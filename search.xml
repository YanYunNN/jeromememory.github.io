<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[排序模板 & 二叉树遍历模板 & 二分模板]]></title>
    <url>%2F2020%2F08%2F02%2F%E6%8E%92%E5%BA%8F%E6%A8%A1%E6%9D%BF%20%26%20%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E6%A8%A1%E6%9D%BF%20%26%20%E4%BA%8C%E5%88%86%E6%A8%A1%E6%9D%BF.html</url>
    <content type="text"><![CDATA[排序这里只简单的放上相应的模板，具体的算法思想见我另外一篇文章 《十大内部排序》) 细节分类 复杂度 补充： 在这里要说明的是，这个图里的快排空间复杂度错了，快排的平均空间复杂度应该是 O(logn)，就是栈的深度，但是最差的情况下，比如全是倒序，则空间复杂度为栈深度 O(n)。归并排序每次递归需要用到一个辅助表，长度与待排序的表相等，虽然递归次数是O(log2n)，但每次递归都会释放掉所占的辅助空间，所以下次递归的栈空间和辅助空间与这部分释放的空间就不相关了，因而空间复杂度还是 O(n)。 稳定性 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 不稳定排序算法：谐音记忆法：快（快排） 些（希尔）选（选择）对（堆）。 总共4个不稳定，6个稳定。 面试点jdk层面实现的sort总共是两类，一个是 Arrays.sort()， Collections.sort()： Arrays.sort() 如果数组内元素是基本数据类型，最主要采用的是双轴快速排序「其实就是三路快排一模一样的思路，只不过三路快排中间是 = pivot1，而双轴快速排序是（pivot1，pivot2），具体戳链接：https://www.cnblogs.com/nullzx/p/5880191.html 。 总结一下：数组长度小于47的时候是用直接插入算法，大于47并且小于286是采用双轴快速排序，大于286如果连续性好「也就是元素大多有序，有一个flag专门用来记录数组元素的升降次数，代表这个数组的连续性」采用的是归并排序，否则还是依旧采用双轴快速排序。 如果数组内元素是对象，采用的是TimSort.sort()，跟 Collections.sort()一样，都是采用的这个函数，这是归并排序算法和插入排序的结合。 Collections.sort()，采用 TimSort.sort()。 TimSort.sort() 大概原理： 当待排序元素小于32个时，采用二分插入排序，是插入排序的一种改进，可以减少插入排序比较的次数。当找到插入位置时，直接利用System.copy()函数即可。 当待排序元素大于等于32个时，进行归并排序（和传统的归并不一样），首先将排序元素分区，每个分区的大小区间为[16,32)，然后依次对每个分区进行排序（具体实现依然是二分插入排序），排完序的分区压入栈（准确的说不是栈，而是一个数组，用来记录排序好的分区），当栈内的分区数满足条件时，进行分区合并，合并为一个更大的分区，当栈中只剩一个分区时，排序完成。 模板交换排序之冒泡1234567891011121314151617181920212223242526272829303132333435363738public class BubbleSort &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); int len = scanner.nextInt(); System.out.println("要输入的数字的个数：" + len); int[] nums = new int[len]; for(int i = 0;i &lt; len;i++)&#123; nums[i] = scanner.nextInt(); &#125; Bubble(nums); System.out.println(Arrays.toString(nums)); &#125; /** * 注意点：外循环是 n - 1 次，所以可以 i 从 1 开始取 * 内循环是从 0 到 n - i // 这个貌似是最容易忘记的 * 比较的是 j 和 j+1 * @param nums */ public static void Bubble(int[] nums)&#123; for(int i = 1;i &lt; nums.length;i++)&#123; boolean flag = true; for(int j = 0;j &lt; nums.length - i;j++)&#123; if(nums[j] &gt; nums[j+1])&#123; swap(nums,j,j+1); flag = false; &#125; &#125; if(flag) return; &#125; &#125; private static void swap(int[] nums, int j, int i) &#123; int temp = nums[j]; nums[j] = nums[i]; nums[i] = temp; &#125;&#125; 交换排序之快排123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.print("请输入数组的大小： "); int len = scanner.nextInt(); int[] nums = new int[len]; for (int i = 0;i &lt; len;i++)&#123; nums[i] = scanner.nextInt(); &#125; quick(nums,0,len-1); System.out.println(Arrays.toString(nums)); &#125;public static void quick(int[] nums,int left,int right)&#123; if(left &lt; right)&#123; int partition = partition(nums,left,right); quick(nums,left,partition-1); quick(nums,partition+1,right); &#125; &#125; private static int partition(int[] nums, int left, int right) &#123; // 基准值 // 优化，就是尽量能做到分治，也就是选取的基准值尽量在中间是性能最好的 int mid = (left + right)/2; dealPivot(nums,left,mid,right); int pivot = left; // 定义两个指针 int p = left; int q = right; while(p &lt; q)&#123; while(p &lt; q &amp;&amp; nums[q] &gt;= nums[pivot])&#123; q--; &#125; while(p &lt; q &amp;&amp; nums[p] &lt;= nums[pivot])&#123; p++; &#125; swap(nums,p,q); &#125; swap(nums,p,pivot); return p; &#125; private static void dealPivot(int[] nums, int left, int mid, int right) &#123; if((left - mid) * (mid - right) &gt; 0)&#123; // left &lt; mid &lt; right swap(nums,left,mid); return; &#125; else if((mid - left) * (left - right) &gt; 0)&#123; // left 是中间值 swap(nums,left,right); return; &#125; return; &#125; 交换排序之三路快排123456789101112131415161718192021222324252627282930313233343536373839private static void quickSortByThreeWays(int[] nums, int left, int right) &#123; if(left &lt; right)&#123; int[] par = quickThreePartition(nums,left,right); int lt = par[0]; int gt = par[1]; quickSortByThreeWays(nums,left,lt-1); quickSortByThreeWays(nums,gt+1,right); &#125; &#125; // nums[left...lt-1] 都是 小于 pivot 的 // nums[gt+1...right] 都是 大于 pivot 的 private static int[] quickThreePartition(int[] nums, int left, int right) &#123; int pivot = left; // 三个指针，类似于三色旗，这里是三色旗的进阶版，因为多了一个 pivot int lt = left + 1; int cur = left + 1; int gt = right; while(cur &lt;= gt)&#123; if(nums[cur] &gt; nums[pivot])&#123; swap(nums,cur,gt); gt--; &#125; else if(nums[cur] &lt; nums[pivot])&#123; swap(nums,lt,cur); cur++; lt++; &#125; else cur++; &#125; swap(nums,pivot,lt-1); return new int[]&#123;lt-1,gt&#125;; &#125; private static void swap(int[] nums, int p, int q) &#123; int temp = nums[p]; nums[p] = nums[q]; nums[q] = temp; &#125; 插入排序之直接插入排序123456789101112131415161718192021222324252627282930/** * 直接插入排序 */public class EasyInsertSort &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.print("输入的数的个数：" ); int len = scanner.nextInt(); int[] nums = new int[len]; for(int i = 0;i &lt; len;i++)&#123; nums[i] = scanner.nextInt(); &#125; EasyInsert(nums); System.out.println(Arrays.toString(nums)); &#125; // 直接插入排序， n-1 轮 private static void EasyInsert(int[] nums) &#123; for(int i = 1;i &lt; nums.length;i++)&#123; int temp = nums[i]; int j = i; while(j &gt; 0 &amp;&amp; nums[j-1] &gt; temp)&#123; nums[j] = nums[j-1]; j--; &#125; nums[j] = temp; &#125; &#125;&#125; 插入排序之希尔排序1234567891011121314151617181920212223/*** 其实就是将直接插入排序的间隔1变为更大，也就是其实就多了一个循环，就是多次缩小间隔的循环，其他跟直接插入是一样的** @param sortArray* @return*/public static int[] ShellSort2(int[] sortArray) &#123; int[] arr = Arrays.copyOf(sortArray, sortArray.length); int size = sortArray.length; for (int increment = size / 2; increment &gt; 0; increment /= 2) &#123; for (int i = increment; i &lt; size; i++) &#123; int temp = arr[i]; int j = i; for (; (j &gt; 0 &amp;&amp; arr[j - increment] &gt; temp); j -= increment) &#123; //这一步最关键，因为要注意每当插入一个数字，其后面都要往后顺移increment位，不然的话就空不出要插入的位置！！！ arr[j] = arr[j - increment]; &#125; //如果没有变化，则temp依然等于arr[i],如果变化，则将arr[i]移动过来就行 arr[j] = temp; &#125; &#125; return arr;&#125; 选择排序之简单选择排序12345678910111213141516171819202122232425262728/** * 简单选择排序 */public class SelectSort &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.print("please input length: "); int len = scanner.nextInt(); int[] nums = new int[len]; for(int i = 0;i &lt; nums.length;i++)&#123; nums[i] = scanner.nextInt(); &#125; for(int i = 0;i &lt; nums.length;i++)&#123; int min = i; for(int j = i+1;j &lt; nums.length;j++)&#123; if(nums[min] &gt; nums[j]) min = j; &#125; if(min != i) swap(nums,i,min); &#125; System.out.println(Arrays.toString(nums)); &#125; private static void swap(int[] nums, int i, int min) &#123; int temp = nums[i]; nums[i] = nums[min]; nums[min] = temp; &#125;&#125; 选择排序之堆排序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 堆排序 */public class HeapSort &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.print("要输入的数字的个数："); int len = scanner.nextInt(); int[] nums = new int[len]; for(int i = 0;i &lt; len;i++)&#123; nums[i] = scanner.nextInt(); &#125; heapSort(nums,len-1); System.out.println(Arrays.toString(nums)); &#125; public static void heapSort(int[] sortArray, int max_index) &#123; BuildHeap(sortArray,max_index); for(int i = max_index;i &gt; 0;i--)&#123; swap(sortArray,i,0); AdjustDown(sortArray,0,i-1); &#125; &#125; private static void BuildHeap(int[] sortArray, int max_index) &#123; double max = max_index; for(int i = (int) (Math.round(max/2) - 1); i &gt;= 0; i--)&#123; AdjustDown(sortArray,i,max_index); &#125; &#125; private static void AdjustDown(int[] sortArray, int i, int max_index) &#123; int left = 2 * i + 1; int right = 2 * i + 2; int largest = i; if(left &lt;= max_index &amp;&amp; sortArray[left] &gt; sortArray[largest])&#123; largest = left; &#125; if(right &lt;= max_index &amp;&amp; sortArray[right] &gt; sortArray[largest])&#123; largest = right; &#125; if(largest != i)&#123; swap(sortArray,largest,i); AdjustDown(sortArray,largest,max_index); &#125; &#125; private static void swap(int[] sortArray, int largest, int i) &#123; int temp = sortArray[largest]; sortArray[largest] = sortArray[i]; sortArray[i] = temp; &#125;&#125; 二路归并排序123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 归并排序 */public class mergeSort &#123; public static void main(String[] args) &#123; int[] nums = new int[]&#123;3,6,7,1,2,3,1,5,7,9,4,2,1,4,5,6&#125;; mergeSort(nums,0,nums.length-1); System.out.println(Arrays.toString(nums)); &#125; public static void mergeSort(int[] nums,int left,int right)&#123; int mid = left + (right - left)/2; if(left &lt; right)&#123; mergeSort(nums,left,mid); mergeSort(nums,mid+1,right); mergeTwo(nums,left,mid,right); &#125; &#125; private static void mergeTwo(int[] nums, int left, int mid, int right) &#123; // 用一个数组来存储合并后的数组，然后复制给原数组 int[] temp = new int[right-left+1]; int i = left; int j = mid + 1; int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= right)&#123; if(nums[i] &lt; nums[j])&#123; temp[k++] = nums[i++]; &#125; else &#123; temp[k++] = nums[j++]; &#125; &#125; while (i &lt;= mid)&#123; temp[k++] = nums[i++]; &#125; while (j &lt;= right)&#123; temp[k++] = nums[j++]; &#125; for(int t = 0;t &lt; temp.length;t++)&#123; nums[left+t] = temp[t]; &#125; &#125;&#125; 计数排序123456789101112131415161718192021222324252627282930/** * 计数排序 */public class courtSort &#123; public static void main(String[] args) &#123; int[] nums = new int[]&#123;2,4,1,2,4,1,4,3,2,1,-2,3,1,2,-2,3,4,5,1&#125;; court(nums); System.out.println(Arrays.toString(nums)); &#125; public static void court(int[] nums)&#123; if(nums.length == 0) return; int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; for(int i = 0;i &lt; nums.length;i++)&#123; max = Math.max(max,nums[i]); min = Math.min(min,nums[i]); &#125; int[] temp = new int[max-min+1]; for(int i = 0;i &lt; nums.length;i++)&#123; temp[nums[i] - min]++; &#125; int t = 0; for(int i = 0;i &lt; temp.length;i++)&#123; while (temp[i] != 0)&#123; nums[t++] = i + min; temp[i]--; &#125; &#125; &#125;&#125; 桶排序1234567891011121314151617181920212223242526272829303132333435363738394041public class BucketSort &#123; public static void main(String[] args) &#123; int[] array = new int[]&#123;200,123,978,2123,3092,3412,300,321,302,9991,2329,7618,9283&#125;; radixSort(array); &#125; public static void radixSort(int[] array) &#123; if (array == null || array.length &lt; 2) return; // 1.先算出最大数的位数； int max = array[0]; for (int i = 1; i &lt; array.length; i++) &#123; max = Math.max(max, array[i]); &#125; int maxDigit = 0; while (max != 0) &#123; max /= 10; maxDigit++; &#125; int mod = 10, div = 1; //桶可以用二维数组实现，也可以用ArrayList实现，推荐列表，因为是动态的 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucketList = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; bucketList.add(new ArrayList&lt;Integer&gt;()); &#125; for (int i = 0; i &lt; maxDigit; i++, mod *= 10, div *= 10) &#123; for (int j = 0; j &lt; array.length; j++) &#123; int num = (array[j] % mod) / div; bucketList.get(num).add(array[j]); &#125; int index = 0; for (int j = 0; j &lt; bucketList.size(); j++) &#123; for (int k = 0; k &lt; bucketList.get(j).size(); k++) &#123; array[index++] = bucketList.get(j).get(k); &#125; bucketList.get(j).clear(); &#125; &#125; System.out.println(Arrays.toString(array)); &#125;&#125; 基数排序123456789101112131415161718192021222324252627282930313233343536373839/** 基数排序 * 可以采用桶的思想，也可以采用队列的思想 * * @param array */ public static void radixSort(int[] array) &#123; if (array == null || array.length &lt; 2) return; // 1.先算出最大数的位数； int max = array[0]; for (int i = 1; i &lt; array.length; i++) &#123; max = Math.max(max, array[i]); &#125; int maxDigit = 0; while (max != 0) &#123; max /= 10; maxDigit++; &#125; int mod = 10, div = 1; //桶可以用二维数组实现，也可以用ArrayList实现，推荐列表，因为是动态的 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucketList = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; bucketList.add(new ArrayList&lt;Integer&gt;()); &#125; for (int i = 0; i &lt; maxDigit; i++, mod *= 10, div *= 10) &#123; for (int j = 0; j &lt; array.length; j++) &#123; int num = (array[j] % mod) / div; bucketList.get(num).add(array[j]); &#125; int index = 0; for (int j = 0; j &lt; bucketList.size(); j++) &#123; for (int k = 0; k &lt; bucketList.get(j).size(); k++) &#123; array[index++] = bucketList.get(j).get(k); &#125; bucketList.get(j).clear(); &#125; &#125; System.out.println(Arrays.toString(array)); &#125; 二叉树遍历这里只提供二叉树遍历模板，具体有关二叉树的内容可以移步我的另外一篇文章：二叉树专题总结 前序递归1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; result.add(root.val); // 访问根节点 preorderHelper(root.left, result); // 递归遍历左子树 preorderHelper(root.right, result); //递归遍历右子树 &#125;&#125; 非递归1234567891011121314151617181920212223242526272829303132// 1.1 非递归先序private static void preOrder(TreeNode root) &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; while (cur != null) &#123; arrayList.add(cur.val); stack.push(cur); cur = cur.left; &#125; cur = stack.pop(); cur = cur.right; &#125; System.out.println(arrayList);&#125;// 1.2 非递归先序2private static void preOrder2(TreeNode root) &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; stack.push(cur); while (!stack.isEmpty()) &#123; cur = stack.pop(); arrayList.add(cur.val); if (cur.right != null) stack.push(cur.right); if (cur.left != null) stack.push(cur.left); &#125; System.out.println(arrayList);&#125; 中序递归1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; preorderHelper(root.left, result); // 递归遍历左子树 result.add(root.val); // 访问根节点 preorderHelper(root.right, result); //递归遍历右子树 &#125;&#125; 非递归12345678910111213141516// 2. 中序非递归private static void InOrder(TreeNode root) &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; while (cur != null) &#123; stack.push(cur); cur = cur.left; &#125; cur = stack.pop(); arrayList.add(cur.val); cur = cur.right; &#125; System.out.println(arrayList);&#125; 后序递归1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; preorderHelper(root.left, result); // 递归遍历左子树 preorderHelper(root.right, result); //递归遍历右子树 result.add(root.val); // 访问根节点 &#125;&#125; 非递归1234567891011121314151617//3. 后序非递归 左右根 反过来讲就是 根右左private static void postOrder(TreeNode root) &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !stack.isEmpty()) &#123; while (cur != null) &#123; arrayList.add(cur.val); stack.push(cur); cur = cur.right; &#125; cur = stack.pop(); cur = cur.left; &#125; Collections.reverse(arrayList); System.out.println(arrayList);&#125; 层序递归123456789101112131415161718192021public List&lt;List&lt;Integer&gt;&gt; levelOrder(Node root) &#123; if (root == null) return res; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); helper(root, 0, res); return res;&#125;private void helper(Node root, int depth, List&lt;List&lt;Integer&gt;&gt; res) &#123; if (root == null) return; //判断是否是新的一层 if (depth + 1 &gt; res.size()) &#123; res.add(new ArrayList&lt;&gt;()); &#125; res.get(depth).add(root.val); //处理子节点 for (Node node : root.children) &#123; if (node != null) &#123; helper(node, depth + 1, res); &#125; &#125;&#125; 非递归1234567891011121314151617181920212223242526272829303132333435363738//4.1 层序遍历private static void levelOrder(TreeNode root) &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); TreeNode cur = root; queue.add(cur); while (!queue.isEmpty()) &#123; cur = queue.poll(); arrayList.add(cur.val); if (cur.left != null) queue.add(cur.left); if (cur.right != null) queue.add(cur.right); &#125; System.out.println(arrayList);&#125;//4.2 层序遍历 2private static void levelOrder2(TreeNode root) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; arrayLists = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); TreeNode cur = root; queue.add(cur); int count; while (!queue.isEmpty())&#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;&gt;(); count = queue.size(); while (count &gt; 0)&#123; cur = queue.poll(); count--; arrayList.add(cur.val); if(cur.left != null) queue.add(cur.left); if(cur.right != null) queue.add(cur.right); &#125; arrayLists.add(arrayList); &#125; System.out.println(arrayLists);&#125; 二分面试的时候经常碰到二分，比如求根号n，求最左侧target的下标，求翻转后的有序数组对应的最左侧target下标等等。 模板左闭右开1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class binarySearch &#123; int binarySearch(int nums[],int target)&#123; int left = 0; int right = nums.length; // 查找范围为 [a,b) while(left &lt; right)&#123; // left == right 时退出，此时[a,a)，所以此时都遍历到了 //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) return mid; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid; &#125; return -1; &#125; /** * 返回的是比target小的数量，同时也是最左target的下标 * @param nums * @param target * @return */ int binarySearchLeft(int nums[],int target)&#123; int left = 0; int right = nums.length; while(left &lt; right)&#123; //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) right = mid; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid; &#125; //要时刻注意处理边界，如果整个数组都没有这个target，则left会到nums.length //left的取值是[0,nums.length] // target 比所有数都大 if (left == nums.length) return -1; // 类似之前算法的处理方式 return nums[left] == target ? left : -1; &#125; /** * 返回的是最右target的下标 * @param nums * @param target * @return */ int binarySearchRight(int nums[],int target)&#123; int left = 0; int right = nums.length; while(left &lt; right)&#123; //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) left = mid + 1; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid; &#125; //要时刻注意处理边界，如果整个数组都没有这个target，则left会到nums.length // target 比所有数都大 // return left - 1; if (left == 0) return -1; // 类似之前算法的处理方式 return nums[left-1] == target ? (left-1) : -1; &#125;&#125; 左闭右闭12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package 二分查找;import java.util.Arrays;public class binarySearch2 &#123; public static void main(String[] args) &#123; binarySearch2 binarySearch2 = new binarySearch2(); int[] nums = new int[]&#123;1,2,2,2,2,2,3&#125;; int target = 2;// int res = binarySearch2.binarySearch(nums,target);// int res = binarySearch2.binarySearchLeft(nums,target); int res = binarySearch2.binarySearchRight(nums,target); System.out.println(res); &#125; public int binarySearch(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while (left &lt;= right) &#123; int mid = left + (right - left) / 2; if (nums[mid] == target) return mid; else if (nums[mid] &gt; target) right = mid - 1; else if (nums[mid] &lt; target) left = mid + 1; &#125; return -1; &#125; public int binarySearchLeft(int[] nums,int target)&#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; int mid = left + (right - left)/2; if(nums[mid] == target) right = mid - 1; else if(nums[mid] &gt; target) right = mid -1; else if(nums[mid] &lt; target) left = mid + 1; &#125; if(left &gt;= nums.length || nums[left] != target)&#123; return -1; &#125; return left; &#125; public int binarySearchRight(int[] nums,int target)&#123; int left = 0, right = nums.length - 1; while (left &lt;= right) &#123; int mid = left + (right - left) / 2; if (nums[mid] &lt; target) &#123; left = mid + 1; &#125; else if (nums[mid] &gt; target) &#123; right = mid - 1; &#125; else if (nums[mid] == target) &#123; // 别返回，锁定右侧边界 left = mid + 1; &#125; &#125; // 最后要检查 right 越界的情况，这里 left = right + 1 if (right &lt; 0 || nums[right] != target) return -1; return right; &#125;&#125; 相关题目根号n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package 练手算法;import java.text.DecimalFormat;class Main &#123; public static double sqrt(double num) &#123; if (num &lt; 0) &#123; return -1; &#125; double low = 0; double high = num / 2; double precision = 0.000001; //格式化，保证输出位数 DecimalFormat df = new DecimalFormat("#.00"); double res = high; while (Math.abs(num - (res * res)) &gt; precision) &#123; if (high * high &gt; num) &#123; double n = high - (high - low) / 2; if (n * n &gt; num) &#123; high = n; &#125; else if (n * n &lt; num) &#123; low = n; &#125; else &#123; return Double.valueOf(df.format(n)); &#125; res = n; &#125; else if (high * high &lt; num) &#123; double m = high + (high - low) / 2; if (m * m &gt; num) &#123; low = high; high = m; &#125; else if (m * m &lt; num) &#123; low = high; high = m; &#125; else &#123; return Double.valueOf(df.format(m)); &#125; res = m; &#125; else &#123; return Double.valueOf(df.format(high)); &#125; &#125; return Double.valueOf(df.format(res)); &#125; public static void main(String[] args) &#123; System.out.println(sqrt(16)); &#125;&#125;public class SqrtN &#123; public static void main(String[] args) &#123; &#125;&#125; 34. 在排序数组中查找元素的第一个和最后一个位置题目给定一个按照升序排列的整数数组 nums，和一个目标值 target。找出给定目标值在数组中的开始位置和结束位置。 你的算法时间复杂度必须是 O(log n) 级别。 如果数组中不存在目标值，返回 [-1, -1]。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123; public int[] searchRange(int[] nums, int target) &#123; if(nums.length == 0) return new int[]&#123;-1,-1&#125;; return new int[]&#123;searchLeft(nums,target),searchRight(nums,target)&#125;; &#125; public int searchLeft(int[] nums,int target)&#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; // 防止溢出 int mid = left + (right - left)/2; if(nums[mid] &gt;= target) right = mid - 1; else left = mid + 1; &#125; // 上面的while循环后，最后一次循环 left == mid, right = left - 1 = mid - 1 // 若有符合要求的target，则会有两种情况： // 1. left 和 right 在 target 值的最左索引处的前一位相遇，此时 left 会向右一位，到达最左 target 处,而 right 会停在原地，弹出 while // 2. left 和 right 在 target 值的最左索引处相遇，此时 left 不动，到达最左 target 处,而 right 会向左一位，弹出 while // 注意：此时的 left 都是处在 target 最左索引处 // 如果没有符合要求的target，则有三种情况： // 1.所有值均小于target，此时，left会在 nums[nums.length-1]处与right相遇，然后left 加 1，跳出循环，此时left == nums.length // 2.所有值均大于target，此时right会不断向左，直至 left = right = 0 相遇，此时 right 减 1，跳出循环，此时 left == 0 // 3.target位于值的中间，但是没有值取到，此时跟有target情况是类似的，最终 left 会停留在比 target 大的第一个数上 // 总结上面 5 种情况，left 为 nums.length 时，另其为 nums.length - 1,此时直接判断 nums[left] 即可，若为target则直接返回 left // 否则返回 -1 int pos = (left == nums.length) ? nums.length - 1 : left; if(nums[pos] != target) return -1; return left; &#125; public int searchRight(int[] nums,int target)&#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; // 防止溢出 int mid = left + (right - left)/2; if(nums[mid] &lt;= target) left = mid + 1; else right = mid - 1; &#125; // 同上分析 int pos = (right == -1)? 0 : right; if(nums[pos] != target) return -1; return right; &#125;&#125; 33. 搜索旋转排序数组题目假设按照升序排序的数组在预先未知的某个点上进行了旋转。 ( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。 搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。 你可以假设数组中不存在重复的元素。 你的算法时间复杂度必须是 O(log n) 级别。 代码123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public int search(int[] nums, int target) &#123; int start = 0; int end = nums.length - 1; while(start &lt;= end)&#123; int mid = start + (end - start)/2; if(nums[mid] == target) return mid; if (nums[start] == nums[mid]) &#123; start++; continue; &#125; //最容易错的点，就是列表只有两个数字时，mid和start是同一个数，此时必须是前半部分有序 // num[mid] == nums[start] 只会在列表只有两个数时才相等，所以才可以上面那样处理 // 正常其实是不应该那样处理，而应该是当num[mid] == nums[start]，直接start++，然后进行下一次循环 if(nums[mid] &gt; nums[start])&#123; // 说明前半部分有序 if(nums[start] &lt;= target &amp;&amp; target &lt; nums[mid])&#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; else &#123; // 说明后半部分有序 if(nums[mid] &lt; target &amp;&amp; target &lt;= nums[end])&#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; &#125; return -1; &#125;&#125; 81. 搜索旋转排序数组 II在上面的基础上，去除不可重复，这里的数组中的元素包含了可重复的元素，同时加大难度，要求找出第左侧的target的下标「这个我不会 」。 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; public boolean search(int[] nums, int target) &#123; if (nums == null || nums.length == 0) &#123; return false; &#125; int start = 0; int end = nums.length - 1; int mid; while (start &lt;= end) &#123; mid = start + (end - start) / 2; if (nums[mid] == target) &#123; return true; &#125; if (nums[start] == nums[mid]) &#123; start++; continue; &#125; //前半部分有序 if (nums[start] &lt;= nums[mid]) &#123; //target在前半部分 if (nums[mid] &gt; target &amp;&amp; nums[start] &lt;= target) &#123; end = mid - 1; &#125; else &#123; //否则，去后半部分找 start = mid + 1; &#125; &#125; else &#123; //后半部分有序 //target在后半部分 if (nums[mid] &lt; target &amp;&amp; nums[end] &gt;= target) &#123; start = mid + 1; &#125; else &#123; //否则，去后半部分找 end = mid - 1; &#125; &#125; &#125; //一直没找到，返回false return false; &#125;&#125; 287. 寻找重复数题目给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。 代码1234567891011121314151617181920212223class Solution &#123; public int findDuplicate(int[] nums) &#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; int mid = left + (right - left)/2; int count = 0; int mid_count = 0; for(int i = 0;i &lt; nums.length;i++)&#123; if(nums[i] &lt;= mid) count++; if(nums[i] == mid) mid_count++; &#125; // 如果 [left,mid] 没有出现重复数字，count &lt;= mid // 否则说明在这个区间出现了重复的 // 这里跟常见的 二分有一点点不同，这里 right = mid，不是 right = mid - 1 // 因为我在上面计算 count 的时候 是有计算 mid 的 if(mid_count &gt; 1) return mid; if(count &gt; mid) right = mid - 1; else left = mid + 1; &#125; return -1; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
        <tag>模板</tag>
        <tag>二叉树排序</tag>
        <tag>二分法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重构项目代码]]></title>
    <url>%2F2020%2F05%2F05%2F%E4%BA%94%E4%B8%80%E5%81%87%E6%9C%9F%E4%B9%8B%E9%87%8D%E6%9E%84%E9%A1%B9%E7%9B%AE.html</url>
    <content type="text"><![CDATA[前言趁着五一假期，自己也没出去玩，所以有大把的空闲时间，想到马上实习去了，可能项目要交接，于是自己决定重构一下代码… 因为我是在师兄的代码的基础上进行维护修改的，改到后面真的是越来越复杂，越来越麻烦，以至于现在我随便加点新功能，完全没有可扩展性，而且代码没有任何可读性…我都不知道我是怎么读下去这个代码并且修改的，简直就是在 ”shi上雕花“，想想还是不能坑别人，于是狠下心来，决定将原来用 node 写的改成 java，还好，一切都比较顺利，经过2天的重构 + 1天的测试，能够顺利跑通了，如果后期要交接的话，就将这份代码上线… 为了最大限度的减少修改和对前期代码的兼容，我只修改了 node 部分的代码，全部转成 java，这样同时也解决了 kafka 对 node 无法进行 kerberos 加密的缺点，终于不用裸奔了… 但是目前代码对谷歌验证的封堵能力还没有进行考验，所以不到万不得已这份代码还是不会上线的…除非真的要添加新功能或者要交接… 现在记录一下在重构中遇到的一些小问题。「代码直接在 leetcode 工程中写的，没有独立出来，文件夹名字为 ReWrite_xxx_List」 问题Q1：Java版本的 Kafka 中的序列化和反序列化，不支持 JSON，而 Python 那个模块的代码要求用 json 传递消息 我发现 Java 这边的 String，要想正常的在 python 模块变为 json，就必须使得 String 中的单引号全部变为双引号，否则就无法转换「json.loads(str) 会报错」。 例如： 12&gt; String msg = "&#123;\"params\": \"apple\", \"task_id\": \"W2155_1588604670842_0310\",\"is_cycle\": 0, \"task_type\": 9, \"cron_expression\": \"---\", \"priority \": 1, \"operation\": \"init\", \"quantity\": 100, \"comments\": 0, \"follower\": 0, \"followed\": 0, \"u_id\": \"0452mlppa555\", \"address\": \"0452_m\", \"remain_count\": 50&#125;";&gt; 这样就没问题，同理，在我尝试抓取列表页的简历的 name 时，也同样发现了这个问题，但是当时没有注意到，在 selenium_test 文件夹下的 url_test 中，有 12&gt; JSONObject resultsJson = (JSONObject) JSONObject.parse(results);&gt; 要想尝试转换成功，就必须把 results 中的 单引号 全部替换成 双引号。 话说，通过此次重构，将自己前几天看的 kafka 完美的融入了进来，简直美滋滋… Q2：遇到的第一个大问题，其实还是考虑如何将登录后的界面拿到，因为我后续的操作都是要基于已经登录后的这个标签页的 这的确是一个难题，我这边最后的做法是将 driver 这个chrome的驱动变为一个 static volatile，这样在多线程操作的时候，能够同步 driver 的状态，也就是说，整个 Crawl 类只有一个 driver 实例，而且我用了单例模式下的饿汉模式，直接在第一次初始化的时候就将 driver 进行实例化了，login() 执行的时候是拿到唯一的 driver 实例，然后后期 crawl 的时候，由于是单例化的，所以必然也是拿到同一个 driver，恩，我真是个天才… Q3：最费时间的还是要对 python 模块进行一个完整的对接 在不改变 python模块的代码的前提下，进行代码的重构，这的确有点浪费时间，尤其是要考虑到 kafka 消息传递的不变，所以可能会比较花费时间吧.. Q4：如何启动爬虫的 我是直接将业务全部写在 Consumer 这个类中了，然后 Crawl 这个类就负责进行登录、爬取等操作，其中，Consumer 类中直接通过线程池调用 Crawl 中的方法，并通过 future.get() 进行结果的获取。 Producer 这个类负责进行任务的发送，而 Utils 则是包含一些工具方法，比如对 keyword 的处理，说到对 keyword 的处理，主要还是要满足 url 编码的规范，所以需要进行一定的处理。 Q5：抓取名字是抓取 html 还是通过 rpc 获取？ 经过自己的测试，还是 rpc 的靠谱一些，但是两者都不够靠谱… 123&gt; // 1. 通过 rpc，这种方式的好处是当页面一直处于 loading 状态并且无法加载数据时，通过 rpc 反而可以获得数据，坏处就是有的时候页面加载出来了数据但是却拦截不到该请求&gt; // 2. 直接通过页面抓取我们需要的数据，好处就是不用承担 rpc 可能拦截不到的风险，坏处就是有的时候会 loading，导致抓不到数据&gt; 感觉最好的方式是：先进行页面的抓取，如果发现是 loading 状态，则转为 rpc 调用，两者都没有数据，说明 no match。 懒得试了…到时候如果要上线，再试试… Q6： java 中如何使用 webdriver？ http://www.testclass.net/selenium_java 主要是参考这个网址，剩下的看chrome中的 ”其他书签“ Q7：设置 cookie，不需要再登录了 收获 对 kafka 和 java 的对接的熟悉，直接将前几天看的全部给用上了，顺带自己写了个自定义的序列化器。 线程池、selenium的运用、request的运用，以前都只是在 python 中用，现在感觉在 java 中也同样是十分方便的。 自己顺带重写了个 request 有关的工具类，里面有很多实用的工具方法，在 leetcode 文件夹下的 爬虫demo。 对业务的再次熟悉，感觉一整套下来，逻辑已经是非常清晰了，我的代码自己觉得写的也是很清晰的，终于不用担心交接的时候坑别人了… 最大的收获当然是更加自信了，感觉写起来还是挺简单的，以前花了个把月才熟悉的 node，用 java 写两天不到就搞定了，最终可能这份代码也用不上，但是给自己的信心是实打实的！ 还需要做的工作 将该工程独立出去，形成一个maven管理的单独的工程 跟原来的部分代码对接好，尤其是 topic 不要弄错了 「已完成」 加密协议写好 「changze 那边已经写完了」 服务器上的某些东西和mac是不一样的，比如 chromeDriver 的地址需要改动，还有就是付费账号是拥有真实姓名，所以在通过keyword的第二种方法获取名字时，className 是有变化的 「了解了」 尽量减少谷歌验证出现的次数 写一个 ReadMe.txt 「已完成」 好了，项目这块就这样了… 得抓紧时间写一下实验和复习考试科目了… 加油！！！]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>重构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 kafka（三）]]></title>
    <url>%2F2020%2F04%2F29%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%20kafka%EF%BC%88%E4%B8%89%EF%BC%89.html</url>
    <content type="text"><![CDATA[第四章 主题和分区从Kafka的底层实现来说，主题和分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段（LogSegment），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。 主题的管理主题的管理包括创建主题、查看主题信息、修改主题和删除主题等操作。可以通过 kafka 提供的 kafka-topics.sh 脚本或者 KafkaAdminClient 的方式来实现。 这一小节，我们主要讲 kafka-topics.sh 的实现。 创建主题首先建议先将broker端配置参数auto.create.topics.enable设置为false（默认值就是true），因为如果是默认值，当生产者或者消费者与一个不存在的主题有关系时会自动创建该 topic，这种自动创建主题的行为是非预期的，会增加主题的管理与维护的难度，所以在生产环境中最好不要这样。 更加推荐的方式是通过 kafka-topics.sh 脚本来创建主题，如图所示： 我们不仅可以通过日志文件的根目录来查看集群中各个broker的分区副本的分配情况，还可以通过ZooKeeper客户端来获取。当创建一个主题时会在ZooKeeper的/brokers/topics/目录下创建一个同名的实节点，该节点中记录了该主题的分区副本分配方案。示例如下：￼ 示例数据中的”2”：[1，2]表示分区 2 分配了 2 个副本，分别在 brokerId 为 1 和 2 的 broker节点中。 到目前为止，创建主题时的分区副本都是按照既定的内部逻辑来进行分配的。 kafka-topics.sh 脚本中还提供了一个 replica-assignment 参数来手动指定分区副本的分配方案，如图所示： 注意事项： 主题的命名同样不推荐（虽然可以这样做）使用双下画线“”开头，因为以双下画线开头的主题一般看作Kafka的内部主题，比如consumer_offsets和__transaction_state。 分区副本的分配这里的分区分配，跟前面的生产者和消费者的分区分配不一样。生产者的分区分配是指为每条消息指定其所要发往的分区，消费者中的分区分配是指为消费者指定其可以消费消息的分区，而这里的分区分配是指为集群制定创建主题时的分区副本分配方案，即在哪个broker中创建哪些分区的副本。 在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建；如果没有使用replica-assignment参数，那么就需要按照内部的逻辑来计算分配方案了。 使用kafka-topics.sh脚本创建主题时的内部分配逻辑按照机架信息划分成两种策略：未指定机架信息和指定机架信息。默认就是 未指定机架信息的分配策略「即 broker.rack 参数为 null」。 查看主题在 kafka-topics.sh 中使用 list 或者 describe 就可以查看主题信息。list 可以查看所有的当前可用主题， describe 可以查看单个信息的主题的详细信息。]]></content>
      <categories>
        <category>中间件系列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>topic</tag>
        <tag>partition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 kafka（二）]]></title>
    <url>%2F2020%2F04%2F25%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%20kafka%EF%BC%88%E4%BA%8C%EF%BC%89.html</url>
    <content type="text"><![CDATA[第二章 生产者客户端开发12345678910111213141516171819202122232425262728293031323334353637383940package Kafka.Demo1;import org.apache.kafka.clients.Metadata;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.clients.producer.RecordMetadata;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;import java.util.concurrent.Future;public class ProducerFastStart &#123; public static final String brokerList = "localhost:9092"; public static final String topic = "topic-demo"; public static void main(String[] args) &#123; Properties properties = new Properties();// properties.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer"); properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());// properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer"); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); properties.put("bootstrap.servers",brokerList); KafkaProducer&lt;String,String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); // 构造要发送的消息 ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic,"hello,baby"); // 发送消息 try&#123; Future&lt;RecordMetadata&gt; future = producer.send(record); RecordMetadata recordMetadata = future.get(); System.out.println(recordMetadata.topic() + "-" + recordMetadata.partition() + "-" + recordMetadata.offset()); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; producer.close(); &#125;&#125; 由上述代码可知，在生产者客户端，一个正常的生产逻辑需要具备以下几个步骤： 配置生产者客户端参数及创建相应的生产者实例。 构建待发送的消息。 发送消息。 关闭生产者实例。 配置生产者客户端参数及创建生产者实例首先是有三个必须配置的参数，分别是 bootstrap.servers、key.serializer、value.serializer。 Bootstrap.servers 该参数用来指定生产者客户端连接Kafka集群所需的broker地址清单。注意这里并非需要所有的broker地址，因为生产者会从给定的broker里查找到其他broker的信息。不过建议至少要设置两个以上的broker 地址信息，当其中任意一个宕机时，生产者仍然可以连接到 Kafka集群上。 key.serializer &amp; value.serializer broker 端接收的消息必须以字节数组（byte[]）的形式存在。在发往broker之前需要将消息中对应的key和value做相应的序列化操作来转换成字节数组。key.serializer和value.serializer这两个参数分别用来指定key和value序列化操作的序列化器，这两个参数无默认值。注意这里必须填写序列化器的全限定名。 配置完参数之后，直接创建生产者实例就可以了，注意这里的 KafkaProducer 是线程安全的，可以在多个线程中共享单个KafkaProducer实例，也可以将KafkaProducer实例进行池化来供其他线程调用。 消息的发送消息的发送有三种方式： 发后即忘。也就是直接显式调用 producer.send(record)，不关心消息是否正确到达服务器端； 同步。producer.send(record) 的返回类型为 Future&lt;RecordMetadata&gt;，所以可以使用 future.get() 进行同步，RecordMetadata 对象包含了消息的一些元数据信息，比如当前消息的主题、分区号、分区中的偏移量、时间戳等等。 异步。如下图所示，直接在 send() 中使用回调函数。 消息在通过 send() 方法发往broker的过程中，有可能需要经过拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner）的一系列作用之后才能被真正地发往 broker。生产者拦截器可以用来在消费消息钱做一些准备工作，然后消息经过序列化之后就需要确定它发往的分区，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号。 拦截器，一般不是必需的。生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。KafkaProducer中不仅可以指定一个拦截器，还可以指定多个拦截器以形成拦截链。 生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给Kafka，这是必须的。 分区器的作用就是为消息分配分区。如果消息ProducerRecord中没有指定partition字段，那么就需要依赖分区器，根据key这个字段来计算partition的值。如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。 原理分析在前面的章节中，我们已经了解了KafkaProducer的具体使用方法，而本节的内容主要是对Kafka 生产者客户端的内部原理进行分析，通过了解生产者客户端的整体脉络可以让我们更好地使用它，避免因为一些理解上的偏差而造成使用上的错误。 整体架构 由前一小结可知，生产者客户端的send()是异步的，说明有两个线程在运行，也就是主线程和send线程。在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。Sender 线程负责从RecordAccumulator中获取消息并将其发送到Kafka中。RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch，ProducerBatch中可以包含一至多个 ProducerRecord。所以我们其实最终发送的消息是 ProducerBatch。 元数据的更新元数据是指Kafka集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的leader副本分配在哪个节点上，follower副本分配在哪些节点上，哪些副本在AR、ISR等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 在上面的第 8 步之前，我们要挑出负载最小的 broker「每个Node在InFlightRequests中还未确认的请求数决定的，未确认的请求越多则认为负载越大」，这样就能够尽快的发出，因为在发出的时候，我们要先获得对应的 partition 在哪个 broker、broker 是哪个ip，所以在发出之前首先要进行元数据的更新，这里我们肯定是先挑选出leastLoadedNode，然后向这个Node发送MetadataRequest请求来获取具体的元数据信息。这个更新操作是由Sender线程发起的，在创建完MetadataRequest之后同样会存入InFlightRequests，之后的步骤就和发送消息时的类似。元数据虽然由Sender线程负责更新，但是主线程也需要读取这些信息，这里的数据同步通过synchronized和final关键字来保障。 重要的生产者参数 acks。这个参数用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的。 acks = 1。生产者发送消息之后，只要分区的leader副本成功写入消息，那么它就会收到来自服务端的成功响应，是消息可靠性和吞吐量的折中方案。 acks=0。生产者发送消息之后不需要等待任何服务端的响应，可以获取最大的吞吐量。 acks=-1或acks=all。生产者在消息发送之后，需要等待ISR中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应。但这并不意味着消息就一定可靠，因为ISR中可能只有leader副本，这样就退化成了acks=1的情况。 注意哦，这里的 acks 参数配置的值是一个字符串，而不是整数型。 Max.request.size。这个参数用来限制生产者客户端能发送的消息的最大值，默认值为 1048576B，即 1MB。 retries和retry.backoff.ms。retries参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。retry.backoff.ms参数用来设定两次重试之间的时间间隔。 compression.type。默认为 “none”，压缩可以减少网络传输量，降低网络I/O。 request.timeout.ms。这个参数用来配置Producer等待请求响应的最长时间，默认值为30000（ms）。请求超时之后可以选择进行重试。 总结本章主要讲述了生产者客户端的具体用法及其整体架构，主要内容包括配置参数的详解、消息的发送方式、序列化器、分区器、拦截器等。 对于KafkaProducer而言，它是线程安全的，我们可以在多线程的环境中复用它，而对于下一章的消费者客户端KafkaConsumer而言，它是非线程安全的，因为它具备了状态。 第三章 消费者消费者和消费者组的概念消费者（Consumer）负责订阅Kafka中的主题（Topic），并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是：在Kafka的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 对于消息中间件而言，一般有两种消息投递模式：点对点（P2P，Point-to-Point）模式和发布/订阅（Pub/Sub）模式。 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。 客户端开发一个正常的消费逻辑需要具备以下几个步骤： 配置消费者客户端参数及创建相应的消费者实例。 订阅主题。 拉取消息并消费。 提交消费位移。 关闭消费者实例。 Demo: 1234567891011121314151617181920212223242526272829303132333435package Kafka.Chapter1;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.time.Duration;import java.util.Collections;import java.util.Properties;public class ConsumerFastStart &#123; public static final String brokerList = "localhost:9092"; public static final String topic = "topic-demo"; public static final String groupId = "group.demo"; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer"); properties.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer"); properties.put("bootstrap.servers",brokerList); properties.put("group.id",groupId); KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); // 订阅主题 consumer.subscribe(Collections.singletonList(topic)); // 循环读取消息 while (true)&#123; ConsumerRecords&lt;String,String&gt; records = consumer.poll(Duration.ZERO); for(ConsumerRecord&lt;String,String&gt; records1 : records)&#123; System.out.println(records1.value()); &#125; &#125; &#125;&#125; 配置消费者客户端参数及创建相应的消费者实例跟生产者客户端参数差不多，这里多了一个 group.id。 bootstrap.servers。 group.id：消费者隶属的消费组的名称，默认值为“”。如果设置为空，则会报出异常：Exception in thread “main” org.apache.kafka.common.errors.InvalidGroupIdException：The configured groupId is invalid。一般而言，这个参数需要设置成具有一定的业务意义的名称。 key.deserializer 和 value.deserializer。消费者从broker端获取的消息格式都是字节数组（byte[]）类型，所以需要执行相应的反序列化操作才能还原成原有的对象格式。这两个参数分别用来指定消息中key和value所需反序列化操作的反序列化器，这两个参数无默认值。注意这里必须填写反序列化器类的全限定名，比如示例中的org.apache.kafka.common.serialization.StringDeserializer，单单指定StringDeserializer是错误的。 消息的消费订阅主题配置完参数、创建完消费者实例之后，就是选择订阅主题了。集合订阅的方式subscribe（Collection）、正则表达式订阅的方式subscribe（Pattern）和指定分区的订阅方式 assign（Collection）分表代表了三种不同的订阅状态：AUTO_TOPICS、AUTO_PATTERN和USER_ASSIGNED（如果没有订阅，那么订阅状态为NONE）。 集合订阅的方式subscribe（Collection）、正则表达式订阅的方式subscribe（Pattern）和指定分区的订阅方式 assign（Collection）分表代表了三种不同的订阅状态：AUTO_TOPICS、AUTO_PATTERN和USER_ASSIGNED（如果没有订阅，那么订阅状态为NONE）。 通过 assign() 方法订阅分区时，是不具备消费者自动均衡的功能的，因为其指定了 TopicPartition。 拉取消息订阅完主题之后，就要开始拉取消息了。Kafka中的消费是基于拉模式的。消息的消费一般有两种模式：推模式和拉模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。一般就是采用 poll() 进行消息的消费，该方法涉及消费位移、消费者协调器、组协调器、消费者的选举、分区分配的分发、再均衡的逻辑、心跳等内容，后续章节会详细介绍。 修改偏移量消费完消息之后，就需要修改偏移量了。在每次调用poll（）方法时，它返回的是还没有被消费过的消息集，要做到这一点，就需要记录上一次消费时的消费位移。消费位移存储在Kafka内部的主题__consumer_offsets中。 在 Kafka 中，有 自动提交偏移量 和 手动提交偏移量，默认的是自动提交偏移量，，好处就是免去了复杂的位移提交逻辑，坏处就是偏移量的修改交由服务器自身去处理的话会导致重复消费和消息丢失的情况，自动位移提交的动作是在poll（）方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。 所以，有的时候，我们需要手动控制位移提交，开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 false。手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync（）和commitAsync（）两种类型的方法。同步提交会阻塞消费者线程直到位移提交完成，异步提交不会阻塞，但是很可能导致重复消费的问题，所以一般采用以下的方式： 指定位移消费在 Kafka 中每当消费者查找不到所记录的消费位移时，就会根据消费者客户端参数auto.offset.reset的配置来决定从何处开始进行消费，这个参数的默认值为“latest”，表示从分区末尾开始消费消息。还有一个对应的值是”earliest“。 auto.offset.reset参数还有一个可配置的值—“none”，配置为此值就意味着出现查到不到消费位移的时候，既不从最新的消息位置处开始消费，也不从最早的消息位置处开始消费，此时会报出NoOffsetForPartitionException异常。 到目前为止，我们一直都是使用 poll() 中的逻辑来进行数据的拉取，比如自动修改偏移量、使用 auto.offset.reset 来决定从末尾开始消费，但是我们只能粗粒度的从开头或者末尾进行消费，那有没有一个更细粒度的方法呢？当然有，那就是KafkaConsumer 中的 seek（）方法。 1public void seek(TopicPartition partition,long offset) seek（）方法中的参数partition表示分区，而offset参数用来指定从分区的哪个位置开始消费。seek（）方法只能重置消费者分配到的分区的消费位置，而分区的分配是在 poll（）方法的调用过程中实现的。也就是说，在执行seek（）方法之前需要先执行一次poll（）方法，等到分配到分区之后才可以重置消费位置。 seek（）方法为我们提供了从特定位置读取消息的能力，我们可以通过这个方法来向前跳过若干消息，也可以通过这个方法来向后回溯若干消息，这样为消息的消费提供了很大的灵活性。seek（）方法也为我们提供了将消费位移保存在外部存储介质中的能力，还可以配合再均衡监听器来提供更加精准的消费能力。 多线程实现消费者客户端KafkaProducer是线程安全的，然而KafkaConsumer却是非线程安全的。 KafkaConsumer 非线程安全并不意味着我们在消费消息的时候只能以单线程的方式执行。如果生产者发送消息的速度大于消费者处理消息的速度，那么就会有越来越多的消息得不到及时的消费，造成了一定的延迟。 消息堆积问题的解决措施，简而言之就是使用多线程，具体来说，可以采用三种方式： 每一个线程对应一个 KafkaConsumer，每一个 KafkaConsumer 对应 topic 中的一个 partition，这些 KafkaConsumer 隶属于同一个消费组，这样能保证同一条消息不被多个 Consumer 消费。好处就是可以不用处理偏移量和顺序控制，因为 partition 内部使按序的，缺点也很明显，就是该多线程的能力受限于分区数，如果需求是所有消息都必须按序，也就是说所有消息都在一个 partition 内，此时，该方法失效； 为了解决上述问题，也就是在只有一个分区的情况下，如何解决消息堆积。这个就必须通过手动控制offset了，通过 assign()、seek() 等方法解决。好处就是打破了原有的消费线程的个数受分区数的限制，坏处就是这种实现方式对于位移提交和顺序控制的处理会变得十分复杂，实际应用基本不会涉及。 上述两种方法都是从提高拉取消息的速率考虑的，但是在实际情况中，poll() 拉取消息的速度是相当快的，整体的消费性能瓶颈并不在于此，而是在于处理消息这一块。所以，我们可以在处理消息模块改成多线程的实现方式，但是这样同样会遇到一个问题，就是消息的顺序问题，因为多线程处理消息，原本 partition 的消息是按序拉取的，但是多线程处理完之后可能就不是按顺序的了，所以我们需要进一步进行处理，作者在该书中提出了滑动窗口的解决思路，即如果按序拉取后的正在处理的消息处理完成，将其放到窗口缓存，当全部完成之后窗口才向后滑动，类似于 TCP 中的 GBN 协议和 SR 协议。 ## 重要的消费者参数 fetch.min.bytes 该参数用来配置Consumer在一次拉取请求（调用poll（）方法）中能从Kafka中拉取的最小数据量，默认值为1（B）。 fetch.max.bytes 与上述对应，默认值为 50 MB。 Max.poll.records 这个参数用来配置Consumer在一次拉取请求中拉取的最大消息数，默认值为500（条）。 connections.max.idle.ms 这个参数用来指定在多久之后关闭限制的连接，默认值是540000（ms），即9分钟。 metadata.max.age.ms这个参数用来配置元数据的过期时间，默认值为300000（ms），即5分钟。如果元数据在此参数所限定的时间范围内没有进行更新，则会被强制更新，即使没有任何分区变化或有新的broker加入。]]></content>
      <categories>
        <category>中间件系列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>生产者&amp;消费者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 kafka（一）]]></title>
    <url>%2F2020%2F04%2F23%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%20kafka%EF%BC%88%E4%B8%80%EF%BC%89.html</url>
    <content type="text"><![CDATA[前言项目最近 kafka 又崩了… 心态炸了… 总是 nobroker，自己也总是没有彻底解决这个问题，所以，受不了了，我要开始深入理解一下 kafka 了… 由于之前一篇文章都是几万字，导致可读性比较拉胯，所以从现在开始，我每一章出一篇文章…这样可读性强一些… 第一章 初识 kafkaKafka起初是由LinkedIn公司采用Scala语言开发的一个多分区、多副本且基于ZooKeeper协调的分布式消息系统，现已被捐献给Apache基金会。目前Kafka已经定位为一个分布式流式处理平台，它以高吞吐、可持久化、可水平扩展、支持流数据处理等多种特性而被广泛使用。 Kafka主要扮演三大角色： 消息系统。提供传统的消息中间件的功能：系统解耦、冗余存储、流量削峰、异步通信等，同时还提供了消息的顺序性保障以及回溯消费的功能。 存储系统。Kafka有多副本机制，所以可以用来当数据存储系统使用。 流式处理平台。为每个流行的流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库。 基本概念组成 Producer。生产者，负责创建消息发送到 broker 中。 Consumer。从 broker 中接收消息。 Zookeeper。负责集群元数据的管理、控制器的选举等等。 Broker。负责将收到的消息存储到磁盘中，可以是一个集群。 重要的概念Topic 和 PartitionKafka中的消息以 topic 为单位进行归类，生产者负责将消息发送到特定的主题，而消费者负责订阅主题并进行消费。 topic 仅仅是一个逻辑上的概念，实际的实现还是分区，一个主题可以有多个分区，每个分区都可以分散到不同 broker 中，分区在存储层面可以看作是一个可追加的 log 文件，而 offset 是消息在分区中的唯一标识，Kafka 通过它来保证分区内的消息的顺序性，所以 Kafka 仅保证分区有序，不保证主题有序。 多副本机制Kafka 为分区引入了多副本机制，保证高可用。在这里和 Mysql 一样，同样是一主多从，所以关键就是主从同步的问题，同时还会涉及到主备切换、主从延迟等通用问题。 分区中所有副本统称为 AR(Assigned Replicas)，AR = ISR(In-Sync Replicas) + OSR(Out-of-Sync Replicas)，当 leader副本发生故障时，只有在 ISR 中的 副本才有资格被选取为新的 leader。 在读取 offset 的过程中，也引入了 HW(高水位) 和 LEO(Log End Offset)的概念，在 Mysql 中实现 mvcc 中，也同样采用了 高水位的概念，不过在那里是在事务的视图中出现的。这里的 HW 和 LEO 同样让我想到了 NIO 中的 buffer 也是拥有同样的概念的。 Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。这里我们可以类比事务处理。同步意味着所有都完成才提交事务「即要求所有 follower 副本全部复制完才可以进行读取消息」，而异步则意味着只要leader读入成功则直接提交事务「Mysql 是异步复制」。前者影响效率，后者则存在不可靠性，因为一旦leader宕机而follower没有同步完成，数据就会出现问题。所以这里采用折中的方案，每同步完一个消息，就进行一次事务提交「即每全部同步完一条消息就提交一次事务」。 安装和配置博主使用的 macOS 系统，所以接下来对在 macOS 下如何安装运行 kafka 做一个详细的叙述。 JDK 的安装和配置Kafka是建立在 jvm 之上的，所以需要先安装 jvm，这个很简单啦… https://www.jianshu.com/p/194531d106ae 下载 配环境 查看配置是否成功 这里我不详细叙述，直接看该网址就行，下面的我会详细叙述。 Zookeeper 的安装和配置首先再简单介绍一下 Zookeeper，其实 kafka 现在已经内置了 Zookeeper，这里我们就暂时不使用内置的 zk 了，因为内置的只能是单机版，但是在生产环境中 zk 一般都是集群，所以这里我们再单独进行 zk 的安装和配置。 ZooKeeper是一个开源的分布式协调服务，是Google Chubby的一个开源实现。分布式应用程序可以基于ZooKeeper实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、配置维护等功能。在ZooKeeper中共有3个角色：leader、follower和observer，同一时刻 ZooKeeper集群中只会有一个leader，其他的都是follower和observer。observer不参与投票，默认情况下 ZooKeeper 中只有 leader 和 follower 两个角色。 下载 Zookeeper brew install zookeeper 这里需要注意的是，下载之后，Zookeeper 中的 bin 目录和配置文件不在一块。 bin 目录的位置：”/usr/local/Cellar/zookeeper/3.4.14” 配置文件的位置：”/usr/local/etc/zookeeper” Zookeeper 的 启动停止日志：/usr/local/var/log/zookeeper 配环境 命令：open -e .bash_profile 123456789101112131415MONGODB_HOME=/usr/local/mongodbKAFKA_HOME=/usr/local/Cellar/kafka/2.4.0ZOOKEEPER_HOME=/usr/local/Cellar/zookeeper/3.4.14PATH="/Library/Frameworks/Python.framework/Versions/3.7/bin:$ZOOKEEPER_HOME/bin:$KAFKA_HOME/bin:$MONGODB_HOME/bin:$&#123;PATH&#125;:"export PATHexport PATH="/Users/yangweijie/anaconda3/bin:$PATH"export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Homeexport CLASSPAHT=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$JAVA_HOME/bin:$PATH: 命令：source .bash_profile 让配置文件生效 修改 Zookeeper 配置文件 进入到 /usr/local/etc/zookeeper，然后将 zoo_sample.cfg 文件修改为 zoo.cfg cd /usr/local/etc/zookeepercp zoo_sample.cfg zoo.cfg 修改 zoo.cfg 配置文件，主要是创建数据目录和日志目录，因为默认是没有的，我们需要自己创建 open -e zoo.cfgmkdir -p tmp/datamkdir -p tmp/log 123456789101112131415161718192021222324252627282930313233343536373839404142# zoo.cfg 文件内容# The number of milliseconds of each tick# zookeeper 服务器心跳时间，单位为 mstickTime=2000# The number of ticks that the initial # synchronization phase can take# 投票选举新leader的初始化时间initLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgement# leader与follower 心跳检测最大容忍时间，响应时间超过 syncLimit * tickTime，lead# er就认为 follower 死掉了，从服务器中删除 followersyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.# 数据目录dataDir=/usr/local/etc/zookeeper/tmp/data# the port at which the clients will connect# 对外服务端口clientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60# 日志目录dataLogDir=/usr/local/etc/zookeeper/tmp/log# # Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to "0" to disable auto purge feature#autopurge.purgeInterval=1 dataDir：”/usr/local/etc/zookeeper/tmp/data” Log: “/usr/local/etc/zookeeper/tmp/log” 启动 Zookeeper启动 zookeeper： zkServer start 查看 zookeeper 状态： zkServer status 这个其实看不出来 Zookeeper 是否真的运行起来了，我在下面的问题模块也有讲到，如果想看 Zookeeper 是否真的运行起来了，可以查看 “/usr/local/etc/zookeeper/tmp/data/zookeeper_server.pid” 是否存在，当 zk stop时该文件就会消失。 Kafka 的安装和配置下载 kafka brew install kafka 同样的，bin 和 配置文件位置不在一起。 bin 目录的位置：”/usr/local/Cellar/kafka/2.4.0” 配置文件的位置：”/usr/local/etc/kafka” Kafka 的启动停止日志：/usr/local/var/log/kafka 配环境见上面的 bash_profile 修改 Kafka 配置文件将 /usr/local/etc/kafka/server.properties 进行相应的修改，主要是对日志文件的地址进行修改，然后修改完之后自己创建好相应的目录。 这里我的 kafka 的消息 log 是 /usr/local/etc/kafka/tmp/kafka-logs。 启动 Kafka 启动 kafka 1kafka-server-start /usr/local/etc/kafka/server.properties 常用指令： 创建 topic： 1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看 topic : 1kafka-topics --list --zookeeper localhost:2181 发送消息： 1kafka-console-producer --broker-list localhost:9092 --topic test 消费消息： 1kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning 查看 java 有关的进程： 1jps -l 遇到的问题Q1问题系统是 macOS，kafka 版本为 2.4.0，zookeeper 版本为3.4.14，在安装启动遇到这样一个问题： 用 “kafka-server-stop /usr/local/etc/kafka/server.properties” 无法杀死 kafka 进程… 使用完这个命令之后 用命令 “jps -l” 可以看到，原先的Kafka.kafka 对应的进程的确没了，但是莫名其妙又起了一个进程来运行 kafka，这是啥情况… 为啥会杀不死。 「从后文可以知道，jps 实际上无法准备的确认 kafka服务 是否正常启动。」 尝试解决方案 重写kafka-server-stop.sh 脚本 12345678PIDS=$(ps -ef |grep java|grep kafka | grep -v grep | awk '&#123;print $2&#125;')for PID in $PIDSdo kill -9 $PIDdoneecho -e "Stop Finished!\n" 原 kafka-server-stop.sh 内容： 123456789SIGNAL=$&#123;SIGNAL:-TERM&#125;PIDS=$(ps ax | grep -i 'kafka\.Kafka' | grep java | grep -v grep | awk '&#123;print $1&#125;')if [ -z "$PIDS" ]; then echo "No kafka server to stop" exit 1else kill -s $SIGNAL $PIDSfi 结果：无效。。。。 参考： https://blog.csdn.net/dengjili/article/details/95041267 依旧是修改 kafka-server-stop.sh 的内容 1PIDS=$(ps ax | grep -i ‘kafka.Kafka’ | grep java | grep -v grep | awk ‘&#123;print $1&#125;’) 修改为： 1PIDS=$(jps -lm | grep -i 'kafka.Kafka' | awk '&#123;print $1&#125;') 结果：依旧失败。。。。。心态炸了 在解决第三个问题的时候发现，发现并不能仅仅通过 jps 去看 kafka 是否停止运行，因为其非常的不准确，其实 kafka 可能已经停止了，也就是说，”kafka-server-stop /usr/local/etc/kafka/server.properties” 其实已经杀死 kafka 服务进程，只是我们不知道而已。。。。 我们可以通过生产消息：kafka-console-producer --broker-list localhost:9092 --topic test来判断 kafka 是否是正常启动的。 结果：已解决！！！ Q2问题使用命令 “zkServer stop” 无法停止 zookeeper，报错： 123ZooKeeper JMX enabled by defaultUsing config: /usr/local/etc/zookeeper/zoo.cfgStopping zookeeper ... no zookeeper to stop (could not find file /usr/local/etc/zookeeper/tmp/data/zookeeper_server.pid) 尝试解决方案原来是因为我根本没有把 Zookeeper 起来，使用 zkServer status并不能显示 zk 是否真的起来了… 在我再次启动 zk 时，的确在 “/usr/local/etc/zookeeper/tmp/data 下 出现了 zookeeper_server.pid” 这个文件。 所以说，想看 zk 到底是否真的起来了，可以查看 “/usr/local/etc/zookeeper/tmp/data/zookeeper_server.pid” 是否存在，当 zk stop时该文件就会消失。 结果：该问题解决！！！ Q3问题在我一直无法关闭 kafka 之后，我进行了电脑重启，重启之后，使用 “jps -l” 的确没有再出现 “kafka.Kafka”，说明这时 kafka 的确停止了，但是新的问题又出现了… 在使用 kafka-server-start /usr/local/etc/kafka/server.properties 命令之后，再也无法启动 kafka 了… 我太难了… 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[2020-04-24 23:16:58,980] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[2020-04-24 23:16:59,391] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)[2020-04-24 23:16:59,392] INFO starting (kafka.server.KafkaServer)[2020-04-24 23:16:59,392] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)[2020-04-24 23:16:59,406] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)[2020-04-24 23:16:59,410] INFO Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,410] INFO Client environment:host.name=localhost (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,411] INFO Client environment:java.version=1.8.0_222 (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,411] INFO Client environment:java.vendor=AdoptOpenJDK (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,411] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,411] INFO Client environment:java.class.path=/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/activation-1.1.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/aopalliance-repackaged-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/argparse4j-0.7.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/audience-annotations-0.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/commons-cli-1.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/commons-lang3-3.8.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-api-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-basic-auth-extension-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-file-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-json-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-mirror-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-mirror-client-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-runtime-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-transforms-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/guava-20.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-api-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-locator-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-utils-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-annotations-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-core-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-databind-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-dataformat-csv-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-datatype-jdk8-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-jaxrs-base-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-jaxrs-json-provider-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-jaxb-annotations-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-paranamer-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-scala_2.12-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.activation-api-1.2.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.annotation-api-1.3.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.inject-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javassist-3.22.0-CR2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javax.servlet-api-3.1.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javax.ws.rs-api-2.1.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jaxb-api-2.3.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-client-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-common-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-container-servlet-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-container-servlet-core-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-hk2-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-media-jaxb-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-server-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-client-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-continuation-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-http-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-io-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-security-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-server-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-servlet-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-servlets-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-util-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jopt-simple-5.0.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-clients-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-log4j-appender-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-examples-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-scala_2.12-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-test-utils-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-tools-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka_2.12-2.4.0-sources.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka_2.12-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/log4j-1.2.17.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/lz4-java-1.6.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/maven-artifact-3.6.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/metrics-core-2.2.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-buffer-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-codec-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-common-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-handler-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-resolver-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-native-epoll-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-native-unix-common-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/osgi-resource-locator-1.0.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/paranamer-2.8.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/plexus-utils-3.2.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/reflections-0.9.11.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/rocksdbjni-5.18.3.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-collection-compat_2.12-2.1.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-java8-compat_2.12-0.9.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-library-2.12.10.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-logging_2.12-3.9.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-reflect-2.12.10.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/slf4j-api-1.7.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/slf4j-log4j12-1.7.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/snappy-java-1.1.7.3.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/validation-api-2.0.1.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zookeeper-3.5.6.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zookeeper-jute-3.5.6.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zstd-jni-1.4.3-1.jar (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,411] INFO Client environment:java.library.path=/Users/yangweijie/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:java.io.tmpdir=/var/folders/cp/p4lwr_6n66s065dhcw80dzd80000gn/T/ (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:java.compiler=&lt;NA&gt; (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:os.name=Mac OS X (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:os.arch=x86_64 (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:os.version=10.15.4 (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:user.name=yangweijie (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:user.home=/Users/yangweijie (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:user.dir=/usr/local/etc/kafka/tmp (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:os.memory.free=978MB (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,412] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,414] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7722c3c3 (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,417] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)[2020-04-24 23:16:59,422] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)[2020-04-24 23:16:59,426] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)[2020-04-24 23:16:59,428] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)[2020-04-24 23:16:59,430] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)[2020-04-24 23:16:59,440] INFO Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:51131, server: localhost/0:0:0:0:0:0:0:1:2181 (org.apache.zookeeper.ClientCnxn)[2020-04-24 23:16:59,444] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x10000053312004f, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)[2020-04-24 23:16:59,447] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)[2020-04-24 23:16:59,612] INFO Cluster ID = I5sBHS6MSMG4MmUddyviFQ (kafka.server.KafkaServer)[2020-04-24 23:16:59,621] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)kafka.common.InconsistentClusterIdException: The Cluster ID I5sBHS6MSMG4MmUddyviFQ doesn't match stored clusterId Some(Lz_cYIXrTryPr_06DLt6hQ) in meta.properties. The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong. at kafka.server.KafkaServer.startup(KafkaServer.scala:220) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44) at kafka.Kafka$.main(Kafka.scala:84) at kafka.Kafka.main(Kafka.scala)[2020-04-24 23:16:59,623] INFO shutting down (kafka.server.KafkaServer)[2020-04-24 23:16:59,625] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)[2020-04-24 23:16:59,732] INFO Session: 0x10000053312004f closed (org.apache.zookeeper.ZooKeeper)[2020-04-24 23:16:59,732] INFO EventThread shut down for session: 0x10000053312004f (org.apache.zookeeper.ClientCnxn)[2020-04-24 23:16:59,734] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)[2020-04-24 23:16:59,737] INFO shut down completed (kafka.server.KafkaServer)[2020-04-24 23:16:59,737] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)[2020-04-24 23:16:59,739] INFO shutting down (kafka.server.KafkaServer) 通过 “jps -l” 的结果看，的确是没有启动起来。 123yangweijieMacBook-Pro:tmp yangweijie$ jps -l640 org.apache.zookeeper.server.quorum.QuorumPeerMain30653 sun.tools.jps.Jps 然后我就去洗澡了.. 回来 “jps -l”，我擦，kafka 起来了，自己起来了？？？我的妈，受不了了… 1234yangweijieMacBook-Pro:~ yangweijie$ jps -l26240 sun.tools.jps.Jps25920 kafka.Kafka640 org.apache.zookeeper.server.quorum.QuorumPeerMain 然后我不信，我就试着去生产一个消息 1kafka-console-producer --broker-list localhost:9092 --topic test 的确，报错了 1WARN [Producer clientId=console-producer] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient) 这我就想不通了… 这个 kafka.Kafka 难道不能代表 kafka 服务端活着？ 然后我又去尝试的 “jps -l” ，妈的，那个进程又死了… 123yangweijieMacBook-Pro:~ yangweijie$ jps -l640 org.apache.zookeeper.server.quorum.QuorumPeerMain32381 sun.tools.jps.Jps 为了证明我眼睛没花，我又尝试的一遍，妈的，又出现了一个新的 kafka 进程。。。 1234yangweijieMacBook-Pro:~ yangweijie$ jps -l32704 sun.tools.jps.Jps640 org.apache.zookeeper.server.quorum.QuorumPeerMain32384 kafka.Kafka 然后我又试了一遍，kafka 进程又没了… 然后我接着试了很多遍，kafka进程一直没有… 这次，我决定再次启动一下 kafka 试试… 还是启动不了… 然后我无聊的测试下 “jsp -l” 这个命令，发现其执行 5-6 次就会有一次 “kafka.Kafka” 的进程产生，然后又会自己消失… 玄学？？？？？？崩了… 尝试解决方案 参考：https://blog.csdn.net/Sakitaf/article/details/104954268 我们回到第三个最初的问题，也就是一直启动不了 kafka，也就是上面的报错： 1kafka.common.InconsistentClusterIdException: The Cluster ID I5sBHS6MSMG4MmUddyviFQ doesn't match stored clusterId Some(Lz_cYIXrTryPr_06DLt6hQ) in meta.properties. The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong. 于是，我就尝试去找 meta.properties，原来这个 meta.properties 在/usr/local/etc/kafka/tmp/kafka-logs/，然后将其里面的 meta.properties 中的 Cluster ID 改为报错中需要的。这应该就是 2.4.0 的bug了，在我上次暴力关闭 kafka 时，不会主动的与新的zk实例建立连接。 结果：已解决！！！！ Q4问题最近想查一下 kafka 的启动日志，结果不查不知道，一查吓一跳，一个启动日志占了4个多g的存储… 看了下，全是启动日志，10分钟就更新一次… 吓得我赶紧删了，得研究一下如何不让他更新的这么频繁，我的256g的电脑瑟瑟发抖… 尝试解决方案看了下，应该是以前把 Zookeeper 关闭了，但是 kafka 一直没关闭，然后 kafka 一直失败重连，日志疯狂增加… 再次尝试的使用命令 “kafka-server-stop /usr/local/etc/kafka/server.properties”，然后看 “/usr/local/var/log/kafka” 下的日志。 首先，的确有 shutdown，这是我搜索 shutdown 后过滤得到的日志，如下： 12345678910111213141516171819202122232425 controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000[2020-04-29 18:21:19,508] ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)[2020-04-29 18:21:19,517] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)[2020-04-29 18:21:19,518] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)[2020-04-29 18:21:19,519] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)[2020-04-29 18:21:19,637] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,911] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,912] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:20,187] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:20,262] INFO Shutdown complete. (kafka.log.LogManager)[2020-04-29 18:21:20,714] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:21,714] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:22,786] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:22,814] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer) controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 但是，使用 jps -l 看的确 kafka 进程还没死，收发消息也正常，所以我们再看一下完整的日志，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517[2020-04-29 18:21:17,883] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[2020-04-29 18:21:18,347] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)[2020-04-29 18:21:18,348] INFO starting (kafka.server.KafkaServer)[2020-04-29 18:21:18,348] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)[2020-04-29 18:21:18,364] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:18,370] INFO Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,370] INFO Client environment:host.name=localhost (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,370] INFO Client environment:java.version=1.8.0_222 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,370] INFO Client environment:java.vendor=AdoptOpenJDK (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,370] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,370] INFO Client environment:java.class.path=/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/activation-1.1.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/aopalliance-repackaged-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/argparse4j-0.7.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/audience-annotations-0.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/commons-cli-1.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/commons-lang3-3.8.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-api-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-basic-auth-extension-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-file-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-json-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-mirror-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-mirror-client-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-runtime-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-transforms-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/guava-20.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-api-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-locator-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-utils-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-annotations-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-core-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-databind-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-dataformat-csv-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-datatype-jdk8-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-jaxrs-base-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-jaxrs-json-provider-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-jaxb-annotations-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-paranamer-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-scala_2.12-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.activation-api-1.2.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.annotation-api-1.3.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.inject-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javassist-3.22.0-CR2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javax.servlet-api-3.1.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javax.ws.rs-api-2.1.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jaxb-api-2.3.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-client-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-common-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-container-servlet-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-container-servlet-core-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-hk2-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-media-jaxb-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-server-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-client-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-continuation-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-http-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-io-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-security-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-server-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-servlet-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-servlets-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-util-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jopt-simple-5.0.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-clients-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-log4j-appender-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-examples-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-scala_2.12-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-test-utils-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-tools-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka_2.12-2.4.0-sources.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka_2.12-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/log4j-1.2.17.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/lz4-java-1.6.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/maven-artifact-3.6.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/metrics-core-2.2.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-buffer-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-codec-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-common-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-handler-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-resolver-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-native-epoll-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-native-unix-common-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/osgi-resource-locator-1.0.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/paranamer-2.8.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/plexus-utils-3.2.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/reflections-0.9.11.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/rocksdbjni-5.18.3.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-collection-compat_2.12-2.1.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-java8-compat_2.12-0.9.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-library-2.12.10.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-logging_2.12-3.9.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-reflect-2.12.10.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/slf4j-api-1.7.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/slf4j-log4j12-1.7.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/snappy-java-1.1.7.3.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/validation-api-2.0.1.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zookeeper-3.5.6.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zookeeper-jute-3.5.6.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zstd-jni-1.4.3-1.jar (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,370] INFO Client environment:java.library.path=/Users/yangweijie/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:java.io.tmpdir=/var/folders/cp/p4lwr_6n66s065dhcw80dzd80000gn/T/ (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:java.compiler=&lt;NA&gt; (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:os.name=Mac OS X (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:os.arch=x86_64 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:os.version=10.15.4 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:user.name=yangweijie (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:user.home=/Users/yangweijie (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:user.dir=/usr/local (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:os.memory.free=978MB (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,371] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,373] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7722c3c3 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:18,377] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)[2020-04-29 18:21:18,383] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)[2020-04-29 18:21:18,388] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:18,389] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:18,392] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:18,403] INFO Socket connection established, initiating session, client: /127.0.0.1:56049, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:18,408] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100000533120320, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:18,411] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:18,600] INFO Cluster ID = I5sBHS6MSMG4MmUddyviFQ (kafka.server.KafkaServer)[2020-04-29 18:21:18,659] INFO KafkaConfig values: advertised.host.name = null advertised.listeners = null advertised.port = null alter.config.policy.class.name = null alter.log.dirs.replication.quota.window.num = 11 alter.log.dirs.replication.quota.window.size.seconds = 1 authorizer.class.name = auto.create.topics.enable = true auto.leader.rebalance.enable = true background.threads = 10 broker.id = 0 broker.id.generation.enable = true broker.rack = null client.quota.callback.class = null compression.type = producer connection.failed.authentication.delay.ms = 100 connections.max.idle.ms = 600000 connections.max.reauth.ms = 0 control.plane.listener.name = null controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controller.socket.timeout.ms = 30000 create.topic.policy.class.name = null default.replication.factor = 1 delegation.token.expiry.check.interval.ms = 3600000 delegation.token.expiry.time.ms = 86400000 delegation.token.master.key = null delegation.token.max.lifetime.ms = 604800000 delete.records.purgatory.purge.interval.requests = 1 delete.topic.enable = true fetch.purgatory.purge.interval.requests = 1000 group.initial.rebalance.delay.ms = 0 group.max.session.timeout.ms = 1800000 group.max.size = 2147483647 group.min.session.timeout.ms = 6000 host.name = inter.broker.listener.name = null inter.broker.protocol.version = 2.4-IV1 kafka.metrics.polling.interval.secs = 10 kafka.metrics.reporters = [] leader.imbalance.check.interval.seconds = 300 leader.imbalance.per.broker.percentage = 10 listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listeners = PLAINTEXT://:9092 log.cleaner.backoff.ms = 15000 log.cleaner.dedupe.buffer.size = 134217728 log.cleaner.delete.retention.ms = 86400000 log.cleaner.enable = true log.cleaner.io.buffer.load.factor = 0.9 log.cleaner.io.buffer.size = 524288 log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308 log.cleaner.max.compaction.lag.ms = 9223372036854775807 log.cleaner.min.cleanable.ratio = 0.5 log.cleaner.min.compaction.lag.ms = 0 log.cleaner.threads = 1 log.cleanup.policy = [delete] log.dir = /tmp/kafka-logs log.dirs = /usr/local/etc/kafka/tmp/kafka-logs log.flush.interval.messages = 9223372036854775807 log.flush.interval.ms = null log.flush.offset.checkpoint.interval.ms = 60000 log.flush.scheduler.interval.ms = 9223372036854775807 log.flush.start.offset.checkpoint.interval.ms = 60000 log.index.interval.bytes = 4096 log.index.size.max.bytes = 10485760 log.message.downconversion.enable = true log.message.format.version = 2.4-IV1 log.message.timestamp.difference.max.ms = 9223372036854775807 log.message.timestamp.type = CreateTime log.preallocate = false log.retention.bytes = -1 log.retention.check.interval.ms = 300000 log.retention.hours = 168 log.retention.minutes = null log.retention.ms = null log.roll.hours = 168 log.roll.jitter.hours = 0 log.roll.jitter.ms = null log.roll.ms = null log.segment.bytes = 1073741824 log.segment.delete.delay.ms = 60000 max.connections = 2147483647 max.connections.per.ip = 2147483647 max.connections.per.ip.overrides = max.incremental.fetch.session.cache.slots = 1000 message.max.bytes = 1000012 metric.reporters = [] metrics.num.samples = 2 metrics.recording.level = INFO metrics.sample.window.ms = 30000 min.insync.replicas = 1 num.io.threads = 8 num.network.threads = 3 num.partitions = 1 num.recovery.threads.per.data.dir = 1 num.replica.alter.log.dirs.threads = null num.replica.fetchers = 1 offset.metadata.max.bytes = 4096 offsets.commit.required.acks = -1 offsets.commit.timeout.ms = 5000 offsets.load.buffer.size = 5242880 offsets.retention.check.interval.ms = 600000 offsets.retention.minutes = 10080 offsets.topic.compression.codec = 0 offsets.topic.num.partitions = 50 offsets.topic.replication.factor = 1 offsets.topic.segment.bytes = 104857600 password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding password.encoder.iterations = 4096 password.encoder.key.length = 128 password.encoder.keyfactory.algorithm = null password.encoder.old.secret = null password.encoder.secret = null port = 9092 principal.builder.class = null producer.purgatory.purge.interval.requests = 1000 queued.max.request.bytes = -1 queued.max.requests = 500 quota.consumer.default = 9223372036854775807 quota.producer.default = 9223372036854775807 quota.window.num = 11 quota.window.size.seconds = 1 replica.fetch.backoff.ms = 1000 replica.fetch.max.bytes = 1048576 replica.fetch.min.bytes = 1 replica.fetch.response.max.bytes = 10485760 replica.fetch.wait.max.ms = 500 replica.high.watermark.checkpoint.interval.ms = 5000 replica.lag.time.max.ms = 10000 replica.selector.class = null replica.socket.receive.buffer.bytes = 65536 replica.socket.timeout.ms = 30000 replication.quota.window.num = 11 replication.quota.window.size.seconds = 1 request.timeout.ms = 30000 reserved.broker.max.id = 1000 sasl.client.callback.handler.class = null sasl.enabled.mechanisms = [GSSAPI] sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.principal.to.local.rules = [DEFAULT] sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism.inter.broker.protocol = GSSAPI sasl.server.callback.handler.class = null security.inter.broker.protocol = PLAINTEXT security.providers = null socket.receive.buffer.bytes = 102400 socket.request.max.bytes = 104857600 socket.send.buffer.bytes = 102400 ssl.cipher.suites = [] ssl.client.auth = none ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = null ssl.keystore.password = null ssl.keystore.type = JKS ssl.principal.mapping.rules = DEFAULT ssl.protocol = TLS ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = null ssl.truststore.password = null ssl.truststore.type = JKS transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000 transaction.max.timeout.ms = 900000 transaction.remove.expired.transaction.cleanup.interval.ms = 3600000 transaction.state.log.load.buffer.size = 5242880 transaction.state.log.min.isr = 1 transaction.state.log.num.partitions = 50 transaction.state.log.replication.factor = 1 transaction.state.log.segment.bytes = 104857600 transactional.id.expiration.ms = 604800000 unclean.leader.election.enable = false zookeeper.connect = localhost:2181 zookeeper.connection.timeout.ms = 6000 zookeeper.max.in.flight.requests = 10 zookeeper.session.timeout.ms = 6000 zookeeper.set.acl = false zookeeper.sync.time.ms = 2000 (kafka.server.KafkaConfig)[2020-04-29 18:21:18,667] INFO KafkaConfig values: advertised.host.name = null advertised.listeners = null advertised.port = null alter.config.policy.class.name = null alter.log.dirs.replication.quota.window.num = 11 alter.log.dirs.replication.quota.window.size.seconds = 1 authorizer.class.name = auto.create.topics.enable = true auto.leader.rebalance.enable = true background.threads = 10 broker.id = 0 broker.id.generation.enable = true broker.rack = null client.quota.callback.class = null compression.type = producer connection.failed.authentication.delay.ms = 100 connections.max.idle.ms = 600000 connections.max.reauth.ms = 0 control.plane.listener.name = null controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controller.socket.timeout.ms = 30000 create.topic.policy.class.name = null default.replication.factor = 1 delegation.token.expiry.check.interval.ms = 3600000 delegation.token.expiry.time.ms = 86400000 delegation.token.master.key = null delegation.token.max.lifetime.ms = 604800000 delete.records.purgatory.purge.interval.requests = 1 delete.topic.enable = true fetch.purgatory.purge.interval.requests = 1000 group.initial.rebalance.delay.ms = 0 group.max.session.timeout.ms = 1800000 group.max.size = 2147483647 group.min.session.timeout.ms = 6000 host.name = inter.broker.listener.name = null inter.broker.protocol.version = 2.4-IV1 kafka.metrics.polling.interval.secs = 10 kafka.metrics.reporters = [] leader.imbalance.check.interval.seconds = 300 leader.imbalance.per.broker.percentage = 10 listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listeners = PLAINTEXT://:9092 log.cleaner.backoff.ms = 15000 log.cleaner.dedupe.buffer.size = 134217728 log.cleaner.delete.retention.ms = 86400000 log.cleaner.enable = true log.cleaner.io.buffer.load.factor = 0.9 log.cleaner.io.buffer.size = 524288 log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308 log.cleaner.max.compaction.lag.ms = 9223372036854775807 log.cleaner.min.cleanable.ratio = 0.5 log.cleaner.min.compaction.lag.ms = 0 log.cleaner.threads = 1 log.cleanup.policy = [delete] log.dir = /tmp/kafka-logs log.dirs = /usr/local/etc/kafka/tmp/kafka-logs log.flush.interval.messages = 9223372036854775807 log.flush.interval.ms = null log.flush.offset.checkpoint.interval.ms = 60000 log.flush.scheduler.interval.ms = 9223372036854775807 log.flush.start.offset.checkpoint.interval.ms = 60000 log.index.interval.bytes = 4096 log.index.size.max.bytes = 10485760 log.message.downconversion.enable = true log.message.format.version = 2.4-IV1 log.message.timestamp.difference.max.ms = 9223372036854775807 log.message.timestamp.type = CreateTime log.preallocate = false log.retention.bytes = -1 log.retention.check.interval.ms = 300000 log.retention.hours = 168 log.retention.minutes = null log.retention.ms = null log.roll.hours = 168 log.roll.jitter.hours = 0 log.roll.jitter.ms = null log.roll.ms = null log.segment.bytes = 1073741824 log.segment.delete.delay.ms = 60000 max.connections = 2147483647 max.connections.per.ip = 2147483647 max.connections.per.ip.overrides = max.incremental.fetch.session.cache.slots = 1000 message.max.bytes = 1000012 metric.reporters = [] metrics.num.samples = 2 metrics.recording.level = INFO metrics.sample.window.ms = 30000 min.insync.replicas = 1 num.io.threads = 8 num.network.threads = 3 num.partitions = 1 num.recovery.threads.per.data.dir = 1 num.replica.alter.log.dirs.threads = null num.replica.fetchers = 1 offset.metadata.max.bytes = 4096 offsets.commit.required.acks = -1 offsets.commit.timeout.ms = 5000 offsets.load.buffer.size = 5242880 offsets.retention.check.interval.ms = 600000 offsets.retention.minutes = 10080 offsets.topic.compression.codec = 0 offsets.topic.num.partitions = 50 offsets.topic.replication.factor = 1 offsets.topic.segment.bytes = 104857600 password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding password.encoder.iterations = 4096 password.encoder.key.length = 128 password.encoder.keyfactory.algorithm = null password.encoder.old.secret = null password.encoder.secret = null port = 9092 principal.builder.class = null producer.purgatory.purge.interval.requests = 1000 queued.max.request.bytes = -1 queued.max.requests = 500 quota.consumer.default = 9223372036854775807 quota.producer.default = 9223372036854775807 quota.window.num = 11 quota.window.size.seconds = 1 replica.fetch.backoff.ms = 1000 replica.fetch.max.bytes = 1048576 replica.fetch.min.bytes = 1 replica.fetch.response.max.bytes = 10485760 replica.fetch.wait.max.ms = 500 replica.high.watermark.checkpoint.interval.ms = 5000 replica.lag.time.max.ms = 10000 replica.selector.class = null replica.socket.receive.buffer.bytes = 65536 replica.socket.timeout.ms = 30000 replication.quota.window.num = 11 replication.quota.window.size.seconds = 1 request.timeout.ms = 30000 reserved.broker.max.id = 1000 sasl.client.callback.handler.class = null sasl.enabled.mechanisms = [GSSAPI] sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.principal.to.local.rules = [DEFAULT] sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism.inter.broker.protocol = GSSAPI sasl.server.callback.handler.class = null security.inter.broker.protocol = PLAINTEXT security.providers = null socket.receive.buffer.bytes = 102400 socket.request.max.bytes = 104857600 socket.send.buffer.bytes = 102400 ssl.cipher.suites = [] ssl.client.auth = none ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = null ssl.keystore.password = null ssl.keystore.type = JKS ssl.principal.mapping.rules = DEFAULT ssl.protocol = TLS ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = null ssl.truststore.password = null ssl.truststore.type = JKS transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000 transaction.max.timeout.ms = 900000 transaction.remove.expired.transaction.cleanup.interval.ms = 3600000 transaction.state.log.load.buffer.size = 5242880 transaction.state.log.min.isr = 1 transaction.state.log.num.partitions = 50 transaction.state.log.replication.factor = 1 transaction.state.log.segment.bytes = 104857600 transactional.id.expiration.ms = 604800000 unclean.leader.election.enable = false zookeeper.connect = localhost:2181 zookeeper.connection.timeout.ms = 6000 zookeeper.max.in.flight.requests = 10 zookeeper.session.timeout.ms = 6000 zookeeper.set.acl = false zookeeper.sync.time.ms = 2000 (kafka.server.KafkaConfig)[2020-04-29 18:21:18,688] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:18,688] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:18,688] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:18,721] INFO Loading logs. (kafka.log.LogManager)[2020-04-29 18:21:18,768] INFO [Log partition=__consumer_offsets-9, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,770] INFO [Log partition=__consumer_offsets-9, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,802] INFO [Log partition=__consumer_offsets-9, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,804] INFO [Log partition=__consumer_offsets-9, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 60 ms (kafka.log.Log)[2020-04-29 18:21:18,812] INFO [Log partition=__consumer_offsets-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,812] INFO [Log partition=__consumer_offsets-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,814] INFO [Log partition=__consumer_offsets-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,815] INFO [Log partition=__consumer_offsets-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,819] INFO [Log partition=__consumer_offsets-7, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,819] INFO [Log partition=__consumer_offsets-7, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,821] INFO [Log partition=__consumer_offsets-7, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,822] INFO [Log partition=__consumer_offsets-7, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)[2020-04-29 18:21:18,826] INFO [Log partition=__consumer_offsets-31, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,826] INFO [Log partition=__consumer_offsets-31, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,828] INFO [Log partition=__consumer_offsets-31, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,829] INFO [Log partition=__consumer_offsets-31, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,832] INFO [Log partition=__consumer_offsets-36, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,832] INFO [Log partition=__consumer_offsets-36, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,834] INFO [Log partition=__consumer_offsets-36, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,835] INFO [Log partition=__consumer_offsets-36, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,838] INFO [Log partition=__consumer_offsets-38, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,839] INFO [Log partition=__consumer_offsets-38, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,841] INFO [Log partition=__consumer_offsets-38, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,841] INFO [Log partition=__consumer_offsets-38, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,845] INFO [Log partition=__consumer_offsets-6, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,845] INFO [Log partition=__consumer_offsets-6, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,847] INFO [Log partition=__consumer_offsets-6, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,847] INFO [Log partition=__consumer_offsets-6, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,850] INFO [Log partition=__consumer_offsets-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,851] INFO [Log partition=__consumer_offsets-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,853] INFO [Log partition=__consumer_offsets-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,853] INFO [Log partition=__consumer_offsets-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,857] INFO [Log partition=__consumer_offsets-8, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,857] INFO [Log partition=__consumer_offsets-8, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,859] INFO [Log partition=__consumer_offsets-8, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,859] INFO [Log partition=__consumer_offsets-8, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,863] INFO [Log partition=__consumer_offsets-39, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,863] INFO [Log partition=__consumer_offsets-39, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,865] INFO [Log partition=__consumer_offsets-39, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,865] INFO [Log partition=__consumer_offsets-39, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,869] INFO [Log partition=__consumer_offsets-37, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,869] INFO [Log partition=__consumer_offsets-37, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,871] INFO [Log partition=__consumer_offsets-37, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,871] INFO [Log partition=__consumer_offsets-37, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,874] INFO [Log partition=__consumer_offsets-30, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,874] INFO [Log partition=__consumer_offsets-30, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,876] INFO [Log partition=__consumer_offsets-30, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,877] INFO [Log partition=__consumer_offsets-30, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,881] INFO [Log partition=test-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,881] INFO [Log partition=test-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,891] INFO [ProducerStateManager partition=test-0] Writing producer snapshot at offset 13 (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,898] INFO [Log partition=test-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 13 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,899] INFO [ProducerStateManager partition=test-0] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/test-0/00000000000000000013.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,906] INFO [Log partition=test-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 13 in 27 ms (kafka.log.Log)[2020-04-29 18:21:18,909] INFO [Log partition=__consumer_offsets-12, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,909] INFO [Log partition=__consumer_offsets-12, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,924] INFO [ProducerStateManager partition=__consumer_offsets-12] Writing producer snapshot at offset 430 (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,926] INFO [Log partition=__consumer_offsets-12, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 430 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,926] INFO [ProducerStateManager partition=__consumer_offsets-12] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-12/00000000000000000430.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,927] INFO [Log partition=__consumer_offsets-12, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 430 in 20 ms (kafka.log.Log)[2020-04-29 18:21:18,929] INFO [Log partition=__consumer_offsets-15, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,930] INFO [Log partition=__consumer_offsets-15, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,931] INFO [Log partition=__consumer_offsets-15, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,932] INFO [Log partition=__consumer_offsets-15, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,935] INFO [Log partition=__consumer_offsets-23, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,935] INFO [Log partition=__consumer_offsets-23, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,936] INFO [ProducerStateManager partition=__consumer_offsets-23] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,938] INFO [Log partition=__consumer_offsets-23, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,938] INFO [ProducerStateManager partition=__consumer_offsets-23] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-23/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,938] INFO [Log partition=__consumer_offsets-23, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 5 ms (kafka.log.Log)[2020-04-29 18:21:18,941] INFO [Log partition=__consumer_offsets-24, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,941] INFO [Log partition=__consumer_offsets-24, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,943] INFO [Log partition=__consumer_offsets-24, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,944] INFO [Log partition=__consumer_offsets-24, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,947] INFO [Log partition=test2-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,947] INFO [Log partition=test2-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,948] INFO [Log partition=test2-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,949] INFO [Log partition=test2-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:18,951] INFO [Log partition=__consumer_offsets-48, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,952] INFO [Log partition=__consumer_offsets-48, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,953] INFO [Log partition=__consumer_offsets-48, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,954] INFO [Log partition=__consumer_offsets-48, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,957] INFO [Log partition=__consumer_offsets-41, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,957] INFO [Log partition=__consumer_offsets-41, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,959] INFO [Log partition=__consumer_offsets-41, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,960] INFO [Log partition=__consumer_offsets-41, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)[2020-04-29 18:21:18,962] INFO [Log partition=__consumer_offsets-46, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,962] INFO [Log partition=__consumer_offsets-46, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,964] INFO [Log partition=__consumer_offsets-46, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,965] INFO [Log partition=__consumer_offsets-46, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,968] INFO [Log partition=__consumer_offsets-25, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,968] INFO [Log partition=__consumer_offsets-25, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,970] INFO [Log partition=__consumer_offsets-25, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,970] INFO [Log partition=__consumer_offsets-25, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:18,973] INFO [Log partition=__consumer_offsets-22, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,973] INFO [Log partition=__consumer_offsets-22, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,975] INFO [Log partition=__consumer_offsets-22, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,975] INFO [Log partition=__consumer_offsets-22, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:18,978] INFO [Log partition=__consumer_offsets-14, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,978] INFO [Log partition=__consumer_offsets-14, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,980] INFO [Log partition=__consumer_offsets-14, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,980] INFO [Log partition=__consumer_offsets-14, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:18,983] INFO [Log partition=__consumer_offsets-13, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,984] INFO [Log partition=__consumer_offsets-13, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,985] INFO [ProducerStateManager partition=__consumer_offsets-13] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,986] INFO [Log partition=__consumer_offsets-13, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,987] INFO [ProducerStateManager partition=__consumer_offsets-13] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-13/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:18,987] INFO [Log partition=__consumer_offsets-13, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 5 ms (kafka.log.Log)[2020-04-29 18:21:18,990] INFO [Log partition=__consumer_offsets-47, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,990] INFO [Log partition=__consumer_offsets-47, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,992] INFO [Log partition=__consumer_offsets-47, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,992] INFO [Log partition=__consumer_offsets-47, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:18,995] INFO [Log partition=__consumer_offsets-40, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,995] INFO [Log partition=__consumer_offsets-40, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,997] INFO [Log partition=__consumer_offsets-40, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:18,997] INFO [Log partition=__consumer_offsets-40, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:18,999] INFO [Log partition=__consumer_offsets-49, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:18,999] INFO [Log partition=__consumer_offsets-49, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,001] INFO [Log partition=__consumer_offsets-49, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,002] INFO [Log partition=__consumer_offsets-49, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,004] INFO [Log partition=__consumer_offsets-35, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,004] INFO [Log partition=__consumer_offsets-35, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,006] INFO [Log partition=__consumer_offsets-35, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,007] INFO [Log partition=__consumer_offsets-35, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,009] INFO [Log partition=__consumer_offsets-32, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,009] INFO [Log partition=__consumer_offsets-32, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,010] INFO [ProducerStateManager partition=__consumer_offsets-32] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)[2020-04-29 18:21:19,012] INFO [Log partition=__consumer_offsets-32, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,012] INFO [ProducerStateManager partition=__consumer_offsets-32] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-32/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:19,012] INFO [Log partition=__consumer_offsets-32, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,015] INFO [Log partition=__consumer_offsets-4, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,015] INFO [Log partition=__consumer_offsets-4, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,016] INFO [Log partition=__consumer_offsets-4, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,017] INFO [Log partition=__consumer_offsets-4, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,019] INFO [Log partition=__consumer_offsets-3, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,019] INFO [Log partition=__consumer_offsets-3, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,021] INFO [Log partition=__consumer_offsets-3, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,022] INFO [Log partition=__consumer_offsets-3, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,024] INFO [Log partition=__consumer_offsets-33, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,024] INFO [Log partition=__consumer_offsets-33, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,026] INFO [Log partition=__consumer_offsets-33, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,027] INFO [Log partition=__consumer_offsets-33, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,028] INFO [Log partition=__consumer_offsets-34, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,029] INFO [Log partition=__consumer_offsets-34, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,030] INFO [Log partition=__consumer_offsets-34, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,031] INFO [Log partition=__consumer_offsets-34, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,033] INFO [Log partition=__consumer_offsets-2, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,033] INFO [Log partition=__consumer_offsets-2, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,039] INFO [Log partition=__consumer_offsets-2, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,040] INFO [Log partition=__consumer_offsets-2, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)[2020-04-29 18:21:19,042] INFO [Log partition=__consumer_offsets-5, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,042] INFO [Log partition=__consumer_offsets-5, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,044] INFO [Log partition=__consumer_offsets-5, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,044] INFO [Log partition=__consumer_offsets-5, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,046] INFO [Log partition=jerome-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,046] INFO [Log partition=jerome-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,048] INFO [Log partition=jerome-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,048] INFO [Log partition=jerome-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,050] INFO [Log partition=__consumer_offsets-45, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,050] INFO [Log partition=__consumer_offsets-45, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,052] INFO [Log partition=__consumer_offsets-45, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,052] INFO [Log partition=__consumer_offsets-45, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,054] INFO [Log partition=__consumer_offsets-42, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,055] INFO [Log partition=__consumer_offsets-42, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,056] INFO [Log partition=__consumer_offsets-42, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,057] INFO [Log partition=__consumer_offsets-42, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,059] INFO [Log partition=__consumer_offsets-29, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,059] INFO [Log partition=__consumer_offsets-29, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,060] INFO [Log partition=__consumer_offsets-29, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,061] INFO [Log partition=__consumer_offsets-29, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,063] INFO [Log partition=__consumer_offsets-16, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,063] INFO [Log partition=__consumer_offsets-16, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,065] INFO [Log partition=__consumer_offsets-16, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,065] INFO [Log partition=__consumer_offsets-16, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,067] INFO [Log partition=topic-demo-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,067] INFO [Log partition=topic-demo-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,069] INFO [ProducerStateManager partition=topic-demo-0] Writing producer snapshot at offset 20 (kafka.log.ProducerStateManager)[2020-04-29 18:21:19,070] INFO [Log partition=topic-demo-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 20 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,071] INFO [ProducerStateManager partition=topic-demo-0] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/topic-demo-0/00000000000000000020.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:19,071] INFO [Log partition=topic-demo-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 20 in 5 ms (kafka.log.Log)[2020-04-29 18:21:19,073] INFO [Log partition=__consumer_offsets-11, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,073] INFO [Log partition=__consumer_offsets-11, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,075] INFO [Log partition=__consumer_offsets-11, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,075] INFO [Log partition=__consumer_offsets-11, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,077] INFO [Log partition=__consumer_offsets-18, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,077] INFO [Log partition=__consumer_offsets-18, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,080] INFO [Log partition=__consumer_offsets-18, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,080] INFO [Log partition=__consumer_offsets-18, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,083] INFO [Log partition=__consumer_offsets-27, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,083] INFO [Log partition=__consumer_offsets-27, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,084] INFO [ProducerStateManager partition=__consumer_offsets-27] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)[2020-04-29 18:21:19,085] INFO [Log partition=__consumer_offsets-27, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,086] INFO [ProducerStateManager partition=__consumer_offsets-27] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-27/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:19,086] INFO [Log partition=__consumer_offsets-27, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 5 ms (kafka.log.Log)[2020-04-29 18:21:19,088] INFO [Log partition=__consumer_offsets-20, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,088] INFO [Log partition=__consumer_offsets-20, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,090] INFO [Log partition=__consumer_offsets-20, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,090] INFO [Log partition=__consumer_offsets-20, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,092] INFO [Log partition=__consumer_offsets-43, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,092] INFO [Log partition=__consumer_offsets-43, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,094] INFO [Log partition=__consumer_offsets-43, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,094] INFO [Log partition=__consumer_offsets-43, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,096] INFO [Log partition=__consumer_offsets-44, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,096] INFO [Log partition=__consumer_offsets-44, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,098] INFO [Log partition=__consumer_offsets-44, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,098] INFO [Log partition=__consumer_offsets-44, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,100] INFO [Log partition=jerome-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,100] INFO [Log partition=jerome-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,102] INFO [Log partition=jerome-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,103] INFO [Log partition=jerome-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,104] INFO [Log partition=__consumer_offsets-21, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,104] INFO [Log partition=__consumer_offsets-21, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,106] INFO [Log partition=__consumer_offsets-21, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,107] INFO [Log partition=__consumer_offsets-21, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,108] INFO [Log partition=__consumer_offsets-19, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,108] INFO [Log partition=__consumer_offsets-19, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,110] INFO [Log partition=__consumer_offsets-19, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,111] INFO [Log partition=__consumer_offsets-19, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,112] INFO [Log partition=__consumer_offsets-26, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,112] INFO [Log partition=__consumer_offsets-26, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,114] INFO [Log partition=__consumer_offsets-26, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,115] INFO [Log partition=__consumer_offsets-26, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,116] INFO [Log partition=__consumer_offsets-10, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,116] INFO [Log partition=__consumer_offsets-10, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,118] INFO [Log partition=__consumer_offsets-10, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,119] INFO [Log partition=__consumer_offsets-10, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,120] INFO [Log partition=__consumer_offsets-28, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,120] INFO [Log partition=__consumer_offsets-28, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,122] INFO [Log partition=__consumer_offsets-28, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,122] INFO [Log partition=__consumer_offsets-28, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)[2020-04-29 18:21:19,124] INFO [Log partition=__consumer_offsets-17, dir=/usr/local/etc/kafka/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)[2020-04-29 18:21:19,124] INFO [Log partition=__consumer_offsets-17, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,126] INFO [Log partition=__consumer_offsets-17, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:19,127] INFO [Log partition=__consumer_offsets-17, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)[2020-04-29 18:21:19,128] INFO Logs loading complete in 407 ms. (kafka.log.LogManager)[2020-04-29 18:21:19,137] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)[2020-04-29 18:21:19,138] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)[2020-04-29 18:21:19,399] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)[2020-04-29 18:21:19,420] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)[2020-04-29 18:21:19,421] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)[2020-04-29 18:21:19,437] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,437] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,437] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,438] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,447] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)[2020-04-29 18:21:19,492] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)[2020-04-29 18:21:19,502] ERROR Error while creating ephemeral at /brokers/ids/0, node already exists and owner '72057616369582879' does not match current session '72057616369582880' (kafka.zk.KafkaZkClient$CheckedEphemeral)[2020-04-29 18:21:19,508] ERROR [KafkaServer id=0] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists at org.apache.zookeeper.KeeperException.create(KeeperException.java:126) at kafka.zk.KafkaZkClient$CheckedEphemeral.getAfterNodeExists(KafkaZkClient.scala:1815) at kafka.zk.KafkaZkClient$CheckedEphemeral.create(KafkaZkClient.scala:1753) at kafka.zk.KafkaZkClient.checkedEphemeralCreate(KafkaZkClient.scala:1720) at kafka.zk.KafkaZkClient.registerBroker(KafkaZkClient.scala:93) at kafka.server.KafkaServer.startup(KafkaServer.scala:270) at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:44) at kafka.Kafka$.main(Kafka.scala:84) at kafka.Kafka.main(Kafka.scala)[2020-04-29 18:21:19,509] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)[2020-04-29 18:21:19,510] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)[2020-04-29 18:21:19,514] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)[2020-04-29 18:21:19,516] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)[2020-04-29 18:21:19,517] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)[2020-04-29 18:21:19,517] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)[2020-04-29 18:21:19,517] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)[2020-04-29 18:21:19,517] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)[2020-04-29 18:21:19,518] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)[2020-04-29 18:21:19,519] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)[2020-04-29 18:21:19,519] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)[2020-04-29 18:21:19,519] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,637] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,637] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,638] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,911] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,911] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,912] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,912] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,912] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:19,913] INFO [ExpirationReaper-0-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:20,187] INFO [ExpirationReaper-0-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:20,187] INFO [ExpirationReaper-0-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:20,194] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)[2020-04-29 18:21:20,195] INFO Shutting down. (kafka.log.LogManager)[2020-04-29 18:21:20,262] INFO Shutdown complete. (kafka.log.LogManager)[2020-04-29 18:21:20,263] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:20,441] INFO Session: 0x100000533120320 closed (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:20,441] INFO EventThread shut down for session: 0x100000533120320 (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:20,442] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:20,442] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:20,714] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:20,714] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:20,714] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:21,714] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:21,714] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:21,714] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:22,786] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:22,786] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:22,788] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)[2020-04-29 18:21:22,814] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)[2020-04-29 18:21:22,818] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)[2020-04-29 18:21:22,819] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)[2020-04-29 18:21:22,825] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)[2020-04-29 18:21:28,872] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[2020-04-29 18:21:29,333] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)[2020-04-29 18:21:29,334] INFO starting (kafka.server.KafkaServer)[2020-04-29 18:21:29,335] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)[2020-04-29 18:21:29,350] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:29,355] INFO Client environment:zookeeper.version=3.5.6-c11b7e26bc554b8523dc929761dd28808913f091, built on 10/08/2019 20:18 GMT (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,355] INFO Client environment:host.name=localhost (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,355] INFO Client environment:java.version=1.8.0_222 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,355] INFO Client environment:java.vendor=AdoptOpenJDK (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,355] INFO Client environment:java.home=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:java.class.path=/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/activation-1.1.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/aopalliance-repackaged-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/argparse4j-0.7.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/audience-annotations-0.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/commons-cli-1.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/commons-lang3-3.8.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-api-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-basic-auth-extension-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-file-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-json-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-mirror-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-mirror-client-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-runtime-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/connect-transforms-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/guava-20.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-api-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-locator-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/hk2-utils-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-annotations-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-core-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-databind-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-dataformat-csv-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-datatype-jdk8-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-jaxrs-base-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-jaxrs-json-provider-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-jaxb-annotations-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-paranamer-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jackson-module-scala_2.12-2.10.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.activation-api-1.2.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.annotation-api-1.3.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.inject-2.5.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.ws.rs-api-2.1.5.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jakarta.xml.bind-api-2.3.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javassist-3.22.0-CR2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javax.servlet-api-3.1.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/javax.ws.rs-api-2.1.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jaxb-api-2.3.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-client-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-common-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-container-servlet-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-container-servlet-core-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-hk2-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-media-jaxb-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jersey-server-2.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-client-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-continuation-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-http-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-io-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-security-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-server-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-servlet-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-servlets-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jetty-util-9.4.20.v20190813.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/jopt-simple-5.0.4.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-clients-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-log4j-appender-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-examples-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-scala_2.12-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-streams-test-utils-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka-tools-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka_2.12-2.4.0-sources.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/kafka_2.12-2.4.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/log4j-1.2.17.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/lz4-java-1.6.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/maven-artifact-3.6.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/metrics-core-2.2.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-buffer-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-codec-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-common-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-handler-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-resolver-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-native-epoll-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/netty-transport-native-unix-common-4.1.42.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/osgi-resource-locator-1.0.1.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/paranamer-2.8.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/plexus-utils-3.2.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/reflections-0.9.11.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/rocksdbjni-5.18.3.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-collection-compat_2.12-2.1.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-java8-compat_2.12-0.9.0.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-library-2.12.10.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-logging_2.12-3.9.2.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/scala-reflect-2.12.10.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/slf4j-api-1.7.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/slf4j-log4j12-1.7.28.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/snappy-java-1.1.7.3.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/validation-api-2.0.1.Final.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zookeeper-3.5.6.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zookeeper-jute-3.5.6.jar:/usr/local/Cellar/kafka/2.4.0/libexec/bin/../libs/zstd-jni-1.4.3-1.jar (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:java.library.path=/Users/yangweijie/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:. (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:java.io.tmpdir=/var/folders/cp/p4lwr_6n66s065dhcw80dzd80000gn/T/ (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:java.compiler=&lt;NA&gt; (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:os.name=Mac OS X (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:os.arch=x86_64 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:os.version=10.15.4 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:user.name=yangweijie (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:user.home=/Users/yangweijie (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:user.dir=/usr/local (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:os.memory.free=978MB (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,356] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,358] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7722c3c3 (org.apache.zookeeper.ZooKeeper)[2020-04-29 18:21:29,362] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)[2020-04-29 18:21:29,367] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)[2020-04-29 18:21:29,372] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:29,374] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:29,377] INFO Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:29,387] INFO Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:56065, server: localhost/0:0:0:0:0:0:0:1:2181 (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:29,391] INFO Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x100000533120321, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)[2020-04-29 18:21:29,394] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)[2020-04-29 18:21:29,570] INFO Cluster ID = I5sBHS6MSMG4MmUddyviFQ (kafka.server.KafkaServer)[2020-04-29 18:21:29,625] INFO KafkaConfig values: advertised.host.name = null advertised.listeners = null advertised.port = null alter.config.policy.class.name = null alter.log.dirs.replication.quota.window.num = 11 alter.log.dirs.replication.quota.window.size.seconds = 1 authorizer.class.name = auto.create.topics.enable = true auto.leader.rebalance.enable = true background.threads = 10 broker.id = 0 broker.id.generation.enable = true broker.rack = null client.quota.callback.class = null compression.type = producer connection.failed.authentication.delay.ms = 100 connections.max.idle.ms = 600000 connections.max.reauth.ms = 0 control.plane.listener.name = null controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controller.socket.timeout.ms = 30000 create.topic.policy.class.name = null default.replication.factor = 1 delegation.token.expiry.check.interval.ms = 3600000 delegation.token.expiry.time.ms = 86400000 delegation.token.master.key = null delegation.token.max.lifetime.ms = 604800000 delete.records.purgatory.purge.interval.requests = 1 delete.topic.enable = true fetch.purgatory.purge.interval.requests = 1000 group.initial.rebalance.delay.ms = 0 group.max.session.timeout.ms = 1800000 group.max.size = 2147483647 group.min.session.timeout.ms = 6000 host.name = inter.broker.listener.name = null inter.broker.protocol.version = 2.4-IV1 kafka.metrics.polling.interval.secs = 10 kafka.metrics.reporters = [] leader.imbalance.check.interval.seconds = 300 leader.imbalance.per.broker.percentage = 10 listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listeners = PLAINTEXT://:9092 log.cleaner.backoff.ms = 15000 log.cleaner.dedupe.buffer.size = 134217728 log.cleaner.delete.retention.ms = 86400000 log.cleaner.enable = true log.cleaner.io.buffer.load.factor = 0.9 log.cleaner.io.buffer.size = 524288 log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308 log.cleaner.max.compaction.lag.ms = 9223372036854775807 log.cleaner.min.cleanable.ratio = 0.5 log.cleaner.min.compaction.lag.ms = 0 log.cleaner.threads = 1 log.cleanup.policy = [delete] log.dir = /tmp/kafka-logs log.dirs = /usr/local/etc/kafka/tmp/kafka-logs log.flush.interval.messages = 9223372036854775807 log.flush.interval.ms = null log.flush.offset.checkpoint.interval.ms = 60000 log.flush.scheduler.interval.ms = 9223372036854775807 log.flush.start.offset.checkpoint.interval.ms = 60000 log.index.interval.bytes = 4096 log.index.size.max.bytes = 10485760 log.message.downconversion.enable = true log.message.format.version = 2.4-IV1 log.message.timestamp.difference.max.ms = 9223372036854775807 log.message.timestamp.type = CreateTime log.preallocate = false log.retention.bytes = -1 log.retention.check.interval.ms = 300000 log.retention.hours = 168 log.retention.minutes = null log.retention.ms = null log.roll.hours = 168 log.roll.jitter.hours = 0 log.roll.jitter.ms = null log.roll.ms = null log.segment.bytes = 1073741824 log.segment.delete.delay.ms = 60000 max.connections = 2147483647 max.connections.per.ip = 2147483647 max.connections.per.ip.overrides = max.incremental.fetch.session.cache.slots = 1000 message.max.bytes = 1000012 metric.reporters = [] metrics.num.samples = 2 metrics.recording.level = INFO metrics.sample.window.ms = 30000 min.insync.replicas = 1 num.io.threads = 8 num.network.threads = 3 num.partitions = 1 num.recovery.threads.per.data.dir = 1 num.replica.alter.log.dirs.threads = null num.replica.fetchers = 1 offset.metadata.max.bytes = 4096 offsets.commit.required.acks = -1 offsets.commit.timeout.ms = 5000 offsets.load.buffer.size = 5242880 offsets.retention.check.interval.ms = 600000 offsets.retention.minutes = 10080 offsets.topic.compression.codec = 0 offsets.topic.num.partitions = 50 offsets.topic.replication.factor = 1 offsets.topic.segment.bytes = 104857600 password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding password.encoder.iterations = 4096 password.encoder.key.length = 128 password.encoder.keyfactory.algorithm = null password.encoder.old.secret = null password.encoder.secret = null port = 9092 principal.builder.class = null producer.purgatory.purge.interval.requests = 1000 queued.max.request.bytes = -1 queued.max.requests = 500 quota.consumer.default = 9223372036854775807 quota.producer.default = 9223372036854775807 quota.window.num = 11 quota.window.size.seconds = 1 replica.fetch.backoff.ms = 1000 replica.fetch.max.bytes = 1048576 replica.fetch.min.bytes = 1 replica.fetch.response.max.bytes = 10485760 replica.fetch.wait.max.ms = 500 replica.high.watermark.checkpoint.interval.ms = 5000 replica.lag.time.max.ms = 10000 replica.selector.class = null replica.socket.receive.buffer.bytes = 65536 replica.socket.timeout.ms = 30000 replication.quota.window.num = 11 replication.quota.window.size.seconds = 1 request.timeout.ms = 30000 reserved.broker.max.id = 1000 sasl.client.callback.handler.class = null sasl.enabled.mechanisms = [GSSAPI] sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.principal.to.local.rules = [DEFAULT] sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism.inter.broker.protocol = GSSAPI sasl.server.callback.handler.class = null security.inter.broker.protocol = PLAINTEXT security.providers = null socket.receive.buffer.bytes = 102400 socket.request.max.bytes = 104857600 socket.send.buffer.bytes = 102400 ssl.cipher.suites = [] ssl.client.auth = none ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = null ssl.keystore.password = null ssl.keystore.type = JKS ssl.principal.mapping.rules = DEFAULT ssl.protocol = TLS ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = null ssl.truststore.password = null ssl.truststore.type = JKS transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000 transaction.max.timeout.ms = 900000 transaction.remove.expired.transaction.cleanup.interval.ms = 3600000 transaction.state.log.load.buffer.size = 5242880 transaction.state.log.min.isr = 1 transaction.state.log.num.partitions = 50 transaction.state.log.replication.factor = 1 transaction.state.log.segment.bytes = 104857600 transactional.id.expiration.ms = 604800000 unclean.leader.election.enable = false zookeeper.connect = localhost:2181 zookeeper.connection.timeout.ms = 6000 zookeeper.max.in.flight.requests = 10 zookeeper.session.timeout.ms = 6000 zookeeper.set.acl = false zookeeper.sync.time.ms = 2000 (kafka.server.KafkaConfig)[2020-04-29 18:21:29,633] INFO KafkaConfig values: advertised.host.name = null advertised.listeners = null advertised.port = null alter.config.policy.class.name = null alter.log.dirs.replication.quota.window.num = 11 alter.log.dirs.replication.quota.window.size.seconds = 1 authorizer.class.name = auto.create.topics.enable = true auto.leader.rebalance.enable = true background.threads = 10 broker.id = 0 broker.id.generation.enable = true broker.rack = null client.quota.callback.class = null compression.type = producer connection.failed.authentication.delay.ms = 100 connections.max.idle.ms = 600000 connections.max.reauth.ms = 0 control.plane.listener.name = null controlled.shutdown.enable = true controlled.shutdown.max.retries = 3 controlled.shutdown.retry.backoff.ms = 5000 controller.socket.timeout.ms = 30000 create.topic.policy.class.name = null default.replication.factor = 1 delegation.token.expiry.check.interval.ms = 3600000 delegation.token.expiry.time.ms = 86400000 delegation.token.master.key = null delegation.token.max.lifetime.ms = 604800000 delete.records.purgatory.purge.interval.requests = 1 delete.topic.enable = true fetch.purgatory.purge.interval.requests = 1000 group.initial.rebalance.delay.ms = 0 group.max.session.timeout.ms = 1800000 group.max.size = 2147483647 group.min.session.timeout.ms = 6000 host.name = inter.broker.listener.name = null inter.broker.protocol.version = 2.4-IV1 kafka.metrics.polling.interval.secs = 10 kafka.metrics.reporters = [] leader.imbalance.check.interval.seconds = 300 leader.imbalance.per.broker.percentage = 10 listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL listeners = PLAINTEXT://:9092 log.cleaner.backoff.ms = 15000 log.cleaner.dedupe.buffer.size = 134217728 log.cleaner.delete.retention.ms = 86400000 log.cleaner.enable = true log.cleaner.io.buffer.load.factor = 0.9 log.cleaner.io.buffer.size = 524288 log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308 log.cleaner.max.compaction.lag.ms = 9223372036854775807 log.cleaner.min.cleanable.ratio = 0.5 log.cleaner.min.compaction.lag.ms = 0 log.cleaner.threads = 1 log.cleanup.policy = [delete] log.dir = /tmp/kafka-logs log.dirs = /usr/local/etc/kafka/tmp/kafka-logs log.flush.interval.messages = 9223372036854775807 log.flush.interval.ms = null log.flush.offset.checkpoint.interval.ms = 60000 log.flush.scheduler.interval.ms = 9223372036854775807 log.flush.start.offset.checkpoint.interval.ms = 60000 log.index.interval.bytes = 4096 log.index.size.max.bytes = 10485760 log.message.downconversion.enable = true log.message.format.version = 2.4-IV1 log.message.timestamp.difference.max.ms = 9223372036854775807 log.message.timestamp.type = CreateTime log.preallocate = false log.retention.bytes = -1 log.retention.check.interval.ms = 300000 log.retention.hours = 168 log.retention.minutes = null log.retention.ms = null log.roll.hours = 168 log.roll.jitter.hours = 0 log.roll.jitter.ms = null log.roll.ms = null log.segment.bytes = 1073741824 log.segment.delete.delay.ms = 60000 max.connections = 2147483647 max.connections.per.ip = 2147483647 max.connections.per.ip.overrides = max.incremental.fetch.session.cache.slots = 1000 message.max.bytes = 1000012 metric.reporters = [] metrics.num.samples = 2 metrics.recording.level = INFO metrics.sample.window.ms = 30000 min.insync.replicas = 1 num.io.threads = 8 num.network.threads = 3 num.partitions = 1 num.recovery.threads.per.data.dir = 1 num.replica.alter.log.dirs.threads = null num.replica.fetchers = 1 offset.metadata.max.bytes = 4096 offsets.commit.required.acks = -1 offsets.commit.timeout.ms = 5000 offsets.load.buffer.size = 5242880 offsets.retention.check.interval.ms = 600000 offsets.retention.minutes = 10080 offsets.topic.compression.codec = 0 offsets.topic.num.partitions = 50 offsets.topic.replication.factor = 1 offsets.topic.segment.bytes = 104857600 password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding password.encoder.iterations = 4096 password.encoder.key.length = 128 password.encoder.keyfactory.algorithm = null password.encoder.old.secret = null password.encoder.secret = null port = 9092 principal.builder.class = null producer.purgatory.purge.interval.requests = 1000 queued.max.request.bytes = -1 queued.max.requests = 500 quota.consumer.default = 9223372036854775807 quota.producer.default = 9223372036854775807 quota.window.num = 11 quota.window.size.seconds = 1 replica.fetch.backoff.ms = 1000 replica.fetch.max.bytes = 1048576 replica.fetch.min.bytes = 1 replica.fetch.response.max.bytes = 10485760 replica.fetch.wait.max.ms = 500 replica.high.watermark.checkpoint.interval.ms = 5000 replica.lag.time.max.ms = 10000 replica.selector.class = null replica.socket.receive.buffer.bytes = 65536 replica.socket.timeout.ms = 30000 replication.quota.window.num = 11 replication.quota.window.size.seconds = 1 request.timeout.ms = 30000 reserved.broker.max.id = 1000 sasl.client.callback.handler.class = null sasl.enabled.mechanisms = [GSSAPI] sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.principal.to.local.rules = [DEFAULT] sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism.inter.broker.protocol = GSSAPI sasl.server.callback.handler.class = null security.inter.broker.protocol = PLAINTEXT security.providers = null socket.receive.buffer.bytes = 102400 socket.request.max.bytes = 104857600 socket.send.buffer.bytes = 102400 ssl.cipher.suites = [] ssl.client.auth = none ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = null ssl.keystore.password = null ssl.keystore.type = JKS ssl.principal.mapping.rules = DEFAULT ssl.protocol = TLS ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = null ssl.truststore.password = null ssl.truststore.type = JKS transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000 transaction.max.timeout.ms = 900000 transaction.remove.expired.transaction.cleanup.interval.ms = 3600000 transaction.state.log.load.buffer.size = 5242880 transaction.state.log.min.isr = 1 transaction.state.log.num.partitions = 50 transaction.state.log.replication.factor = 1 transaction.state.log.segment.bytes = 104857600 transactional.id.expiration.ms = 604800000 unclean.leader.election.enable = false zookeeper.connect = localhost:2181 zookeeper.connection.timeout.ms = 6000 zookeeper.max.in.flight.requests = 10 zookeeper.session.timeout.ms = 6000 zookeeper.set.acl = false zookeeper.sync.time.ms = 2000 (kafka.server.KafkaConfig)[2020-04-29 18:21:29,654] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:29,654] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:29,655] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)[2020-04-29 18:21:29,686] INFO Loading logs. (kafka.log.LogManager)[2020-04-29 18:21:29,744] INFO [Log partition=__consumer_offsets-9, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,752] INFO [Log partition=__consumer_offsets-9, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 43 ms (kafka.log.Log)[2020-04-29 18:21:29,760] INFO [Log partition=__consumer_offsets-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,761] INFO [Log partition=__consumer_offsets-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,765] INFO [Log partition=__consumer_offsets-7, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,765] INFO [Log partition=__consumer_offsets-7, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,769] INFO [Log partition=__consumer_offsets-31, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,769] INFO [Log partition=__consumer_offsets-31, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,773] INFO [Log partition=__consumer_offsets-36, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,773] INFO [Log partition=__consumer_offsets-36, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,777] INFO [Log partition=__consumer_offsets-38, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,777] INFO [Log partition=__consumer_offsets-38, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,781] INFO [Log partition=__consumer_offsets-6, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,781] INFO [Log partition=__consumer_offsets-6, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,785] INFO [Log partition=__consumer_offsets-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,785] INFO [Log partition=__consumer_offsets-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,788] INFO [Log partition=__consumer_offsets-8, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,788] INFO [Log partition=__consumer_offsets-8, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,792] INFO [Log partition=__consumer_offsets-39, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,792] INFO [Log partition=__consumer_offsets-39, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,796] INFO [Log partition=__consumer_offsets-37, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,796] INFO [Log partition=__consumer_offsets-37, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,799] INFO [Log partition=__consumer_offsets-30, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,799] INFO [Log partition=__consumer_offsets-30, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,814] INFO [Log partition=test-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 13 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,818] INFO [ProducerStateManager partition=test-0] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/test-0/00000000000000000013.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,828] INFO [Log partition=test-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 13 in 27 ms (kafka.log.Log)[2020-04-29 18:21:29,833] INFO [Log partition=__consumer_offsets-12, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 430 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,833] INFO [ProducerStateManager partition=__consumer_offsets-12] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-12/00000000000000000430.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,834] INFO [Log partition=__consumer_offsets-12, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 430 in 4 ms (kafka.log.Log)[2020-04-29 18:21:29,837] INFO [Log partition=__consumer_offsets-15, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,837] INFO [Log partition=__consumer_offsets-15, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,841] INFO [Log partition=__consumer_offsets-23, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,842] INFO [ProducerStateManager partition=__consumer_offsets-23] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-23/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,842] INFO [Log partition=__consumer_offsets-23, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 3 ms (kafka.log.Log)[2020-04-29 18:21:29,845] INFO [Log partition=__consumer_offsets-24, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,846] INFO [Log partition=__consumer_offsets-24, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,849] INFO [Log partition=test2-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,849] INFO [Log partition=test2-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,852] INFO [Log partition=__consumer_offsets-48, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,852] INFO [Log partition=__consumer_offsets-48, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,855] INFO [Log partition=__consumer_offsets-41, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,855] INFO [Log partition=__consumer_offsets-41, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,858] INFO [Log partition=__consumer_offsets-46, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,859] INFO [Log partition=__consumer_offsets-46, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,862] INFO [Log partition=__consumer_offsets-25, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,862] INFO [Log partition=__consumer_offsets-25, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,865] INFO [Log partition=__consumer_offsets-22, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,865] INFO [Log partition=__consumer_offsets-22, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,868] INFO [Log partition=__consumer_offsets-14, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,868] INFO [Log partition=__consumer_offsets-14, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,872] INFO [Log partition=__consumer_offsets-13, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,873] INFO [ProducerStateManager partition=__consumer_offsets-13] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-13/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,873] INFO [Log partition=__consumer_offsets-13, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 4 ms (kafka.log.Log)[2020-04-29 18:21:29,876] INFO [Log partition=__consumer_offsets-47, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,876] INFO [Log partition=__consumer_offsets-47, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,879] INFO [Log partition=__consumer_offsets-40, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,879] INFO [Log partition=__consumer_offsets-40, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,882] INFO [Log partition=__consumer_offsets-49, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,882] INFO [Log partition=__consumer_offsets-49, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,885] INFO [Log partition=__consumer_offsets-35, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,885] INFO [Log partition=__consumer_offsets-35, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,889] INFO [Log partition=__consumer_offsets-32, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,890] INFO [ProducerStateManager partition=__consumer_offsets-32] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-32/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,890] INFO [Log partition=__consumer_offsets-32, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 4 ms (kafka.log.Log)[2020-04-29 18:21:29,892] INFO [Log partition=__consumer_offsets-4, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,892] INFO [Log partition=__consumer_offsets-4, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,895] INFO [Log partition=__consumer_offsets-3, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,895] INFO [Log partition=__consumer_offsets-3, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,898] INFO [Log partition=__consumer_offsets-33, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,898] INFO [Log partition=__consumer_offsets-33, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,901] INFO [Log partition=__consumer_offsets-34, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,901] INFO [Log partition=__consumer_offsets-34, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,903] INFO [Log partition=__consumer_offsets-2, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,904] INFO [Log partition=__consumer_offsets-2, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,906] INFO [Log partition=__consumer_offsets-5, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,906] INFO [Log partition=__consumer_offsets-5, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,909] INFO [Log partition=jerome-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,909] INFO [Log partition=jerome-1, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,911] INFO [Log partition=__consumer_offsets-45, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,911] INFO [Log partition=__consumer_offsets-45, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,914] INFO [Log partition=__consumer_offsets-42, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,914] INFO [Log partition=__consumer_offsets-42, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,916] INFO [Log partition=__consumer_offsets-29, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,917] INFO [Log partition=__consumer_offsets-29, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,919] INFO [Log partition=__consumer_offsets-16, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,919] INFO [Log partition=__consumer_offsets-16, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,923] INFO [Log partition=topic-demo-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 20 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,923] INFO [ProducerStateManager partition=topic-demo-0] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/topic-demo-0/00000000000000000020.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,923] INFO [Log partition=topic-demo-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 20 in 3 ms (kafka.log.Log)[2020-04-29 18:21:29,926] INFO [Log partition=__consumer_offsets-11, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,926] INFO [Log partition=__consumer_offsets-11, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,928] INFO [Log partition=__consumer_offsets-18, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,928] INFO [Log partition=__consumer_offsets-18, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,931] INFO [Log partition=__consumer_offsets-27, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 3 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,932] INFO [ProducerStateManager partition=__consumer_offsets-27] Loading producer state from snapshot file '/usr/local/etc/kafka/tmp/kafka-logs/__consumer_offsets-27/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)[2020-04-29 18:21:29,932] INFO [Log partition=__consumer_offsets-27, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 3 ms (kafka.log.Log)[2020-04-29 18:21:29,934] INFO [Log partition=__consumer_offsets-20, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,934] INFO [Log partition=__consumer_offsets-20, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,937] INFO [Log partition=__consumer_offsets-43, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,937] INFO [Log partition=__consumer_offsets-43, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,939] INFO [Log partition=__consumer_offsets-44, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,939] INFO [Log partition=__consumer_offsets-44, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,941] INFO [Log partition=jerome-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,942] INFO [Log partition=jerome-0, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,944] INFO [Log partition=__consumer_offsets-21, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,944] INFO [Log partition=__consumer_offsets-21, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)[2020-04-29 18:21:29,946] INFO [Log partition=__consumer_offsets-19, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,946] INFO [Log partition=__consumer_offsets-19, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,948] INFO [Log partition=__consumer_offsets-26, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,948] INFO [Log partition=__consumer_offsets-26, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,950] INFO [Log partition=__consumer_offsets-10, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,950] INFO [Log partition=__consumer_offsets-10, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,952] INFO [Log partition=__consumer_offsets-28, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,952] INFO [Log partition=__consumer_offsets-28, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,954] INFO [Log partition=__consumer_offsets-17, dir=/usr/local/etc/kafka/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)[2020-04-29 18:21:29,954] INFO [Log partition=__consumer_offsets-17, dir=/usr/local/etc/kafka/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)[2020-04-29 18:21:29,956] INFO Logs loading complete in 270 ms. (kafka.log.LogManager)[2020-04-29 18:21:29,964] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)[2020-04-29 18:21:29,965] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)[2020-04-29 18:21:30,221] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)[2020-04-29 18:21:30,243] INFO [SocketServer brokerId=0] Created data-plane acceptor and processors for endpoint : EndPoint(null,9092,ListenerName(PLAINTEXT),PLAINTEXT) (kafka.network.SocketServer)[2020-04-29 18:21:30,244] INFO [SocketServer brokerId=0] Started 1 acceptor threads for data-plane (kafka.network.SocketServer)[2020-04-29 18:21:30,259] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,260] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,260] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,260] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,270] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)[2020-04-29 18:21:30,310] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)[2020-04-29 18:21:30,320] INFO Stat of the created znode at /brokers/ids/0 is: 12126,12126,1588155690317,1588155690317,1,0,0,72057616369582881,188,0,12126 (kafka.zk.KafkaZkClient)[2020-04-29 18:21:30,321] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(localhost,9092,ListenerName(PLAINTEXT),PLAINTEXT)), czxid (broker epoch): 12126 (kafka.zk.KafkaZkClient)[2020-04-29 18:21:30,366] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,368] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,369] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,402] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:21:30,402] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:21:30,407] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,416] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:6000,blockEndProducerId:6999) by writing to Zk with path version 7 (kafka.coordinator.transaction.ProducerIdManager)[2020-04-29 18:21:30,434] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)[2020-04-29 18:21:30,441] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)[2020-04-29 18:21:30,442] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)[2020-04-29 18:21:30,462] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-04-29 18:21:30,489] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)[2020-04-29 18:21:30,508] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)[2020-04-29 18:21:30,512] INFO Kafka version: 2.4.0 (org.apache.kafka.common.utils.AppInfoParser)[2020-04-29 18:21:30,512] INFO Kafka commitId: 77a89fcf8d7fa018 (org.apache.kafka.common.utils.AppInfoParser)[2020-04-29 18:21:30,512] INFO Kafka startTimeMs: 1588155690509 (org.apache.kafka.common.utils.AppInfoParser)[2020-04-29 18:21:30,513] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)[2020-04-29 18:21:30,585] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, test-0, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-38, __consumer_offsets-17, jerome-1, __consumer_offsets-48, __consumer_offsets-19, jerome-0, __consumer_offsets-11, __consumer_offsets-13, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, topic-demo-0, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)[2020-04-29 18:21:30,607] INFO [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,608] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,617] INFO [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,617] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,619] INFO [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,619] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,621] INFO [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,621] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,623] INFO [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,623] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,625] INFO [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,625] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,626] INFO [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,627] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,628] INFO [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,628] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,630] INFO [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,630] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,632] INFO [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 3 (kafka.cluster.Partition)[2020-04-29 18:21:30,632] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,633] INFO [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,633] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,634] INFO [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,634] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,636] INFO [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,636] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,638] INFO [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,638] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,640] INFO [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,640] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,641] INFO [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,642] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,643] INFO [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,643] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,645] INFO [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,645] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,647] INFO [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,647] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,649] INFO [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,649] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,650] INFO [Partition test-0 broker=0] Log loaded for partition test-0 with initial high watermark 13 (kafka.cluster.Partition)[2020-04-29 18:21:30,650] INFO [Partition test-0 broker=0] test-0 starts at Leader Epoch 0 from offset 13. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,651] INFO [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,651] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,653] INFO [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 3 (kafka.cluster.Partition)[2020-04-29 18:21:30,653] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,654] INFO [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,654] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,656] INFO [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,656] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,658] INFO [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,658] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,659] INFO [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,659] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,661] INFO [Partition jerome-0 broker=0] Log loaded for partition jerome-0 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,661] INFO [Partition jerome-0 broker=0] jerome-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,662] INFO [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,662] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,664] INFO [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,664] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,666] INFO [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,666] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,667] INFO [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,667] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,669] INFO [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,669] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,670] INFO [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,670] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,672] INFO [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,672] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,674] INFO [Partition topic-demo-0 broker=0] Log loaded for partition topic-demo-0 with initial high watermark 20 (kafka.cluster.Partition)[2020-04-29 18:21:30,674] INFO [Partition topic-demo-0 broker=0] topic-demo-0 starts at Leader Epoch 0 from offset 20. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,675] INFO [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 430 (kafka.cluster.Partition)[2020-04-29 18:21:30,675] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 430. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,676] INFO [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,676] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,677] INFO [Partition jerome-1 broker=0] Log loaded for partition jerome-1 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,677] INFO [Partition jerome-1 broker=0] jerome-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,679] INFO [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,679] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,680] INFO [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,680] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,682] INFO [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,682] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,683] INFO [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,683] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,684] INFO [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,685] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,686] INFO [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,686] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,687] INFO [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,688] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,689] INFO [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,689] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,690] INFO [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,690] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,692] INFO [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,692] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,693] INFO [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,693] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,694] INFO [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,694] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,696] INFO [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 3 (kafka.cluster.Partition)[2020-04-29 18:21:30,696] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,697] INFO [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)[2020-04-29 18:21:30,697] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,698] INFO [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 3 (kafka.cluster.Partition)[2020-04-29 18:21:30,698] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,705] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,706] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,707] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,709] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,710] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,710] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,711] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,711] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,711] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,711] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,711] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,712] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,712] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,712] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,712] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,713] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,713] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,713] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,714] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,714] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,734] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 20 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,735] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,735] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,735] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,735] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,736] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,736] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,736] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,736] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,736] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,739] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,740] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,740] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,742] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,743] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,743] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,743] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,743] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,743] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,743] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,763] INFO [GroupCoordinator 0]: Loading group metadata for group.demo with generation 18 (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:21:30,763] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 20 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,763] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,763] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,763] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,764] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,766] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,766] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,767] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,767] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,767] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,767] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,767] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:21:30,767] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)[2020-04-29 18:25:57,932] INFO [GroupCoordinator 0]: Preparing to rebalance group group.demo in state PreparingRebalance with old generation 18 (__consumer_offsets-12) (reason: Adding new member consumer-group.demo-1-6e7f0048-7df1-4574-8f74-31c7e1680f88 with group instanceid None) (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:25:57,937] INFO [GroupCoordinator 0]: Stabilized group group.demo generation 19 (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:25:57,945] INFO [GroupCoordinator 0]: Assignment received from leader for group group.demo for generation 19 (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:26:23,049] INFO [GroupCoordinator 0]: Member consumer-group.demo-1-6e7f0048-7df1-4574-8f74-31c7e1680f88 in group group.demo has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:26:23,050] INFO [GroupCoordinator 0]: Preparing to rebalance group group.demo in state PreparingRebalance with old generation 19 (__consumer_offsets-12) (reason: removing member consumer-group.demo-1-6e7f0048-7df1-4574-8f74-31c7e1680f88 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)[2020-04-29 18:26:23,051] INFO [GroupCoordinator 0]: Group group.demo with generation 20 is now empty (__consumer_offsets-12) (kafka.coordinator.group.GroupCoordinator) 最终，我在 https://www.orchome.com/32 找到一个看上去很不错的答案，但是很遗憾，依旧失败了，我强制 kill 也没用，依然会自动重新创建一个 kafka 服务… 至于文章中提到的 controlled.shutdown.enable，开启(true)后，执行shutdown时，broker主动将自己有leader身份的partition转移给ISR里的其他broker，但是如果是默认值false，对关闭依然不会有影响，按照默认的规则切换只是会慢一点点而已… 所以，很遗憾，没有解决… Kafka 在 java 中使用实现maven 依赖123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.28&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.2&lt;/version&gt; &lt;/dependency&gt; 生产者客户端123456789101112131415161718192021222324252627282930package Kafka.Demo1;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;public class ProducerFastStart &#123; public static final String brokerList = "localhost:9092"; public static final String topic = "topic-demo"; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer"); properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer"); properties.put("bootstrap.servers",brokerList); KafkaProducer&lt;String,String&gt; producer = new KafkaProducer&lt;String, String&gt;(properties); // 构造要发送的消息 ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic,"hello,I am jerome_memory"); // 发送消息 try&#123; producer.send(record); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; producer.close(); &#125;&#125; 消费者客户端123456789101112131415161718192021222324252627282930313233343536package Kafka.Demo1;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.protocol.types.Field;import java.time.Duration;import java.util.Collections;import java.util.Properties;public class ConsumerFastStart &#123; public static final String brokerList = "localhost:9092"; public static final String topic = "topic-demo"; public static final String groupId = "group.demo"; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer"); properties.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer"); properties.put("bootstrap.servers",brokerList); properties.put("group.id",groupId); KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(properties); // 订阅主题 consumer.subscribe(Collections.singletonList(topic)); // 循环读取消息 while (true)&#123; ConsumerRecords&lt;String,String&gt; records = consumer.poll(Duration.ZERO); for(ConsumerRecord&lt;String,String&gt; records1 : records)&#123; System.out.println(records1.value()); &#125; &#125; &#125;&#125; 遇到的问题在启动生产者和消费者后，报错如下：「但是程序可以正常运行…」 123SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details. 解决方案： 采用 其提示的 url 中的解决方案进行解决，失败。。。还是会有上面的错误… 12345678910111213141516171819&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt; org.apache.cassandra&lt;/groupId&gt; &lt;artifactId&gt;cassandra-all&lt;/artifactId&gt; &lt;version&gt;0.8.1&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 参考：https://www.cnblogs.com/felixzh/p/12487644.html 直接上slf4j官网找到相应的maven包添加到pom.xml 网址：https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12/1.8.0-alpha2 好吧… 依旧不好使 最终得以解决，以下是最终解决方案： 参考：https://www.shuzhiduo.com/A/pRdBO486zn/ 在添加了以下依赖,还是会报错…「这里的版本我是通过查询 cd /usr/local/Cellar/kafka/2.4.0/libexec/lib/ 得到的 slf4j 和 log4j 的版本」 123456789101112&gt; &lt;dependency&gt;&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;&gt; &lt;version&gt;1.7.28&lt;/version&gt;&gt; &lt;scope&gt;test&lt;/scope&gt;&gt; &lt;/dependency&gt;&gt; &lt;dependency&gt;&gt; &lt;groupId&gt;log4j&lt;/groupId&gt;&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt;&gt; &lt;version&gt;1.2.17&lt;/version&gt;&gt; &lt;/dependency&gt;&gt; 之后，看到了上面的参考文章，再次添加 123456&gt; &lt;dependency&gt;&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;&gt; &lt;version&gt;1.7.2&lt;/version&gt;&gt; &lt;/dependency&gt;&gt; 得以解决该问题…]]></content>
      <categories>
        <category>中间件系列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从双亲委派模型到 jdbc]]></title>
    <url>%2F2020%2F03%2F19%2F%E4%BB%8E%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%A8%A1%E5%9E%8B%E5%88%B0%20jdbc.html</url>
    <content type="text"><![CDATA[引子双亲委派模型，jdbc，貌似没啥关系，之间的联系还得从周志华老师的《深入理解java虚拟机》这本书说起。 上图可以看到，jdbc 这种涉及 SPI 的类加载方式破坏了双亲委派模型，接下来，我们来具体分析一波。 回顾 — 双亲委派模型类加载器谈到双亲委派模型，必然是要谈到类加载器啦，而类加载器要做的事情，就是完成类加载中的第一个动作，顺带回顾一下类加载的三个动作： 通过类的全限定名「一般是 .class 文件的形式，也可以是其他的，没有限定」产生一个二进制数据流； 将该二进制数据流解析为方法区的运行时数据结构； 在 Java 堆中创建一个表示该类型的java.lang.Class类的实例，作为方法区这个类的各种数据的访问入口。 讲完类加载器的工作，再来回顾一下类加载器的分类，类加载器的分类可以从两个角度看， 如果是 jvm 角度看，那就只需要分为 启动类加载器 「native 语言写的」和 其他类加载器「java 写的」； 如果从程序员角度看，那就分为四类，一类是 启动类加载器「Bootstrap ClassLoader」，一类是 扩展类加载器「Extension ClassLoader」，一类是 应用程序类加载器「Application ClassLoader，这个就是我们开发程序的默认使用过的类加载器」，最后一类是 自定义类加载器「自己实现的类加载器，其实只要去继承CLassLoader 下的 findCLass() 方法就行了，这样是符合双亲委派模型的」。 双亲委派模型接下来我们从程序员角度的分类来看双亲委派模型。 概念 「什么是双亲委派模型」除了顶层的启动类加载器以外，所有的类加载器都应该有父类加载器，这里的父子关系不是通过继承来的，而是通过组合关系。「要特别注意哦，是组合关系，是 has-a 关系，这个地方跟后面为何 jdbc 要违反双亲委派模型有不可分割的关系哈」 模型怎么用「双亲委派模型的工作流程」如果一个类加载器收到了类加载的请求，先把这个请求委派给父类加载器去完成（所以所有的加载请求最终都应该传送到顶层的启动类加载器中），只有当父加载器反馈自己无法完成加载请求(它的搜索范围没有找到所需要的类)时，子加载器才会尝试自己去加载。 具体我们在源代码中可以看到，这样看的更清楚一点，在 ClassLoader 类中，完成这个工作流程： 1234567891011121314151617181920212223242526272829303132333435363738394041protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded // 首先检查是否已经加载过了，加载过了就没必要加载了。 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; // 可以看到这里是个递归调用，就是意味着将该请求不断地向上委派 c = parent.loadClass(name, false); &#125; else &#123; // 没有老父亲了，判断是不是 BootStrap ClassLoader c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 为何要引入这个双亲委派模型，直接用一个类加载器不好吗？当然不好！我觉得双亲委派模型是很有必要的：能够最大限度的保证内库的安全性，比如 String 类，如果我自己去实现，双亲委派模型是不会加载我这个类的，而是会使用启动了加载器去加载内库中的 String 类。 换句话说，双亲委派带来的这个好处，就是 java类随着它的类加载器一起具备了一种带有优先级的层次关系，内库的类优先级高，所以就更加安全了。 那我弄多个类加载器，我不搞这种工作流程可不可以？不这样的话，很容易导致一个类被多个不同的类加载器加载，这样会产生很多本应该一样的二进制流，但是由于类加载器不同，在方法区就不同了。 既然这么好，为何要违反这个模型，违反不会带来问题吗？好的确是好啊，但是这个模型他有致命的弱点啊，就是 has-a，也就是他们只是组合的关系，这样会导致一个问题，就是我顶层加载器加载类时，遇到问题了，比如说 jdbc，就是顶层加载器加载了 driver 这个接口，但是没有实现类，这个时候顶层加载器已经在加载这个类了，但是卡住了，此时双亲委派模型也帮不上忙了啊，因为这个模型并不允许你顶层加载器去获得下一级的加载器去帮忙加载类，所以此时就必须违背这个模型去做一些事情了。 回顾 — jdbc在正式讲 jdbc 破坏双亲委派模型之前，我们先来好好回顾一下 jdbc 的概念，这是 mybatis 的基础。 定义JDBC（Java DataBase Connectivity,java数据库连接）是一种用于执行SQL语句的Java API，可以为多种关系数据库提供统一访问，它由一组用 Java 语言编写的类和接口组成。JDBC提供了一种基准，据此可以构建更高级的工具和接口，使数据库开发人员能够编写数据库应用程序。 一句话总结就是，jdbc 是 sun 公司提出来 java 操作 数据库的规范，具体如何去实现，需要各大厂商自己实现。 使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117package 类加载器;import java.sql.*;import java.util.ArrayList;import java.util.List;// 这里使用的是 Class.forName("")，没有使用 spi 去加载，所以这个并没有违反双亲委派模型/** * 在mysql中创建test数据库，并建立user表，两个字段，name和age */public class jdbc &#123; /** * 数据库相关参数 */ // maven 导包就好，mysql 8.0之后的包，官方也是建议使用 SPI 哦！！ // 使用 spi 就可以不用硬编码了，不用每次换包都改代码了！ public static final String JDBC_DRIVER = "com.mysql.cj.jdbc.Driver"; //连接数据库的url，各个数据库厂商不一样，此处为mysql的;后面是创建的数据库名称 public static final String JDBC_URL = "jdbc:mysql://localhost:3306/test"; //连接数据库所需账户名 public static final String JDBC_USERNAME = "root"; //用户名对应的密码，我的mysql密码是123456 public static final String JDBC_PASSWORD ="jwyjwy9951206-=-"; public static void main(String[] args) &#123; List&lt;Student&gt; students = new ArrayList&lt;Student&gt;(); Connection connection = null; PreparedStatement preparedStatement = null; ResultSet resultSet = null; try &#123; //第一步：加载Driver类，注册数据库驱动 Class.forName(JDBC_DRIVER); //第二步：通过DriverManager,使用url，用户名和密码建立连接(Connection) connection = DriverManager.getConnection(JDBC_URL, JDBC_USERNAME, JDBC_PASSWORD); //第三步：通过Connection，使用sql语句打开Statement对象； preparedStatement = connection.prepareStatement("select * from user where age =?"); //传入参数，之所以这样是为了防止sql注入 preparedStatement.setInt(1, 21); //第四步：执行语句，将结果返回resultSet resultSet = preparedStatement.executeQuery(); //第五步：对结果进行处理 while (resultSet.next())&#123; String name = resultSet.getString("name"); int age = resultSet.getInt("age"); Student student = new Student(); student.setAge(age); student.setName(name); students.add(student); &#125; &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125;finally &#123; //第六步：倒叙释放资源resultSet-》preparedStatement-》connection try &#123; if (resultSet!=null &amp;&amp; !resultSet.isClosed())&#123; resultSet.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; if(preparedStatement!=null &amp;&amp; !preparedStatement.isClosed())&#123; preparedStatement.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; try &#123; if(connection!=null &amp;&amp; connection.isClosed())&#123; connection.close(); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; for (Student student:students) &#123; System.out.println(student.getName()+"="+student.getAge()); &#125; &#125;&#125;class Student&#123; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 总结一下，有以下几步： 使用 DriverManager 去建立连接，这里是直接用 Class.forName()去进行 driver驱动的加载，这样的话这个driver 是由 Application ClassLoader 加载的，而 DriverManager 是由启动类加载器加载的，由于 driver 先加载了，所以这里并不会违反双亲委派原则，在下面我还会详细讲到这个加载过程； 建立连接后，通过 connection 对象，传入 sql 语句，包装成 Statement 对象，然后可以对其传参； 然后执行 sql 语句，会返回结果结果集 ResultSet； 处理结果即可； 最后释放资源。 jdbc &amp; 双亲委派模型的爱恨情仇主要参考： https://blog.csdn.net/justloveyou_/article/details/72231425 在讲正题之前，还是要再铺垫一下，先介绍几个需要知道的概念：SPI 机制、线程上下文类加载器。 SPI简介SPI（Service Provider Interface），针对厂商或者插件的一种机制。我们系统里抽象的各个模块，往往有很多不同的实现方案，比如日志模块的方案，xml解析模块、jdbc模块的方案等。面向的对象的设计里，我们一般推荐模块之间基于接口编程，模块之间不对实现类进行硬编码。一旦代码里涉及具体的实现类，就违反了可拔插的原则，如果需要替换一种实现，就需要修改代码。为了实现在模块装配的时候能不在程序里动态指明，这就需要一种服务发现机制。 java spi就是提供这样的一个机制：为某个接口寻找服务实现的机制。有点类似IOC的思想，就是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要。 ————————————————版权声明：本文为CSDN博主「sigangjun」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/sigangjun/article/details/79071850 约定当服务的提供者提供了服务接口的一种实现之后，在jar包的META-INF/services/目录里同时创建一个以服务接口命名的文件，该文件里就是实现该服务接口的具体实现类。而当外部程序装配这个模块的时候，就能通过该jar包META-INF/services/里的配置文件找到具体的实现类名，并装载实例化，完成模块的注入。基于这样一个约定就能找到服务接口的实现类，不需要在代码里指定。jdk提供服务实现查找的工具类：java.util.ServiceLoader。 带来的问题SPI （服务提供接口）是在 rt.jar 下定义的，这是系统核心库，是由启动类加载器去加载的，但是接口的实现却在各供应商提供的 jar 包下，启动类加载器肯定是无法去进行加载的，所以此时双亲委派模型无法解决这个问题。 线程上下文类加载器正是因为 SPI 带来的问题，所以我们引入 线程上下文类加载器「Context ClassLoader」 这个武器来进行双亲委派模型的违反。 线程上下文类加载器是从 JDK 1.2 开始引入的。Java.lang.Thread中的方法 getContextClassLoader()和 setContextClassLoader(ClassLoader cl)用来获取和设置线程的上下文类加载器。如果没有通过 setContextClassLoader(ClassLoader cl)方法进行设置的话，线程将继承其父线程的上下文类加载器。Java 应用运行的初始线程的上下文类加载器是系统类加载器「Application ClassLoader」，在线程中运行的代码可以通过此类加载器来加载类和资源。 有了上下文类加载器，那就可以直接在 SPI 中去加载各厂商提供的实现类了。 jdbc 的加载jdbc 可以通过两种方式进行加载： 这种方式并没有违反双亲委派模型，因为其并没有调用 SPI 接口，直接去硬编码加载 Driver 驱动，所以根本不会用到 BootStrap ClassLoader 去调用 SPI。 12Class.forName("com.mysql.cj.jdbc.Driver");connection = DriverManager.getConnection(JDBC_URL, JDBC_USERNAME, JDBC_PASSWORD); SPI 机制去加载 Driver 驱动，可以避免硬编码，驱动改了也无需修改代码。因为 jdk 默认就是 spi 机制，所以其实上面那一行是多余的，去掉 Class.forName(&quot;com.mysql.cj.jdbc.Driver&quot;);程序是照样执行的。 我们可以深入源码去看一下实现的具体步骤： 在 DriverManager 的 static 代码块中 12345678/** * Load the initial JDBC drivers by checking the System property * jdbc.properties and then use the &#123;@code ServiceLoader&#125; mechanism */static &#123; loadInitialDrivers(); println("JDBC DriverManager initialized");&#125; 我们注意到有个 loadIntialDrivers()，继续看 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162private static void loadInitialDrivers() &#123; String drivers; try &#123; drivers = AccessController.doPrivileged(new PrivilegedAction&lt;String&gt;() &#123; public String run() &#123; return System.getProperty("jdbc.drivers"); &#125; &#125;); &#125; catch (Exception ex) &#123; drivers = null; &#125; // If the driver is packaged as a Service Provider, load it. // Get all the drivers through the classloader // exposed as a java.sql.Driver.class service. // ServiceLoader.load() replaces the sun.misc.Providers() AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; // 这里是重点！！！！！！！ ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); /* Load these drivers, so that they can be instantiated. * It may be the case that the driver class may not be there * i.e. there may be a packaged driver with the service class * as implementation of java.sql.Driver but the actual class * may be missing. In that case a java.util.ServiceConfigurationError * will be thrown at runtime by the VM trying to locate * and load the service. * * Adding a try catch block to catch those runtime errors * if driver not available in classpath but it's * packaged as service and that service is there in classpath. */ try&#123; while(driversIterator.hasNext()) &#123; driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null; &#125; &#125;); println("DriverManager.initialize: jdbc.drivers = " + drivers); if (drivers == null || drivers.equals("")) &#123; return; &#125; String[] driversList = drivers.split(":"); println("number of Drivers:" + driversList.length); for (String aDriver : driversList) &#123; try &#123; println("DriverManager.Initialize: loading " + aDriver); Class.forName(aDriver, true, ClassLoader.getSystemClassLoader()); &#125; catch (Exception ex) &#123; println("DriverManager.Initialize: load failed: " + ex); &#125; &#125; &#125; 我们看到了我上文提及的 ServiceLoader，我们说过他其实就是去进行服务实现查找的，继续看 ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class);做了些什么「其实我们肯定都能猜到了，肯定是拿到了驱动的列表。」 12345678910public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl);&#125; public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service, ClassLoader loader)&#123; return new ServiceLoader&lt;&gt;(service, loader);&#125; 在这里，终于出现了线程上下文类加载器，然后再经过一系列的函数调用「我就不展开了，还是非常复杂的，我们来看看最终的有效代码」。 12345678910111213141516171819202122232425private boolean hasNextService() &#123; if (nextName != null) &#123; return true; &#125; if (configs == null) &#123; try &#123; // 等同于 Class.forName() String fullName = PREFIX + service.getName(); if (loader == null) configs = ClassLoader.getSystemResources(fullName); else configs = loader.getResources(fullName); &#125; catch (IOException x) &#123; fail(service, "Error locating configuration files", x); &#125; &#125; while ((pending == null) || !pending.hasNext()) &#123; if (!configs.hasMoreElements()) &#123; return false; &#125; pending = parse(service, configs.nextElement()); &#125; nextName = pending.next(); return true; &#125; 所以，就是利用线程上下文类加载器去完成违反双亲委派模型的行为的。 ​ 拓展除了 SPI 机制，违反了双亲委派模型，在 Spring 中 和 Tomcat 中也同样违反了双亲委派模型。 ​ 「Tomcat这一块没太搞明白。。。。。」 Tomcat 中违反的原因是每个 web应用程序对应的类库都应该是互相隔离的，但是如果是双亲委派机制，那么都会去用顶层加载器加载相应的类，那么一个相同的类会被加载多次，而又是属于不同的webapp，导致发生冲突，所以在 Tomcat 中是 WebApp 自己直接加载，不转发给上级加载器，也就是所谓的“子优先”。「这块我自己也有些不确定，在周志华老师的书中也讲到了这个例子，但是我也没太明白他讲这个例子的意思。。。」 有个跟我类似想法的博客，写的比较详细，可以看看：https://www.cnblogs.com/aspirant/p/8991830.html 这个谈到了子优先，不过也没太看明白：https://zhuanlan.zhihu.com/p/24168200 Spring，这个问题周志华老师的书中也有提及。 如下：如果有 10 个 Web 应用程序都用到了spring的话，可以把Spring的jar包放到 common 或 shared 目录下让这些程序共享。Spring 的作用是管理每个web应用程序的bean，getBean时自然要能访问到应用程序的类，而用户的程序显然是放在 /WebApp/WEB-INF 目录中的（由 WebAppClassLoader 加载），那么在 CommonClassLoader 或 SharedClassLoader 中的 Spring 容器如何去加载并不在其加载范围的用户程序（/WebApp/WEB-INF/）中的Class呢？ spring根本不会去管自己被放在哪里，它统统使用线程上下文类加载器来加载类，而线程类加载器默认设置为了WebAppClassLoader。也就是说，哪个WebApp应用调用了Spring，Spring就去取该应用自己的WebAppClassLoader来加载bean。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>双亲委派模型</tag>
        <tag>jdbc</tag>
        <tag>tomcat</tag>
        <tag>SPI</tag>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试经历]]></title>
    <url>%2F2020%2F03%2F17%2F%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C.html</url>
    <content type="text"><![CDATA[2020.3.17 15:38 - 16:35 「57 分钟」阿里一面电话面。自己第一次面试，面得很糟糕，面对问题很慌… 整个面试分三块进行： 基础知识，主要问了计网和数据库以及算法 TCP 的三次握手和四次挥手过程 腾讯的面试官大大也问了这个问题，并且特别提醒了我要注意记住 tcp 连接和断开时客户端和服务器端的状态，真是超级感谢啊，这点原来还真的一直没有注意过。 首先我们来回顾一下 TCP 的数据传输单元，TCP 传送的数据单元称为报文段。一个 TCP 报文段分为 TCP 首部和 TCP 数据两部分，整个 TCP 报文段都封装在 IP 数据报中的数据部分，TCP 首部长度是4的整数倍，其中有固定的20个字节，剩余的可变动的就是选项和填充「最常见的可选字段是最长报文大小，又称为MSS（Maximum Segment Size），每个连接方通常都在通信的第一个报文段（为建立连接而设置SYN标志为1的那个段）中指明这个选项，它表示本端所能接受的最大报文段的长度。」，20个固定的字节包括了源端口号（2 字节）、目的端口（2字节）、seq序列号（4字节）、确认号ack（4字节）、以及确认位ACK 等等。 其次，我们来详细讲解一下三次握手、四次挥手的过程： 三次握手 首先，在三次握手建立连接的阶段，是不会传输 TCP 报文段的，传输的是 传输控制块（TCB），传输控制块 TCB(Transmission Control Block)存储了每一个连接中的一些重要信息，如：TCP 连接表，指向发送和接收缓存的指针，指向重传队列的指针，当前的发送和接收序号等等。 最开始的 Client 和 Server 都是处于 Closed，由于服务器端不知道要跟谁建立连接，所以其只能被动打开，然后监听端口，此时 Server 处于 Listen 状态； 而 Client 会主动打开，然后构建好 TCB 「SYN= 1，seq = x」，发送给服务器端，此时 Client 会将状态置为 SYN_SEND 「同步已发送」； 服务器端收到客户端发来的同步请求后，会将状态置为 SYN_RECV「同步已接收」，同时会构建好 TCB「SYN = 1，seq = y，ACK = 1，ack = x + 1」发送给客户端； 客户端接收到了服务器端发来的传输控制块之后，会将自己的状态改为 ESTABLISHED「建立连接」，然后发送确认报文（ACK= 1，seq = x + 1，ack = y + 1）； 服务器端在收到了客户端发来的报文之后，也将状态置为 ESTABLISHED「建立连接」，至此，三次握手结束，当然在这里，可以带 tcp 报文段信息过来了，因为此时客户端已经可以保证是可靠的传输了，所以在这一端可以发送报文段了。 几个问题： 为何不直接在第一次握手就带上报文段消息，非要第三次才可以带？ 因为 TCP 是要保证数据的不丢失且可靠，如果在第一次就带上报文段消息，此次建立连接很有可能就会失败，那么就不能保证数据的不丢失了，在不可靠的机制上进行这种操作，换来的代价太大，每次发送报文段的资源也会增大，得不偿失； 而第三次握手的时候，客户端已经知道服务器端准备好了，所以只要告诉服务器端自己准备好了就okay了，所以此时带上报文段信息没有任何问题。 可不可以只握手两次？ 肯定是不可以的，三次握手主要是解决这样一个常见的问题，客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。 如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。————————————————版权声明：本文为CSDN博主「小书go」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qzcsu/article/details/72861891 四次挥手 最开始客户端和服务器端都是 ESTABLISHED 的状态，然后客户端会主动关闭，而服务器端则是被动关闭。 客户端发送一个 FIN 报文段，seq = 结束的报文段序号 + 1「假设为 u」，告诉服务器端，客户端需要关闭了，此时将客户端的状态变为 FIN-WAIT-1，等待服务器端的反馈； 服务器在接收到了客户端发来的 FIN 包之后，会发一条 ack报文反馈给客户端，其中报文中包括 ACK = 1，seq = v，ack = u+1，告诉客户端收到了客户端要关闭的消息了，同时服务器端会通知应用进程需要关闭连接了，并将自己的状态置为 CLOSE-WAIT； 由于服务器端可能还有一些数据没处理完，所以需要一段时间的等待，当处理完了之后，会再发一条报文，其中 FIN = 1，ACK = 1，seq = w，ack = u+1，告知客户端，服务器端现在可以关闭了，并将服务器端的状态由 CLOSE-WAIT 变为 LAST-ACK； 客户端在收到了服务器端发来的消息之后，会发一条ack报文「ACK = 1，seq = u+1，ack = w+1」回去，告知服务器端，客户端已经知道了你准备好关闭了，此时会将客户端的状态由 FIN-WAIT-2 置为 TIME-WAIT，在两个最长报文段传输时间过后，会自动将客户端的状态由 TIME-WAIT 置为 CLOSED。 服务器端收到消息之后，就将状态由 LAST-ACK 置为了 CLOSED，自此，四次挥手全部结束。 一个很常见的问题，为何不能三次挥手呢？ 首先如果去掉最后一次挥手，那么服务器端就不知道自己要关闭的报文有没有传输成功，可能半路上就失败了，但是此时客户端不知道，导致客户端一直在等待服务器关闭，但是此时服务器端直接就关闭了； 如果中间的两次挥手合并，那是肯定不行的，因为此时服务器端可能还有很多报文未处理完，此时直接关闭肯定会对传输有很大影响。 为什么客户端在收到 服务器端发来的 FIN 包后要等 2 个最长报文段传输时间？ 防止最后自己发去的 ack 没传送到服务器，如果服务器没收到客户端的 ack，肯定会选择重发一次 FIN 包，那么此时如果客户端已经关闭了，客户端就不能再发 ack 确认收到了，至于为何是 2 个报文段传输时间，因为刚好一去一回嘛… 2 个最长报文传输时间没有 FIN 包发来，就说明服务器已经关闭了，客户端也就可以安心关闭了。 这文章整挺好：https://blog.csdn.net/qzcsu/article/details/72861891 http 和 tcp 的区别 tcp 是传输层协议，http是应用层协议，http在传输层就是使用的 tcp。 排序算法有哪些 这个简单。排序算法分为比较算法和非比较算法，其中比较算法包括交换排序「冒泡和快排」、选择排序「简单选择排序和堆排序」、插入排序「直接插入排序、希尔排序」、归并排序「二路归并和多路归并」，非比较排序有计数排序、桶排序、基数排序。 「公式： 不稳定的有：快些选堆」 冒泡排序。稳定的，平均时间复杂度为 O(n²)，最好时间复杂度那肯定就是一次循环 O(n)，最坏时间复杂度为 O(n²)。空间复杂度 O(1)。 快速排序。不稳定，平均时间复杂度为O(nlogn)，最好的时间复杂度为O(nlogn)，最坏就是选定的基准值在最边上，这样就是O(n²)，注意哦，快排的空间复杂度平均是 O(logn)，最差 O(n)。 简单选择排序。不稳定，平均、最好、最坏时间复杂度都为O(n²)。空间复杂度 O(1)。 堆排序。不稳定，平均、最好、最坏的时间复杂度为O(nlogn)。空间复杂度 O(1)。 直接插入排序。稳定。最好O(n)，平均、最坏时间复杂度O(n²)。空间复杂度 O(1)。 希尔排序。不稳定。最好O(n)，平均O(n1.3)，最坏肯定是O(n²)。空间复杂度O(1)。 归并排序。稳定。最好、最坏、最差时间复杂度O(nlogn)，空间复杂度O(n)。 计数排序。稳定，空间换时间。适合数比较集中在一起的，这样k就少了，时间复杂度为 O(n+k)，空间复杂度也为O(n+k)。「个人还是觉得其实空间复杂度为O(k)，因为我可以把值放回去的时候可以放到原数组上，所以是O(k)。」 桶排序，桶越多，时间复杂度很简单，为O(n+k)，空间复杂度最坏为O(n+k)，其中 n 是因为桶内部所有元素得排序， k 是指桶的数量。 基数排序，时间复杂度O(n*k)，k为最大数的位数，空间复杂度为O(n)。 堆排序的稳定性，如何实现堆排序，具体细节 这个很简单，就不详细说了。 归并排序的稳定性，如何实现归并排序，具体细节 简单。 说一下jdk自带的排序用到了哪些排序算法，展开讲一下 Arrays.sort() &amp; Collections.sort() JDK中的自带的排序算法实现原理精彩总结 jdk层面实现的sort总共是两类，一个是 Arrays.sort()， Collections.sort()； Arrays.sort() 如果数组内元素是基本数据类型，最主要采用的是双轴快速排序「其实就是三路快排一模一样的思路，只不过三路快排中间是 = pivot1，而双轴快速排序是（pivot1，pivot2），具体戳链接：https://www.cnblogs.com/nullzx/p/5880191.html 。 总结就是数组长度小于47的时候是用直接插入算法，大于47并且小于286是采用双轴快速排序，大于286如果连续性好「也就是元素大多有序，有一个flag专门用来记录数组元素的升降次数，代表这个数组的连续性」采用的是归并排序，否则还是依旧采用双轴快速排序。 如果数组内元素是对象，采用的是TimSort.sort()，跟 Collections.sort()一样，都是采用的这个函数，这是归并排序算法和插入排序的结合。 Collections.sort()，采用 TimSort.sort()。 TimSort.sort() 大概原理： 当待排序元素小于32个时，采用二分插入排序，是插入排序的一种改进，可以减少插入排序比较的次数。当找到插入位置时，直接利用System.copy()函数即可。 当待排序元素大于等于32个时，进行归并排序（和传统的归并不一样），首先将排序元素分区，每个分区的大小区间为[16,32)，然后依次对每个分区进行排序（具体实现依然是二分插入排序），排完序的分区压入栈（准确的说不是栈，而是一个数组，用来记录排序好的分区），当栈内的分区数满足条件时，进行分区合并，合并为一个更大的分区，当栈中只剩一个分区时，排序完成。 mysql如何优化 建索引 索引的建立的原则有哪些 除了运用最左前缀、索引下推、考虑索引长度，还有哪些是建立索引需要考虑的 「这个实在想不到了」 红黑树和平衡二叉树的区别，各自的优势特点，以及红黑树如何进行添加数据「具体说一下旋转过程，我只说了我博客上写了，具体的给忘了…」 这个二面真得复习复习。 java方面 讲一下双亲委派模型，为什么要设计双亲委派模型 双亲委派模型：一个类加载器在加载类时，先把这个请求委托给自己的父类加载器去执行，如果父类加载器还存在父类加载器，就继续向上委托，直到顶层的启动类加载器。如果父类加载器能够完成类加载，就成功返回，如果父类加载器无法完成加载，那么子加载器才会尝试自己去加载。 好处：java类随着它的类加载器一起具备了一种带有优先级的层次关系。 这种双亲委派模式的设计原因：可以避免类的重复加载，另外也避免了java的核心API被篡改。 违反双亲委派模型，我不小心说了jdbc，然后问我jdbc是如何连接到数据库的，具体流程是什么，我就说了个反射…没复习到位 见我写的 《从双亲委派模型到jdbc》 讲一下jmm，为何这样设计 java memeory model ，java 内存模型，设计的目的是屏蔽掉各种硬件和操作系统之间的差异性，实现让 Java 在各种平台下都能能到一致的并发效果。jmm 中，分为主内存和工作内存，其中每个线程拥有自己的工作内存，而主内存是所有线程共享的。这里我遇到疑问了，在周志华老师那本书中，先是讲主内存中存储了所有的变量，那必然就包括了线程的局部变量，那难道我线程使用自己的局部变量，也要从主内存中拷贝一份副本到工作内存中呢？这是不是和说主内存是共享区域产生了矛盾呢？书中还说可以类比，主内存就是跟堆差不多，而工作内存类似于栈，那之前说的主内存存储了所有的变量，这句话是不是有问题呢？个人觉得主内存不可能存储所有的变量…应该就是类似于堆存储共享变量… 为何要有工作内存，有了主内存和工作内存不是更麻烦啊，要不断的复制移动数据，为何不能直接对主内存操作「这个也没答上来」 这就跟为何要提出寄存器和缓存一样的道理，如果所有的操作都在内存中完成，那速度实在是太慢了，只有工作在寄存器和缓存中，速度才能让人满意，而这里的主内存就类比为内存，工作内存就类比为寄存器和缓存。 什么是线程安全 「多线程方面一个问题都没问…血亏」 多个线程访问一个对象，无需考虑环境和额外同步，调用这个对象的行为就能得到正确的答案，就说明这个对象是线程安全的。 举三个例子分别描述 jmm的三个特性「原子性、有序性、可见性」导致的线程安全问题 不遵循原子性：volatile 变量的自加，复合操作，导致线程不安全； 不遵循有序性：比如共享变量，多个线程同时访问，不按序，每个都拷贝一份到自己的工作内存，必然会导致线程不安全； 不遵循可见性：普通变量，跟有序性一样的例子，每个都从主内存拷贝一份变量的副本到工作内存，必然会导致线程不安全。 讲一下 RunTimeException 的造成的原因「非检查型异常」，并说一下为什么不处理 RunTimeException？ RuntimeException是Exception子类。而Exception还有其它类型的异常，我们统一称为非Runtime异常。RuntimeException的特点是非检查型异常，也就是Java系统中允许可以不被catch，在运行时抛出。而其它定非运行时异常如果抛出的话必须显示的catch，否则编译不过。 RuntimeException常见异常： 1 NullPointerException，空指针异常。 2 NumberFormatException，字符串转化成数字时。 3 ArrayIndexOutOfBoundsException， 数组越界时。 4 StringIndexOutOfBoundsException， 字符串越界时。 5 ClassCastException，类型转换时。 6 UnsupportedOperationException，该操作不支持，一般子类不实现父类的某些方法时。 7 ArithmeticException，零作为除数等。 8 IllegalArgumentException，表明传递了一个不合法或不正确的参数 运行时出现错误，说明你的代码有问题，程序已经无法继续运行，所以对RuntimeException的处理时不必要的。之所以不处理，目的就是要是程序停止，修正代码。 项目 主要就问了下我最近在做什么项目，到什么阶段了，有多少人用； 问我爬虫的实现「面试官貌似没有做过这方面的东西」 我提了一嘴 xxl-job，面试官应该也没用过，就没有深究，本来还想讲讲kafka的，结果直接没问…白准备了 最后就问了一个很常见的算法问题： 256M 的内存如何对 16g的数组进行排序 多路归并，因为没要求存储，只要求了内存，可以多路归并，加入每个元素都是 1M，则可以最多分成 256 组，然后进行归并。 具体描述：采用外部排序，先将16 g数组分成 256 M 一组，然后分别读入内存进行内部排序「比如说可以使用快排」，将这些组内元素全部排好序之后，然后运用败者树和置换-选择排序，进行多路归并，即可。 这里其实可以引申出好多问题： 海量数据 求最大的 K个数问题，如何解决？ 按位划分区域，可以尽快的缩小范围，比如最高位 0 分一堆，1 分成一堆而且不用排序，这是第一选择。 最经典的方法当然是 堆 了，比如要求前1000个最大的数，那就直接建一个 1000 大小的小根堆，然后遍历，只要发现后面的数比小根堆的根节点大，就把根节点和该数交换，重新调整堆，遍历完之后，堆中的数自然就是最大的 1000 个数了； 当然能使用堆排序的前提是内存中要能够放得下这个 K，如果放不下呢？那就只能外部排序了，排序完之后拿到第 K 大的数即可，当然排序前可以和方法一搭配一下。 海量数据求中位数，如何解决？ 可以按照位来分组，比如说最高位是0的一组，是 1 的一组，这样可以统计出那一组更少，这样就排除了一大半，然后继续这样排查，最终缩小范围后直接内部排序。 直接外部排序，然后取中间值，最笨的方法。 在海量数据中找出出现频率最高的前k个数，例如，在搜索引擎中，统计搜索最热门的10个查询词；在歌曲库中统计下载最高的前10首歌等。 如果重复率很高，可以采用前缀树，因为 trie 树适用于数据量大，重复多，但是数据种类小必须得可以放入内存； 按照 hash 进行分组，这样就能避免相同的数分到不同区域去了，导致不好统计。hash 分组完毕后，然后用前缀树 或者 hashmap 来计算每个组的前 k 个频率最高的数，最后对各个组的前 k 个数进行统计即可。 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？ 这里我们把40亿个数中的每一个用32位的二进制来表示假设这40亿个数开始放在一个文件中。 然后将这40亿个数分成两类: 1.最高位为0 2.最高位为1 并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=20亿，而另一个&gt;=20亿（这相当于折半了）；与要查找的数的最高位比较并接着进入相应的文件再查找 再然后把这个文件为又分成两类: 1.次最高位为0 2.次最高位为1 并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=10亿，而另一个&gt;=10亿（这相当于折半了）； 与要查找的数的次最高位比较并接着进入相应的文件再查找。 ……. 以此类推，就可以找到了,而且时间复杂度为O(logn)。 大概统计一下，海量数据求 TopK 的普遍方法： 最快的不需要排序就能排除一大堆的数据的方法就是看 “位”，比如最高位为 0 的分一块，为 1 的分一块，这样迅速就分出一大块不需要的了，尤其适合找中位数，等分的差不多了就可以进行内部排序了。 堆排序，适用于求海量数据最大 K or 最小的 K 个数； 分治hash，适用于那些内存很小，数据很大，但是又想求最大的 K 个众数的问题，可以先 hash 到很多个组，然后在组内部使用 hashmap 或者 前缀树 「google等字符」，取到组内前 K 个众数，最后进行组间比较久okay了； 当然不能忘了万能法，那就是外部排序，然后再进行相应的处理。 最后的最后，让我们再来做个附加题： 先来看几个比较常见的例子 字处理软件中，需要检查一个英语单词是否拼写正确 在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上 在网络爬虫里，一个网址是否被访问过 yahoo, gmail等邮箱垃圾邮件过滤功能 这几个例子有一个共同的特点： 如何判断一个元素是否存在一个集合中？ 这里必须介绍一下 bitmap 这个方法了，例如我要从海量数据中找一个数是否出现过，就可以用位图的思路去做，如果数字是 7 ，那就在第 7 位 置 1，如果该位置已经是 1 了，那就代表出现过，不用更改。 如果问题变为从海量数据中找一个数是否出现过一次，那这个时候就得用 2 bitmap 来表示了，也就是一个数如果出现一次，置为 01 ，出现过两次，置为 10，然后再出现，都是10，这个时候如果我们只用一位，是不能表示出出现的次数的。 至于我们常说的布隆过滤器，其实也就是在bitmap之前进行一个hash，例如将字符串进行hash成数组，然后使用位图，解决这类问题。 2020.3.18 09:28 - 11:12 「105 分钟」 腾讯一面视频面。本次面试分为两大块： 牛客网写代码 「LeetCode easy难度」，这个阶段大概 5-10 分钟 123456789101112131415161718192021翻转数列小Q定义了一种数列称为翻转数列:给定整数n和m, 满足n能被2m整除。对于一串连续递增整数数列1, 2, 3, 4..., 每隔m个符号翻转一次, 最初符号为'-';。例如n = 8, m = 2, 数列就是: -1, -2, +3, +4, -5, -6, +7, +8.而n = 4, m = 1, 数列就是: -1, +2, -3, + 4.小Q现在希望你能帮他算算前n项和为多少。输入描述输入包括两个整数n和m(2 &lt;= n &lt;= 109, 1 &lt;= m), 并且满足n能被2m整除。输出描述输出一个整数, 表示前n项和。示例1输入8 2输出8 正式进入面试 「100分钟」 腾讯看来的确全部是 c++，面试官也是说基本上都是 c++，没有专门搞 java 的组，所以大家 java 投腾讯还是务必慎重，最开始问我的技术栈是什么，c++是否了解，用的比较多？得知我说基本没咋用过之后，就开始尝试问计算机网络方面的问题。 问的很深入，比如说三次握手四次挥手，客户端服务器端各自的状态是什么，对整个建立连接和关闭连接的具体流程是什么，深入到 OSI 模型去讲； 这个我在上面已经详细说过了… 然后主要问的是 Socket 编程，讲套接字编程的具体实现流程，代码如何写，如何实现类似于 Nginx 的多服务器的 Socket 编程； 然后谈到 select、epoll等相关 c++ 的知识，我是在 java 层面去讲的如何实现的 「NIO」； ​ 嘿嘿嘿，这个没问题了，可以看我的最新的文章 —《零拷贝及其周边》 再者就是聊到数据库，这个是必问的，问我索引如何建立、如何优化索引，然后是一些具体问题对索引的分析； 用 key or index 建立索引，优化索引的方法：尽量复用索引，利用好最左前缀和索引下推原则，尽量减少回表次数，利用覆盖索引。 然后谈到事务的四个特性，如何实现原子性、隔离性、一致性、持久性的，内部机制具体如何实现； 原子性：利用 undolog，回滚机制，完成要么全部成功，要么全部失败； 隔离性：主要是靠一致性视图+当前行的 row_id_transaction，来完成的。 一致性：主要靠加锁防止事务冲突，一致性是另外三个的顶层，只要他们三完成了他才有可能完成，还有mvcc的加持，以及 undolog、redolog。 持久性：redolog保证crash-safe，bin-log保证归档。 借此谈到 undolog、redolog、binlog，以及 mvcc 的实现； 略 Redolog 到底是保存在磁盘中的还是在内存中？ redo log包括两部分：一是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的；二是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。 详细分析 Mysql 中的三个日志：redolog、undolog、binlog 谈一谈 mysql 的运行机制，整个运行过程是怎样的，如何处理的； 这个简单，略 mysql的索引实现，B+的优点等等； 简单，略 全程谈 一致性问题 谈的很多，包括了 mysql 主从复制的一致性如何保证，我说不太清楚，但是借此讲了 kafka 中的高可用机制「ISR」，以及 kafka中的 ack 机制和 kafka 中的消息语义「如何保证数据的一致性」； Mysql 保证主从一致性： 主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写binlog。 备库B跟主库A之间维持了一个长连接。主库A内部有一个线程，专门用于服务备库B的这个长连接。一个事务日志同步的完整过程是这样的： 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个位置开始请求binlog，这个位置包含文件名和日志偏移量。 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和sql_thread。其中io_thread负责与主库建立连接。 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。 备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。 sql_thread读取中转日志，解析出日志里的命令，并执行。 这里需要说明，后来由于多线程复制方案的引入，sql_thread演化成为了多个线程。 因为语言上还是有很多区别的，在后面又问了几个有关于 c++ 的问题，答得不是很好 虚函数是什么 「没答上来」； 略。 因为看到我博客有些滑动窗口算法，就问了 tcp 滑动窗口底层的代码实现； 进程、线程、协程的区别，我说完之后，又延伸到线程是如何保证同步的，借此谈到了线程安全，然后我自己拓展了 synchronized「详细介绍了锁升级过程」、lock体系、CAS 的实现以及 final 关键字和 ThreadLocal； 协程的应用场景主要在于 ：I/O 密集型任务。 这一点与多线程有些类似，但协程调用是在一个线程内进行的，是单线程，切换的开销小，因此效率上略高于多线程。当程序在执行 I/O 时操作时，CPU 是空闲的，此时可以充分利用 CPU 的时间片来处理其他任务。在单线程中，一个函数调用，一般是从函数的第一行代码开始执行，结束于 return 语句、异常或者函数执行（也可以认为是隐式地返回了 None ）。 有了协程，我们在函数的执行过程中，如果遇到了耗时的 I/O 操作，函数可以临时让出控制权，让 CPU 执行其他函数，等 I/O 操作执行完毕以后再收回控制权。 简单来讲协程的好处： 跨平台 跨体系架构 无需线程上下文切换的开销 无需原子操作锁定及同步的开销 方便切换控制流，简化编程模型 高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。 缺点： 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。 进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序：这一点和事件驱动一样，可以使用异步IO操作来解决 最后再贴个图来总结一下，更清楚： 作者：程序猿杂货铺链接：https://juejin.im/post/5d5df6b35188252ae10bdf42来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 进行间通信 IPC 有哪些方式「我只说了信号量、共享区域、管道这几个，面试官也没追问」 管道。管道的实质是一个内核缓冲区，进程以先进先出的方式从缓冲区存取数据，管道一端的进行顺序的将进程数据写入缓冲区，另一端的进则顺序地读取数据，该缓冲区可以看做一个循环队列，读和写的位置都是自动增加的，一个数据只能被读一次，读出以后在缓冲区就不复存在了。当缓冲区读空或者写满时，有一定的规则控制相应的读进程或写进程是否进入等待队列，当空的缓冲区有新数据写入或满的缓冲区有数据读出时，就唤醒等待队列中的进程继续读写。管道是一种半双工通信方式，数据只能单向流动。需要进行通信时，需要建立2个管道。 信号量。进程之间通信的机制，例如 Semphore ？ 共享内存。进程的不同虚拟内存映射到用一个物理内存上，实现共享。 内存泄露问题如何排查，主要问linux如何进行排查「没答上来，就说可以用可视化界面」 内存泄露会发生什么情况，系统会假死吗？ 最后谈了谈一些数据结构和算法： 两个堆如何实现队列「貌似堆不就是优先级队列嘛…」，两个队列如何实现堆；「面试官表述不清楚，可能就是想指堆栈？？？」 两个栈如何实现队列，两个队列如何实现栈； 链表如何查找是否有环； 链表如何确定环的起点； 两条链表找公共处的起点。 还有一些细枝末节的问题，印象已经不深了，大概就是这些吧。 总结一下：面试官非常擅长挖掘面试者的优势，对面试者不太懂的全部不问，基本上我会什么就问什么，所以大概面试了半小时后，就开始对着我的博客问，所以整体上给人的感觉是很好的。大家如果要面腾讯的话建议多看看c++，并且对常见的 计网 和 os 的问题尽量往深处走，面试官只看中你对问题的深度，不会的问题他不会追问。 最大的收获： 面试官对自己方方面面的建议。并且给自己推荐了三本书《unix网络编程卷一》《unix网络编程卷二》《linux内核》 部分拓展 输入 www.taobao.com 后发生了什么 如果是第一次访问 www.taobao.com，客户端「浏览器」会首先去 dns 服务器查找对应的ip，如果是第一次访问 这个网站，那么首先会去走 http 协议，客户端「浏览器」和服务器的 80 端口进行 tcp 连接，然后 服务器 80 端口会返回一个 301/302「具体区别下文会讲」重定向，在服务器响应头中还会添加 HTTP-Strict-Transport-Security，里面有 max-age，用户访问时，服务器种下这个头，下次如果使用 http 访问，只要 max-age 未过期「客户端检验」，客户端会进行内部跳转，可以看到 307 Redirect Internel 的响应码。然后就直接变成和 443 端口建立连接，走 https 连接。 如果不是第一次访问，那在拿到ip之后客户端直接校验自己的 max-age字段，如果没过期直接 307 内部跳转，直接和服务器的 443 端口进行 https 的连接了。剩下的就是https的过程了。 作者：蜗牛的北极星之旅链接：https://juejin.im/post/5d8a34ea6fb9a04dfa09561b来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。告诉我们需要建立 https 301、302、307的区别 301重定向是永久的重定向，搜索引擎在抓取新的内容的同时也将旧的网址替换为了重定向之后的网址。 302重定向只是暂时的重定向，搜索引擎会抓取新的内容而保留旧的地址，因为服务器返回302，所以，搜索搜索引擎认为新的网址是暂时的。所以302，会导致劫持问题：A站通过重定向到B站的资源xxoo，A站实际上什么都没做但是有一个比较友好的域名，web资源xxoo存在B站并由B站提供，但是B站的域名不那么友好，因此对搜索引擎而言，可能会保存A站的地址对应xxoo资源而不是B站，这就意味着B站出了资源版权、带宽、服务器的钱，但是用户通过搜索引擎搜索xxoo资源的时候出来的是A站，A站什么都没做却被索搜引擎广而告之用户，B站做了一切却不被用户知道，价值被A站窃取了。这里 A 就相当于是我们在输入框输入的域名，然后对 B 重定向 302，导致自己的域名不会被替换却还享用 B 的资源。 307意思就是客户端内部自己做了重定向，就比如 www.baidu.com ，我在有了 HSTS 的 max-age 之后会自动从 http://www.baidu.com 跳转到 https://www.baidu.com。 https 的形成过程 在传输层和应用层中间有个 ssl/tls 层 流程： 验证过程「具体流程：https://www.jianshu.com/p/b0b6b88fe9fe」 客户端（通常是浏览器）先向服务器发出加密通信的请求，与服务器的443端口建立连接； 服务器收到请求,然后响应，确认加密方法，如 RSA非对称加密，然后将服务器的证书发过去； 客户端验证服务器的证书，然后使用公钥加密发送消息； 服务器用私钥解密，得到明文。 正常传输 跟正常的http传输一致，只不过是密文传输。 对称加密算法和非对称加密算法的区别 https://blog.csdn.net/u013320868/article/details/54090295 对称加密，加密解密时间更快，但是密钥管理是个大问题； 非对称加密，更安全，但是加密解密时间较长一些。 RSA 算法流程 https://zhuanlan.zhihu.com/p/44185847 之所以难解，是因为位数很长，并且去对一个超级大的数进行因式分解，如果没有私钥的话几乎不可能做到。 主要用到的数学知识有欧拉函数、费马小定理等等 Mysql 的高可用机制是如何实现的 见下面写的👇 2020.3.26 16:00 - 17:00 「60 分钟」 美团一面电话面。 跟前两面风格完全不一样！ 先问我学过什么科目？ 我说计算机网络、操作系统、编译原理、数据结构、计算机组成原理、数据库等等； 对什么科目感兴趣？ 我说计算机网络吧 为什么喜欢这门课？ 我说因为上课听得懂，这门课的体系也比较清晰，分层来学，循序渐进，分数考得高，有成就感所以就喜欢了。 这门课有哪些让你记忆深刻的地方？ 我说 tcp、ip吧，tcp 比较记忆深刻因为他很重要，出现的地方比较多，ip的话应该我们现实生活中也总是提起，所以记忆比较深刻一些。 那我们现实中说到的 ip 和 你这里的 ip 有什么区别？ 我说一个是 ip 协议，主要功能是定义IP地址格式，数据包的格式，分组转发规则，而我们现实中的 ip，是指的 ip 地址，这是主机在网络中的标识，一般我们接触的都是 IPv4，即 32 位的地址。 那你讲讲 IPV4 有什么吧 「我自己加的问题」 IPv4 分为网络号和主机号，网络号就是主机或者路由器所连接到的网络的标识，主机号就是主机或者路由器在这个网络内的标识。 既然谈到了路由器，那路由器有 ip 地址吗，路由器又有什么功能呢？ 路由器肯定是有 ip 地址的，并且路由器总是有两个或两个以上的 ip 地址，路由器的每一个端口都有一个不同网络号的 ip 地址，因为路由器最主要的功能就是分组转发路由，通过路由表对报文进行相应的转发。 那网关又是什么？和路由器有什么关系呢？ 这个真的难…平时还真不会去思考这些问题… 如上图所示，路由器其实就是实现了网关的功能，网关是一个逻辑概念，指的是网络的出口和入口「网络边界」，而路由器则是一个物理概念，实现了网关的功能，是不同网关的沟通桥梁和物理基础。 你说的 ip 协议是什么？属于哪一层？ 我们通常使用的协议是 IPv4 协议，属于网络层。 介绍一下 IPv4 协议有哪些内容，然后说说网络层的一些其他协议吧？ 首部固定20字节，包括版本，首部长度，源地址，目的地址等等。 网络层的其他协议包括 ARP「地址解析协议，用于IP地址和MAC地址的映射」、NAT「网络地址转换，对外隐藏内部的 ip」、ICMP「网络报文控制协议，允许主机和路由器报告差错和异常情况」、CIDR「子网划分协议，无分类域间路由选择，没有子网概念，但是有用子网掩码」 还要 DHCP，不过这个是应用层协议，基于 UDP，用于给主机动态分配 IP 地址，我们的笔记本突然接入 wifi 获得的 IP 就是 DHCP 协议获取的。 第一段落告终，因为我实在是听不清对面面试官说话，声音太低了，并且由于他使用的公司的 vpn，压根听不清…我所有的注意力基本上都集中在听他说话上了…根本没心思思考问题…莫名的紧张… 然后他换了电话打过来，终于听得清楚了，也终于不用尽全力听他讲话了，于是就不紧张了，然后我们就继续聊了下去。 除了计算机网络，还对哪门课程比较有印象？ 数据库吧，自己因为做项目也一直有用。 那你平时用的是什么数据库？ Mysql、mongodb 这两个数据库有什么区别？ 一个是关系型数据库，一个是非关系型数据库。 那什么是关系型数据库，什么是非关系型数据库？为什么要分成这两种数据库呢？各自的优势和使用场景在哪呢？ 关系型数据库指采用了关系模型来组织数据的数据库，关系模型可以简单的理解为一个二维表，所以里面的字段名称和字段类型都是在建表的时候就确定好了的； 非关系型数据库则是结构不固定，集合内数据字段可以不一样，数据比较松散，一般以键值对的形式存储，比如一般都是json数据直接存储。 适合使用SQL开发的项目： 可以预先定义逻辑相关的离散数据的需求 数据一致性是必要的{acid} 具有良好的开发者经验和技术支持的标准的成熟技术 适合使用NoSQL开发的项目： 不相关，不确定和逐步发展的数据需求 更简单或者更宽松的能够快速开始编程的项目 速度和可扩展性至关重要的 非关系型数据库的优势： 性能NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 关系型数据库的优势： 复杂查询可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持使得对于安全性能很高的数据访问要求得以实现。 对于这两类数据库，对方的优势就是自己的弱势，反之亦然。 但是近年来这两种数据库都在向着另外一个方向进化。例如：NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能的雏形，比如Couchbase的index以及MONGO的复杂查询。对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国。SQL数据库也开始慢慢进化，比如HandlerSocker技术的实现，可以在MYSQL上实现对于SQL层的穿透，用NOSQL的方式访问数据库，性能可以上可以达到甚至超越NOSQL数据库。可扩展性上例如Percona Server，可以实现无中心化的集群。 虽然这两极都因为各自的弱势而开始进化出另一极的一些特性，但是这些特性的增加也会消弱其本来具备的优势，比如Couchbase上的index的增加会逐步降低数据库的读写性能。所以怎样构建系统的短期和长期存储策略，用好他们各自的强项是架构师需要好好考虑的重要问题。 作者：陈鼎星链接：https://www.zhihu.com/question/24225007/answer/81501685来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 那你讲讲 mysql 中你印象深刻的地方吧 第一个，对 mysql 支持的 RR 隔离级别非常的印象深刻，竟然可以做到修改了但不去读这种隔离级别； 还有就是 Mysql 的高可用机制；「必须疯狂转入自己熟悉的地方啊」 Mysql 的 Write Ahead Logging 也是一个很大的特点，有去使用 redolog、undolog、binlog； Mysql 的锁也是一个很大的特点，里面有丰富的锁，跟 juc 下的锁有的一拼，甚至更丰富； 还有就是 Mysql 中的索引，能提高检索速度。 『机会来了就要把握住，这种问题是最适合展现自己的学习深度』 那你分别讲讲这五个吧 其实面试官并没有让我讲这五个，只是让我讲讲 rr 级别如何实现的，但是为了复习，我还是把这五个再串一遍吧。 先讲第一个，mysql 如何实现的 RR 隔离级别 主要是采用了事务的一致性视图，和当前行的一个 row_tranc_id，根据一致性视图里面的低水位和高水位和 row_tranc_id 进行比较，判断是否需要用 undolog 拿到上一个值，undolog 在这里就是实现 mvcc 的基础。这里有一个值得注意的地方，就是如果 select 是不加悲观锁的去读，没有问题，是rr级别的读取，但是如果 select 显式的加锁，比如说加了行锁中的读锁「在语句最后加 in shared mode」或者写锁「for update」，这样跟 update 一样强制去进行一个加锁，导致只能去当前读，此时 mvcc 是失效的。 再讲第二个，mysql 的高可用机制是如何实现的？ 这里我在腾讯面试部分也有提到，但是腾讯那部分主要侧重讲了主从一致是如何实现的，而高可用则是建立在主从一致的基础上的。 先上一个自己画的图： mysql 的高可用的实现，基础就是能够做到主备切换，而主备切换的前提是要保证主从的一致性，这样才能切换，不然肯定会影响数据，但是要保证主从一致性，就会带来主从延迟的情况，具体的主从一致性的实现可以看我上面讲的，主从延迟可能会有图中那些原因，针对不可避免的主从延迟，主备切换有两种策略： 可靠性优先策略。具体步骤是等延迟小于某个数时，例如 5 秒，将主库 A 改成只读状态，然后再看主从延迟「seconds_behind_master」，等其变为 0 后，将备库 B 改成可读写状态，然后把业务切换到备库 B 上，这个策略的好处就是能保证主从的完全一致性，但是会有少量时间导致系统处于不可写的状态，不过影响不大，这也是最常用的策略； 可用性优先策略。不等主备数据同步，直接把连接切到备库B，并且让备库B可以读写，那么系统几乎就没有不可用时间了。好处就是没有不可用时间，但是容易导致数据逻辑出现问题。 再讲第三个，Mysql 的 Write Ahead Logging Mysql 中常见的三个日志文件分别是 undolog、redolog以及binlog，redolog 主要是负责 crash-safe，也就是崩溃恢复的工作，binlog主要是做一个归档的作用，redolog和binlog是两阶段提交的，这也是常用的分布式的手段吧，大家都okay了才commit，redolog是可擦除的，存储的是物理日志，但是binlog是顺序写，存储的是逻辑日志，并且 redolog 是 Innodb 独有的，binlog则是 server端有的。undolog 主要是负责 mvcc 和回滚，在数据修改的时候，不仅记录了redo，还记录了相对应的undo，如果因为某些原因导致事务失败或回滚了，可以借助该undo进行回滚。undo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。 再讲第四个，Mysql 中的锁 Mysql 从锁的范围上讲分为全局锁、表级锁以及行锁。 全局锁称为 Flash Tables with Read Lock，主要用来全库逻辑备份，这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。如果这个命令在主库操作的话，会导致业务停摆，如果再备库操作的话，会导致备库无法写从主库传来的binlog，造成主备延迟，所以我们很少使用它，一般都是使用mysql自带的 mysqldump 去进行全库逻辑备份，因为 mysqldump 有 mvcc 的支持，所以可以一边备份一边读写，但是这个必须要数据库引擎支持rr这个隔离级别，像 MyISAM 就不行。 表级锁又分为表锁和元数据锁MDL，其中表锁需要我们去显式加锁，例如 lock tables … read/write，如果一个线程对其显式加了表读锁，那么其他线程和该线程只能读该表，如果该线程加了表写锁，那么其他线程啥也干不了，所以这个锁表的粒度还是太大，一般不会采用。 表级锁的另外一种是元数据锁MDL，这是系统默认加上的，对表进行 DML 是加读锁，对表进行 DDL 是加写锁，读写互斥，读共享，相当于 ReadWriteLock，但是这里没有写降级的过程。 行锁是我们最经常用的了，也分为读锁和写锁，这里的读单指 select，这里的写指的是 update、delete、insert等等，当然我们也可以对 select 进行显式的加锁，此时 select 就从乐观读锁「跟 StampedLock 类似，一个是通过 version 判断数据有没有变化，一个是通过 stamp 判断」变成了悲观读锁或者写锁了，此时mvcc是失效的，是跟 update等语句一样强制去当前读的。 再讲第五个，Mysql 的索引？ 索引有很多，常见的就是聚集索引和非聚集索引，聚集索引就是元素的逻辑位置和物理位置保持一致，也称为主键索引，是B+树，因为支持范围查询，并且索引节点只有指针和索引，这样单个节点就能容纳更多的指针域，从而使得树的高度下降。 非聚集索引要注意减少回表的次数，常用的方法就是覆盖索引、使用最左前缀原则尽量减少索引的建立，同时注意可以使用索引下推来减少回表的次数。 好勒，数据库差不多了，最后讲一讲范式吧 mysql中常用的范式就是三大范式 第一范式：确保每列的原子性，比如说地址这一属性，有的业务可能对城市用的比较多，我们就就应该细分，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式：确保表中的每列都和主键相关，去除部分依赖。最典型的场景就是多对多的场景，比如订单-商品表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键，如果此时把商品信息也写在这个表中，就不符合第二范式了，这样会造成数据冗余，分表会更好。 第三范式：确保没有传递依赖，也就是每个非主属性都要依赖于主属性，例如订单表，和用户是一对多的关系，此时用户的id会是这个表的外键，如果在这里加上用户的名字，就违反了第三范式，也是会造成数据的冗余的。 参考: https://www.cnblogs.com/linjiqin/archive/2012/04/01/2428695.html 2020.3.28 15:30 - 16:30 「60 分钟」腾讯二面电话面。这一轮面试貌似没遇到什么大问题，问的问题也不难，没什么太大印象了… 只记得一个关于内存的。 GC roots 有哪些类型？ 就是那些确保存活的对象，例如 栈中本地变量表中引用的对象； native 方法栈中引用的对象； 方法区(non-heap)中的类静态属性引用的变量； 方法区(non-heap)中的常量引用的对象； 为什么要选定这些对象为 GC roots？ 因为 gc 的目的是回收那些不用的对象，这些对象可以确保需要用到，所以肯定就不能回收。 如何判断一个对象不可达？举个具体的例子？ 当一个对象到 GC roots 的对象没有任何引用链相连，这个对象就是不可用的，最典型的例子就是 123&gt; A a = new A();&gt; a = null;&gt; 此时 a 开始指向的对象，已经没有 GC roots的对象指向它了，所以它应该被标记为不可达，我们始终注意的是，回收的永远都是堆上的不可用对象，当然它也有自救的机会，就是 A 这个类重写 finalize() 方法，然后在里面加上 B.b = this，这里假设 b 是类 B 的一个静态变量，这样这个对象依旧不会被回收，因为有 GC roots 到它的引用链。 根搜索算法是如何去实现的呢？ HotSpot首先需要枚举所有的GC Roots根节点，虚拟机栈的空间不大，遍历一次的时间或许可以接受，但是方法区的空间很可能就有数百兆，遍历一次需要很久。更加关键的是，当我们遍历所有GC Roots根节点时，我们需要暂停所有用户线程，因为我们需要一个此时此刻的”虚拟机快照”，找到此时此刻的可达性分析关系图。基于这种情况，HotSpot实现了一种叫做OopMap的数据结构，存储GC Roots对象，同时并不是每个指令都会对OopMap进行修改，这样OopMap很庞大，这里Hotspot引入了安全点，safePoint，只会在Safe Point处记录GC Roots信息。这也就是 CMS 常说的初始阶段的第一步，先把 GC roots 找到，然后去标记那些与 GC roots 相关的对象，我个人认为在 gc roots 中会标记引用了他的对象的符号引用的位置，这样就能够实现找到 GC roots 相关的对象了，这个跟类加载过程的加载过程的第三个动作一样，在堆上 去 new 一个对象作为一个类的入口，那这个对象肯定是知道方法区中类的位置的，所以就是说引用了 gc roots的对象的引用链都会有记录。 上面说的虚拟机快照是什么东西？ 快照，就是存储在这个点上的所有vm的状态，包括内存和硬盘，当然也就包括了方法区中的 GC roots 根节点。 回收的过程是怎样的？ 这里以 cms 收集器为代表。 初始标记：先去判断对象的可达性，如果不可达，标记为 dead，然后看是否有继承 finalize 方法，没有的话直接标记为需要 gc，如果继承了看是否是第一次执行，是第一次执行把其标记为 alive，否则标记为 dead。 并发标记：safepoint到达之后，一边继续标记还可以一边让用户并行； 最终标记：在让用户程序并行的过程中，还会产生 gc 的对象，所以还需要再标记一下； 并发清除：多线程清除。 为什么要将对象分为新生代老年代 早期提出这个分代的思想我认为主要是解决 stop the world的时间长度，因为如果时间太长很影响用户体验，所以只去扫可能很快就死亡的那一些块区域是很正常的想法，而老年代由于逃过了很多次gc，说明他是有在被活跃使用的，所以我们在平时的gc的时候可以不去考虑他，这样就加快了gc的速度； 后期由于引入了并行机制，也就是用户进程和垃圾回收进程可以同步进行，并且可以多线程进行回收，那 stop the world 这个时间就不是重点了，重点就在于GC能够应付的应用内存分配速率(allocation rate)，也就是说 gc 的回收速度要跟上应用的分配内存的速度，所以这个时候，gc肯定是选择疯狂回收那种回收率很高的区域，其他回收率不高的肯定得等等。 所以，前者是从时间角度考虑，所以我们需要分代，因为扫小区域比扫大区域时间更短； 而后者则是从追赶用户分配内存的角度考虑，需要分代，这样扫的区域的回报率更高。 放一个链接，写的还是比较有条理的：https://allenwu.itscoder.com/java-gc 2020.3.29 15:00 - 17:30 「150 分钟」 腾讯三面视频面。这一面前面面的还算okay，但是最后代码撕了一个多小时，生死未卜了… 主要问题还是在计算机网络方面吧。 大端小端是什么？详细叙述一下？ 大端和小端是指数据在内存中的存储模式，它由 CPU 决定： 1) 大端模式（Big-endian）是指将数据的低位（比如 1234 中的 34 就是低位）放在内存的高地址上，而数据的高位（比如 1234 中的 12 就是高位）放在内存的低地址上。这种存储模式有点儿类似于把数据当作字符串顺序处理，地址由小到大增加，而数据从高位往低位存放。 2) 小端模式（Little-endian）是指将数据的低位放在内存的低地址上，而数据的高位放在内存的高地址上。这种存储模式将地址的高低和数据的大小结合起来，高地址存放数值较大的部分，低地址存放数值较小的部分，这和我们的思维习惯是一致，比较容易理解。 为什么有大小端模式之分? 计算机中的数据是以字节（Byte）为单位存储的，每个字节都有不同的地址。现代 CPU 的位数（可以理解为一次能处理的数据的位数）都超过了 8 位（一个字节），PC机、服务器的 CPU 基本都是 64 位的，嵌入式系统或单片机系统仍然在使用 32 位和 16 位的 CPU。 对于一次能处理多个字节的CPU，必然存在着如何安排多个字节的问题，也就是大端和小端模式。以 int 类型的 0x12345678 为例，它占用 4 个字节，如果是小端模式（Little-endian），那么在内存中的分布情况为（假设从地址 0x 4000 开始存放）： 内存地址 0x4000 0x4001 0x4002 0x4003存放内容 0x78 0x56 0x34 0x12 如果是大端模式（Big-endian），那么分布情况正好相反： 内存地址 0x4000 0x4001 0x4002 0x4003存放内容 0x12 0x34 0x56 0x78 我们的 PC 机上使用的是 X86 结构的 CPU，它是小端模式；51 单片机是大端模式；很多 ARM、DSP 也是小端模式（部分 ARM 处理器还可以由硬件来选择是大端模式还是小端模式）。 cookie 和 session的区别 cookie是为会话存储的键值信息，不可跨域名（只能拿到当前域名下的cookie，包含父级域名），有有效期限，是在客户端的浏览器保存。 session是基于内存的缓存技术，用来保存针对每个用户的会话数据，通过session ID 来区分用户，存储于服务器端。 浏览器在第一次请求时，无cookie，然后服务器收到请求后，创建一个 session，用sessionid 标识，将其放入cookie中，然后客户端以后请求都带上cookie，服务器端收到消息后，解析里面的sessionid即可。 java nio 中如何解决半包、粘包问题？ 对于粘包问题先读出包头即包体长度n，然后再读取长度为n的包内容，这样数据包之间的边界就清楚了。 对于半包问题先读出包头即包体长度n，由于此次读取的缓存池长度小于n，这时候就需要先缓存这部分的内容，等待下次read事件来时拼接起来形成完整的数据包。 数据库中 varchar 和 char 的区别是什么？ 字符串数据类型 MySQL数据类型 含义 char(n) 固定长度，申请的长度就是最终的长度，类似于静态数组，英文占一个字节，汉字占两个字节 varchar(n) 可变长度，类似于可变数组—列表，英文和汉字都占两个字节，实际长度是它的值的实际长度+1 text 存储可变长度的非Unicode数据，最大长度为2^31-1个字符。text列不能有默认值，存储或检索过程中，不存在大小写转换，后面如果指定长度，不会报错误，但是这个长度是不起作用的，意思就是你插入数据的时候，超过你指定的长度还是可以正常插入。 经常变化的字段用varchar； 知道固定长度的用char； 尽量用 varchar； 超过255字节的只能用varchar或者text； 能用varchar的地方不用text； 超长的，例如存储整个html用text。 然后就一直在聊项目了最后出了一道算法题，就是 LeetCode 上的求下一个排列数的变体题，我竟然…紧张了… 2020.3.30 17:24 - 18:30 「66 分钟」 阿里二面电话面。这一面，项目为主，基础知识为辅，重点是需要将自己做的项目用比较清楚的话语表述清楚，让面试官能够最短时间了解到你做的项目，同时切忌注意，自己不了解不深入的知识点尽量不要提及，这是大忌。 当将业务水平分库后，转账业务如何保证事务的一致性？ 这是典型的分布式事务，理论有 CAP 「C (一致性)，A (可用性)，P (分区容错性)，只能选择 AP or CP」，BASE「Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展」 常见的分布式解决方案： 2PC，两阶段提交，事务管理器来协调，全部okay了才okay，这样效率很低，因为是如果没成功就一直阻塞。 TCC（Try-Confirm-Cancel）最大努力交付，在更新多个资源时，将多个资源的提交尽量延后到最后一刻处理，这样的话，如果业务流程出现问题，则所有的资源更新都可以回滚，事务仍然保持一致。唯一可能出现问题的情况是在提交多个资源时发生了系统问题，比如网络问题等，但是这种情况是非常罕见的，一旦出现这种情况，就需要进行实时补偿，将已提交的事务进行回滚。 事务补偿机制。在数据库分库分表后，如果涉及的多个更新操作在某一个数据库范围内完成，则可以使用数据库内的本地事务保证一致性；对于跨库的多个操作，可通过补偿和重试，使其在一定的时间窗口内完成操作，这样就可以实现事务的最终一致性，突破事务遇到问题就滚回的传统思路。 参考：https://juejin.im/post/5b5a0bf9f265da0f6523913b#heading-16 https://cloud.tencent.com/developer/news/200316 说实话…还没看太懂… zookeeper中的两阶段提交是怎么去做的？ CAP理论中，zookeeper就是CP，放弃可用性，追求一致性和分区容错性，追求的是强一致。 在项目中假如一级调度器挂了，怎么处理？ 我说没处理…正常应该是类似于 Mysql 一样有个主备切换的机制。 SpringBoot中的 AOP 分为几类， AOP 主要是两种方式，一种是直接通过 JDK 动态代理，一种是通过cglib。 先来回顾一下 Spring 中 AOP 的流程： 代理的创建。 需要创建代理工厂，代理工厂需要 3 个重要的信息：拦截器数组，目标对象接口数组，目标对象。 创建代理工厂时，默认会在拦截器数组尾部再增加一个默认拦截器 —— 用于最终的调用目标方法。 当调用 getProxy 方法的时候，会根据接口数量大余 0 条件返回一个代理对象（JDK or Cglib）。 注意：创建代理对象时，同时会创建一个外层拦截器，这个拦截器就是 Spring 内核的拦截器。用于控制整个 AOP 的流程。 代理的调用 当对代理对象进行调用时，就会触发外层拦截器。 外层拦截器根据代理配置信息，创建内层拦截器链。创建的过程中，会根据表达式判断当前拦截是否匹配这个拦截器。而这个拦截器链设计模式就是职责链模式。 当整个链条执行到最后时，就会触发创建代理时那个尾部的默认拦截器，从而调用目标方法。最后返回。 参考：https://www.jianshu.com/p/e18fd44964eb 讲讲 cglib 如何使用并实现的？ 动态代理再熟悉不过了，是只能代理接口，cglib现在我们来具体看一下。 我们知道，动态代理是代理类实现被代理类的接口，而cglib则是代理类继承被代理类，也就是子类增强父类的手段。cglib其实也就是字节码增强类库。 具体如何使用 cglib：https://zhuanlan.zhihu.com/p/37886319 动态代理和cglib的区别？ 一个是代理类实现接口，一个是代理类继承类，我觉得差不太多。 动态代理用到的接口有 InnvocationHandler，通过实现其 invoke 方法增强方法，并且通过 Proxy.newInstace() 实现代理过程。而cglib则使用 MethodInterceptor 接口，通过实现其 intercept() 方法增强方法，并且通过 Enhancer.create() 方法实现代理过程。 下面我放一下自己写的动态代理和 cglib 的实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// 动态代理，总共有四个类// 1. 接口package AOP.Proxy;public interface Person &#123; void play(); void dance();&#125;// 2. 实现类，也就是要被代理的类package AOP.Proxy;public class Universities implements Person &#123; @Override public void play() &#123; System.out.println("I like play computer"); &#125; @Override public void dance() &#123; System.out.println("I like dance"); &#125;&#125;// 3. 实现 InnocationHandler 接口，完成代理的任务package AOP.Proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class Dynamic implements InvocationHandler &#123; // 整容的人是谁，我得知道 private Object obj; public Dynamic(Object obj)&#123; this.obj = obj; &#125; // 整容的过程 public Object myDynamic()&#123; return Proxy.newProxyInstance(this.obj.getClass().getClassLoader(),this.obj.getClass().getInterfaces(),this); &#125; // 整容的地方交代清楚 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("开始为" + method.getName() + "方法进行代理"); Object result = method.invoke(obj,args); System.out.println("结束" + method.getName() + "方法的代理"); return result; &#125;&#125;// 4. 测试类// 这里我采用了两种方法测试，一种是直接在 test 类中写出整容的过程// 另外一种是直接调用我在 Dynamic 已经包装好整容过程的方法，建议使用这种// 因为这样代码就少了，下面 cglib 就是采用方法二哈package AOP.Proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class test &#123; public static void main(String[] args) &#123; // 相当于做生意的过程，比如我去医院整容，首先我要确保我有钱(也就是有接口) Universities person = new Universities(); // 其次，我得去医院找到相应的医生，也就是把我想代理的对象，即我本人，告知给医生 // 医生肯定得有能整容的技术，那就是得继承 InvocationHandler InvocationHandler dynamic_person = new Dynamic(person); // 进行交易的过程，一手交钱一手交换，医生拿到钱，会返回一个有钱的处理好的美女，也就是代理完成了 // 这里必须强调代理返回的是 接口对象，也就是医生只会对有钱人进行代理，没钱的代理就失败了 // 如果最开始我没钱，我就去找医生了，那在这一步交易的过程就会出错，因为医生只会处理有钱人，并且返回有钱人的代理好的对象 // 有钱人得到代理后的对象，就可以为所欲为了 Person pp = (Person) Proxy.newProxyInstance(person.getClass().getClassLoader(),person.getClass().getInterfaces(),dynamic_person); pp.dance(); System.out.println("-----------"); Person pp2 = (Person) new Dynamic(person).myDynamic(); pp2.dance(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// cglib 实现//1. 导包，在pom.xml 导包 &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib-nodep&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; // 2. 需要被代理的类package AOP.Cglib;public class SomeService &#123; public String doFirst() &#123; System.out.println("执行doFirst()方法"); return "abc"; &#125; public void doSecond() &#123; System.out.println("doSecond()方法"); &#125;&#125;// 3. 进行代理过程package AOP.Cglib;import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;public class CglibFactory implements MethodInterceptor &#123; private Object object; public CglibFactory(Object object) &#123; this.object = object; &#125; public Object myCglibCreator() &#123; Enhancer enhancer = new Enhancer(); //将目标类设置为父类，cglib动态代理增强的原理就是子类增强父类,cglib不能增强目标类为final的类 //因为final类不能有子类 enhancer.setSuperclass(this.object.getClass()); //设置回调接口,这里的MethodInterceptor实现类回调接口，而我们又实现了MethodInterceptor,其实 //这里的回调接口就是本类对象,调用的方法其实就是intercept()方法 enhancer.setCallback(this); //create()方法用于创建cglib动态代理对象 return enhancer.create(); &#125; //回调接口的方法 //回调接口的方法执行的条件是：代理对象执行目标方法时会调用回调接口的方法 @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; Object result = methodProxy.invokeSuper(o, objects); //这里实现将返回值字符串变为大写的逻辑 if(result != null) &#123; result = ((String) result).toUpperCase(); &#125; return result; &#125;&#125;// 4. 测试package AOP.Cglib;public class Test &#123; public static void main(String[] args) &#123; SomeService target = new SomeService(); SomeService proxy = (SomeService) new CglibFactory(target).myCglibCreator(); String result = proxy.doFirst(); System.out.println(result); proxy.doSecond(); &#125;&#125; Spring中 Ioc 和 DI 讲一下？ 参考：https://juejin.im/post/5df5bab0e51d45582427104e https://www.jianshu.com/p/17b66e6390fd IoC(Inversion of Control 控制反转)：是一种面向对象编程中的一种设计原则，用来减低计算机代码之间的耦合度。其基本思想是：借助于“第三方”实现具有依赖关系的对象之间的解耦。 DI(Dependence Injection 依赖注入)：将实例变量传入到一个对象中去(Dependency injection means giving an object its instance variables)。 也就是说 DI 是 Ioc 的 实现，Ioc 是一种设计原则。 Spring 作者 Rod Johnson 设计了两个接口用以表示容器。 BeanFactory ApplicationContext BeanFactory 粗暴简单，可以理解为就是个 HashMap，Key 是 BeanName，Value 是 Bean 实例。通常只提供注册（put），获取（get）这两个功能。我们可以称之为 “低级容器”。 ApplicationContext 可以称之为 “高级容器”。因为他比 BeanFactory 多了更多的功能。他继承了多个接口。因此具备了更多的功能。例如资源的获取，支持多种消息（例如 JSP tag 的支持），对 BeanFactory 多了工具级别的支持等待。所以你看他的名字，已经不是 BeanFactory 之类的工厂了，而是 “应用上下文”， 代表着整个大容器的所有功能。该接口定义了一个 refresh 方法，此方法是所有阅读 Spring 源码的人的最熟悉的方法，用于刷新整个容器，即重新加载/刷新所有的 bean。 Ioc 的过程： a. 加载配置文件，解析成 BeanDefinition 放在 Map 里。 b. 调用 getBean 的时候，从 BeanDefinition 所属的 Map 里，拿出 Class 对象进行实例化，同时，如果有依赖关系，将递归调用 getBean 方法 —— 完成依赖注入。 ORM框架有哪些？有什么好处？什么是mysql注入？$ 和 # 有什么区别 JPA是orm框架标准，主流的orm框架都实现了这个标准。 MyBatis没有实现JPA，他和orm框架的设计思路完全不一样。MyBatis是拥抱sql，而orm则更靠近面向对象，不建议写sql，实在要写推荐你写hql代替。 Mybatis是sql mapping框架而不是orm框架，当然orm和Mybatis都是持久层框架。 所以说hibernate是典型的ORM框架，好处就是我们可以用面向对象的思想去对数据库进行操作，我觉得主要就是比较省代码，解决面向对象的设计方式和关系型数据库之间的关联，Java主要面向对象设计，因此在分析业务的时候会以对象的角度来看待问题。然而数据库是关系型的，对于Java程序员而言是不符合面向对象设计的，因此才会出现ORM这种东西。有了ORM，Java开发人员在整个代码设计都将遵循对象的思维模式，这就是好处。 mysql注入：参数进行转义与过滤 如何解决：使用 Prepare Statement $ 和 # 的区别： Sql: delete from student where name=${name} 假如 name = jerome OR 1 = 1 在 Mybatis 中，如果写 ${name}，那就是直接将 name 拼接到 sql 中，结局就是全删； 如果写 #{name}，则 sql 变成 delete from student where name= ’jerome OR 1 = 1‘，这样只有name = jerome OR 1 = 1 的才会删除，否则不会删除任何东西。 2020.3.31 20:02 - 20:45 「43 分钟」 阿里三面从这一面开始面试官就是阿里 P9 了，所以说实话压力还是非常大的，这一面还是跟前面一样是电话面。这一面基本上没有问任何基础，全部在讲项目，建议大家一定要有一个讲的很溜的项目，我因为提及了一下毕设，然后就被一直抓着问，而自己本身其实是没有好好准备这个项目的，所以有些问题竟然被问了后，答得不是特别理想。但是还好，答得也没有很差，后面的项目介绍的还是让面试官很满意的。 介绍完后，面试官可能觉得才半小时，于是就问了几个问题，只是探寻一下我知识的广度吧，全部没有深入。 例如： 了解 tomcat 吗？「当然了这是我在讲双亲委派模型引申出来的」 用过 nginx 吗？ spring 中用过吗？ 这些，我因为说不太了解 or 用过没深入，所以面试官也就索性没有问下去。 最后，花了10分钟介绍了下自己的部门，然后就到了反转环节了，我觉得大家可以好好抓住反转环节，因为我们往往可以从这个环节探到面试官对本次面试的看法，比如，我问的第一个问题就是： ”我觉得今天的表现不是很好，第一个项目没讲的非常清楚，您后面问的几个知识点我也不太会，没深入了解过。“ 很让我意外的是，面试官给我的回答让我备受鼓舞： ”前面可能是你太着急太紧张了，所以讲的不是很好，但是后面讲的很清楚了，也能听得出来全是自己用心做了的，我也听明白了，所以不用担心，至于后面的知识点不会也非常正常，你还年轻的很，不会是非常正常的，你要都会了我还需要在这吗？你还年轻，是非常有潜力的。“ 至此，开始期待四面哈哈哈哈。 2020.4.01 23:04 - 00:05 「61 分钟」 阿里四面在四面之前，出现了个小插曲。就是面试官上午估计是有跟我打过电话，但是貌似跟饿了么的外卖员跟我打电话冲突了「应该都是走的阿里的电话系统」，导致我压根没接到电话「外卖员还说给我打电话我一直不接，我说我压根没接到电话…」，最后还是下午5点的时候二面的面试官打电话给我叫我注意一下电话…我当时一脸懵逼…所以说关键时刻不要点饿了么外卖……. 然后约的 8 点半，但是我因为有携程的笔试推到了晚上 10 点半，然后 10 点我就开始坐在桌前等，说真的有点紧张，毕竟这面是交叉面，又是一个 P9 大佬…等啊等，一直等到晚上 11 点面试官都没有出现，我一度怀疑是我听错了时间，突然，11:04，面试官非常抱歉的打开时视频，终于看到了…于是面试正式开始… 视频面。有点太晚了，所以自己在面之前也是有跟面试官交代，可能反应会慢一些…面试官还是非常理解的，然后这一面可以说是这半个多月来面的最棒的一次了，下面是主要内容： 面试依旧是从项目出发，注意，阿里后面四面基本上全部围绕项目展开，所以务必要有一个吃透的项目，了解其中用到的技术栈，并且延伸到操作系统级别的调用，同时要关注项目中遇到的难点以及自己是如何解决的，这是最最最高频的问题了，基本上每一面的面试官都有问这个问题。 同时，一定要将项目中的某些部门转移到自己擅长的领域，比如我讲的项目中其实就是一个爬虫项目，但是可以扩展的地方太多了，我就举几个例子： 讲到爬虫，那务必要讲反爬措施，这是一个可以展开的点，可以讲很久，讲反爬如何解决； 中间件用到kafka，这又是一大块可以讲的，kafka 的通信机制，以及内部构造，以及高可用机制和吞吐量大； 谈到 kafka 的吞吐量大，又可以总结一波为何吞吐量大，然后延伸到 NIO 和 零拷贝技术，又可以讲一堆； 讲到 NIO，又可以讲 IO 那一大块知识点，同时谈到 NIO 必定会谈到 select/epoll，这样又可以讲很多。 总之，这些知识，自己都是可以提前准备好的，然后到时候面试抛出来就行了，面试官一般都会顺着你的思路进行下去的，要把话语权掌握在自己的手上，一定要记得特意的抛出某些知识点「这里的特意是指跟你的项目挂钩的」 同时，我发现从三面到四面，都有去关心，自己在项目中用到的设计模式以及自己看源码发现的一些设计模式。 我之前只准备了三种设计模式，并且自己亲自写了一些小demo： 单例模式，四面就问了这个，谈什么是饿汉模式和懒汉模式； 12345678910&gt; // 饿汉&gt; public class Singleton &#123;&gt; // 类加载时就初始化&gt; private static final Singleton instance = new Singleton();&gt; private Singleton() &#123;&#125;&gt; public static Singleton getInstance()&#123;&gt; return instance;&gt; &#125;&gt; &#125;&gt; 123456789101112&gt; // 懒汉&gt; public class Singleton &#123;&gt; private static Singleton instance;&gt; private Singleton () &#123;&#125;&gt; public static Singleton getInstance() &#123;&gt; if (instance == null) &#123;&gt; instance = new Singleton();&gt; &#125;&gt; return instance;&gt; &#125;&gt; &#125;&gt; 模板方法模式，这个是IO延伸过来的知识点，然后我顺带提了一下 AQS； IO 和 AQS 都用了模板方法的设计模式。 代理模式，提了一下动态代理，以及Spring中的 AOP。 上面已经总结了 然后就是一些比较简单的问题： 为何要分新生代和老年代，如何区分的？ 这个我在《深入理解java虚拟机笔记》中有非常详细的谈到，并且我在 “腾讯二面”这部分也有很详细的总结！可以往上翻翻看到。 Full gc 如何排查？ 先看一下相关的图形化界面，看每一次 full gc后对象的存活率 如果存活率高，可能是老年代的内存太小了，导致很容易触发 full gc； 如果存活率低，说明可能是很多大对象进入到老年代区域，主要是可能自己代码写的有点问题，或者说明可能是 eden 区太小了，导致年轻代很容易就进入了老年代区域。 讲一讲 synchronized 和 Lock 的区别，以及 ThreadLocal？ Synchronized 和 lock 最典型的几个区别： synchronized 关键字，jvm层面；lock 是类，jdk层面； synchronized 不支持中断，lock支持； synchronized如果阻塞了会一直等待，而lock可以定时取消等待； synchronized只能有一个条件队列，而lock可以有多个。 ThreadLocal 主要谈到 ThreadLocalMap，注意它是线性探测法解决的冲突，并且加载因子为2/3，初始值16，当然还得谈到其内存泄漏问题，解决方案就是使用弱引用以及用一些方法处理脏 Entry ，当然并不能完全避免内存泄漏，但是只要你不和线程池搞在一块就应该不会产生内存泄漏。 网络中的加密算法有了解吗？ 必须了解。 从https「上面已经讲过了」，到 RSA 加密算法，再到对称非对称加密算法，再到 token 等等。 讲讲非对称和对称加密算法「这块我讲了md5算是，很明显面试官说不是」 严格来说：MD5、sha-1只是散列算法，或者叫摘要算法，不能算加密算法。 也就是说，MD5 是不可逆的，你根本解不了，而对称非对称加密算法是可逆的，可以通过密文得到明文，也可以通过明文得到密文。 加密对应解密，即加密后的密文可以解密成明文，但是MD5无法从密文（散列值）反过来得到原文，即没有解密算法。 大家知道加密算法分为对称加密和非对称加密，不管对称加密和非对称加密，都是能够从密文解密得到明文的。从这点上讲MD5不是加密算法，更谈不上属于对称加密、非对称加密。所以不要再讨论MD5是属于对称加密、非对称加密了，MD5既不属于对称加密也不属于非对称加密，MD5根本就没法解密，也没有秘钥（加盐并不是秘钥），所以可以认为MD5不属于加密算法。 一些人认为MD5处理后看不到原文，即已经将原文加密，所以认为MD5属于加密算法。如果这么看的，那么求余也可以算加密算法了。 作者：GeCoder链接：https://www.zhihu.com/question/68735830/answer/327762693来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 所以，md5 并不是加密算法，那典型的加密算法有哪些呢？ 加密技术通常分为两大类:”对称式”和”非对称式”。 对称性加密算法：对称式加密就是加密和解密使用同一个密钥。信息接收双方都需事先知道密匙和加解密算法且其密匙是相同的，之后便是对数据进行加解密了。对称加密算法用来对敏感数据等信息进行加密。 非对称算法：非对称式加密就是加密和解密所使用的不是同一个密钥，通常有两个密钥，称为”公钥”和”私钥”，它们两个必需配对使用，否则不能打开加密文件。发送双方A,B事先均生成一堆密匙，然后A将自己的公有密匙发送给B，B将自己的公有密匙发送给A，如果A要给B发送消息，则先需要用B的公有密匙进行消息加密，然后发送给B端，此时B端再用自己的私有密匙进行消息解密，B向A发送消息时为同样的道理。 散列算法：散列算法，又称哈希函数，是一种单向加密算法。在信息安全技术中，经常需要验证消息的完整性，散列(Hash)函数提供了这一服务，它对不同长度的输入消息，产生固定长度的输出。这个固定长度的输出称为原输入消息的”散列”或”消息摘要”(Message digest)。散列算法不算加密算法，因为其结果是不可逆的，既然是不可逆的，那么当然不是用来加密的，而是签名。 对称性加密算法有：AES、DES、3DES用途：对称加密算法用来对敏感数据等信息进行加密 DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合。 3DES（Triple DES）：是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高。 AES（Advanced Encryption Standard）：高级加密标准，是下一代的加密算法标准，速度快，安全级别高；AES是一个使用128为分组块的分组加密算法，分组块和128、192或256位的密钥一起作为输入，对4×4的字节数组上进行操作。众所周之AES是种十分高效的算法，尤其在8位架构中，这源于它面向字节的设计。AES 适用于8位的小型单片机或者普通的32位微处理器,并且适合用专门的硬件实现，硬件实现能够使其吞吐量（每秒可以到达的加密/解密bit数）达到十亿量级。同样，其也适用于RFID系统。 非对称性算法有：RSA、DSA、ECC RSA：由 RSA 公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的。RSA在国外早已进入实用阶段，已研制出多种高速的RSA的专用芯片。 DSA（Digital Signature Algorithm）：数字签名算法，是一种标准的 DSS（数字签名标准），严格来说不算加密算法。 ECC（Elliptic Curves Cryptography）：椭圆曲线密码编码学。ECC和RSA相比，具有多方面的绝对优势，主要有：抗攻击性强。相同的密钥长度，其抗攻击性要强很多倍。计算量小，处理速度快。ECC总的速度比RSA、DSA要快得多。存储空间占用小。ECC的密钥尺寸和系统参数与RSA、DSA相比要小得多，意味着它所占的存贮空间要小得多。这对于加密算法在IC卡上的应用具有特别重要的意义。带宽要求低。当对长消息进行加解密时，三类密码系统有相同的带宽要求，但应用于短消息时ECC带宽要求却低得多。带宽要求低使ECC在无线网络领域具有广泛的应用前景。 散列算法（签名算法）有：MD5、SHA1、HMAC用途：主要用于验证，防止信息被修。具体用途如：文件校验、数字签名、鉴权协议 MD5：MD5是一种不可逆的加密算法，目前是最牢靠的加密算法之一，尚没有能够逆运算的程序被开发出来，它对应任何字符串都可以加密成一段唯一的固定长度的代码。 SHA1：是由NISTNSA设计为同DSA一起使用的，它对长度小于264的输入，产生长度为160bit的散列值，因此抗穷举(brute-force)性更好。SHA-1设计时基于和MD4相同原理,并且模仿了该算法。SHA-1是由美国标准技术局（NIST）颁布的国家标准，是一种应用最为广泛的Hash函数算法，也是目前最先进的加密技术，被政府部门和私营业主用来处理敏感的信息。而SHA-1基于MD5，MD5又基于MD4。 HMAC：是密钥相关的哈希运算消息认证码（Hash-based Message Authentication Code）,HMAC运算利用哈希算法，以一个密钥和一个消息为输入，生成一个消息摘要作为输出。也就是说HMAC是需要一个密钥的。所以，HMAC_SHA1也是需要一个密钥的，而SHA1不需要。 其他常用算法： Base64：其实不是安全领域下的加密解密算法，只能算是一个编码算法，通常用于把二进制数据编码为可写的字符形式的数据，对数据内容进行编码来适合传输(可以对img图像编码用于传输)。这是一种可逆的编码方式。 HTTPS（全称：Hypertext Transfer Protocol over Secure Socket Layer），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL(SSL使用40 位关键字作为RC4流加密算法，这对于商业信息的加密是合适的。)，因此加密的详细内容就需要SSL。https:URL表明它使用了HTTP，但HTTPS存在不同于HTTP的默认端口及一个加密/身份验证层（在HTTP与TCP之间），提供了身份验证与加密通讯方法，现在它被广泛用于万维网上安全敏感的通讯，例如交易支付方面。它的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。Https 就可以使用上面的对称加密算法和非对称加密算法进行消息的加密。 项目应用总结： 加密算法是可逆的，用来对敏感数据进行保护。散列算法(签名算法、哈希算法)是不可逆的，主要用于身份验证。 对称加密算法使用同一个密匙加密和解密，速度快，适合给大量数据加密。对称加密客户端和服务端使用同一个密匙，存在被抓包破解的风险。 非对称加密算法使用公钥加密，私钥解密，私钥签名，公钥验签。安全性比对称加密高，但速度较慢。非对称加密使用两个密匙，服务端和客户端密匙不一样，私钥放在服务端，黑客一般是拿不到的，安全性高。 Base64不是安全领域下的加解密算法，只是一个编码算法，通常用于把二进制数据编码为可写的字符形式的数据，特别适合在http，mime协议下的网络快速传输数据。UTF-8和GBK中文的Base64编码结果是不同的。采用Base64编码不仅比较简短，同时也具有不可读性，即所编码的数据不会被人用肉眼所直接看到，但这种方式很初级，很简单。Base64可以对图片文件进行编码传输。 https协议广泛用于万维网上安全敏感的通讯，例如交易支付方面。它的主要作用可以分为两种：一种是建立一个信息安全通道，来保证数据传输的安全；另一种就是确认网站的真实性。 大量数据加密建议采用对称加密算法，提高加解密速度；小量的机密数据，可以采用非对称加密算法。在实际的操作过程中，我们通常采用的方式是：采用非对称加密算法管理对称算法的密钥，然后用对称加密算法加密数据，这样我们就集成了两类加密算法的优点，既实现了加密速度快的优点，又实现了安全方便管理密钥的优点。 MD5标准密钥长度128位（128位是指二进制位。二进制太长，所以一般都改写成16进制，每一位16进制数可以代替4位二进制数，所以128位二进制数写成16进制就变成了128/4=32位。16位加密就是从32位MD5散列中把中间16位提取出来）；sha1标准密钥长度60位(比MD5摘要长32位)，Base64转换后的字符串理论上将要比原来的长1/3。 以上内容来自于：https://www.cnblogs.com/sochishun/p/7028056.html 个人理解的对称加密算法的过程： A 与 B 商定好加密算法，然后 A 如果给 B 发消息，会先运用加密算法，进行加密，B 收到后，会运用逆加密算法，进行解密，也就是其密钥是一样的。 个人理解的非对称加密算法的过程： A 和 B 各自拥有一对公钥和私钥，也就是说二者发送消息的加密算法可以互不相同，且对方可以不用知道。例如 A 要给 B 发消息，要拿到 B 的公钥，然后进行加密即可，B 收到后用自己的私钥就可以解开了。 介绍BIO、NIO、AIO 这个简单…详细的叙述见我 《零拷贝及其周边》 BIO：普通的同步阻塞 I/O NIO：同步非阻塞 I/O AIO ：异步非阻塞 I/O 虚拟内存 这个是知识盲区，需要好好科普一下。 我的天啊！！！！自己的《零拷贝及其周边》早就介绍过虚拟内存，面试的时候竟然没有想起来，还知识盲区呢？？？真的是智障啊啊啊啊。。。。。这个没答上来真的太可惜了，不过还好面试官没有深究… 虚拟内存 虚拟内存是计算机系统内存管理的一种技术。 它使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间）。而实际上，虚拟内存通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换，加载到物理内存中来。 目前，大多数操作系统都使用了虚拟内存，如 Windows 系统的虚拟内存、Linux 系统的交换空间等等。 离开进程谈虚拟内存没有任何意义，不同进程里的同一个虚拟地址指向的物理地址是不一样的。每个用户进程维护了一个单独的页表（Page Table），虚拟内存和物理内存就是通过这个页表实现地址空间的映射的，页表（Page Table）里面的数据由操作系统维护。 引入虚拟内存的好处 在进程和物理内存之间，加了一层虚拟内存的概念，好处有： 提供更大的地址空间，因为虚拟内存还可以放在磁盘上或者寄存器中，而物理内存并不行，而且虚拟地址空间是连续的，我们不需要操心具体是如何存放的，操作系统会帮我们映射好； 安全性更好，虚拟内存设有读写属性，并且不同进程互不影响； 可以懒加载，只有在需要读相应的文件的时候，才将它真正的从磁盘上加载到内存中来，而在内存吃紧的时候又可以将这部分内存清空掉，提高物理内存利用效率，并且所有这些对应用程序是都透明的； 可以共享内存，动态库只需要在内存中存一份就够了，然后将它映射到不同进程的虚拟地址空间中，让进程觉得自己独占了这个文件。进程间的内存共享也可以通过映射同一块物理内存到进程的不同虚拟地址空间来实现共享。 如果想了解更底层的话，戳链接：https://sylvanassun.github.io/2017/10/29/2017-10-29-virtual_memory/#comments 话说最拿手的数据库竟然一点都没问，感觉有点意犹未尽哈哈哈哈哈… 2020.4.06 17:30 - 17:45 「15 分钟」 腾讯 hr 面 自我介绍 项目中遇到的最大的困难以及如何解决的 项目中如何分工 未来 3-5 年的规划 优缺点 现在面试了几家公司，各自具体流程是什么 如果给你发offer，你会怎么选 2020.4.08 14:30 - 15:00 「30 分钟」阿里 hr 面就是正常的聊天 自我介绍 做的项目讲一下，在里面的职责 然后就是介绍我要去的部门和架构组了 等offer，希望一切好运！！！ 祝大家都能够找到心仪的实习吧！！加油各位！！！]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>实习</tag>
        <tag>校招</tag>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅入浅出 NIO]]></title>
    <url>%2F2020%2F03%2F15%2F%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA%20NIO.html</url>
    <content type="text"><![CDATA[引子本文从最基本的 IO 出发，引出零拷贝技术，继而发散到用到零拷贝技术的 NIO，谈及 NIO，就必须谈一谈著名的网络通信框架 Netty 了，而 Netty 又是 metaQ、HSF等熟知技术栈的基础，本文旨在以 NIO 为核心，辐射到其相关的知识点。 Zero Copy 这一部分基本上参考：深入剖析Linux IO原理和几种零拷贝机制的实现 预读知识物理内存和虚拟内存物理内存物理内存指通过物理内存条而获得的内存空间。 虚拟内存虚拟内存是计算机系统内存管理的一种技术。 它使得应用程序认为它拥有连续的可用的内存（一个连续完整的地址空间）。而实际上，虚拟内存通常是被分隔成多个物理内存碎片，还有部分暂时存储在外部磁盘存储器上，在需要时进行数据交换，加载到物理内存中来。 目前，大多数操作系统都使用了虚拟内存，如 Windows 系统的虚拟内存、Linux 系统的交换空间等等。 离开进程谈虚拟内存没有任何意义，不同进程里的同一个虚拟地址指向的物理地址是不一样的。每个用户进程维护了一个单独的页表（Page Table），虚拟内存和物理内存就是通过这个页表实现地址空间的映射的，页表（Page Table）里面的数据由操作系统维护。 引入虚拟内存的好处在进程和物理内存之间，加了一层虚拟内存的概念，好处有： 提供更大的地址空间，因为虚拟内存还可以放在磁盘上或者寄存器中，而物理内存并不行，而且虚拟地址空间是连续的，我们不需要操心具体是如何存放的，操作系统会帮我们映射好； 安全性更好，虚拟内存设有读写属性，并且不同进程互不影响； 可以懒加载，只有在需要读相应的文件的时候，才将它真正的从磁盘上加载到内存中来，而在内存吃紧的时候又可以将这部分内存清空掉，提高物理内存利用效率，并且所有这些对应用程序是都透明的； 可以共享内存，动态库只需要在内存中存一份就够了，然后将它映射到不同进程的虚拟地址空间中，让进程觉得自己独占了这个文件。进程间的内存共享也可以通过映射同一块物理内存到进程的不同虚拟地址空间来实现共享。 Tip：我们后文讲的，都是虚拟内存哦。 内核空间和用户空间为了避免用户直接操作内核「可以操作一切，牛逼得很」，保证内核安全，所以将虚拟内存划分为用户空间和内核空间，进程在访问到这两个空间的时候需要进行状态的转变（内核态、用户态）。 像我们在 jvm 中谈到的，堆、栈、方法区等等都是默认是用户空间，因为我们可以直接访问的到。 内核态 &amp; 用户态内核态可以执行任意命令，调用系统的一切资源，而用户态只能执行简单的运算，不能直接调用系统资源。用户态必须通过系统接口（System Call），才能向内核发出指令。比如，当用户进程启动一个 bash 时，它会通过 getpid() 对内核的 pid 服务发起系统调用，获取当前用户进程的 ID；当用户进程通过 cat 命令查看主机配置时，它会对内核的文件子系统发起系统调用。 I/O 读写方式CPU 轮询CPU 对 I/O 端口进行不断地检测，直到数据准备好了，这个就很吃 cpu，pass！ I/O 中断在 DMA 技术出现之前，应用程序与磁盘之间的 I/O 操作都是通过 CPU 的中断完成的。每次用户进程读取磁盘数据时，都需要 CPU 中断，然后发起 I/O 请求等待数据读取和拷贝完成，等数据准备完毕之后，发起 I/O 中断信号，提醒 CPU 数据已经准备好了。 用户进程向 CPU 发起 read 系统调用读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回。 CPU 在接收到指令以后对磁盘发起 I/O 请求，将磁盘数据先放入磁盘控制器缓冲区。 数据准备完成以后，磁盘向 CPU 发起 I/O 中断。 CPU 收到 I/O 中断以后将磁盘缓冲区中的数据拷贝到内核缓冲区，然后再从内核缓冲区拷贝到用户缓冲区。 用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时间钟。 DMA传输原理DMA（Direct Memory Access），又称直接内存存取，也就是通过硬件直接访问主内存，不需要通过 cpu，有点像协处理器，目前大多数的硬件都支持 DMA，例如网卡、显卡、声卡、磁盘等等。 有了 DMA，CPU就不用去处理将数据从磁盘缓冲区读到内核缓冲区「这个过程是很耗时的」了，直接交由 DMA 操作就行了。 建立在 DMA 传输上的读写的一般步骤在 Linux 系统中，传统的访问方式是通过 write() 和 read() 两个系统调用实现的，通过 read() 函数读取文件到到缓存区中，然后通过 write() 方法把缓存中的数据输出到网络端口。 具体步骤如下： 用户空间调用 read() 方法，会触发系统调用，进程由用户态转为内核态； 然后 DMA 会将磁盘缓冲区的数据加载到内核缓冲区中； CPU 会将内核缓冲区的数据 COPY 到用户缓冲区中，此时进程由内核态转为用户态； 然后用户空间调用 write() 方法，会触发系统调用，CPU 会将用户缓冲区的数据 COPY 到 Socket 缓冲区中「在内核空间中」，此时进程由用户态转成内核态； 然后 Socket缓冲区中的数据会通过 DMA 的方式 COPY到网卡处，传送完成； 此时 CPU 会再次由内核态切回到用户态，方便下一次的工作。 由此可知，一共有 4 次拷贝过程，其中 2 次 CPU 参与工作， 2 次DMA参与工作，最要命的是有 4 次的上下文切换。 自此，我们的预读知识告一段落，这里我们已经知道了普通的读写过程，切换上下文的次数和拷贝数据的次数很多，所以我们急需改进，由此，零拷贝技术运应而生。 零拷贝技术零拷贝技术是指在计算机执行操作时，减少 CPU 将数据从一个内存区域复制到另一个内存区域的次数。 零拷贝带来的显而易见的好处就是可以减少上下文切换「用户空间和内核空间之间切换导致的内核态和用户态之间的切换」以及 CPU 的拷贝次数。 实现零拷贝用到的最主要技术是 DMA 数据传输技术和 内存区域映射技术。由此，我们介绍四种零拷贝的实现思路： 用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。 mmap + write：直接将内核空间中的内核缓冲区和用户空间中的用户缓冲区做一个地址映射，这样就省去了在 read 过程中将内核缓冲区的数据 COPY 到用户缓冲区中。很适合文件的修改读写。 sendfile：不仅将 CPU 参与的拷贝次数减少了，而且直接减少了上下文切换的次数，因为在用户缓冲区根本就不会有数据，当然坏处也很明显，就是无法修改数据，只能进行传输，所以很适合大文件的传送。 当然了，还会讲到 sendfile 的升级版。 splice：就是在内核缓冲区和Socket缓冲区之间建立管道。 用户态直接I/O进程直接在用户态访问硬件设备「这个我也不知道是怎么做到的…」，这类进程称为自缓存应用程序。 直接 I/O 访问文件方式可以减少 CPU 的使用率以及内存带宽的占用，但是直接 I/O 有时候也会对性能产生负面影响。所以在使用直接 I/O 之前一定要对应用程序有一个很清醒的认识，只有在确定了设置缓冲 I/O 的开销非常巨大的情况下，才考虑使用直接 I/O。直接 I/O 经常需要跟异步 I/O 结合起来使用，因为CPU 和磁盘 I/O 之间的执行时间差距，会造成大量的浪费。 mmap + writemmap，称为内存地址映射，可以将内核缓冲区的地址映射到用户缓冲区。 如图所示，基本和传统的读写差不多，只是减少了一次 CPU 的 COPY 工作，但是上下文切换的次数没有变化，依旧是 4 次。mmap 主要的用处是提高 I/O 性能，特别是针对大文件。对于小文件，内存映射文件反而会导致碎片空间的浪费，因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。 sendfile数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。 相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。 sendfile + DMA gather copyLinux 2.4 版本的内核对 sendfile 系统调用进行修改，为 DMA 拷贝引入了 gather 操作。它将内核空间（kernel space）的读缓冲区（read buffer）中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区（ socket buffer）中，由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区（read buffer）拷贝到网卡设备中，这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作。 在硬件的支持下，sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝，这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。 sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。 splicesplice 系统调用可以在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。splice 拷贝方式也同样存在用户程序不能对数据进行修改的问题。除此之外，它使用了 Linux 的管道缓冲机制，可以用于任意两个文件描述符中传输数据，但是它的两个文件描述符参数中有一个必须是管道设备。 一句话归纳，就是直接在内核缓冲区和Socket缓存区之间建立管道。 总结 拷贝方式 CPU拷贝次数 DMA拷贝次数 系统调用 上下文切换次数 传统方式（read + write） 2 2 read + write 4 内存映射（mmap + write） 1 2 mmap + write 4 sendfile 1 2 sendfile 2 sendfile + DMA gather copy 0 2 sendfile 2 splice 0 2 splice 2 NIO 编程新说，就是这篇文章把我拉入了 NIO 的深坑 一段一段式，个人觉得可读性很强 Java3y 跟我想的一样，文件I/O 主要使用零拷贝技术，而网络 I/O 主要使用 I/O 复用技术 简单的总结，言简意赅，可惜没有代码demo 这里，我分三部分讲： 先介绍一下 NIO 的基本概念，主要是三剑客「Selector、Buffer、Channel」； 然后讲一下 NIO 中用到的零拷贝技术； 最后再谈一下 NIO 中最为重要的 SocketChannel 用到的 I/O 复用技术。 我建议学习 NIO 的路线是： 先系统了解各个概念，有什么：Java NIO 系统教程，总共有 17 讲 然后看 Java3y 的总结，个人觉得总结的很到位，尤其是将 NIO 分解成两大块技术「零拷贝技术 &amp; I/O 复用技术」，虽然他没有细致的谈到文件 I/O 使用的技术「这是一大缺陷」，但是有在重点强调 Selector 带来的同步非阻塞的 I/O 读写方式，这点是很好的，链接：如何学习Java的NIO？ - Java3y的回答 - 知乎 可以再好好补充下这两项技术的具体应用，首先是文件 I/O 中的零拷贝技术，主要就是通过 FileChannel 中的 map() 使用了 mmap，transferTo() &amp; transferFrom() 使用了 sendfile。「为了更好地理解流程，可以简单的将 Buffer 看作是用户缓冲区，Channel 看作是内核缓冲区」，尤其是 mmap 的在这里面的使用，可以参见这个博客里面的 demo，浅显易懂，下面我也会具体讲的：mmap在 NIO 中的具体使用 其次就是网络 I/O 中使用到的 I/O 复用技术了，这个在很多地方都有提及到，甚至大家认为 NIO 是 (Non-Blocking I/O)的意思，可见大家对 NIO 中这项技术的重视程度「虽然我认为 NIO 是 New I/O 的意思，因为零拷贝的实现也很牛逼啊！」，这里呢，还是建议大家看学习路线的第一个链接，里面其实也有许多小 demo的，当然了，如果想看具体的套接字传输的 demo，我推荐：编程新说，就是这篇文章带我入的坑，里面还提及了 AIO 哦 基本概念这玩意是什么Java NIO是 Java 1.4之后新出的一套IO接口，这里的的新是相对于原有标准的Java IO和Java Networking接口。NIO提供了一种完全不同的操作方式。 Non-Blocking I/O or New I/O New 在哪里，特点是啥 标准的IO编程接口是面向字节流和字符流的。而NIO是面向通道和缓冲区的，数据总是从通道中读到buffer缓冲区内，或者从buffer写入到通道中。「知识是相通的，这块跟零拷贝技术中的用户缓冲区和内核缓冲区的形式一模一样，用户缓冲区就是 Buffer，我们可以在这里进行操作数据，而内核缓冲区数据就是 Channel，我们在这里不能去操作数据，但是可以传输数据。」这个特点在 文件I/O 和 网络 I/O 中都有很大的用处，在文件 I/O 中我已经讲过了好处了，就是可以和零拷贝那一套结合上，在网络 I/O 中，这个特点可以很好的控制数据，毕竟缓冲区相对字节字符流来说要强得多。 Non-Blocking I/O。我们可以进行非阻塞IO操作。比如说，单线程中从通道读取数据到buffer，同时可以继续做别的事情，当数据读取到buffer中后，线程再继续处理数据。写数据也是一样的。就是用 Selector 实现的。 三剑客「Channel、Buffer、Selector」channel用来运输数据的，是全双工的，双向都可以，如果觉得不好理解的话，就看成内核缓冲区就行了，所以我们在程序中想要用到数据的话必须搭配 buffer，channel 从 buffer 中读数据，相当于内核缓冲区中的数据传送到用户缓冲区，buffer 写数据至 channel，相当于用户缓冲区中的数据传送到 channel。 主要的 channel 有： FileChannel 「文件 I/O 用到的」 DatagramChannel 「网络 I/O 用到，udp传输」 SocketChannel 「网络 I/O 用到，tcp传输」 ServerSocketChannel 「网络 I/O 用到，用于建立 tcp连接的channel」 bufferbuffer本质上就是一块内存区，可以用来写入数据，并在稍后读取出来。这块内存被NIO Buffer包裹起来，对外提供一系列的读写方便开发的接口。就是用来跟 channel 交互的，我们对数据的操作都是在这里完成的，buffer应该是三剑客中最值得讲的。 分类Java NIO有如下具体的Buffer类型： ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 正如你看到的，Buffer的类型代表了不同数据类型，换句话说，Buffer中的数据可以是上述的基本类型； 其中最为重要的就是 ByteBuffer，在网络 I/O 中我们基本上就是使用 ByteBuffer 了，在文件 I/O 中由于可以使用零拷贝技术，所以主要用的是用到mmap的 MappedByteBuffer，聪明的你肯定已经知道怎么用这个MappedByteBuffer了，也就是直接让 MappedByteBuffer 与 FileChannel 形成内存地址映射「类比用户缓冲区和内核缓冲区做地址映射」，减少拷贝次数。 数据结构我们来好好了解一下 buffer，先来看主要的属性 12345// Invariants: mark &lt;= position &lt;= limit &lt;= capacityprivate int mark = -1; // 默认 -1，标记，可以配合 mark()、reset()方法使用private int position = 0; private int limit;private int capacity; 容量（Capacity） 作为一块内存，buffer有一个固定的大小，叫做capacity容量。一旦buffer写满了就需要清空已读数据以便下次继续写入新的数据。 位置（Position） 当写入数据到Buffer的时候需要中一个确定的位置开始，默认初始化时这个位置position为0，一旦写入了数据比如一个字节，整型数据，那么position的值就会指向数据之后的一个单元，position最大可以到capacity-1. 当从Buffer读取数据时，也需要从一个确定的位置开始。buffer从写入模式变为读取模式时，position会归零，每次读取后，position向后移动。 上限（Limit） 在写模式，limit的含义是我们所能写入的最大数据量。它等同于buffer的容量。 一旦切换到读模式，limit则代表我们所能读取的最大数据量，他的值等同于写模式下position的位置。 数据读取的上限时buffer中已有的数据，也就是limit的位置（原position所指的位置）。 读模式和写模式在读模式和写模式下，position、mark、limit的值都会变化。如果不显示调用 mark() 函数，mark值不会变，如果显式调用，那么 mark 会等于 当前的 position，如果调用 flip()，由写模式切换成读模式，此时不仅position和limit会发生变化，mark 也会变为 -1。当然了，clear 也会将 mark 置为初始状态 -1。 123456789101112public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this;&#125;public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125; 基本用法利用Buffer读写数据，通常遵循四个步骤： 把数据写入buffer； 调用flip； 从Buffer中读取数据； 调用buffer.clear()或者buffer.compact() 当写入数据到buffer中时，buffer会记录已经写入的数据大小。当需要读数据时，通过flip()方法把buffer从写模式调整为读模式；在读模式下，可以读取所有已经写入的数据。 当读取完数据后，需要清空buffer，以满足后续写入操作。清空buffer有两种方式：调用clear()或compact()方法。clear会清空整个buffer，compact则只清空已读取的数据，未被读取的数据会被移动到buffer的开始位置，写入位置则近跟着未读数据之后。 123456789101112131415161718192021222324252627282930313233public class Buffer_Demo &#123; public static void main(String[] args) throws IOException &#123; RandomAccessFile aFile = new RandomAccessFile("file.txt", "rw"); FileChannel inChannel = aFile.getChannel(); //create buffer with capacity of 48 bytes ByteBuffer buf = ByteBuffer.allocate(48); // 调用 channel 的 read or write，就相当于在内核态系统调用一样 // 至于是 read 还是 write 取决于 new 的时候文件是 源文件还是目的文件 // 如果是源文件，那就是 read 到 随便一个地方 // 如果是目的文件，那就是随便将东西 write 到 目的文件 int bytesRead = inChannel.read(buf); //read into buffer. int count = 0 ; while (bytesRead != -1) &#123; buf.flip(); // 切换到读模式 count++; System.out.println(count); while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); // read 1 byte at a time &#125; buf.clear(); //make buffer ready for writing // 这一行一定要在一定ByteBuffer.clear()之后，因为如果 position == limit，就代表ByteBuffer不能存入东西了, // 因此连流终止信息都不能接受，不返回-1，而是返回 0 // 而clear会将 position 置为 0，不会有 position == limit，并且 buf 已经读完了，故会返回 -1. // 再三提醒，调用 read() 方法，会判断是否读完了，如果读完了就会返回 -1，所以文件读取只会读一遍，但是buffer可以读多遍 bytesRead = inChannel.read(buf); &#125; aFile.close(); &#125;&#125; 其他一些函数的用法 mark() &amp; reset() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 用于对mark属性的测试 * 调用buffer.remak()时，会将当前的position值赋给remak属性， * 保存当前操作的状态，然后get继续执行，当调用buffer.reset（）时，会将之前当前的remark值赋予position, * 实现状态的恢复. * flip clear ( reset mark ) rewind * @author lenovo * */class BufferMark &#123; public static void main(String[] args) &#123; // TODO Auto-generated method stub IntBuffer intBuffer = IntBuffer.allocate(10); for(int i = 0;i&lt;intBuffer.capacity();i++) &#123; int b = new SecureRandom().nextInt(20); //此方法为相对方法(relative),他会导致position的变化 intBuffer.put(b); //此方法是绝对方法（absolute),他的使用只会讲对应位置的值替换到，并不会更改position //intBuffer.put(i,new SecureRandom().nextInt(20)); System.out.print(b+" "); &#125; //翻转buffer intBuffer.flip(); System.out.println(""); System.out.println("读取到的数据："+intBuffer.get()); System.out.println("读取到的数据："+intBuffer.get()); System.out.println("1:buffer position="+intBuffer.position()+" limit="+intBuffer.limit()); //标记buffer状态 intBuffer.mark(); System.out.println("2: buffer position="+intBuffer.position()+" limit="+intBuffer.limit()); System.out.println("读取到的数据："+intBuffer.get()); System.out.println("读取到的数据："+intBuffer.get()); System.out.println("3: buffer position="+intBuffer.position()+" limit="+intBuffer.limit()); //恢复buffer状态 intBuffer.reset(); System.out.println("读取到的数据："+intBuffer.get()); System.out.println("读取到的数据："+intBuffer.get()); System.out.println("4: buffer position="+intBuffer.position()+" limit="+intBuffer.limit()); /** * 4 10 3 11 17 5 18 10 2 6 读取到的数据：4 读取到的数据：10 1: buffer position=2 limit=10 2: buffer position=2 limit=10 读取到的数据：3 读取到的数据：11 3: buffer position=4 limit=10 读取到的数据：3 读取到的数据：11 4: buffer position=4 limit=10 */ &#125;&#125; buffer.get() 读取缓冲区的数据，默认按 字节 读取； rewind() Buffer.rewind()方法将position置为0，这样我们可以重复读取buffer中的数据。limit保持不变。 clear() &amp; compact() 一旦我们从buffer中读取完数据，需要复用buffer为下次写数据做准备。只需要调用clear或compact方法。 clear方法会重置position为0，limit为capacity，也就是整个Buffer清空。实际上Buffer中数据并没有清空，我们只是把标记为修改了。 如果Buffer还有一些数据没有读取完，调用clear就会导致这部分数据被“遗忘”，因为我们没有标记这部分数据未读。 针对这种情况，如果需要保留未读数据，那么可以使用compact。 因此compact和clear的区别就在于对未读数据的处理，是保留这部分数据还是一起清空。 selectorSelector是Java NIO中的一个组件，用于检查一个或多个NIO Channel的状态是否处于可读、可写。如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。这也是 I/O 复用技术的核心，也是整个 NIO 最核心的组件。在 I/O 复用技术再展开吧。 用到的零拷贝技术NIO中通过FileChannel来提供Zero-Copy的支持，分别是 FileChannel.map: 将文件的一部分映射到内存 FileChannel.transferTo: 将本Channel的文件字节转移到指定的可写Channel mmap12345678910111213141516171819202122232425262728293031323334/** * 测试FileChannel的用法 * * @author sound2gd * */public class FileChannnelTest &#123; public static void main(String[] args) &#123; File file = new File("src/com/cris/chapter15/f6/FileChannnelTest.java"); try ( // FileInputStream打开的FileChannel只能读取 FileChannel fc = new FileInputStream(file).getChannel(); // FileOutputStream打开的FileChannel只能写入 FileChannel fo = new FileOutputStream("src/com/cris/chapter15/f6/a.txt").getChannel();) &#123; // 将FileChannel的数据全部映射成ByteBuffer MappedByteBuffer mbb = fc.map(MapMode.READ_ONLY, 0, file.length()); // 使用UTF-8的字符集来创建解码器 Charset charset = Charset.forName("UTF-8"); // 直接将buffer里的数据全部输出 fo.write(mbb); mbb.clear(); // 创建解码器 CharsetDecoder decoder = charset.newDecoder(); // 使用解码器将byteBuffer转换为CharBuffer CharBuffer decode = decoder.decode(mbb); System.out.println(decode); &#125; catch (Exception e) &#123; &#125; &#125;&#125; 更多详细源码解析见：https://zhuanlan.zhihu.com/p/83398714 sendfile transferTo() 和 transferFrom() 方法的底层实现原理，这两个方法也是 java.nio.channels.FileChannel 的抽象方法，由子类 sun.nio.ch.FileChannelImpl.java 实现。transferTo() 和 transferFrom() 底层都是基于 sendfile 实现数据传输的。这里就是 Channel 之间的传输。 I/O 复用技术其实说白了，就是一个 Selector 对应多个 Channel，可以专门用一个管理多个，就是 I/O 复用。 下面直接放一个 Socket I/O 实例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151package NIO.编程新说;import java.io.IOException;import java.net.InetSocketAddress;import java.net.Socket;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Iterator;import java.util.Random;import java.util.Set;import java.util.concurrent.atomic.AtomicInteger;public class NIO &#123;&#125;/** * @author yang * @since 2020-03-21 */class Client2 &#123; public static void main(String[] args) &#123; try &#123; for (int i = 0; i &lt; 20; i++) &#123; Socket s = new Socket(); s.connect(new InetSocketAddress("127.0.0.1", 27771)); processWithNewThread(s, i); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; static void processWithNewThread(Socket s, int i) &#123; Runnable run = () -&gt; &#123; try &#123; //睡眠随机的5-10秒，模拟数据尚未就绪 Thread.sleep((new Random().nextInt(6) + 5) * 1000); //写1M数据，为了拉长服务器端读数据的过程 s.getOutputStream().write(prepareBytes()); //睡眠1秒，让服务器端把数据读完 Thread.sleep(1000); s.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;; new Thread(run).start(); &#125; static byte[] prepareBytes() &#123; byte[] bytes = new byte[1024*1024*1]; for (int i = 0; i &lt; bytes.length; i++) &#123; bytes[i] = 1; &#125; return bytes; &#125;&#125;/** * @author yang * @since 2020-03-21 */class NioServer2 &#123; static int clientCount = 0; static AtomicInteger counter = new AtomicInteger(0); static SimpleDateFormat sdf = new SimpleDateFormat("HH:mm:ss"); public static void main(String[] args) &#123; try &#123; Selector selector = Selector.open(); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT); ssc.bind(new InetSocketAddress("127.0.0.1", 27771)); while (true) &#123; selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) &#123; ServerSocketChannel ssc1 = (ServerSocketChannel)key.channel(); SocketChannel sc = null; while ((sc = ssc1.accept()) != null) &#123; sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ); InetSocketAddress rsa = (InetSocketAddress)sc.socket().getRemoteSocketAddress(); System.out.println(time() + "-&gt;" + rsa.getHostName() + ":" + rsa.getPort() + "-&gt;" + Thread.currentThread().getId() + ":" + (++clientCount)); &#125; &#125; else if (key.isReadable()) &#123; //先将“读”从感兴趣操作移出，待把数据从通道中读完后，再把“读”添加到感兴趣操作中 //否则，该通道会一直被选出来 key.interestOps(key.interestOps() &amp; (~ SelectionKey.OP_READ)); processWithNewThread((SocketChannel)key.channel(), key); &#125; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; static void processWithNewThread(SocketChannel sc, SelectionKey key) &#123; Runnable run = () -&gt; &#123; counter.incrementAndGet(); try &#123; String result = readBytes(sc); //把“读”加进去 key.interestOps(key.interestOps() | SelectionKey.OP_READ); System.out.println(time() + "-&gt;" + result + "-&gt;" + Thread.currentThread().getId() + ":" + counter.get()); sc.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; counter.decrementAndGet(); &#125;; new Thread(run).start(); &#125; static String readBytes(SocketChannel sc) throws Exception &#123; long start = 0; int total = 0; int count = 0; ByteBuffer bb = ByteBuffer.allocate(1024); //开始读数据的时间 long begin = System.currentTimeMillis(); while ((count = sc.read(bb)) &gt; -1) &#123; if (start &lt; 1) &#123; //第一次读到数据的时间 start = System.currentTimeMillis(); &#125; total += count; bb.clear(); &#125; //读完数据的时间 long end = System.currentTimeMillis(); return "wait=" + (start - begin) + "ms,read=" + (end - start) + "ms,total=" + total + "bs"; &#125; static String time() &#123; return sdf.format(new Date()); &#125;&#125; 具体的语句含义我这里就不再单独说明了，可以移步：https://wiki.jikexueyuan.com/project/java-nio-zh/java-nio-selector.html，这里有说明一些具体函数的用法，这里我们知道思想就好了。 对输入输出流的一些看法在写上面的一些demo的时候，突然又对输入流输出流有点困惑，到底什么时候该输入流，什么时候该输出流，又为什么需要分输出流输入流呢？ 为什么要分输入输出流？ 因为目的不一样，输入流是表示从外部输入到内存中去，关注的重点在字节流从哪里来，而输出流是表示将内存中的数据存储到外部去，关注的重点是字节流到哪去，分开就是为了分工明确。二者操作的对象不一样，一个是源文件「输入流」，一个是目标文件「输出流」。 输入流和输出流何时用？ 只要确定了要操作的文件是源文件还是目的文件，就能很清楚的知道输入输出流应该如何选择，像最简单的 FileOutputStream 和 FileInputStream ，在使用时分别会先指定目的文件和源文件，至于 fos.write()具体写啥我们并不关心，至于 fis.read()，读到哪里去我们也并不关心。 最后，由于这篇文章也不是主要写 I/O 流的，还是该收就收吧，最后放一个大招链接，没事的时候多看看，可以加深对 I/O 流的理解。 最详尽的 I/O 流大全 简述输入输出流 极具争议的一个话题这就是 NIO 到底是同步阻塞，还是同步非阻塞，还是异步非阻塞呢？ 我看过很多答案，也看了很多解释，真的云里雾里… 我先表明我的观点，我认为是同步非阻塞的。同步非同步，我觉得看的是消息来了，你是不是按序接收，比如我们的程序在一个线程内肯定是同步的，因为他是按序进行的，再比如在 node js 中所有的语句都不是按序来的，这个就算做异步，从这个角度看，NIO 是同步的，因为 Selector 要一个一个轮询，每个有请求的I/O 都是需要按序来接收的。再来谈谈非阻塞，我觉得谈阻塞非阻塞焦点在于这个线程的状态，如果他接收消息后，自己去忙着处理，其他线程必须等你处理完 I/O 操作然后一直等着，这个就肯定是阻塞了。在 NIO 中毫无疑问是非阻塞的，因为 Selector 在接收到 I/O 读写准备好了之后，会起一个线程去处理 I/O 读写，不会让这个 Selector 去处理，其他I/O 请求也就不用阻塞等待。 在知乎上，有人这么总结： 同步和异步说的是消息的通知机制「同步就是需要线程主动询问，异步就是压根不需要线程主动询问」，阻塞和非阻塞说的是线程的状态。 我认为勉强后半句 okay，前半句多少有些勉强「不过也能说得过去，下面我会解释」，你在 node js 中谈同步异步的时候有去考虑什么消息的通知机制吗？不过话说回来，主动询问的代价就是得一个一个询问，所以肯定同步，不需要去询问，每个 I/O 请求自己去进行处理，那这个也就不存在一个一个处理请求的问题了，必然是异步了。所以我觉得，同步还是异步，看的是 I/O 请求来了是不是按序去处理的，这里肯定是按序处理，因为都是由 Selector 去接收，所以是同步。阻塞还是非阻塞，就看这个 I/O 请求有没有导致线程阻塞了，实际上是没有，因为他找了 worker 线程去处理了，所以这里是非阻塞。 再补充两句吧，同步异步我认为始终是要以顺序为中心要义的，node js中我们谈同步异步，始终都是看执行语句的顺序，这里我觉得也是一样。在这里只不过是消息的通知机制的不同带来了顺序的变化，所以我觉得因果关系还是要确定好，同步异步的概念我认为不可能用消息通知机制来衡量，我们在讲同步代码块中有提到过什么通知机制的概念吗？我们始终都是在强调执行的顺序，以此来衡量同步异步。 综合来说，NIO 就是同步非阻塞的。 至于 AIO 「jdk1.7 引入」采用的是异步非阻塞，通过我上面的方式去理解就很好理解了。采用的是回调机制，I/O 请求来了，我们根本不需要去管，也就不存在请求按序处理的情况了，直接由操作系统自己处理，等到数据okay了，直接通知我就好了，全程都是异步非阻塞的状态。 当然了 AIO 这么好为啥我们还在用 NIO 呢？我觉得有几个原因吧，一个是这个 AIO 支持的太少了，还没完全发展起来，一个是不支持 udp，只支持 tcp。 NIO底层之奇妙的三兄弟我擦，就一点半了…明天接着写，透露下，就是 select/poll/epoll。 回来了，回来了，接着昨天的彩蛋，我们今天来讲一讲 select/poll/epoll。 其实这个 select/poll/epoll 和 零拷贝没有关系，但是 跟 NIO 紧密相连，我们知道，NIO 中总共就是使用了两个技术：I/O 复用技术「网络 I/O」和零拷贝技术「文件 I/O」，我在上面其实也有讲，NIO 中最重要的技术就是 I/O 复用技术，那它是如何实现的呢？没错，底层就是 select/poll/epoll。 这三兄弟是啥 参考： https://zhuanlan.zhihu.com/p/95872805 https://mp.weixin.qq.com/s?__biz=MzUyNzgyNzAwNg==&amp;mid=2247483925&amp;idx=1&amp;sn=1ac3e863594745c7466b0e88a688b203&amp;scene=21#wechat_redirect select、poll、epoll都是I/O多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个文件描述符，一旦某个描述符就绪（读就绪或写就绪），能够通知程序进行相应的读写操作 。 但是，select，poll，epoll本质还是同步I/O（I/O多路复用本身就是同步IO）的范畴，因为它们都需要在读写事件就绪后线程自己进行读写，读写的过程阻塞的。而异步I/O的实现是系统会把负责把数据从内核空间拷贝到用户空间，无需线程自己再进行阻塞的读写，内核已经准备完成。 Tip：linux中 socket 的 fd 是什么？ 这个FD就是File Discriptor 中文翻译为文件描述符。 Socket起源于unix，Unix中把所有的资源都看作是文件，包括设备，比如网卡、打印机等等，所以，针对Socket通信，我们在使用网卡，网卡又处理N多链接，每个链接都需要一个对应的描述，也就是惟一的ID，即对应的文件描述符。简单点说也就是 int fd = socket(AF_INET,SOCK_STREAM, 0); 函数socket()返回的就是这个描述符。在传输中我们都要使用这个惟一的ID来确定要往哪个链接上传输数据。 SelectAPI简介linux系统中/usr/include/sys/select.h文件中对select方法的定义如下： 1234567891011121314151617181920212223242526/* fd_set for select and pselect. */typedef struct &#123; /* XPG4.2 requires this member name. Otherwise avoid the name from the global namespace. */ #ifdef __USE_XOPEN __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS]; # define __FDS_BITS(set) ((set)-&gt;fds_bits) #else __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS]; # define __FDS_BITS(set) ((set)-&gt;__fds_bits) #endif &#125; fd_set;/* Check the first NFDS descriptors each in READFDS (if not NULL) for read readiness, in WRITEFDS (if not NULL) for write readiness, and in EXCEPTFDS (if not NULL) for exceptional conditions. If TIMEOUT is not NULL, time out after waiting the interval specified therein. Returns the number of ready descriptors, or -1 for errors. This function is a cancellation point and therefore not marked with __THROW. */extern int select (int __nfds, fd_set *__restrict __readfds, fd_set *__restrict __writefds, fd_set *__restrict __exceptfds, struct timeval *__restrict __timeout); int __nfds是fd_set中最大的描述符+1，当调用select时，内核态会判断fd_set中描述符是否就绪，__nfds告诉内核最多判断到哪一个描述符。 readfds、writefds、__exceptfds都是结构体fd_set，fd_set可以看作是一个描述符的集合。 select函数中存在三个fd_set集合，分别代表三种事件，readfds表示读描述符集合，writefds表示读描述符集合，exceptfds表示异常描述符集合。当对应的fd_set = NULL时，表示不监听该类描述符。 timeval __timeout用来指定select的工作方式，即当文件描述符尚未就绪时，select是永远等下去，还是等待一定的时间，或者是直接返回 函数返回值int表示： 就绪描述符的数量，如果为-1表示产生错误 。 运行机制Select会将全部fd_set『文件描述符』从用户空间拷贝到内核空间，并注册回调函数， 在内核态空间来判断每个请求是否准备好数据 。select在没有查询到有文件描述符就绪的情况下，将一直阻塞（select是一个阻塞函数）。如果有一个或者多个描述符就绪，select 会去轮询整个 fd_set「无差别轮询」。 Select的缺陷 每次调用select，都需要把fd集合从用户态拷贝到内核态，fd越多开销则越大； 每次调用select，都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 select支持的文件描述符数量有限，默认是1024。参见/usr/include/linux/posix_types.h中的定义： 1# define __FD_SETSIZE 1024 pollAPI简介linux系统中/usr/include/sys/poll.h文件中对poll方法的定义如下： 1234567891011121314151617/* Data structure describing a polling request. */struct pollfd &#123; int fd; /* File descriptor to poll. */ short int events; /* Types of events poller cares about. */ short int revents; /* Types of events that actually occurred. */ &#125;;/* Poll the file descriptors described by the NFDS structures starting at FDS. If TIMEOUT is nonzero and not -1, allow TIMEOUT milliseconds for an event to occur; if TIMEOUT is -1, block until an event occurs. Returns the number of file descriptors with events, zero if timed out, or -1 for errors. This function is a cancellation point and therefore not marked with __THROW. */extern int poll (struct pollfd *__fds, nfds_t __nfds, int __timeout); __fds参数时Poll机制中定义的结构体pollfd，用来指定一个需要监听的描述符。结构体中fd为需要监听的文件描述符，events为需要监听的事件类型，而revents为经过poll调用之后返回的事件类型，在调用poll的时候，一般会传入一个pollfd的结构体数组，数组的元素个数表示监控的描述符个数。 __nfds和__timeout参数都和Select机制中的同名参数含义类似 运行机制poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd 结构代替select的fd_set（网上讲：类似于位图）结构，其他的本质上都差不多。所以Poll机制突破了Select机制中的文件描述符数量最大为1024的限制。 Poll的缺陷Poll机制相较于Select机制中，解决了文件描述符数量上限为1024的缺陷。但另外两点缺陷依然存在： 每次调用poll，都需要把fd集合从用户态拷贝到内核态，fd越多开销则越大； 每次调用poll，都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 epollEpoll在Linux2.6内核正式提出，是基于事件驱动的I/O方式。相对于select来说，epoll没有描述符个数限制；使用一个文件描述符管理多个描述符，将用户关心的文件描述符的事件存放到内核的一个事件表中，通过内存映射，使其在用户空间也可直接访问，省去了拷贝带来的资源消耗。 API简介linux系统中/usr/include/sys/epoll.h文件中有如下方法： 123456789101112131415161718192021222324252627/* Creates an epoll instance. Returns an fd for the new instance. The "size" parameter is a hint specifying the number of file descriptors to be associated with the new instance. The fd returned by epoll_create() should be closed with close(). */extern int epoll_create (int __size) __THROW;/* Manipulate an epoll instance "epfd". Returns 0 in case of success, -1 in case of error ( the "errno" variable will contain the specific error code ) The "op" parameter is one of the EPOLL_CTL_* constants defined above. The "fd" parameter is the target of the operation. The "event" parameter describes which events the caller is interested in and any associated user data. */extern int epoll_ctl (int __epfd, int __op, int __fd, struct epoll_event *__event) __THROW;/* Wait for events on an epoll instance "epfd". Returns the number of triggered events returned in "events" buffer. Or -1 in case of error with the "errno" variable set to the specific error code. The "events" parameter is a buffer that will contain triggered events. The "maxevents" is the maximum number of events to be returned ( usually size of "events" ). The "timeout" parameter specifies the maximum wait time in milliseconds (-1 == infinite). This function is a cancellation point and therefore not marked with __THROW. */extern int epoll_wait (int __epfd, struct epoll_event *__events, int __maxevents, int __timeout); epoll_create函数：创建一个epoll实例并返回，该实例可以用于监控__size个文件描述符 epoll_ctl函数：向epoll中注册事件，该函数如果调用成功返回0，否则返回-1。用红黑树来管理这些事件，每新增一个事件，就新增一个红黑树节点。 __epfd为epoll_create返回的epoll实例 __op表示要进行的操作 __fd为要进行监控的文件描述符 __event要监控的事件 epoll_wait函数：类似与select机制中的select函数、poll机制中的poll函数，等待内核返回监听描述符的事件产生。该函数返回已经就绪的事件的数量，如果为-1表示出错。 __epfd为epoll_create返回的epoll实例 __events数组为 epoll_wait要返回的已经产生的事件集合 maxevents为希望返回的最大的事件数量（通常为events的大小） __timeout和select、poll机制中的同名参数含义相同 运行机制 epoll 将等待队列和就绪队列分开，红黑树存储等待队列中的进程，而一旦 I/O 就绪就将红黑树节点转移到链表，所以只要看到链表中的节点，就能知道哪些事件准备好了。 epoll操作过程需要上述三个函数，也正是通过三个函数完成Select机制中一个函数完成的事情，解决了Select机制的三大缺陷。epoll的工作机制更为复杂，我们就解释一下，它是如何解决Select机制的三大缺陷的。 对于第一个缺点，epoll的解决方案是：它的fd是共享在用户态和内核态之间的，所以可以不必进行从用户态到内核态的一个拷贝，大大节约系统资源。至于如何做到用户态和内核态，大家可以查一下“mmap”，它是一种内存映射的方法。 对于第二个缺点，epoll的解决方案不像select或poll一样每次都把当前线程轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把当前线程挂一遍（这一遍必不可少），并为每个fd指定一个回调函数。当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表。那么当我们调用epoll_wait时，epoll_wait只需要检查链表中是否有存在就绪的fd即可，效率非常可观。 对于第三个缺点，fd数量的限制，也只有Select存在，Poll和Epoll都不存在。由于Epoll机制中只关心就绪的fd，它相较于Poll需要关心所有fd，在连接较多的场景下，效率更高。在1GB内存的机器上大约是10万左右，一般来说这个数目和系统内存关系很大。 工作模式相较于Select和Poll，Epoll内部还分为两种工作模式： LT水平触发（level trigger）和ET边缘触发（edge trigger）。 LT模式： 默认的工作模式，即当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序可以不立即处理该事件；事件会被放回到就绪链表中，下次调用epoll_wait时，会再次通知此事件。 ET模式： 当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应并通知此事件。 由于上述两种工作模式的区别，LT模式同时支持block和no-block socket两种，而ET模式下仅支持no-block socket。即epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个fd的阻塞I/O操作把多个处理其他文件描述符的任务饿死。ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。 Epoll的优点 文件描述符数量不再受限。 epoll 是线程安全的，而 select 和 poll 不是。 epoll 内部使用了 mmap 共享了用户和内核的部分空间，避免了数据的来回拷贝。 select、poll采用轮询的方式来检查文件描述符是否处于就绪态，而epoll采用回调机制。造成的结果就是，随着fd的增加，select和poll的效率会线性降低，而epoll不会受到太大影响，除非活跃的socket很多。 epoll 基于事件驱动，epoll_ctl 注册事件并注册 callback 回调函数，epoll_wait 只返回发生的事件避免了像 select 和 poll 对事件的整个轮询操作。 虽然epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。 三者比较1. 用户态将文件描述符传入内核的方式 select：创建3个文件描述符集并拷贝到内核中，分别监听读、写、异常动作。这里受到单个进程可以打开的fd数量限制，默认是1024。 poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听。 epoll：执行epoll_create会在内核的高速cache区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数添加文件描述符会在红黑树上增加相应的结点。 2. 内核态检测文件描述符读写状态的方式 select：采用轮询方式，遍历所有fd，最后返回一个描述符读写操作是否就绪的mask掩码，根据这个掩码给fd_set赋值。 poll：同样采用轮询方式，查询每个fd的状态，如果就绪则在等待队列中加入一项并继续遍历。 epoll：采用回调机制。在执行epoll_ctl的add操作时，不仅将文件描述符放到红黑树上，而且也注册了回调函数，内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。 3. 找到就绪的文件描述符并传递给用户态的方式 select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。 poll：将之前传入的fd数组拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。 epoll：epoll_wait只用观察就绪链表中有无数据即可，最后将链表的数据返回给数组并返回就绪的数量。内核将就绪的文件描述符放在传入的数组中，所以只用遍历依次处理即可。这里返回的文件描述符是通过mmap让内核和用户空间共享同一块内存实现传递的，减少了不必要的拷贝。 4. 重复监听的处理方式 select：将新的监听文件描述符集合拷贝传入内核中，继续以上步骤。 poll：将新的struct pollfd结构体数组拷贝传入内核中，继续以上步骤。 epoll：无需重新构建红黑树，直接沿用已存在的即可。 NIO 如何使用的这三兄弟 具体的 demo 分析可以看：https://blog.csdn.net/nieyanshun_me/article/details/52397153 在 windows 下，只支持 select，不支持 epoll，而 linux 2.6是支持 epoll的，从 demo 中可以知道 selector.select(); 这一步调用了 epoll 系统调用「复杂度 O(1)」，只要客户端准备好了数据，就会去告知 Selector 数据准备好了，可以进行 I/O 读写了，而不是像 select 「复杂度 O(n)」，知道有数据准备好了之后，把所有的连接全部轮一遍，看看是谁准备好了…费时费力。所以说啊，还是事务驱动好啊，就跟我们学习一样，还是一边面试一边学某些知识会更快一点，select 就像我们自己看书一样，知道自己有不足了，但是不知道哪块不行，于是就把所有知识全部复习一遍，epoll 就相当于面试的时候面试官直接告诉你哪块菜的不行，然后你就能对症下药并且最快效率的补齐自己的短板。 从 NIO 看 Netty在 NIO 一节中，我主要是介绍了下其文件 IO 用到的零拷贝技术以及 IO 复用技术，这一小节，我会从 NIO 的角度，一窥著名的 Netty。 Netty 产生的缘由 JDK 的 NIO 编程复杂。模型不友好，ByteBuffer 的 api 反人类； 没有实现线程模型，连自定义协议拆包都需要自己实现； JDK 的 NIO，bug 太多，维护成本过高。 所以，与之对应，Netty 的好处在于： Netty底层IO模型随意可切换； 自带拆包解包，异常检测等机制，只需要关心业务逻辑； Netty 的线程模型优化的好，精心设计的 Reactor 模型可以做到非常高效； 自带各种协议栈； 社区活跃、dubbo、hsf、kafka、metaQ都在使用，健壮性经受考验。 基本概念 Netty is an asynchronous event-driven network application framework for rapid development of maintainable high performance protocol servers &amp; clients. 从官网上的介绍可以看到，Netty 是一个网络应用框架，提供了异步事件驱动 的方式，使用 Netty 可以快速开发出一个高性能的网络应用程序，实际上，我们可以理解成 Netty 是一套在 java NIO 的基础上封装的便于用户开发网络应用程序的 api 。 整体架构让我们来看看 Netty 的整体架构： 可以看到，Netty 主要分为三部分： 底层的零拷贝技术、事件驱动模型以及统一的通信模型—Reactor； 基于 JVM 实现的传输层； 常用协议的支持，如 HTTP、SSL、gzip等等。 重新实现 ByteBufferNetty 重新实现了 NIO 中的 ByteBuffer，可以更好的解决很多问题，例如半包、粘包。 半包：接收端将一个发送端的 ByteBuffer “拆开” 了，收到了多个破碎的包； 粘包：接收端收到了多个 ByteBuffer。 解决思路就是根据自定义协议，将数据读取到缓冲区进行二次拼装，重新组装得到我们应用层的数据包。 统一的 I/O 接口Netty 有 Channel 这个统一的编程接口，所以可以在传输层进行实现的切换，例如可以将 Socket 切换成 Datagram，还可以灵活的在 NIO 和 BIO 中进行切换，由于核心 API 具有高度的可扩展性，可以很容易的定制自己的传输实现。 高级组件Netty 提供了一系列高级组件，如 SSL/TLS 支持、HTTP实现等等。 基于拦截链模式的事件模型一个定义良好并具有扩展能力的事件模型是事件驱动开发的必要条件。Netty 具有定义良好的 I/O 事件模型。由于严格的层次结构区分了不同的事件类型，因此 Netty 也允许你在不破坏现有代码的情况下实现自己的事件类型。这是与其他框架相比另一个不同的地方。很多 NIO 框架没有或者仅有有限的事件模型概念；在你试图添加一个新的事件类型的时候常常需要修改已有的代码，或者根本就不允许你进行这种扩展。 这块暂时没啥体感… 异步事件驱动 — Reactor 模型 主要参考：https://cloud.tencent.com/developer/article/1488120 这里采用的“异步事件驱动”，就是我们上文讲的 IO 复用技术，也可以换个名字，就是 reactor 模型。也称为 Dispatcher 模型，也可以称为同步非阻塞模型。 有人可能会奇怪，这里明明谈到的是 “异步事件驱动”，怎么又是 “同步非阻塞模型”，这里我就要稍微解释一下了，在 “同步非阻塞” 中，其实也有两种形式，一种是非事件驱动，线程主动去询问数据是否准备好了「select」，另外一种呢，就是事件驱动，数据准备好了之后主动告知对应的线程「epoll」，这里的 “异步” ，指的就是线程无需主动的去询问数据是否准备好了，而是可以去干其他事情，此时的线程和数据准备的确是异步的。 举个生活中更常见的例子，一个是： 如果你想吃一份卤肉饭，在饭馆点完餐，就去遛狗了。不过遛一会儿，就回饭馆喊一声：好了没啊！—– 非事件驱动 如果你想吃一份卤肉饭，在饭馆点完餐，就去遛狗了，遛狗的时候，接到饭馆电话，说饭做好了，让您亲自去拿。— 事件驱动 这里能看到我们在遛狗的时候无需回去问饭是否做好了，异步的概念就很明显了！ 而同步非阻塞，则是从整个IO传输的角度看，在数据准备就绪之后，依旧需要内核来调度新的线程来进行IO传输，这样明显是同步的，所以说，二者的同步异步的范围不一样，一个是仅仅聚焦于IO准备就绪，一个是聚焦于整个的IO传输。 概述Reactor模型中定义的三种角色： Reactor：负责监听和分配事件，将I/O事件分派给对应的Handler。新的事件包含连接建立就绪、读就绪、写就绪等。也就是上文代码中的 “select” 角色； Acceptor：处理客户端新连接，并分派请求到处理器链中。就是上文代码中触发 “accept” 语义。 Handler：将自身与事件绑定，执行非阻塞读/写任务，完成channel的读入，完成处理业务逻辑后，负责将结果写出channel。可用资源池来管理。就是上文代码中触发 “read”语义。 Reactor处理请求的流程： 读取操作： 应用程序注册读就绪事件和相关联的事件处理器； 事件分离器等待事件的发生； 当发生读就绪事件的时候，事件分离器调用第一步注册的事件处理器； 写入操作类似于读取操作，只不过第一步注册的是写就绪事件。 单 Reactor 单线程模型Reactor线程负责多路分离套接字，accept新连接，并分派请求到handler。Redis使用单Reactor单进程的模型。 消息处理流程： Reactor对象通过select监控连接事件，收到事件后通过dispatch进行转发。 如果是连接建立的事件，则由acceptor接受连接，并创建handler处理后续事件。 如果不是建立连接事件，则Reactor会分发调用Handler来响应。 handler会完成read-&gt;业务处理-&gt;send的完整业务流程。 这里的 handler 没有异步，所以对于高负载、大并发的应用场景并不合适。 Tip: 这里照应上面的例子，单 Reactor 单线程模型其实就相当于只有一个专门负责接待的服务员，他不仅要负责询问我们是否要进店「注册套接字，绑定端口」，还要负责告诉引导员将顾客带入相应的座位「acceptor接受连接，准备进行读取」，最后还要负责通知上菜员上菜「handler，传输数据」 单 Reactor 多线程模型与单线程模型相比，这里引入了 worker 线程池进行业务流程的处理。 消息处理流程： Reactor对象通过Select监控客户端请求事件，收到事件后通过dispatch进行分发。 如果是建立连接请求事件，则由acceptor通过accept处理连接请求，然后创建一个Handler对象处理连接完成后续的各种事件。 如果不是建立连接事件，则Reactor会分发调用连接对应的Handler来响应。 Handler只负责响应事件，不做具体业务处理，通过Read读取数据后，会分发给后面的Worker线程池进行业务处理。 Worker线程池会分配独立的线程完成真正的业务处理，如何将响应结果发给Handler进行处理。 Handler收到响应结果后通过send将响应结果返回给Client。 Tip: 这里照应上面的例子，单 Reactor 多线程模型其实就相当于最后拿走菜单的那个人回到厨房，然后有很多人上菜。 可以看到，这里的 reactor、accept、handler都专注于分发了，业务处理交由 worker 线程池去处理了，这样整个系统的吞吐就上去了，但是，这个模型还是存在两个比较明显的问题： 如果子线程完成业务处理后，把结果传递给主线程Reactor进行发送，就会涉及共享数据的互斥和保护机制。 Reactor承担所有事件的监听和响应，只在主线程中运行，可能会存在性能问题。例如并发百万客户端连接，或者服务端需要对客户端握手进行安全认证，但是认证本身非常损耗性能。 一句话来说，就是专门负责接待的服务员太忙了，所有通知的活都要他来干… 主从 Reactor 多线程模型比起第二种模型，它是将Reactor分成两部分： mainReactor负责监听server socket，用来处理网络IO连接建立操作，将建立的socketChannel指定注册给subReactor。 subReactor「这里也可以是多线程」主要做和建立起来的socket做数据交互和事件业务处理操作。通常，subReactor个数上可与CPU个数等同。 通俗点来说，就是负责接待的服务员只需要站在门口，问别人是否要进来就餐，剩下的事情，如将其引导到相应的位置「相当于建立连接」、告知顾客菜全部上完了「相当于worker干完了，需要通知给client」，就由 subReactor来全部完成了。这也对应了我们现在饭店的模型：有专门的一个人在门口负责询问顾客是否要进来就餐，顾客进入餐厅后，有专人负责引导到座位，顾客点餐完后，对应的负责人看到后通知后厨做菜，然后有多人分别给顾客端菜，最后菜上齐了，负责人告知顾客上齐了，最后引导顾客结账… Netty 线程模型Netty的线程模型，就是主从 Reactor 多线程模型，当然了，Netty 可以在以上三种 Reactor 模型之间进行切换。 扩充：AIO 中使用的 Proactor 模型由图中可以看到： Procator Initiator负责创建Procator和Handler，并将Procator和Handler都通过Asynchronous operation processor注册到内核。 Asynchronous operation processor负责处理注册请求，并完成IO操作。完成IO操作后会通知procator。 procator根据不同的事件类型回调不同的handler进行业务处理。handler完成业务处理，handler也可以注册新的handler到内核进程。 Reactor 实现了一个被动的事件分离和分发模型，服务等待请求事件的到来，再通过不受间断的同步处理事件，从而做出反应，适用于同时接收多个服务请求，并且依次同步的处理它们的事件驱动程序； Proactor 实现了一个主动的事件分离和分发模型；这种设计允许多个任务并发的执行，从而提高吞吐量；并可执行耗时长的任务（各个任务间互不影响），异步接收和同时处理多个服务请求的事件驱动程序。 Proactor 实现起来比较复杂，因为需要内核的深度参与，同时缓冲区在读或写操作的时间段内必须保持住，可能造成持续的不确定性，并且每个并发操作都要求有独立的缓存，相比Reactor模型，在Socket已经准备好读或写前，是不要求开辟缓存的。现在高并发网络编程基本上还是都采用 Reactor 模型。 从 Netty 看 HSF/Dubbo有了 Netty，我们就可以自己去实现一个 rpc 框架了，其实最为著名的就是 HSF、Dubbo了，二者都是采用了 Netty + Hession。 HSF 框架中采用如今流行的网络通信框架 Netty 加上 Hession 数据序列化协议实现 HSF 服务间的交互，主要考虑点是在大并发量时，服务交互性能达到最佳。这类 RPC 协议采用多路复用的 TCP 长连接方式，在服务提供者和调用者间有多个服务请求同时调用时会共用同一个长连接，即一个连接交替传输不同请求的字节块。它既避免了反复建立连接开销，也避免了连接的等待闲置从而减少了系统连接总数，同时还避免了 TCP 顺序传输中的线头阻塞（head-of-line blocking）问题。 Hessian 是 HSF 框架中默认使用的数据序列化协议，在数据量较小时性能表现出众，Hessian 的优点是精简高效，同时可以跨语言使用，目前支持 Java, C++, .net, Python, ruby 等语言。另外 Hessian 可以充分利用 Web 容器的成熟功能，在处理大量用户访问时很有优势，在资源分配、线程排队、异常处理等方面都可以由 Web 容器保证。 HSF 框架同时也支持切换使用 Java 序列化，Hession 相比 JDK 标准的序列化方式（即基于 Serializable 接口的标准序列化），在典型场景中，其序列化时间开销可能缩短 20 倍。虽然 Hessian 不是最快的序列化协议，但它对于复杂业务对象的序列化正确率、准确性相较于最稳定的 Java 序列化并不逊色太多。 业界还有一些比 Hessian 更快的序列化协议，但它们相对于 Hessian 在复杂场景下的处理能力还是会差一些，所以 Hessian 是在性能和稳定性同时考虑下最优的序列化协议。 阿里巴巴当时在对多种通信协议和数据序列化组件等测试中，Netty + Hession 的组合在互联网高并发量的场景下,特别是在 TPS 上达到 10w 以上时，性能和效率远比 REST 或者 Web Service 高。 从 Netty 看 kafka/metaQ 主要引用： kafka之所以快的六大原因 讲到了kafka快的两个原因：页缓存技术+磁盘顺序写+零拷贝 Kafka中的NIO网络通信模型 先来回答那个老生常谈的问题，kafka为何这么吞吐量大而且还快？ 我认为主要是以下几点： 使用了 NIO 的网络 I/O 通信模型，就是 I/O 复用，所以减去了等待数据准备的时间，加快速度； 使用页缓存技术「Page Cache」、磁盘顺序写以及 sendfile 的零拷贝技术加快落盘速度和传送速度，因为 Kafka 中存在大量的网络数据持久化到磁盘和磁盘文件通过网络发送的过程； 分区分段 + 索引，批量读写、批量压缩。 kafka 中的 NIO 网络通信模型Kafka的网络通信模型是基于NIO的主从Reactor多线程模型来设计的。就是我在 Netty 一节中提及到的，这里再次来回顾一次，先引用Kafka源码中注释的一段话： An NIO socket server. The threading model is 1 Acceptor thread that handles new connections. Acceptor has N Processor threads that each have their own selector and read requests from sockets. M Handler threads that handle requests and produce responses back to the processor threads for writing. 相信大家看了上面的这段引文注释后，大致可以了解到Kafka的网络通信层模型，主要采用了1（1个Acceptor线程）+N（N个Processor线程）+M（M个业务处理线程）。下面的表格简要的列举了下（这里先简单的看下后面还会详细说明）：『Acceptor 就是 NIO 中的 Selector』 线程数 线程名 线程具体说明 1 kafka-socket-acceptor_%x Acceptor线程，负责监听Client端发起的请求 N kafka-network-thread_%d Processor线程，负责对Socket进行读写 M kafka-request-handler-_%d Worker线程，处理具体的业务逻辑并生成Response返回 Kafka网络通信层的完整框架图如下图所示： 这里可以简单总结一下其网络通信模型中的几个重要概念： （1）Acceptor：1个接收线程，负责监听新的连接请求，同时注册OP_ACCEPT 事件，将新的连接按照“round robin”方式交给对应的 Processor 线程处理； （2）Processor：N个处理器线程，其中每个 Processor 都有自己的 selector，它会向 Acceptor 分配的 SocketChannel 注册相应的 OP_READ 事件，N 的大小由“num.networker.threads”决定； （3）KafkaRequestHandler：M个请求处理线程，包含在线程池—KafkaRequestHandlerPool内部，从RequestChannel的全局请求队列—requestQueue中获取请求数据并交给KafkaApis处理，M的大小由“num.io.threads”决定； （4）RequestChannel：其为Kafka服务端的请求通道，该数据结构中包含了一个全局的请求队列 requestQueue和多个与Processor处理器相对应的响应队列responseQueue，提供给Processor与请求处理线程KafkaRequestHandler和KafkaApis交换数据的地方； （5）NetworkClient：其底层是对 Java NIO 进行相应的封装，位于Kafka的网络接口层。Kafka消息生产者对象—KafkaProducer的send方法主要调用NetworkClient完成消息发送； （6）SocketServer：其是一个NIO的服务，它同时启动一个Acceptor接收线程和多个Processor处理器线程。提供了一种典型的Reactor多线程模式，将接收客户端请求和处理请求相分离； （7）KafkaServer：代表了一个Kafka Broker的实例；其startup方法为实例启动的入口； （8）KafkaApis：Kafka的业务逻辑处理Api，负责处理不同类型的请求；比如“发送消息”、“获取消息偏移量—offset”和“处理心跳请求”等； Page Cache(页缓存技术)什么是页缓存操作系统本身有一层缓存，叫做page cache，是在内存里的缓存，我们也可以称之为os cache，意思就是操作系统自己管理的缓存。 好处是什么这种方式相当于读写内存，不是在读写磁盘，个人觉得有点类似于直接将磁盘和内核缓冲区放一块了「我们平常的 I/O 就是用 DMA 将 内核缓冲区和磁盘连接」，既然是读写内存，好处就是很快很快。 如何使用页缓存你在写磁盘文件的时候，可以直接写入os cache 中，也就是仅仅写入内存中，接下来由操作系统自己决定什么时候把os cache 里的数据真的刷入到磁盘中「刷盘其实就是用直接在末尾追加，也快的一批」。 为何不使用 jvm 中的内存 避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多； 避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题； 操作系统对页缓存有很多优化，提供了 write-behind、read-ahead以及flush等多种机制。 磁盘顺序写磁盘顺序写，仅仅将数据追加到文件的末尾(append)，而不是在文件的随机位置来修改数据。减去了寻址的时间，性能甚至比内存随机读写要快，这样就极大的加快了 os cache 落盘的速度，吞吐量更大了。 但是，这种方法有一个缺陷—— 没有办法删除数据 ，所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个Topic都有一个offset用来表示读取到了第几条数据 。 如果不删除硬盘肯定会被撑满，所以Kakfa提供了两种策略来删除数据。一是基于时间，二是基于partition文件大小。 sendfile 的零拷贝技术允许操作系统将数据从Page Cache 直接发送到网络，只需要最后一步的copy操作将数据复制到 NIC 缓冲区， 这样避免重新复制数据 。通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。 这里拓展一下： RocketMQ 选择了 mmap + write 这种零拷贝方式，适用于业务级消息这种小块文件的数据持久化和传输；而 Kafka 采用的是 sendfile 这种零拷贝方式，适用于系统日志消息这种高吞吐量的大块文件的数据持久化和传输。但是值得注意的一点是，Kafka 的索引文件使用的是 mmap + write 方式，数据文件使用的是 sendfile 方式。 分区分段 + 索引 kafka中的 message 是按 topic 存储的，每个 topic 又可以有多个分区 partition「每个partition 都有一个 leader，若干个 follower」，每个partition对应了操作系统上的一个文件夹「也就是单个broker上」，然后 partition 又由多个 segment 组成，分区分段，这样扫描起来性能更快，因为操作文件只需要操作一个 segment 就可以了，细粒度化了； 为了更快的检索 segment，使用了索引文件，而为了索引文件更快的读取，又采用了零拷贝技术中的 mmap + write。「是不是挺牛逼！」 这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。 批量读写Kafka数据读写也是批量的而不是单条的。 除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多。 批量压缩在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，对于需要在广域网上的数据中心之间发送消息的数据流水线尤其如此。进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑，支持多种压缩，我自己有在项目中使用 gzip，的确是有加速，并且消费端无需自己解压，自动解压。 总结kafka快的原因：「从流程上说」 Producer 发送消息时进行批量的发送并且批量压缩，减轻 网络 I/O 压力，然后是使用了 NIO 中的 网络I/O 模型，采用 I/O 复用机制，用 Selector 去轮询是否准备好了发送的数据，避免的其他线程的无效等待，等到数据传送到服务器时，会首先用索引去找到对应的segment，索引文件采用了 mmap + write 减少 cpu 拷贝次数加快访问速度，找到对应的 segment 之后，会将数据发送到 Page Cache，Page Cache 会在合适的时候进行刷盘，刷盘是顺序读写磁盘，所以刷盘也很快，然后消费者要消费时，利用 sendfile 的零拷贝技术直接从 Page Cache 发送到 SocketChannel，无需进行多余的拷贝和上下文切换，直接在内核态中完成，自此，数据发送给客户端。]]></content>
      <categories>
        <category>NIO</category>
      </categories>
      <tags>
        <tag>零拷贝</tag>
        <tag>NIO</tag>
        <tag>Netty</tag>
        <tag>HSF</tag>
        <tag>Kafka</tag>
        <tag>metaQ</tag>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 复习]]></title>
    <url>%2F2020%2F03%2F13%2FMysql%20%E5%86%8D%E5%BA%A6%E5%87%BA%E5%8F%91.html</url>
    <content type="text"><![CDATA[目录：/MySQL实战45讲 [161.8M] ┣━━01讲基础架构：一条SQL查询语句是如何执行的.html [54.2K] ┣━━02讲日志系统：一条SQL更新语句是如何执行的.html [65K] ┣━━03讲事务隔离：为什么你改了我还看不见.html [56.8K] ┣━━04讲深入浅出索引（上）.html [59.9K] ┣━━05讲深入浅出索引（下）.html [63.2K] ┣━━06讲全局锁和表锁：给表加个字段怎么有这么多阻碍.html [63.9K] ┣━━07讲行锁功过：怎么减少行锁对性能的影响.html [60.9K] ┣━━08讲事务到底是隔离的还是不隔离的.html [69K] ┣━━09讲普通索引和唯一索引，应该怎么选择.html [70K] ┣━━10讲MySQL为什么有时候会选错索引.html [67.1K] ┣━━11讲怎么给字符串字段加索引.html [54.6K] ┣━━12讲为什么我的MySQL会“抖”一下.html [62K] ┣━━13讲为什么表数据删掉一半，表文件大小不变.html [60.2K] ┣━━14讲count这么慢，我该怎么办.html [64.2K] ┣━━15讲答疑文章（一）：日志和索引相关问题.html [73.4K] ┣━━16讲“orderby”是怎么工作的.html [74.3K] ┣━━17讲如何正确地显示随机消息.html [63.5K] ┣━━18讲为什么这些SQL语句逻辑相同，性能却差异巨大.html [62.1K] ┣━━19讲为什么我只查一行的语句，也执行这么慢.html [59.6K] ┣━━20讲幻读是什么，幻读有什么问题.html [67.2K] ┣━━21讲为什么我只改一行的语句，锁这么多.html [68.7K] ┣━━22讲MySQL有哪些“饮鸩止渴”提高性能的方法.html [75.2K] ┣━━23讲MySQL是怎么保证数据不丢的.html [65.4K] ┣━━24讲MySQL是怎么保证主备一致的.html [66.4K] ┣━━25讲MySQL是怎么保证高可用的.html [68.4K] ┣━━26讲备库为什么会延迟好几个小时.html [67.5K] ┣━━27讲主库出问题了，从库怎么办.html [63.3K] ┣━━28讲读写分离有哪些坑.html [67.1K] ┣━━29讲如何判断一个数据库是不是出问题了.html [49.4K] ┣━━30讲答疑文章（二）：用动态的观点看加锁.html [38.9K] ┣━━31讲误删数据后除了跑路，还能怎么办.html [65.1K] ┣━━32讲为什么还有kill不掉的语句.html [59.4K] ┣━━33讲我查这么多数据，会不会把数据库内存打爆.html [61.1K] ┣━━34讲到底可不可以使用join.html [37.2K] ┣━━35讲join语句怎么优化.html [67.4K] ┣━━36讲为什么临时表可以重名.html [55.3K] ┣━━37讲什么时候会使用内部临时表.html [57.2K] ┣━━38讲都说InnoDB好，那还要不要使用Memory引擎.html [52.1K] ┣━━39讲自增主键为什么不是连续的.html [50.6K] ┣━━40讲insert语句的锁为什么这么多.html [53.8K] ┣━━41讲怎么最快地复制一张表.html [34K] ┣━━42讲grant之后要跟着flushprivileges吗.html [51.4K] ┣━━43讲要不要使用分区表.html [51.2K] ┣━━44讲答疑文章（三）：说一说这些好问题.html [53.1K] ┣━━开篇词讲这一次，让我们一起来搞懂MySQL.html [40.6K] ┗━━直播回顾讲林晓斌：我的MySQL心路历程.html [51.7K] 1. 一条语句的执行过程以查询语句为例，一条查询语句要经过以下几个步骤： 连接器。首先会在 Server 端验证连接的正确性，判断用户名密码是否正确，进行连接管理； 查询缓存。这个在 Mysql 8.0 之后就没有了，因为缓存命中率一向很低，只要修改了表就会导致缓存的失效； 分析器。主要进行词法语法的分析，判断语句是否符合规范，语句中的表以及字段在表中是否正确； 优化器。对查询语句进行优化，主要是索引的选择； 执行器。这一步才真正开始打开表来操作，需要首先验证是否可以有操作表的权限，然后调用InnoDB 等存储引擎的读写接口来完成语句的操作，最后返回结果； 存储引擎。主要负责存储数据以及提供数据读写的接口。 2. 讲日志系统：一条 SQL 更新语句的执行「MySQL里经常说到的WAL技术，WAL的全称是Write-Ahead Logging，它的关键点就是先写日志，再写磁盘」 InnoDB中特有的日志 redo-log： 在 InnoDB 中可以保证即使数据库发生了重启，之前提交的记录并不会丢失，这个能力称为 “crash-safe”「提供崩溃恢复功能」； 是物理日志，记录的是“这个数据页上做了什么改变”，与存储引擎的作用相符，主要是处理存储方面的功能； 是循环写的，空间固定会用完的。用完了就把之前的擦除，然后继续写，用两个指针来控制。 Server 层独有的日志 binlog： 是 Server 层实现的，所有引擎都可以使用； 作用是用于归档，记录的是“做了什么逻辑操作”，重点在执行逻辑上，不在存储，因为 Server 就是负责 Mysql 功能方面； 是可以追加写的，写完一页换一页，不会擦除以前的。 mysql&gt; update T set c=c+1 where ID=2; 上述的 update 语句执行顺序大体和 1 中一样，具体的执行器的流程如下： 执行器先找引擎，获取 ID = 2 这一行，ID 是主键，引擎会直接用索引树定位到这一行的数据，如果这一行的数据在内存中，那直接返回就好了，如果不在，则需要去磁盘中读取并加载到内存中，然后返回给执行器； 执行器拿到了这一行的数据，将 c 自加一，然后调用引擎去写入数据； 引擎会先将这个数据加载到内存中，并且去写 redo-log，此时注意，redo-log 并未提交，而是处于 prepare 状态，然后告知执行器执行完成了，随时可以提交事务； 执行器收到了存储引擎的消息后，就开始写入 bin-log，并把 bin-log写入磁盘； 执行器写入完毕后，调用引擎的提交事务接口，引擎把刚刚写入的 redo-log 改成 commit 状态，提交事务。 上述值得注意的是， redo-log 是 两阶段提交，也就是 先 prepare 再 commit。之所以这么做的原因是为了让两份日志之间的逻辑一致。 要注意，现阶段的数据恢复还是靠着 binlog + 整库备份去完成的，按理说 redo-log 也能完成而且速度肯定要比 bin-log 快得多，但是由于 redo-log 是会擦除的，不具备bin-log 的归档功能，所以现阶段还是没办法。 思考题：定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？ 一天一备：好处是“最长恢复时间”更短。在一天一备的模式里，最坏情况下需要应用一天的binlog。比如，你每天0点做一次全量备份，而要恢复出一个到昨天晚上23点的备份。但是坏处也是显而易见的，就是你需要大量的存储。 一周一备最坏情况就要应用一周的binlog了。这个恢复时间长，但是明显存储的消耗就小得多了。 3. 事务隔离事务的四大特性： 原子性。要么不做，要么全做； 一致性。在处理事务的过程中，保证数据库状态和其他业务规则保持一致，比如转账，前后两者的账湖总额不变； 隔离性。不同事务之间互不影响； 持久性。事务提交之后需要持久化到数据库中去，即使崩溃，也能恢复。 事务的隔离级别： 读未提交。别人还没提交的东西，就能读取到，会产生幻读、不可重复读、脏读的问题；「一个事务还没提交时，它做的变更就能被别的事务看到。」 读提交。别人提交的东西，就能读取到。会产生幻读、不可重复读；「一个事务提交之后，它做的变更才会被其他事务看到。」 可重复读。在事务执行期间，别人就算提交了东西并改动了，当前事务看不到，不会去读最新的，还是可能会产生幻读的情况；「一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。」 序列化。加锁，牛逼。「顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。」 其中，最难理解的就是可重复读的隔离级别是如何实现的，这也是mysql的默认隔离级别： 就是使用的回滚技术，在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。「由undo log完成的」 如图所示，这也是 数据库的多版本并发控制的基础，采用回滚机制。当然了，回滚日志不能一直保留，所以在必要的时候删除，删除的时间点就是当没有事务再需要用到这些回滚日志时，回滚日志会被删除。所以我们最好不要使用长事务，这样会导致回滚的数据很多很多。除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 如何避免长事务对业务的影响？ 这个问题，我们可以从应用开发端和数据库端来看。 首先，从应用开发端来看： 确认是否使用了set autocommit=0。这个确认工作可以在测试环境中开展，把MySQL的general_log开起来，然后随便跑一个业务逻辑，通过general_log的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成1。 确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用begin/commit框起来。我见过有些是业务并没有这个需要，但是也把好几个select语句放到了事务中。这种只读事务可以去掉。 业务连接数据库的时候，根据业务本身的预估，通过SET MAX_EXECUTION_TIME命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例） 其次，从数据库端来看： 监控 information_schema.Innodb_trx表，设置长事务阈值，超过就报警/或者kill； Percona的pt-kill这个工具不错，推荐使用； 在业务功能测试阶段要求输出所有的general_log，分析日志行为提前发现问题； 如果使用的是MySQL 5.6或者更新版本，把innodb_undo_tablespaces设置成2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。 4. 深入浅出索引（上）为何不使用 哈希表、数组、B 树、二叉搜索树，而采用了 B+？ 为何不用哈希表？ 哈希表进行等值查询的确很快，O(1) 的时间复杂度，但是其进行范围查询就必须遍历全表，并且如果相同的值太多，就容易导致哈希冲突，性能进一步下降。 为何不用数组？ 数组的确可以很快的进行等值查询和范围查询，但是其更新很麻烦，如果中间需要删除，那就要移动后面所有数据，所以只适用于静态存储引擎。 为何不用 B 树？ B 树最大的问题就是非叶子节点有一个 Data 域，我们知道数据库的性能主要取决于磁盘 I/O 的次数，mysql 采用了磁盘预读的机制，也就是每一个树的节点都申请一页的存储空间，这样方便读写，就是因为 B 树存储了 Data 域，相比之下肯定就只能存更少的指针域了，这样就会导致整个树的高度拉高，磁盘 I/O 次数增加，二叉搜索树同样如此，树高过高导致磁盘 I/O 次数增加。 为何用 B+ 树？ 我觉得第一点就是因为其树低，每个非叶子节点只存储了指针域和主键（这里说的是聚集索引）； 叶子节点有链表，方便范围查询； 查询效率很稳定。 为何最好使用自增主键？ 因为这样就可以减少维护索引有序性带来的开销，如果不采用自增主键，那么在插入的时候可能会在中间，造成一个页分裂的过程，开销很大，会导致存储的碎片化。 5. 深入浅出索引（下） 什么叫回表？ 普通索引在找到主键后，回到主键索引树搜索的过程，称为回表。 如何减少回表的次数呢？ 覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。其实就是联合索引，然后符合覆盖的要求就称之为覆盖索引，就是多个字段拼成一个索引，然后对应主键的映射。 最左前缀规则。只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 基于上面对最左前缀索引的说明，我们来讨论一个问题：在建立联合索引的时候，如何安排索引内的字段顺序。 这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。那如果既有联合索引，又有基于 a、b 各自的索引呢？那就考虑空间问题了，如果 a 的字段明显比 b 的长，则一页下可以容纳更多的 b，那肯定就创建一个（a,b)的联合索引和一个(b)的单字段索引。 索引下推。在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。 而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 6. 全局锁和表级锁根据加锁范围：MySQL里面的锁可以分为：全局锁、表级锁、行级锁 一、全局锁：对整个数据库实例加锁。MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL) FTWRL的启动时间点： FTWRL 前有读写的话 ,FTWRL 都会等待 读写执行完毕后才执行； FTWRL 执行的时候要刷脏页的数据到磁盘,因为要保持数据的一致性 ，理解的执行FTWRL时候是 所有事务 都提交完毕的时候。 这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。使用场景：全库逻辑备份。风险：1.如果在主库备份，在备份期间不能更新，业务停摆2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟官方自带的逻辑备份工具mysqldump，当mysqldump使用参数–single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。 一致性读是好，但是前提是引擎要支持这个隔离级别。如果要全库只读，为什么不使用set global readonly=true的方式？1.在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大。2.在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。二、表级锁MySQL里面表级锁有两种，一种是表锁，一种是元数据锁(meta data lock,MDL)表锁的语法是:lock tables … read/write可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。MDL：不需要显式使用，在访问一个表的时候会被自动加上。Server 层面实现的！！！！MDL的作用：保证读写的正确性。在对一个表做增删改查（DML）操作的时候，加MDL读锁；当要对表做结构变更（DDL）操作的时候，加MDL写锁。读锁之间不互斥。因为有隔离级别在，mvcc，所以读锁之间无需复制即可，读写锁之间，写锁之间是互斥的，用来保证变更表结构操作的安全性。MDL 会直到事务提交才会释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。所以小心在给小表做 DDL 时把这个库搞崩。 数据库四种语言： DDL （Data Definition Language）：数据定义语言，无需提交就可以执行的，主要有 CREATE、ALTER、DROP等等； DML（Data Manipulation Language）：数据操作语言，这个也是需要提交事务的哦，INSERT、UPDATE、DELETE、TRUNCATE、SELECT、以及 LOCK TABLE、CALL、EXPLAIN PLAN DCL（Data Control Language）：数据控制语言，包括 GRANT、REVOKE TCL（Transaction Control Language）：事务控制语言，包括 ROLLBACK、COMMIT、SAVEPOINT、SET TRANSACTION。 今日思考题：备份一般都会在备库上执行，你在用–single-transaction方法做逻辑备份的过程中，如果主库上的一个小表做了一个DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 假设这个DDL是针对表t1的， 这里我把备份过程中几个关键的语句列出来： 12345678910111213&gt;Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;&gt;Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT；&gt;/* other tables */&gt;Q3:SAVEPOINT sp;&gt;/* 时刻 1 */&gt;Q4:show create table `t1`;&gt;/* 时刻 2 */&gt;Q5:SELECT * FROM `t1`;&gt;/* 时刻 3 */&gt;Q6:ROLLBACK TO SAVEPOINT sp;&gt;/* 时刻 4 */&gt;/* other tables */&gt; 在备份开始的时候，为了确保RR（可重复读）隔离级别，再设置一次RR隔离级别(Q1); 启动事务，这里用 WITH CONSISTENT SNAPSHOT确保这个语句执行完就可以得到一个一致性视图（Q2)； 设置一个保存点，这个很重要（Q3）； show create 是为了拿到表结构(Q4)，然后正式导数据 （Q5），回滚到SAVEPOINT sp，在这里的作用是释放 t1的MDL锁 （Q6。当然这部分属于“超纲”，上文正文里面都没提到。 DDL从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后，如果开始执行，则很快能够执行完成。 参考答案如下： 如果在Q4语句执行之前到达，现象：没有影响，备份拿到的是DDL后的表结构。 如果在“时刻 2”到达，则表结构被改过，Q5执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump终止； 如果在“时刻2”和“时刻3”之间到达，mysqldump占着t1的MDL读锁，binlog被阻塞，现象：主从延迟，直到Q6执行完成。 从“时刻4”开始，mysqldump释放了MDL读锁，现象：没有影响，备份拿到的是DDL前的表结构。 7. 讲行锁功过：怎么减少行锁对性能的影响行锁是在存储引擎级别实现的，并不是像全局锁和表级锁一样在 Server 端实现。 InnoDB 支持行锁，不支持列锁，并且 innodb 行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。因为不走索引，所以必须遍历整个表，那么就会读取一行，锁一行，又因为是两阶段锁，只有更新成功，事务提交之后才会释放锁。 MyISAM 最小的锁粒度是表，并且由于表锁的存在，同时刻只能是单个线程对表进行操作，这很大程度上限制了并发度，所以这也是 InnoDB 取代 MyISAM 的重要原因吧。 两阶段锁的概念是什么? 对事务使用有什么帮助? 两阶段锁是指行锁是在需要的时候加锁，但是并不是在不用了之后就释放锁，而是等到事务提交才释放锁，这一点跟 MDL （元数据锁）很像； 对事务的帮助就是：一定要将并发度高、时间较长的操作的行放到事务提交前，防止过多的线程等待锁。 为什么要使用两阶段锁协议，为何不在事务启动时就加锁，又为什么不在语句执行完就解锁呢？ 首先回答一下为什么不在事务启动时就加锁，我觉得原因有两个： 减小锁的粒度和加锁时长，提高并发度。 事务启动的时候，并没有明确说明需要修改什么行，此时如果要锁定行必须锁定整个表才行。 再回答一下为什么不在语句执行完就解锁。 因为这样事务如果回滚的话，就会有问题了。例如下面这种情况： 如果事务 A 回滚，那肯定无法回滚了，因为已经有多个事务修改了这个值，并且人家已经提交了，无法回滚。 死锁的概念是什么? 举例说明出现死锁的情况。 两个或多个线程在互相等待对方释放资源的过程称为死锁状态，出现死锁的情况一般都是嵌套锁（跟多线程中嵌套同步代码块一样）。 所以我的建议是在加锁的时候一般都做到顺序加锁，这样就能防止锁互相嵌套问题。 死锁的处理策略有哪两种? 在数据库中，死锁的处理策略有： 有个超时机制，这个超时时间可以通过参数innodb_lock_wait_timeout来设置。时间到了自动释放锁，并且回滚当前事务，这个默认是 50s，时间太长，一般不考虑； 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。这个默认是开启的，但是吧，这个很消耗资源。 有哪些思路可以解决热点更新导致的并发问题? 热点更新导致的并发问题，症结就在于死锁检测非常的消耗资源，所以可以有以下解决措施： 尽量避免死锁，比如上文说的加锁按顺序，然后直接将死锁检测关闭，但是这样容易造成业务有损，一般死锁检测虽然浪费资源但是是业务无损的，所以关闭可能不是特别可取； 控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了； 将热点细分，比如购买电影票，把影院的账户总额拆成10条记录来存储，其10条记录的和等于影院账户余额，这样能尽量减少死锁的情况。 什么时候会进行死锁检测？一旦触发死锁检测是检测所有事务吗？ 如果他要加锁访问的行上有锁，他才要检测。 不是每次死锁检测都都要扫所有事务。比如某个时刻，事务等待状态是这样的： B在等A，D在等C，现在来了一个E，发现E需要等D，那么E就判断跟D、C是否会形成死锁，这个检测不用管B和A 今日面试题： 如果你要删除一个表里面的前10000行数据，有以下三种方法可以做到： 第一种，直接执行delete from T limit 10000; 第二种，在一个连接中循环执行20次 delete from T limit 500; 第三种，在20个连接中同时执行delete from T limit 500。 你会选择哪一种方法呢？为什么呢？ 在一个连接中循环执行20次 delete from T limit 500。 确实是这样的，第二种方式是相对较好的。 第一种方式（即：直接执行delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。 第三种方式（即：在20个连接中同时执行delete from T limit 500），会人为造成锁冲突。 确实是这样的，第二种方式是相对较好的。 第一种方式（即：直接执行delete from T limit 10000）里面，单个语句占用时间长，锁的时间也比较长；而且大事务还会导致主从延迟。 第三种方式（即：在20个连接中同时执行delete from T limit 500），会人为造成锁冲突。 8. 事务到底是隔离还是不隔离的这章知识点很多，难度比较大！ 先回顾一下什么是 MVCC，如何实现的？ MVCC，多版本并发控制，在查询记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本。「存储在undo log 日志中」 什么是 read-view？ 一致性视图，全称是 consistent read-view，这里稍微补充一下，在 mysql中有两种视图： 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view … ，而它的查询方法与表一样。 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。 在查询的时候，我们是需要用到 read-view的，但是在 update 的时候，是必须只能读当前最新值哦！！！「这里的最新值肯定是已经提交的事务去做的，如果未提交的事务去操作这个值，那肯定是加了行锁中的独占锁啦，所以你在 update 的话必须等其他事务释放锁。」 说了这么多，回到问题，read-view其实就是一个数组，每个事务或者语句都有的一个数组，只不过这个数组有点特殊。 InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。 数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。 这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。 查询的时候，是如何用到 read-view 的？ InnoDB的行数据有多个版本，每个数据版本有自己的row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据row trx_id和一致性视图（read-view）确定数据版本的可见性。 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；「因为对于可重复读的隔离级别，一致性视图是在事务开始的时候进行全库快照生成的」 对于读提交，查询只承认在语句启动前就已经提交完成的数据；「因为对于读提交的隔离级别来说，一致性视图是在每一次执行语句的时候生成的，跟事务没有关系」 总结上述两种情况，就是只有视图生成前事务提交了的数据我能看到，其他的我都看不到。 而当前读，总是读取已经提交完成的最新版本。「第 2 个问题已经讲了这个了」 上面一直强调的，当前读，什么时候会出现当前读的情况？ 首先我们要明确，当前读，必须要读的是该行的最新版本，此时的视图快照已经失效了，而且你要想读该行的最新版本，就必须去加锁！！！！「意思很明确，就是 当前读 == 给该行加锁」 所以，什么时候会出现行的当前读呢？那就是给该行加锁『悲观锁』了，那问题就转化成了什么时候会给行加锁呢？ 那这个问题就非常简单了，有以下几种情况会使得当前行加上悲观锁： SELECT … FROM … LOCK IN SHARE MODE，select 的时候强行加上共享锁，如果不加其实就是默认的快照读； SELECT … FROM … FOR UPDATE，select 的时候强行加上独占锁； UPDATE … WHERE … ，独占锁； DELETE FROM … WHERE … ，独占锁； INSERT 语句将在插入的记录上加一把排他锁「独占锁」。 好吧，其实也就是 select 啥都不加会导致不给行锁加悲观锁，其他的都会加的，意味着都是要去读当前值，意味着这个 rc 级别的隔离就无效了，不过话说回来，人家 rc 本身就是 可重复读 的隔离级别，本身就是用于 数据修改了我不去读 。 http://www.fordba.com/locks-set-by-different-sql-statements-in-innodb.html 这篇文章对 sql 语句加锁分析的我觉得是我看到的最好的了，受益匪浅！ 第 4 个问题中提到的“lock in shared mode” “for update”是什么锁？ 这个很简单啦，就是 mysql 中的悲观锁中的共享锁和独占锁。 既然谈到锁了，那你再回顾一下现在接触到的所有锁吧，也可以自己拓展一下！ 其实，上面讲到的锁都是可以跟 lock 体系对应上的，比如说上文的乐观锁悲观锁，对应的就是 lock 体系的乐观锁悲观锁，乐观锁就是底层是 CAS，CAS 中有三个值：当前内存值、预读的值、想修改的值，如果当前值和预读值一样，我就默认没被修改过，直接修改成想修改的值，当然这样会造成一个问题：ABA 问题，在 mysql 中最主要的方法是加一个 version 字段，用 version 字段去判断是否被修改过，在 lock 体系中，最主要的方法是加一个 stamp，类似于时间戳这样的东西，在 StampedLock 里面有去用到。当然了，CAS 还会有另外两个问题：一个是自旋会造成cpu资源的浪费，一个是只能保证单个变量的原子性「你如果想保证多个的话就直接写到一个对象中就完事了」。 回到正题，上面讲完乐观锁，现在讲讲悲观锁吧，mysql 中的独占锁和共享锁和 AQS 中的完全一致，这两思想也完全一样，至于 mysql 中内部如何实现的我不太清楚，因为貌似是用 c 写的，看的兴趣不大， AQS 的底层源码是有看过的，AQS 就是用的模板方法的设计思想嘛，对外我们想要去实现读写锁的话只需要继承几个方法就可以了，比如独占锁 ReentrantLock 只需要继承 tryacquired()、tryRealease()、isHeldExclusive()…「说不定会让我手动去实现一个，这个我得注意一下，提前写一个，刚才写的过程中，又发现了 transient 这个关键词，真的牛皮，这个博客讲的真的好：https://juejin.im/post/5dda467051882572f8249f11#heading-10」。。。。懒得讲了。。。 9. 普通索引和唯一索引，应该如何选择 在查找过程中，普通索引和唯一索引的性能基本一样，但是在更新过程中，如果记录不在内存中，那普通索引和唯一索引的性能差距就很大了，因为普通索引可以使用 changebuffer，而唯一索引因为有唯一性约束，需要将所有数据读入到内存中，这样就失去了 changebuffer 的意义，所以速度很慢。 changebuffer：存储用户更新的语句。在下次查询需要访问这个数据页的时候，才将数据页读入内存，执行 change buffer中与这个页有关的操作，注意哦，change buffer是可以持久化，将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。 故 changebuffer 适合写多读少的场景，这样能最大限度的加快写入的速度，所以说 changebuffer 可以避免写入的时候随机去读取数据到内存中，节省的是随机读磁盘的 IO 消耗。 redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。 tip: 为什么说 redo log 是将磁盘的随机写变成顺序写？ 如果没有 redo log，那么每次数据来了我们都要直接去落盘，每一次数据我们都需要去进行磁盘随机读写，而有了 redo log我们可以先将数据顺序直接追加到文件末尾「redo log文件的大小是固定的，所以可以是顺序写」，然后redo log再在之后进行落盘，这样的话我们的数据就是相当于顺序写入磁盘了。 kafka 的顺序写其实和mysql 顺序写是一样的，都是采用了日志，kafka中使用 log，而 mysql 中采用 redo-log，数据来了先顺序追加到日志中，比如说日志是 4 g大小，那这 4 g 是一起落盘的，所以说这4g就是顺序写入磁盘的，因为他们是在一块的，不需要多次磁盘寻址写入。 这个写的还可以吧: 浅析MySQL事务中的redo与undo 10. Mysql 为什么会选错索引 选索引的任务，是交给了优化器去做的，而优化器选择索引的最优方案主要是看扫描行数「通过取样去判断，所以这块可能会造成误差」、临时表和排序等等； 如果选错了索引，我们可以强制使用 force index，然后如果更快的话，我们可以用 explain 拿到 rows 的大小，然后使用 analyze table t 命令重新统计索引信息，这样下次就不会选错了。 11. 怎么给字符串加索引 直接创建完整索引，但是这样比较占用空间； 创建前缀索引，节省空间，但是容易导致回表次数增多，并且无法使用范围查询； 倒序存储，主要用来解决字符串本身的前缀区分度不高，例如身份证号； 创建hash字段索引，查询稳定，但是不支持范围查询，同时因为要额外增加字段和索引，消耗资源。 12. 为什么 Mysql 会 ”抖“ 一下？我们在第 9 节的内容其实跟这节也有点关系，因为都是有关于 redo log，我们在第 9 节谈到，redo log的出现，将我们随机写磁盘变为了顺序写磁盘「当然 redo log还有crash-safe的功能」，但是我们一直没有去谈写完 redo log之后数据，借此机会，我还是希望能够好好地回顾一下整个数据库更新的过程。 前提：假设我要将 id = 2 中的 c ＋ 1 ，步骤如下： 我们来细致分析一下在 写入 redo log，和提交事务分别发生了什么。 首先，我们需要知道的是，redo log 分为两部分：redo log buffer 和 redo log file，一个是在内存中，一个是在磁盘中，在写入 redo log 的时候，是先将数据写至 redo log buffer，然后再将 redo log buffer 中的数据顺序写入到redo log file，写完后处于 prepare 状态，在提交事务之前，就已经将 binlog 写入文件了，所以当提交事务的时候，redo log 和 binlog 全部持久化。 现在回到标题的问题。 什么叫”抖“一下？ 一条SQL语句，正常执行的时候特别快，但是有时也不知道怎么回事，它就会变得特别慢，并且这样的场景很难复现，它不只随机，而且持续时间还很短。可以认为是”抖“了一下。 为什么 Mysql 会经常抖一下？ 根本原因是数据库此时在大量刷脏页 什么是脏页？ 内存页与磁盘页不一致的页称为脏页 什么时候需要刷脏页？ redo log 满了，需要将脏页落盘； 内存中 buffer pool 满了，需要将脏页落盘，变成干净页； Mysql 空闲，后台进行刷脏页； Mysql 正常关闭前，进行刷脏页。 这四种情况的刷脏页，对系统影响如何？ redo log 写满再刷脏页，对系统影响很大，因为此时系统不能进行更新了，会阻塞； buffer pool 满了刷脏页，是很常见的，但是如果脏页多了，清理起来很慢； Mysql 空闲时刷脏页，对系统不会有啥性能上的影响； Mysql 正常关闭前刷脏页同样对系统不会有什么影响。 那应该如何避免刷脏页带来的性能影响呢？ 控制脏页比例，刷盘速度不能过慢，可以通过 inno_io_capacity来控制速度，当然过快也没有必要，因为这样势必会消耗更多的资源导致用户使用资源变少了。 总结：虽然 redo log 可以带来 crash-safe 和将随机写变为顺序写这两大功能，但是其也带来了内存脏页的问题。 13. 为什么表数据删掉一半，表文件大小不变 表空洞问题 delete 命令删除行是不能回收表空间的，只是当前页被标记为”可复用“，例如300 400 500，删除了400之后，400这个数据页就可以被复用，比如 401 就可以插入到其中，但是如果下一个是 800，那 400 这个数据页就只能被浪费了，成为空洞。 不止是删除数据会导致空洞，插入数据也会。当随机插入数据时，会造成索引的数据页分裂。例如当 300 500 600是一页的时候，此时来了个 550，那么就不得不申请一个新的页面来保存数据了，此时 550 会在新的数据页中，而 600 紧随其后，此时原来的页就只剩下了 300 500 了，这样就造成了数据页空洞。「这也是磁盘按页分配按页预读的坏处吧..」 如何解决表空洞问题？ 重建表。例如表 A 存在空洞情况，可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序将数据一行一行的插入到 B 中，也就是将 B 变成临时表，最后将 A 替换即可。 直接使用 “alter table A engine=InnoDB”即可，系统会自动帮我们完成转存数据、交换表名、删除旧表的操作，而其中最慢的步骤就是往临时表插入数据的过程，在 MySQL 5.6版本之前，不能一边往临时表插入数据一边对原表 A 进行更新，但是在 MySQL 5.6 之后，可以进行 Online DDL。 Online DDL 的流程？ 建立一个临时文件，扫描 A 主键的所有数据页； 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中； 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件中，对应的是图中 state2 的状态； 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上和 A 相同的数据文件，对应的就是 state3 的状态； 用临时文件替换 A 的数据文件。 总结一下，就是用了一个 row log 来保证 Online 的。 optimize table、analyze table、alter table这三种重建表的区别 Alter table 就是上面的步骤啦，转存数据、交换表名、删除旧表； analyze table 其实不是重建表，只是对表的索引信息做一个重新统计，没有修改数据，比如之前修改优化器统计的遍历的 row 数； optimize table 等于 recreate + analyze 14. count 这么慢，我该怎么办？ 为什么 InnoDB 不像 MyISAM 一样，专门来记录 count 值？ 因为 InnoDB 的隔离级别是 rr 级别，可重复读，所以在不同事务下，由于多版本并发控制的原因（MVCC），每一行记录都要判断自己是否对这个会话可现，因此对于 count(*) 来说，只好把数据一行一行的读出并进行判断。 那这种方法太蠢了，没有优化吗？ 你都知道蠢了，当然就有优化啊，你想想，InnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树上的叶子结点则是主键值，所以普通索引树比主键索引树肯定要小很多，而不管遍历哪个索引树其实结果都是一样的，所以 Mysql 会找到最小的那棵树进行遍历。 在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库设计的通用法则之一。 那我们之前在第 10 节谈到的 ”show table status“ 命令中的 row numbers 能用吗？ 当然不能啊，那只是一个预估值，官方说这个误差可以达到 40% ，所以肯定不行啊。 那如果我有一个总是要统总数的业务，该怎么办呢？ 提出这个问题，说明你肯定觉得优化后的方法依旧很慢，是的，所以我们只能自己去操作。 把 count 这个计数直接放到数据库里单独的一张计数表 C 中，一是解决了崩溃丢失的问题，因为InnoDB支持崩溃恢复，二是解决了事务导致的无法精确统计count的问题，直接将表C中的计数值这个操作也加入到事务中就很okay了。 count(字段)、count(主键id)、count(1)、count(*)的区别 count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。 至于分析性能差别的时候，你可以记住这么几个原则： server层要什么就给什么； InnoDB只给必要的值； 现在的优化器只优化了count(*)的语义为“取行数”，其他“显而易见”的优化并没有做。 这是什么意思呢？接下来，我们就一个个地来看看。 对于count(主键id)来说，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。 对于count(1)来说，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 单看这两个用法的差别的话，你能对比出来，count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。 对于count(字段)来说： 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加； 如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。 也就是前面的第一条原则，server层要什么字段，InnoDB就返回什么字段。 但是count(*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加。 看到这里，你一定会说，优化器就不能自己判断一下吗，主键id肯定非空啊，为什么不能按照count(*)来处理，多么简单的优化啊。 当然，MySQL专门针对这个语句进行优化，也不是不可以。但是这种需要专门优化的情况太多了，而且MySQL已经优化过count(*)了，你直接使用这种用法就可以了。 所以结论是：按照效率排序的话，count(字段)&lt;count(主键id)&lt;count(1)≈count(*)，所以我建议你，尽量使用count(*)。 15. 日志和索引相关问题在第 2 讲中，有提到 redo log 和 binlog 的两阶段提交，这也是分布式系统常用的手段。 其中，时刻 A 如果 crash 了，此时还没写 binlog，所以崩溃恢复肯定就将 redo log 中的数据进行回滚就行了，时刻 B 情况，会稍微复杂一点，因为这个时候有三种情况： binlog 写完了并且redo log 已经有了 commit 标志，那这个时候肯定就直接提交就好了； binlog 没写完，redo log 此时只有 prepare 标志，那这个时候肯定得回滚了，因为binlog没写完； binlog有写完了的标志，但是 redo log 此时只有 prepare 标志，没问题，这个也可以直接提交。 追问： binlog 如何判断写完了？ 一个事务的binlog是有完整格式的： statement格式的binlog，最后会有COMMIT； row格式的binlog，最后会有一个XID event。 redo log 如何知晓binlog 写完了，然后将状态从 prepare 变为 commit，又是如何扫描 prepare or commit 标志，然后提交的呢？ 它们有一个共同的数据字段，叫XID。binlog 写完后，有相应的通知机制，此时redo log 对应的 XID 状态变化。 崩溃恢复的时候，会按顺序扫描redo log： 如果碰到既有prepare、又有commit的redo log，就直接提交； 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务。 只要binlog不可以吗，也可以提供顺序写磁盘的功能，如果 binlog 也加一个 crash-safe 的功能，那不是既能归档，又能崩溃恢复吗？ binlog存储的是逻辑日志，而且并没有标记读到哪了，并不能记录数据页的更改，所以不能做到崩溃恢复。 那能不能只要 redo log，不要 binlog？ 从逻辑上可以，因为 redo log 除了不能归档，其他都能做。 但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog都是开着的。因为binlog有着redo log无法替代的功能。 一个是归档。redo log是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log也就起不到归档的作用。 一个就是MySQL系统依赖于binlog。binlog作为MySQL一开始就有的功能，被用在了很多地方。其中，MySQL系统高可用的基础，就是binlog复制。 还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费MySQL的binlog来更新自己的数据。关掉binlog的话，这些下游系统就没法输入了。 总之，由于现在包括MySQL高可用在内的很多系统机制都依赖于binlog，所以“鸠占鹊巢”redo log还做不到。你看，发展生态是多么重要。 数据的最终落盘，是从 redo log更新的还是从 buffer pool 更新的呢？ 实际上，redo log并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由redo log更新过去”的情况。 如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与redo log毫无关系。 在崩溃恢复场景中，InnoDB如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让redo log更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。 在我们第 12 讲的，如果 redo log 满了，也是要刷脏页的，此时会将 redo log对应的内存中的脏页刷入磁盘。 总而言之，其实 redo log 就是一个中间者，比如10 到 19，redo log 只是记录的 +9，所以肯定是从buffer pool 中更新的数据。 redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？ Redo log 是分为两块的：一块就是 redo log buffer，一块就是 redo log 文件，正常肯定是先到缓存，再到文件中啦。这里的写文件是一个顺序写的过程。等不忙的时候再执行 redo log 文件，进行最终的落盘。「也就是我们之前一直提到的刷脏页」 16. ”order by“ 是如何工作的？ order by 是 Mysql 中是如何工作的？ 有两种算法执行 order by。 全字段排序； rowid 排序。 科普： MySQL会为每个线程分配一个内存（sort_buffer）用于排序该内存大小为sort_buffer_size 如果排序的数据量小于sort_buffer_size，排序将会在内存中完成如果排序数据量很大，内存中无法存下这么多数据，则会使用磁盘临时文件来辅助排序，也称外部排序 在使用外部排序时，MySQL会分成好几份单独的临时文件用来存放排序后的数据，然后在将这些文件合并成一个大文件 mysql会通过遍历索引将满足条件的数据读取到sort_buffer，并且按照排序字段进行快速排序 如果查询的字段不包含在辅助索引中，需要按照辅助索引记录的主键返回聚集索引取出所需字段 该方式会造成随机IO，在MySQL5.6提供了MRR的机制，会将辅助索引匹配记录的主键取出来在内存中进行排序，然后在回表 按照情况建立联合索引来避免排序所带来的性能损耗，允许的情况下也可以建立覆盖索引来避免回表 全字段排序过程： 通过索引将所需的字段全部读取到sort_buffer中 按照排序字段进行排序 将结果集返回给客户端 优点： MySQL认为内存足够大时会优先选择全字段排序，因为这种方式比rowid 排序避免了一次回表操作 缺点： 造成sort_buffer中存放不下很多数据，因为除了排序字段还存放其他字段，对sort_buffer的利用效率不高 当所需排序数据量很大时，会有很多的临时文件，排序性能也会很差 rowid 排序 通过控制排序的行数据的长度来让sort_buffer中尽可能多的存放数据，max_length_for_sort_data 只将需要排序的字段和主键读取到sort_buffer中，并按照排序字段进行排序 按照排序后的顺序，取id进行回表取出想要获取的数据 将结果集返回给客户端 优点：更好的利用内存的sort_buffer进行排序操作，尽量减少对磁盘的访问 缺点：回表的操作是随机IO，会造成大量的随机读，不一定就比全字段排序减少对磁盘的访问 两种算法的应用场景？ 如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了MySQL的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。 这个结论看上去有点废话的感觉，但是你要记住它，下一篇文章我们就会用到。 看到这里，你就了解了，MySQL做排序是一个成本比较高的操作。那么你会问，是不是所有的order by都需要排序操作呢？如果不排序就能得到正确的结果，那对系统的消耗会小很多，语句的执行时间也会变得更短。 其实，并不是所有的order by语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。 那如果原来的数据就是有序的，是不是就可以不用排序了？如何做到有序呢？ 是的，如果数据本身就是有序的，就无须排序了。 这里，我们就可以采用覆盖索引，因为索引本身就是有序的，这样就不需要额外排序带来系统的消耗了，但是同样的维护索引也是有维护代价的。 17. 如何正确地显示随机消息MySQL对临时表排序的执行过程。 如果你直接使用order by rand()，这个语句需要Using temporary 和 Using filesort，查询的执行代价往往是比较大的。所以，在设计的时候你要尽量避开这种写法。 18. 为什么这些SQL语句逻辑相同，性能却差异巨大 条件字段函数操作 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 会遍历全表，所以很慢… 隐式类型转换 mysql&gt; select * from tradelog where tradeid=110717; 例如上面这条语句， tradeid 是 varchar 类型，但是传进来整数型，这个时候就会类型转换，导致不能走索引了。 其实类型转换就是相当于函数操作了，所以会走全表而不是走索引。 隐式字符编码转换 字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。 总而言之，这些 sql 语句之所以执行慢，就是因为加了在条件中加了函数操作。 19. 为什么我只查一行的语句，也执行这么慢方法：执行一下show processlist命令，看看当前语句处于什么状态。 原因可能有： 第一类：查询一条数据长时间不返回 等 MDL 锁。使用show processlist命令，发现是 Waiting for table metadata lock。出现这个状态表示的是，现在有一个线程正在表t上请求或者持有MDL写锁，把select语句堵住了。 等行锁。 第二类：查询慢 一致性读的问题，需要不断回滚undo log，所以慢。这里因为只是查一行，所以不能算是慢查询，只能算坏查询，等数据量上去了，就变成慢查询了。 20. 幻读是什么，幻读有什么问题？「未认真研读」 幻读是什么？ 幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。 这里，我需要对“幻读”做一个说明： 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。「加行锁读的时候才会出现，破坏mvcc，例如update、insert」 幻读仅专指“新插入的行”。 幻读有什么问题？ 首先是语义上有问题。session A在T1时刻就声明了，“我要把所有d=5的行锁住，不准别的事务进行读写操作”。而实际上，这个语义被破坏了。 数据一致性问题。 即使把所有的记录都加上锁，还是阻止不了新插入的记录 那这么难对付，Mysql 是如何解决呢？ 产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB只好引入新的锁，也就是间隙锁(Gap Lock)。 间隙锁和行锁合称next-key lock，每个next-key lock是前开后闭区间。 间隙锁和next-key lock的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”。 间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的 21. 为什么我只改一行的语句，锁这么多「未研读」有点难…有时间再研读 22. MySQL有哪些“饮鸩止渴”提高性能的方法 面对短连接风暴时，如何解决？ 第一种方法：先处理掉那些占着连接但是不工作的线程。 第二种方法：减少连接过程的消耗。有的业务代码会在短时间内先大量申请数据库连接做备用，如果现在数据库确认是被连接行为打挂了，那么一种可能的做法，是让数据库跳过权限验证阶段。 跳过权限验证的方法是：重启数据库，并使用–skip-grant-tables参数启动。这样，整个MySQL会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。 慢查询问题如何解决？ 在MySQL中，会引发性能问题的慢查询，大体有以下三种可能： 索引没有设计好； SQL语句没写好； MySQL选错了索引。 导致慢查询的第一种可能是，索引没有设计好。这种场景一般就是通过紧急创建索引来解决。MySQL 5.6版本以后，创建索引都支持Online DDL了，对于那种高峰期数据库已经被这个语句打挂了的情况，最高效的做法就是直接执行alter table 语句。比较理想的是能够在备库先执行。假设你现在的服务是一主一备，主库A、备库B，这个方案的大致流程是这样的： 在备库B上执行 set sql_log_bin=off，也就是不写binlog，然后执行alter table 语句加上索引； 执行主备切换； 这时候主库是B，备库是A。在A上执行 set sql_log_bin=off，然后执行alter table 语句加上索引。 这是一个“古老”的DDL方案。平时在做变更的时候，你应该考虑类似gh-ost这样的方案，更加稳妥。但是在需要紧急处理时，上面这个方案的效率是最高的。 导致慢查询的第二种可能是，语句没写好。 这时，我们可以通过改写SQL语句来处理。MySQL 5.7提供了query_rewrite功能，可以把输入的一种语句改写成另外一种模式。 比如，语句被错误地写成了 select * from t where id + 1 = 10000，你可以通过下面的方式，增加一个语句改写规则。 1234&gt; mysql&gt; insert into query_rewrite.rewrite_rules(pattern, replacement, pattern_database) values ("select * from t where id + 1 = ?", "select * from t where id = ? - 1", "db1");&gt; &gt; call query_rewrite.flush_rewrite_rules();&gt; 这里，call query_rewrite.flush_rewrite_rules()这个存储过程，是让插入的新规则生效，也就是我们说的“查询重写”。 导致慢查询的第三种可能，就是MySQL选错了索引。 这时候，应急方案就是给这个语句加上force index。 如何避免慢查询导致性能问题？ 上面我和你讨论的由慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索引没设计好和语句没写好。而这两种情况，恰恰是完全可以避免的。比如，通过下面这个过程，我们就可以预先发现问题。 上线前，在测试环境，把慢查询日志（slow log）打开，并且把long_query_time设置成0，确保每个语句都会被记录入慢查询日志； 在测试表里插入模拟线上的数据，做一遍回归测试； 观察慢查询日志里每类语句的输出，特别留意Rows_examined字段是否与预期一致。 不要吝啬这段花在上线前的“额外”时间，因为这会帮你省下很多故障复盘的时间。 23 - 28 讲 后续章节暂时感觉没有太多看的必要，最后以一道题结束本篇文章。 我们假设原来的大表有10000行数据，而符合条件的数据有100行，我们最后的结果集是只要10行 我们知道，limit是一个语句最后执行的，也就是说会先把符合条件的数据全部找到，最后再进行 limit。 在第一种情况中，会先去非聚集索引中找符合条件的主键值，然后进行回表，我们可以知道，会遍历大表10000行，回表100次，最后数据全部拿到之后，会进行 limit； 而在第二种情况，会直接先去非聚集索引中找符合条件的主键值，这一步和第一种情况是一致的，也是会遍历大表10000行，但是注意，这里在内部有进行 limit，也就是说，找到了符合条件的主键值，就直接 limit了，这样就会只剩下 10 个有效的主键值，然后再去回表，也是相当于只会回表十次。所以应该快的原因在这了….一个回表100次，一个回表 10 次，这还只是在符合条件的数据只有100行的情况下，如果符合条件的数据是1000000行，那这个回表的次数减少的可就很恐怖了。。。 一开始自己的想法以为是 right join 带来的性能的巨大提升，后面发现并不是，但是也顺带看了一遍 join 提升性能的一些方法。 「第34、35讲」 第一个问题：能不能使用join语句？ 如果可以使用Index Nested-Loop Join算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用Block Nested-Loop Join算法，扫描行数就会过多。尤其是在大表上的join操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种join尽量不要用。 所以你在判断要不要使用join语句时，就是看explain结果里面，Extra字段里面有没有出现“Block Nested Loop”字样。 第二个问题是：如果要使用join，应该选择大表做驱动表还是选择小表做驱动表？ 如果是Index Nested-Loop Join算法，应该选择小表做驱动表； 如果是Block Nested-Loop Join算法： 在join_buffer_size足够大的时候，是一样的； 在join_buffer_size不够大的时候（这种情况更常见），应该选择小表做驱动表。 所以，这个问题的结论就是，总是应该使用小表做驱动表。 总结后续其实还有10多节未看，但是稍微略读了下，感觉可读性不是很强了，毕竟不是专业 DBA，暂时 MySQL 告一段落了…]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>45讲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库专题复习]]></title>
    <url>%2F2020%2F03%2F03%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%8D%E4%B9%A0.html</url>
    <content type="text"><![CDATA[自己部分总结Q：事务的四大特性？ ACID 原子性 Atomicity。事务中所有操作是不可再分割的原子单位，所有操作要么全部执行成功，要么全部执行失败。 一致性 Consistency。事务执行后，数据库状态与其它业务规则保持一致。如转账业务，无论事务执行成功与否，参与转账的两个账号余额之和应该是不变的。 隔离性 Isolation。在并发操作中，不同事务之间应该隔离开来，使每个并发中的事务不会相互干扰。 持久性 Durability。一旦事务提交成功，事务中所有的数据操作都必须被持久化到数据库中，即使提交事务后，数据库马上崩溃，在数据库重启时，也必须能保证通过某种机制恢复数据。 Q：数据库隔离级别？ https://www.cnblogs.com/newsouls/p/4936842.html 数据库事务的隔离级别有4个，由低到高依次为 Read uncommitted 读未提交 Read committed 读提交 Repeatable read 重复读（默认） Serializable 序列化 这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。 脏读: 对于两个事物 T1, T2, T1 读取了已经被 T2 更新但还没有被提交的字段. 之后, 若 T2 回滚, T1读取的内容就是临时且无效的。 不可重复读: 对于两个事物 T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段. 之后, T1再次读取同一个字段, 值就不同了。 幻读: 对于两个事物 T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行. 之后, 如果 T1 再次读取同一个表, 就会多出几行。 √: 可能出现 ×: 不会出现 脏读 不可重复读 幻读 Read uncommitted √ √ √ Read committed × √ √ Repeatable read × × √ Serializable × × × Q：MYSQL的两种存储引擎区别（事务、锁级别等等），各自的适用场景 https://segmentfault.com/a/1190000008227211 MyISAM VS InnoDB 不同点： 存储方式。MyISAM 的索引文件（.MYI）和数据文件（.MYD）分开放，而 InnoDB 存放在一起。 事务支持。MyISAM 不支持事务，但操作具备原子性，InnoDB 支持事务。 表锁差异。 MyISAM 只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，同时，这种表级锁基本就是 ReentrantReadWriteLock 的变体，读锁共享，写锁互斥，读-写也互斥，并且有个特殊点，就是一个进程请求某个MyISAM表的读锁，同时另一个进程也请求同一表的写锁，MySQL如何处理呢？答案是写进程先获得锁。不仅如此，即使读请求先到锁等待队列，写请求后到，写锁也会插到读锁请求之前！这是因为MySQL认为写请求一般比读请求要重要。这也正是MyISAM表不太适合于有大量更新操作和查询操作应用的原因，因为，大量的更新操作会造成查询操作很难获得读锁，从而可能永远阻塞。这种情况有时可能会变得非常糟糕！ 支持行锁，锁粒度细化，因此可以支持写并发。 是否有外键。MyISAM 没有外键，InnoDB 有外键。 索引类型。MyISAM 采用的是非聚集索引，而 InnoDB 采用了聚集索引。MyISAM 叶节点保存的是指向数据文件的指针，而 InnoDB 保存的就是数据本身。 「 https://www.cnblogs.com/rgever/p/9736374.html」 存储总行数。MyISAM 直接存储总行数，而 InnoDB一个个必须遍历。 对于AUTO_INCREMENT类型的字段，在MyISAM表中，可以和其他字段一起建立联合索引。而InnoDB中必须包含只有该字段的索引。 全文索引。MyISAM 支持 fulltext 全文索引，而 InnoDB 不支持。 由于索引类型的区别，所以在是否必须要有主键上也有差距。 MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 适用场景： 采用MyISAM引擎 R/W &gt; 100:1 且update相对较少 并发不高 表数据量小 硬件资源有限 采用InnoDB引擎 R/W比较小，频繁更新大字段 表数据量超过1000万，并发高 安全性和可用性要求高 采用Memory引擎 有足够的内存 对数据一致性要求不高，如在线人数和session等应用 需要定期归档数据 Q：索引有 B+ 索引 和 Hash 索引，二者有何异同，为何主流的数据库引擎都是使用的 B+ 索引？ 索引 区别 Hash 等值查询效率高，但是不能排序(键值经过哈希变成无序)，不能进行范围查询，不支持多列联合索引的最左匹配规则，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。 B+ 平衡的多叉树，数据有序，可范围查询 Q：聚集索引和非聚集索引的区别？ https://www.cnblogs.com/s-b-b/p/8334593.html 聚集索引（主键索引）：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。 非聚集索引：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。 使用聚集索引的查询效率要比非聚集索引的效率要高，但是如果需要频繁去改变聚集索引的值，写入性能并不高，因为需要移动对应数据的物理位置。 非聚集索引在查询的时候可以的话就避免二次查询，这样性能会大幅提升。 不是所有的表都适合建立索引，只有数据量大表才适合建立索引，且建立在选择性高的列上面性能会更好。 Q：索引的优缺点，已经索引的适用场景？ 索引最大的好处是提高查询速度， 缺点是更新数据时效率低，因为要同时更新索引. 对数据进行频繁查询进建立索引，如果要频繁更改数据不建议使用索引。 Q：数据库引擎底层实现为何不用 B 树和红黑树、AVL呢？ 首先回答为何不用 B 树 B 树在每个索引节点都会保存 Data 域，要知道衡量 Mysql 的查询效率就是取决于磁盘 I/O 次数，B 树在每个索引节点都保存 Data 域，意味着每个索引所占的空间就变大了，磁盘 I/O 是按页（16kb）读取的，那么每页的索引数量就自然会变少，这样就意味着需要更多的 I/O 次数，而对比 B+，只在叶子节点才有所有的数据，其他的索引节点所占空间很小，所以这是 B+ 的一个优点； 其次，B+ 的叶子结点有使用链表连接，非常适用于范围查询，相比来说 B 树就没有； 最后，B+ 的查询效率非常稳定，都是要遍历到叶子结点，而 B 树的查询效率就不稳定了。 再回答为何不用红黑树、AVL? 红黑树和 AVL 相比 B+ 来说，树过高，这样带来的直接影响就是磁盘 I/O 次数的增多。 拓展：数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 剩下的是直接复制别人的博客： https://www.cnblogs.com/wenxiaofei/p/9853682.html 博客园 https://www.nowcoder.com/discuss/135748?order=4&amp;pos=23&amp;type=0 牛客网 两个都是一样的，为了看得更方便，直接复制到本地了。 数据库面试知识点汇总一、基本概念1.主键、外键、超键、候选键 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 2.为什么用自增列作为主键 如果我们定义了主键(PRIMARY KEY)，那么InnoDB会选择主键作为聚集索引、 如果没有显式定义主键，则InnoDB会选择第一个不包含有NULL值的唯一索引作为主键索引、 如果也没有这样的唯一索引，则InnoDB会选择内置6字节长的ROWID作为隐含的聚集索引(ROWID随着行记录的写入而主键递增，这个ROWID不像ORACLE的ROWID那样可引用，是隐含的)。 数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置，如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点） 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 3.触发器的作用？ 触发器是一种特殊的存储过程，主要是通过事件来触发而被执行的。它可以强化约束，来维护数据的完整性和一致性，可以跟踪数据库内的操作从而不允许未经许可的更新和变化。可以联级运算。如，某表上的触发器上包含对另一个表的数据操作，而该操作又会导致该表触发器被触发。 4.什么是存储过程？用什么来调用？ 存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。 调用： 1）可以用一个命令对象来调用存储过程。 2）可以供外部程序调用，比如：java程序。 5.存储过程的优缺点？ 优点： 1）存储过程是预编译过的，执行效率高。 2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。 3）安全性高，执行存储过程需要有一定权限的用户。 4）存储过程可以重复使用，可减少数据库开发人员的工作量。 缺点：移植性差 6.存储过程与函数的区别 7.什么叫视图？游标是什么？ 视图： 是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改会影响基本表。它使得我们获取数据更容易，相比多表查询。 游标： 是对查询出来的结果集作为一个单元来有效的处理。游标可以定在该单元中的特定行，从结果集的当前行检索一行或多行。可以对结果集当前行做修改。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。 8.视图的优缺点 优点： 1对数据库的访问，因为视图可以有选择性的选取数据库里的一部分。 2)用户通过简单的查询可以从复杂查询中得到结果。 3)维护数据的独立性，试图可从多个表检索数据。 4)对于相同的数据可产生不同的视图。 缺点： 性能：查询视图时，必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，那么就无法更改数据 9.drop、truncate、 delete区别 最基本： drop直接删掉表。 truncate删除表中数据，再插入时自增长id又从1开始。 delete删除表中数据，可以加where字句。 （1） DELETE语句执行删除的过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。 （2） 表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小，而DELETE操作不会减少表或索引所占用的空间。drop语句将表所占用的空间全释放掉。 （3） 一般而言，drop &gt; truncate &gt; delete （4） 应用范围。TRUNCATE 只能对TABLE；DELETE可以是table和view （5） TRUNCATE 和DELETE只删除数据，而DROP则删除整个表（结构和数据）。 （6） truncate与不带where的delete ：只删除数据，而不删除表的结构（定义）drop语句将删除表的结构被依赖的约束（constrain),触发器（trigger)索引（index);依赖于该表的存储过程/函数将被保留，但其状态会变为：invalid。 （7） delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。 （8） truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚。 （9） 在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。回滚段要足够大。要删除表用drop;若想保留表而将表中数据删除，如果于事务无关，用truncate即可实现。如果和事务有关，或老师想触发trigger,还是用delete。 （10） Truncate table 表名 速度快,而且效率高,因为:truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 （11） TRUNCATE TABLE 删除表中的所有行，但表结构及其列、约束、索引等保持不变。新行标识所用的计数值重置为该列的种子。如果想保留标识计数值，请改用 DELETE。如果要删除表定义及其数据，请使用 DROP TABLE 语句。 （12） 对于由 FOREIGN KEY 约束引用的表，不能使用 TRUNCATE TABLE，而应使用不带 WHERE 子句的 DELETE 语句。由于 TRUNCATE TABLE 不记录在日志中，所以它不能激活触发器。 10.什么是临时表，临时表什么时候删除? 临时表可以手动删除：DROP TEMPORARY TABLE IF EXISTS temp_tb; 临时表只在当前连接可见，当关闭连接时，MySQL会自动删除表并释放所有空间。因此在不同的连接中可以创建同名的临时表，并且操作属于本连接的临时表。创建临时表的语法与创建表语法类似，不同之处是增加关键字TEMPORARY， 如： CREATE TEMPORARY TABLE tmp_table ( NAME VARCHAR (10) NOT NULL, time date NOT NULL); select * from tmp_table; 11.非关系型数据库和关系型数据库区别，优势比较? 非关系型数据库的优势： 性能：NOSQL是基于键值对的，可以想象成表中的主键和值的对应关系，而且不需要经过SQL层的解析，所以性能非常高。 可扩展性：同样也是因为基于键值对，数据之间没有耦合性，所以非常容易水平扩展。 关系型数据库的优势： 复杂查询：可以用SQL语句方便的在一个表以及多个表之间做非常复杂的数据查询。 事务支持：使得对于安全性能很高的数据访问要求得以实现。 其他： 1.对于这两类数据库，对方的优势就是自己的弱势，反之亦然。 2.NOSQL数据库慢慢开始具备SQL数据库的一些复杂查询功能，比如MongoDB。 3.对于事务的支持也可以用一些系统级的原子操作来实现例如乐观锁之类的方法来曲线救国，比如Redis set nx。 12.数据库范式，根据某个场景设计数据表? 第一范式:(确保每列保持原子性)所有字段值都是不可分解的原子值。 第一范式是最基本的范式。如果数据库表中的所有字段值都是不可分解的原子值，就说明该数据库表满足了第一范式。第一范式的合理遵循需要根据系统的实际需求来定。比如某些数据库系统中需要用到“地址”这个属性，本来直接将“地址”属性设计成一个数据库表的字段就行。但是如果系统经常会访问“地址”属性中的“城市”部分，那么就非要将“地址”这个属性重新拆分为省份、城市、详细地址等多个部分进行存储，这样在对地址中某一部分操作的时候将非常方便。这样设计才算满足了数据库的第一范式，如下表所示。上表所示的用户信息遵循了第一范式的要求，这样在对用户使用城市进行分类的时候就非常方便，也提高了数据库的性能。 第二范式:(确保表中的每列都和主键相关)在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。 第二范式在第一范式的基础之上更进一层。第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。也就是说在一个数据库表中，一个表中只能保存一种数据，不可以把多种数据保存在同一张数据库表中。比如要设计一个订单信息表，因为订单中可能会有多种商品，所以要将订单编号和商品编号作为数据库表的联合主键。 第三范式:(确保每列都和主键列直接相关,而不是间接相关) 数据表中的每一列数据都和主键直接相关，而不能间接相关。 第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。比如在设计一个订单数据表的时候，可以将客户编号作为一个外键和订单表建立相应的关系。而不可以在订单表中添加关于客户其它信息（比如姓名、所属公司等）的字段。 BCNF:符合3NF，并且，主属性不依赖于主属性。 若关系模式属于第二范式，且每个属性都不传递依赖于键码，则R属于BC范式。通常BC范式的条件有多种等价的表述：每个非平凡依赖的左边必须包含键码；每个决定因素必须包含键码。BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。一般，一个数据库设计符合3NF或BCNF就可以了。 第四范式:要求把同一表内的多对多关系删除。 第五范式:从最终结构重新建立原始结构。 13.什么是 内连接、外连接、交叉连接、笛卡尔积等? 内连接: 只连接匹配的行 左外连接: 包含左边表的全部行（不管右边的表中是否存在与它们匹配的行），以及右边表中全部匹配的行 右外连接: 包含右边表的全部行（不管左边的表中是否存在与它们匹配的行），以及左边表中全部匹配的行 例如1：SELECT a.,b. FROM luntan LEFT JOIN usertable as b ON a.username=b.username 例如2：SELECT a.,b. FROM city as a FULL OUTER JOIN user as b ON a.username=b.username 全外连接: 包含左、右两个表的全部行，不管另外一边的表中是否存在与它们匹配的行。 交叉连接: 生成笛卡尔积－它不使用任何匹配或者选取条件，而是直接将一个数据源中的每个行与另一个数据源的每个行都一一匹配 例如：SELECT type,pub_name FROM titles CROSS JOIN publishers ORDER BY type 注意： 很多公司都只是考察是否知道其概念，但是也有很多公司需要不仅仅知道概念，还需要动手写sql,一般都是简单的连接查询，具体关于连接查询的sql练习，参见以下链接： 牛客网数据库SQL实战 leetcode中文网站数据库练习 14.varchar和char的使用场景? 1.char的长度是不可变的，而varchar的长度是可变的。 定义一个char[10]和varchar[10]。如果存进去的是‘csdn’,那么char所占的长度依然为10，除了字符‘csdn’外，后面跟六个空格，varchar就立马把长度变为4了，取数据的时候，char类型的要用trim()去掉多余的空格，而varchar是不需要的。 2.char的存取数度还是要比varchar要快得多，因为其长度固定，方便程序的存储与查找。char也为此付出的是空间的代价，因为其长度固定，所以难免会有多余的空格占位符占据空间，可谓是以空间换取时间效率。varchar是以空间效率为首位。 3.char的存储方式是：对英文字符（ASCII）占用1个字节，对一个汉字占用两个字节。varchar的存储方式是：对每个英文字符占用2个字节，汉字也占用2个字节。 4.两者的存储数据都非unicode的字符数据。 15.SQL语言分类 SQL语言共分为四大类： 数据查询语言DQL 数据操纵语言DML 数据定义语言DDL 数据控制语言DCL。 1. 数据查询语言DQL 数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块： SELECTFROMWHERE 2 .数据操纵语言DML 数据操纵语言DML主要有三种形式： 1) 插入：INSERT 2) 更新：UPDATE 3) 删除：DELETE 3. 数据定义语言DDL 数据定义语言DDL用来创建数据库中的各种对象—–表、视图、索引、同义词、聚簇等如：CREATE TABLE/VIEW/INDEX/SYN/CLUSTER 表 视图 索引 同义词 簇 DDL操作是隐性提交的！不能rollback 4. 数据控制语言DCL 数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如： 1) GRANT：授权。 2) ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚—ROLLBACK；回滚命令使数据库状态回到上次最后提交的状态。其格式为：SQL&gt;ROLLBACK; 3) COMMIT [WORK]：提交。 在数据库的插入、删除和修改操作时，只有当事务在提交到数据库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看到所做的事情，别人只有在最后提交完成后才可以看到。提交数据有三种类型：显式提交、隐式提交及自动提交。下面分别说明这三种类型。 (1) 显式提交用COMMIT命令直接完成的提交为显式提交。其格式为：SQL&gt;COMMIT； (2) 隐式提交用SQL命令间接完成的提交为隐式提交。这些命令是：ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP，EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。 (3) 自动提交若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后，系统将自动进行提交，这就是自动提交。其格式为：SQL&gt;SET AUTOCOMMIT ON； 参考文章：https://www.cnblogs.com/study-s/p/5287529.html 16.like %和-的区别 通配符的分类: %百分号通配符:表示任何字符出现任意次数(可以是0次). _下划线通配符:表示只能匹配单个字符,不能多也不能少,就是一个字符. like操作符: LIKE作用是指示mysql后面的搜索模式是利用通配符而不是直接相等匹配进行比较. 注意: 如果在使用like操作符时,后面的没有使用通用匹配符效果是和=一致的,SELECT * FROM products WHERE products.prod_name like ‘1000’;只能匹配的结果为1000,而不能匹配像JetPack 1000这样的结果. %通配符使用: 匹配以”yves”开头的记录:(包括记录”yves”) SELECT FROM products WHERE products.prod_name like ‘yves%’;匹配包含”yves”的记录(包括记录”yves”) SELECT FROM products WHERE products.prod_name like ‘%yves%’;匹配以”yves”结尾的记录(包括记录”yves”,不包括记录”yves “,也就是yves后面有空格的记录,这里需要注意) SELECT * FROM products WHERE products.prod_name like ‘%yves’; _通配符使用: SELECT FROM products WHERE products.prod_name like ‘_yves’; 匹配结果为: 像”yyves”这样记录.SELECT FROM products WHERE products.prodname like ‘yves_‘; 匹配结果为: 像”yvesHe”这样的记录.(一个下划线只能匹配一个字符,不能多也不能少) 注意事项: 注意大小写,在使用模糊匹配时,也就是匹配文本时,mysql是可能区分大小的,也可能是不区分大小写的,这个结果是取决于用户对MySQL的配置方式.如果是区分大小写,那么像YvesHe这样记录是不能被”yves__”这样的匹配条件匹配的. 注意尾部空格,”%yves”是不能匹配”heyves “这样的记录的. 注意NULL,%通配符可以匹配任意字符,但是不能匹配NULL,也就是说SELECT * FROM products WHERE products.prod_name like ‘%;是匹配不到products.prod_name为NULL的的记录. 技巧与建议: 正如所见， MySQL的通配符很有用。但这种功能是有代价的：通配符搜索的处理一般要比前面讨论的其他搜索所花时间更长。这里给出一些使用通配符要记住的技巧。 不要过度使用通配符。如果其他操作符能达到相同的目的，应该 使用其他操作符。 在确实需要使用通配符时，除非绝对有必要，否则不要把它们用 在搜索模式的开始处。把通配符置于搜索模式的开始处，搜索起 来是最慢的。 仔细注意通配符的位置。如果放错地方，可能不会返回想要的数. 参考博文：https://blog.csdn.net/u011479200/article/details/78513632 17.count(*)、count(1)、count(column)的区别 count(*)对行的数目进行计算,包含NULL count(column)对特定的列的值具有的行数进行计算,不包含NULL值。 count()还有一种使用方式,count(1)这个用法和count(*)的结果是一样的。 性能问题: 1.任何情况下SELECT COUNT(*) FROM tablename是最优选择; 2.尽量减少SELECT COUNT(*) FROM tablename WHERE COL = ‘value’ 这种查询; 3.杜绝SELECT COUNT(COL) FROM tablename WHERE COL2 = ‘value’ 的出现。 如果表没有主键,那么count(1)比count(*)快。 如果有主键,那么count(主键,联合主键)比count(*)快。 如果表只有一个字段,count(*)最快。 count(1)跟count(主键)一样,只扫描主键。count(*)跟count(非主键)一样,扫描整个表。明显前者更快一些。 18.最左前缀原则 多列索引： ALTER TABLE people ADD INDEX lname_fname_age (lame,fname,age); 为了提高搜索效率，我们需要考虑运用多列索引,由于索引文件以B－Tree格式保存，所以我们不用扫描任何记录，即可得到最终结果。 注：在mysql中执行查询时，只能使用一个索引，如果我们在lname,fname,age上分别建索引,执行查询时，只能使用一个索引，mysql会选择一个最严格(获得结果集记录数最少)的索引。 最左前缀原则：顾名思义，就是最左优先，上例中我们创建了lname_fname_age多列索引,相当于创建了(lname)单列索引，(lname,fname)组合索引以及(lname,fname,age)组合索引。 二、索引1.什么是索引？ 何为索引： 数据库索引，是数据库管理系统中一个排序的数据结构，索引的实现通常使用B树及其变种B+树。 在数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 2.索引的作用？它的优点缺点是什么？ 索引作用： 协助快速查询、更新数据库表中数据。 为表设置索引要付出代价的： 一是增加了数据库的存储空间 二是在插入和修改数据时要花费较多的时间(因为索引也要随之变动)。 3.索引的优缺点？ 创建索引可以大大提高系统的性能（优点）： 1.通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 2.可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 3.可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 4.在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 5.通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 增加索引也有许多不利的方面(缺点)： 1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 2.索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 3.当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 4.哪些列适合建立索引、哪些不适合建索引？ 索引是建立在数据库表中的某些列的上面。在创建索引的时候，应该考虑在哪些列上可以创建索引，在哪些列上不能创建索引。 一般来说，应该在这些列上创建索引： （1）在经常需要搜索的列上，可以加快搜索的速度； （2）在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； （3）在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； （4）在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； （5）在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； （6）在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度。 对于有些列不应该创建索引： （1）对于那些在查询中很少使用或者参考的列不应该创建索引。 这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 （2）对于那些只有很少数据值的列也不应该增加索引。 这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 （3）对于那些定义为text, image和bit数据类型的列不应该增加索引。 这是因为，这些列的数据量要么相当大，要么取值很少。 (4)当修改性能远远大于检索性能时，不应该创建索引。 这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 5.什么样的字段适合建索引 唯一、不为空、经常被查询的字段 6.MySQL B+Tree索引和Hash索引的区别? Hash索引和B+树索引的特点： Hash索引结构的特殊性，其检索效率非常高，索引的检索可以一次定位; B+树索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问; 为什么不都用Hash索引而使用B+树索引？ Hash索引仅仅能满足”=”,”IN”和””查询，不能使用范围查询,因为经过相应的Hash算法处理之后的Hash值的大小关系，并不能保证和Hash运算前完全一样； Hash索引无法被用来避免数据的排序操作，因为Hash值的大小关系并不一定和Hash运算前的键值完全一样； Hash索引不能利用部分索引键查询，对于组合索引，Hash索引在计算Hash值的时候是组合索引键合并后再一起计算Hash值，而不是单独计算Hash值，所以通过组合索引的前面一个或几个索引键进行查询的时候，Hash索引也无法被利用； Hash索引在任何时候都不能避免表扫描，由于不同索引键存在相同Hash值，所以即使取满足某个Hash键值的数据的记录条数，也无法从Hash索引中直接完成查询，还是要回表查询数据； Hash索引遇到大量Hash值相等的情况后性能并不一定就会比B+树索引高。 补充： 1.MySQL中，只有HEAP/MEMORY引擎才显示支持Hash索引。 2.常用的InnoDB引擎中默认使用的是B+树索引，它会实时监控表上索引的使用情况，如果认为建立哈希索引可以提高查询效率，则自动在内存中的“自适应哈希索引缓冲区”建立哈希索引（在InnoDB中默认开启自适应哈希索引），通过观察搜索模式，MySQL会利用index key的前缀建立哈希索引，如果一个表几乎大部分都在缓冲池中，那么建立一个哈希索引能够加快等值查询。B+树索引和哈希索引的明显区别是： 3.如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；当然了，这个前提是，键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据； 4.如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索；同理，哈希索引没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）； 5.哈希索引也不支持多列联合索引的最左匹配规则； 6.B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在所谓的哈希碰撞问题。 7.在大多数场景下，都会有范围查询、排序、分组等查询特征，用B+树索引就可以了。 7.B树和B+树的区别 B树，每个节点都存储key和data，所有节点组成这棵树，并且叶子节点指针为nul，叶子结点不包含任何关键字信息。 B+树，所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大的顺序链接，所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (而B 树的非终节点也包含需要查找的有效信息) 8.为什么说B+比B树更适合实际应用中操作系统的文件索引和数据库索引？ 1.B+的磁盘读写代价更低 B+的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。 2.B+tree的查询效率更加稳定 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 9.聚集索引和非聚集索引区别? 聚合索引(clustered index): 聚集索引表记录的排列顺序和索引的排列顺序一致，所以查询效率快，只要找到第一个索引值记录，其余就连续性的记录在物理也一样连续存放。聚集索引对应的缺点就是修改慢，因为为了保证表中记录的物理和索引顺序一致，在记录插入的时候，会对数据页重新排序。聚集索引类似于新华字典中用拼音去查找汉字，拼音检索表于书记顺序都是按照a~z排列的，就像相同的逻辑顺序于物理顺序一样，当你需要查找a,ai两个读音的字，或是想一次寻找多个傻(sha)的同音字时，也许向后翻几页，或紧接着下一行就得到结果了。 非聚合索引(nonclustered index): 非聚集索引指定了表中记录的逻辑顺序，但是记录的物理和索引不一定一致，两种索引都采用B+树结构，非聚集索引的叶子层并不和实际数据页相重叠，而采用叶子层包含一个指向表中的记录在数据页中的指针方式。非聚集索引层次多，不会造成数据重排。非聚集索引类似在新华字典上通过偏旁部首来查询汉字，检索表也许是按照横、竖、撇来排列的，但是由于正文中是a~z的拼音顺序，所以就类似于逻辑地址于物理地址的不对应。同时适用的情况就在于分组，大数目的不同值，频繁更新的列中，这些情况即不适合聚集索引。 根本区别： 聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。 三、事务1.什么是事务？ 事务是对数据库中一系列操作进行统一的回滚或者提交的操作，主要用来保证数据的完整性和一致性。 2.事务四大特性（ACID）原子性、一致性、隔离性、持久性? 原子性（Atomicity）:原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响。 一致性（Consistency）:事务开始前和结束后，数据库的完整性约束没有被破坏。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）:隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）:持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 3.事务的并发?事务隔离级别，每个级别会引发什么问题，MySQL默认是哪个级别? 从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题，然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行， 在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行， 事务的隔离级别可以通过隔离事务属性指定。事务的并发问题 1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据 2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。 3、幻读：幻读解决了不重复读，保证了同一个事务里，查询的结果都是事务开始时的状态（一致性）。 例如：事务T1对一个表中所有的行的某个数据项做了从“1”修改为“2”的操作 这时事务T2又对这个表中插入了一行数据项，而这个数据项的数值还是为“1”并且提交给数据库。 而操作事务T1的用户如果再查看刚刚修改的数据，会发现还有跟没有修改一样，其实这行是从事务T2中添加的，就好像产生幻觉一样，这就是发生了幻读。小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 事务的隔离级别 读未提交：另一个事务修改了数据，但尚未提交，而本事务中的SELECT会读到这些未被提交的数据脏读 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致。 可重复读：在同一个事务里，SELECT的结果是事务开始时时间点的状态，因此，同样的SELECT操作读到的结果会是一致的。但是，会有幻读现象 串行化：最高的隔离级别，在这个隔离级别下，不会产生任何异常。并发的事务，就像事务是在一个个按照顺序执行一样 特别注意： MySQL默认的事务隔离级别为repeatable-read MySQL 支持 4 种事务隔离级别. 事务的隔离级别要得到底层数据库引擎的支持, 而不是应用程序或者框架的支持. Oracle 支持的 2 种事务隔离级别：READ_COMMITED , SERIALIZABLE SQL规范所规定的标准，不同的数据库具体的实现可能会有些差异 MySQL中默认事务隔离级别是“可重复读”时并不会锁住读取到的行 事务隔离级别：未提交读时，写数据只会锁住相应的行。 事务隔离级别为：可重复读时，写数据会锁住整张表。 事务隔离级别为：串行化时，读写数据都会锁住整张表。 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大，鱼和熊掌不可兼得啊。对于多数应用程序，可以优先考虑把数据库系统的隔离级别设为Read Committed，它能够避免脏读取，而且具有较好的并发性能。尽管它会导致不可重复读、幻读这些并发问题，在可能出现这类问题的个别场合，可以由应用程序采用悲观锁或乐观锁来控制。 4.事务传播行为 1.PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。 2.PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。 3.PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 4.PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 5.PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 6.PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 7.PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 5.嵌套事务 什么是嵌套事务？ 嵌套是子事务套在父事务中执行，子事务是父事务的一部分，在进入子事务之前，父事务建立一个回滚点，叫save point，然后执行子事务，这个子事务的执行也算是父事务的一部分，然后子事务执行结束，父事务继续执行。重点就在于那个save point。看几个问题就明了了： 如果子事务回滚，会发生什么？ 父事务会回滚到进入子事务前建立的save point，然后尝试其他的事务或者其他的业务逻辑，父事务之前的操作不会受到影响，更不会自动回滚。 如果父事务回滚，会发生什么？ 父事务回滚，子事务也会跟着回滚！为什么呢，因为父事务结束之前，子事务是不会提交的，我们说子事务是父事务的一部分，正是这个道理。那么： 事务的提交，是什么情况？ 是父事务先提交，然后子事务提交，还是子事务先提交，父事务再提交？答案是第二种情况，还是那句话，子事务是父事务的一部分，由父事务统一提交。 参考文章：https://blog.csdn.net/liangxw1/article/details/51197560 四、存储引擎1.MySQL常见的三种存储引擎（InnoDB、MyISAM、MEMORY）的区别? 两种存储引擎的大致区别表现在： 1.InnoDB支持事务，MyISAM不支持， 这一点是非常之重要。事务是一种高级的处理方式，如在一些列增删改中只要哪个出错还可以回滚还原，而MyISAM就不可以了。 2.MyISAM适合查询以及插入为主的应用。 3.InnoDB适合频繁修改以及涉及到安全性较高的应用。 4.InnoDB支持外键，MyISAM不支持。 5.从MySQL5.5.5以后，InnoDB是默认引擎。 6.InnoDB不支持FULLTEXT类型的索引。 7.InnoDB中不保存表的行数，如select count() from table时，InnoDB需要扫描一遍整个表来计算有多少行，但是MyISAM只要简单的读出保存好的行数即可。注意的是，当count()语句包含where条件时MyISAM也需要扫描整个表。 8.对于自增长的字段，InnoDB中必须包含只有该字段的索引，但是在MyISAM表中可以和其他字段一起建立联合索引。 9.DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的 删除，效率非常慢。MyISAM则会重建表。 10.InnoDB支持行锁（某些情况下还是锁整表，如 update table set a=1 where user like ‘%lee%’。 2.MySQL存储引擎MyISAM与InnoDB如何选择 MySQL有多种存储引擎，每种存储引擎有各自的优缺点，可以择优选择使用：MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE。 虽然MySQL里的存储引擎不只是MyISAM与InnoDB这两个，但常用的就是两个。关于MySQL数据库提供的两种存储引擎，MyISAM与InnoDB选择使用： 1.INNODB会支持一些关系数据库的高级功能，如事务功能和行级锁，MyISAM不支持。 2.MyISAM的性能更优，占用的存储空间少，所以，选择何种存储引擎，视具体应用而定。 如果你的应用程序一定要使用事务，毫无疑问你要选择INNODB引擎。但要注意，INNODB的行级锁是有条件的。在where条件没有使用主键时，照样会锁全表。比如DELETE FROM mytable这样的删除语句。 如果你的应用程序对查询性能要求较高，就要使用MyISAM了。MyISAM索引和数据是分开的，而且其索引是压缩的，可以更好地利用内存。所以它的查询性能明显优于INNODB。压缩后的索引也能节约一些磁盘空间。MyISAM拥有全文索引的功能，这可以极大地优化LIKE查询的效率。 有人说MyISAM只能用于小型应用，其实这只是一种偏见。如果数据量比较大，这是需要通过升级架构来解决，比如分表分库，而不是单纯地依赖存储引擎。 现在一般都是选用innodb了，主要是MyISAM的全表锁，读写串行问题，并发效率锁表，效率低，MyISAM对于读写密集型应用一般是不会去选用的。MEMORY存储引擎 MEMORY是MySQL中一类特殊的存储引擎。它使用存储在内存中的内容来创建表，而且数据全部放在内存中。这些特性与前面的两个很不同。每个基于MEMORY存储引擎的表实际对应一个磁盘文件。该文件的文件名与表名相同，类型为frm类型。该文件中只存储表的结构。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。值得注意的是，服务器需要有足够的内存来维持MEMORY存储引擎的表的使用。如果不需要了，可以释放内存，甚至删除不需要的表。 MEMORY默认使用哈希索引。速度比使用B型树索引快。当然如果你想用B型树索引，可以在创建索引时指定。 注意，MEMORY用到的很少，因为它是把数据存到内存中，如果内存出现异常就会影响数据。如果重启或者关机，所有数据都会消失。因此，基于MEMORY的表的生命周期很短，一般是一次性的。 3.MySQL的MyISAM与InnoDB两种存储引擎在，事务、锁级别，各自的适用场景? 事务处理上方面 MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 锁级别 MyISAM：只支持表级锁，用户在操作MyISAM表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 关于存储引擎MyISAM和InnoDB的其他参考资料如下： MySQL存储引擎中的MyISAM和InnoDB区别详解 MySQL存储引擎之MyISAM和Innodb总结性梳理 五、优化1.查询语句不同元素（where、jion、limit、group by、having等等）执行先后顺序? 1.查询中用到的关键词主要包含六个，并且他们的顺序依次为 select–from–where–group by–having–order by 其中select和from是必须的，其他关键词是可选的，这六个关键词的执行顺序 与sql语句的书写顺序并不是一样的，而是按照下面的顺序来执行 from:需要从哪个数据表检索数据 where:过滤表中数据的条件 group by:如何将上面过滤出的数据分组 having:对上面已经分组的数据进行过滤的条件 select:查看结果集中的哪个列，或列的计算结果 order by :按照什么样的顺序来查看返回的数据 2.from后面的表关联，是自右向左解析 而where条件的解析顺序是自下而上的。 也就是说，在写SQL语句的时候，尽量把数据量小的表放在最右边来进行关联（用小表去匹配大表），而把能筛选出小量数据的条件放在where语句的最左边 （用小表去匹配大表） 其他参考资源：http://www.cnblogs.com/huminxxl/p/3149097.html 2.使用explain优化sql和索引? 对于复杂、效率低的sql语句，我们通常是使用explain sql 来分析sql语句，这个语句可以打印出，语句的执行。这样方便我们分析，进行优化 table：显示这一行的数据是关于哪张表的 type：这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、index和ALL all:full table scan ;MySQL将遍历全表以找到匹配的行； index: index scan; index 和 all的区别在于index类型只遍历索引； range：索引范围扫描，对索引的扫描开始于某一点，返回匹配值的行，常见与between ，等查询； ref：非唯一性索引扫描，返回匹配某个单独值的所有行，常见于使用非唯一索引即唯一索引的非唯一前缀进行查找； eq_ref：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常用于主键或者唯一索引扫描； const，system：当MySQL对某查询某部分进行优化，并转为一个常量时，使用这些访问类型。如果将主键置于where列表中，MySQL就能将该查询转化为一个常量。 possible_keys：显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句 key： 实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MySQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MySQL忽略索引 key_len：使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref：显示索引的哪一列被使用了，如果可能的话，是一个常数 rows：MySQL认为必须检查的用来返回请求数据的行数 Extra：关于MySQL如何解析查询的额外信息。将在表4.3中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思MySQL根本不能使用索引，结果是检索会很慢。 3.MySQL慢查询怎么解决? slow_query_log 慢查询开启状态。 slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录）。 long_query_time 查询超过多少秒才记录。 六、数据库锁1.mysql都有什么锁，死锁判定原理和具体场景，死锁怎么解决? MySQL有三种锁的级别：全局锁、表级级、行级锁。 全局锁：对整个数据库实例加锁。提供加全局读锁的方法：Flush tables with read lock(FTWRL)，这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。使用场景是全库逻辑备份。MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL) 表级锁：MySQL里面表级锁有两种，一种是表锁，一种是元数据锁(meta data lock,MDL)，锁住整个表，其中 MDL 不需要显式使用，在访问一个表的时候会被自动加上。Server 层面实现的！！！！ 行级锁：存储引擎级别实现的，具体可以看自己写的另外一篇文章 《Mysql 再度出发》。 什么是死锁？ 死锁: 是指两个或两个以上的进程在执行过程中。因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等竺的进程称为死锁进程。 表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。 死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。 那么对应的解决死锁问题的关键就是：让不同的session加锁有次序。 死锁的解决办法? 1.查出的线程杀死 killSELECT trx_MySQL_thread_id FROM information_schema.INNODB_TRX; 2.设置锁的超时时间Innodb 行锁的等待时间，单位秒。可在会话级别设置，RDS 实例该参数的默认值为 50（秒）。 生产环境不推荐使用过大的 innodb_lock_wait_timeout参数值该参数支持在会话级别修改，方便应用在会话级别单独设置某些特殊操作的行锁等待超时时间，如下：set innodb_lock_wait_timeout=1000; —设置当前会话 Innodb 行锁等待超时时间，单位秒。 3.指定获取锁的顺序 2.有哪些锁（乐观锁悲观锁），select 时怎么加排它锁? 悲观锁（Pessimistic Lock）: 悲观锁特点:先获取锁，再进行业务操作。 即“悲观”的认为获取锁是非常有可能失败的，因此要先确保获取锁成功再进行业务操作。通常所说的“一锁二查三更新”即指的是使用悲观锁。通常来讲在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select for update时会获取被select中的数据行的行锁，因此其他并发执行的select for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。 补充：不同的数据库对select for update的实现和支持都是有所区别的， oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，MySQL就没有no wait这个选项。 MySQL还有个问题是select for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此如果在MySQL中用悲观锁务必要确定走了索引，而不是全表扫描。 乐观锁（Optimistic Lock）: 1.乐观锁，也叫乐观并发控制，它假设多用户并发的事务在处理时不会彼此互相影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，那么当前正在提交的事务会进行回滚。 2.乐观锁的特点先进行业务操作，不到万不得已不去拿锁。即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。 3.一般的做法是在需要锁的数据上增加一个版本号，或者时间戳， 实现方式举例如下： 乐观锁（给表加一个版本号字段） 这个并不是乐观锁的定义，给表加版本号，是数据库实现乐观锁的一种方式。 SELECT data AS old_data, version AS old_version FROM …; 根据获取的数据进行业务操作，得到new_data和new_version UPDATE SET data = new_data, version = new_version WHERE version = old_version if (updated row &gt; 0) { // 乐观锁获取成功，操作完成 } else { // 乐观锁获取失败，回滚并重试 } 注意： 乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能 乐观锁还适用于一些比较特殊的场景，例如在业务操作过程中无法和数据库保持连接等悲观锁无法适用的地方。 总结：悲观锁和乐观锁是数据库用来保证数据并发安全防止更新丢失的两种方法，例子在select … for update前加个事务就可以防止更新丢失。悲观锁和乐观锁大部分场景下差异不大，一些独特场景下有一些差别，一般我们可以从如下几个方面来判断。 响应速度： 如果需要非常高的响应速度，建议采用乐观锁方案，成功就执行，不成功就失败，不需要等待其他并发去释放锁。’ 冲突频率： 如果冲突频率非常高，建议采用悲观锁，保证成功率，如果冲突频率大，乐观锁会需要多次重试才能成功，代价比较大。 重试代价： 如果重试代价大，建议采用悲观锁。 七、其他1.数据库的主从复制 主从复制的几种方式: 同步复制: 所谓的同步复制，意思是master的变化，必须等待slave-1,slave-2,…,slave-n完成后才能返回。 这样，显然不可取，也不是MySQL复制的默认设置。比如，在WEB前端页面上，用户增加了条记录，需要等待很长时间。 异步复制: 如同AJAX请求一样。master只需要完成自己的数据库操作即可。至于slaves是否收到二进制日志，是否完成操作，不用关心,MySQL的默认设置。 半同步复制: master只保证slaves中的一个操作成功，就返回，其他slave不管。 这个功能，是由google为MySQL引入的。 2.数据库主从复制分析的 7 个问题?问题1：master的写操作，slaves被动的进行一样的操作，保持数据一致性，那么slave是否可以主动的进行写操作？ 假设slave可以主动的进行写操作，slave又无法通知master，这样就导致了master和slave数据不一致了。因此slave不应该进行写操作，至少是slave上涉及到复制的数据库不可以写。实际上，这里已经揭示了读写分离的概念。 问题2：主从复制中，可以有N个slave,可是这些slave又不能进行写操作，要他们干嘛？ 实现数据备份:类似于高可用的功能，一旦master挂了，可以让slave顶上去，同时slave提升为master。 异地容灾:比如master在北京，地震挂了，那么在上海的slave还可以继续。主要用于实现scale out,分担负载,可以将读的任务分散到slaves上。【很可能的情况是，一个系统的读操作远远多于写操作，因此写操作发向master，读操作发向slaves进行操作】 问题3：主从复制中有master,slave1,slave2,…等等这么多MySQL数据库，那比如一个JAVA WEB应用到底应该连接哪个数据库? 我们在应用程序中可以这样，insert/delete/update这些更新数据库的操作，用connection(for master)进行操作， select用connection(for slaves)进行操作。那我们的应用程序还要完成怎么从slaves选择一个来执行select，例如使用简单的轮循算法。 这样的话，相当于应用程序完成了SQL语句的路由，而且与MySQL的主从复制架构非常关联，一旦master挂了，某些slave挂了，那么应用程序就要修改了。能不能让应用程序与MySQL的主从复制架构没有什么太多关系呢？找一个组件，application program只需要与它打交道，用它来完成MySQL的代理，实现SQL语句的路由。MySQL proxy并不负责，怎么从众多的slaves挑一个？可以交给另一个组件(比如haproxy)来完成。 这就是所谓的MySQL READ WRITE SPLITE，MySQL的读写分离。 问题4：如果MySQL proxy , direct , master他们中的某些挂了怎么办？ 总统一般都会弄个副总统，以防不测。同样的，可以给这些关键的节点来个备份。 问题5：当master的二进制日志每产生一个事件，都需要发往slave，如果我们有N个slave,那是发N次，还是只发一次？如果只发一次，发给了slave-1，那slave-2,slave-3,…它们怎么办？ 显 然，应该发N次。实际上，在MySQL master内部，维护N个线程，每一个线程负责将二进制日志文件发往对应的slave。master既要负责写操作，还的维护N个线程，负担会很重。可以这样，slave-1是master的从，slave-1又是slave-2,slave-3,…的主，同时slave-1不再负责select。 slave-1将master的复制线程的负担，转移到自己的身上。这就是所谓的多级复制的概念。 问题6：当一个select发往MySQL proxy，可能这次由slave-2响应，下次由slave-3响应，这样的话，就无法利用查询缓存了。 应该找一个共享式的缓存，比如memcache来解决。将slave-2,slave-3,…这些查询的结果都缓存至mamcache中。 问题7：随着应用的日益增长，读操作很多，我们可以扩展slave，但是如果master满足不了写操作了，怎么办呢？ scale on ?更好的服务器？ 没有最好的，只有更好的，太贵了。。。scale out ? 主从复制架构已经满足不了。可以分库【垂直拆分】，分表【水平拆分】。 3.mysql 高并发环境解决方案? MySQL 高并发环境解决方案： 分库 分表 分布式 增加二级缓存。。。。。 需求分析：互联网单位 每天大量数据读取，写入，并发性高。 现有解决方式：水平分库分表，由单点分布到多点数据库中，从而降低单点数据库压力。 集群方案：解决DB宕机带来的单点DB不能访问问题。 读写分离策略：极大限度提高了应用中Read数据的速度和并发量。无法解决高写入压力。 4.数据库崩溃时事务的恢复机制（REDO日志和UNDO日志）?转载：MySQL REDO日志和UNDO日志 Undo Log: Undo Log是为了实现事务的原子性，在MySQL数据库InnoDB存储引擎中，还用了Undo Log来实现多版本并发控制(简称：MVCC)。 事务的原子性(Atomicity)事务中的所有操作，要么全部完成，要么不做任何操作，不能只做部分操作。如果在执行的过程中发生了错误，要回滚(Rollback)到事务开始前的状态，就像这个事务从来没有执行过。原理Undo Log的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为UndoLog）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 之所以能同时保证原子性和持久化，是因为以下特点： 更新数据前记录Undo log。为了保证持久性，必须将数据在事务提交前写到磁盘。只要事务成功提交，数据必然已经持久化。Undo log必须先于数据持久化到磁盘。如果在G,H之间系统崩溃，undo log是完整的， 可以用来回滚事务。如果在A-F之间系统崩溃,因为数据没有持久化到磁盘。所以磁盘上的数据还是保持在事务开始前的状态。 缺陷：每个事务提交前将数据和Undo Log写入磁盘，这样会导致大量的磁盘IO，因此性能很低。如果能够将数据缓存一段时间，就能减少IO提高性能。但是这样就会丧失事务的持久性。因此引入了另外一种机制来实现持久化，即Redo Log。 Redo Log: 原理和Undo Log相反，Redo Log记录的是新数据的备份。在事务提交前，只要将Redo Log持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是Redo Log已经持久化。系统可以根据Redo Log的内容，将所有数据恢复到最新的状态。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（八）--- 总结 & 补充 & api 使用]]></title>
    <url>%2F2020%2F03%2F01%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E5%85%AB%EF%BC%89---%20%E6%80%BB%E7%BB%93%20%26%20api%20%E4%BD%BF%E7%94%A8%20%26%20%E8%A1%A5%E5%85%85.html</url>
    <content type="text"><![CDATA[学习路径 补充以上基本上涵盖了自己看的多线程的内容，但是呢，还有一些 juc 下的类我没有提及到，但是又比较重要，下面我稍微展开说一下，就不从源码上分析了，因为都是大同小异的，主要还是从跟以前谈到的类的异同点出发，然后讲一下实际用途和使用方式。 StampedLock在多线程（一），我们就提及了 ReentrantLock 和 ReentrantReadWriteLock，他们是在 jdk 1. 5 就引入了，而这个 StampedLock 则是在 jdk 1.8 引入的，既然晚了好几年才来，肯定是更加优秀了！答案也是肯定的，StampedLock 继承了前辈们的优点。ReentrantLock 继承了 AQS 的优点，引入自旋锁减少锁的消耗，ReentrantReadWriteLock 则在 ReentrantLock 基础之上，大大解决了读多写少的场景下，并发的效率。读-读之间不互斥，共享，写-读，写-写依然互斥。StampedLock 则在 ReentrantReadWriteLock 基础上继续扩展，为了解决读多写少场景下写饥饿的问题，引入悲观读锁和乐观读锁的概念，所谓的悲观读锁就是 ReentrantReadWriteLock 中的 readLock，即每次读数据时都需要加锁，且读-写互斥，而乐观锁不同，当数据没有发生变化（判断 stamp 是否发生变化）时，可以不加锁，直接读取，也就是可以做到 读-写不互斥「因为人家读压根没要锁…只是去读了个 stamp 而已…」。 部分方法 可以看到，整体上其实 StampedLock 和 ReentrantReadWriteLock 方法差不多，实现方式也是一样，都是用了 CLH 队列搭配自旋解决排队等待问题，最重要的还是乐观读锁的加入。「稍微看了一眼源码，这里的队列好像不是 Node 节点了，而是 WNode 节点，原因是加了一个属性—- stamp。」 所以我们现在来看看官方给的例子，体会一下乐观读锁如何使用的： 12345678910111213141516171819202122232425262728293031323334public class Point &#123; private double x, y;//内部定义表示坐标点 private final StampedLock s1 = new StampedLock();//定义了 StampedLock 锁 void move(double deltaX, double deltaY) &#123; // 写 long stamp = s1.writeLock();//注意 StampedLock.writeLock()可没有响应中断哦 //响应中断需调用StampedLock.asWriteLock()，等同于ReentrantReadWriteLock.writeLock() try &#123; x += deltaX; y += deltaY; &#125; finally &#123; s1.unlockWrite(stamp);//退出临界区,释放写锁 &#125; &#125; double distanceFormOrigin() &#123;//只读方法 long stamp = s1.tryOptimisticRead(); //试图尝试一次乐观读 返回一个类似于时间戳的邮戳整数stamp 这个stamp就可以作为这一个所获取的凭证 //---------------------------------- // 这里就是开始读取内容 double currentX = x, currentY = y;//读取x和y的值 //---------------------------------- // 然后判断一下读的过程中，stamp 有没有变化，就是说期间有没有写锁工作了，只有写锁改变 stamp if (!s1.validate(stamp)) &#123; stamp = s1.readLock(); // 变成悲观读锁，注意这里跟写锁一样不是可中断的 try &#123; currentX = x; currentY = y; &#125; finally &#123; s1.unlockRead(stamp);//退出临界区,释放读锁 &#125; &#125; return Math.sqrt(currentX * currentX + currentY * currentY); &#125;&#125; 总结一下： StampedLock 中的 asReadLock() 和 asWriteLock() 等同于 ReentrantReadWriteLock 中的 readLock() 和 writeLock()，都是可中断的，但是 StampedLock() 也提供了 readLock() 和 writeLock()，但这是不可中断的。 StampedLock 中的读是先乐观读，在乐观读之前，先拿到当前的 stamp，然后再读完之后验证一遍是否中途有写锁改变了 stamp，防止出现脏读，如果 stamp 变化了，则升级成悲观读锁再读一遍。 注意 StampedLock.readLock() 和 Stamped.writeLock()的用法，会返回一个 long stamp，直接 Stamped.readLock() 就是获取锁并且返回 stamp，然后解锁要带着这个 stamp 才能解锁。 一些疑惑首先看到网上说的，因为 Stamped.readLock()不处理中断异常，所以在线程挂起后，手动添加中断，能够将挂起的线程从 wait —&gt; running，然后如果此时想要获取的锁如果一直被写锁持有，那么读锁就会一直自旋，使得CPU 爆炸… 于是我非常好奇的试了一下，发现出现了一些问题，如果我将 StampedLock 退化成 ReentrantReadWriteLock，并且手动触发中断并且不处理中断，的确会导致 CPU 占满： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package 多线程.补充;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.LockSupport;import java.util.concurrent.locks.StampedLock;public class StampedLockDemo &#123; static Thread[] holdCpuThreads = new Thread[3]; static final StampedLock lock = new StampedLock(); static Lock readLock = lock.asReadLock(); static Lock writeLock = lock.asWriteLock(); public static void main(String[] args) throws InterruptedException &#123; new Thread(() -&gt; &#123; writeLock.lock(); try &#123; TimeUnit.SECONDS.sleep(40); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; writeLock.unlock(); System.out.println("123"); &#125;).start(); Thread.sleep(2000); for (int i = 0; i &lt; 3; ++i) &#123; holdCpuThreads[i] = new Thread(new HoldCPUReadThread()); holdCpuThreads[i].start(); &#125; System.out.println("0------------"); Thread.sleep(10000); System.out.println("1------------"); for (int i = 0; i &lt; 3; i++) &#123; holdCpuThreads[i].interrupt(); &#125; &#125; private static class HoldCPUReadThread implements Runnable &#123; public void run() &#123; readLock.lock(); System.out.println(Thread.currentThread().getName() + " get read lock"); readLock.unlock(); System.out.println("--------------------------"); &#125; &#125;&#125; 很容易看到，在中断之前，读线程全部被挂起了，所以并不占用 cpu 资源，但是手动中断后，park()被唤醒，直接导致读线程 running，不停的自旋获取锁，但是锁一直被 write 占有，所以就把单核 CPU 占满，所以我这个应该是4 核处理器？说好的 6 核呢？？ 如果不退化的话，经过测试，的确也会发生一样的情况。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package 多线程.补充;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.LockSupport;import java.util.concurrent.locks.StampedLock;public class StampedLockDemo &#123; static Thread[] holdCpuThreads = new Thread[3]; static final StampedLock lock = new StampedLock(); public static void main(String[] args) throws InterruptedException &#123; new Thread(() -&gt; &#123; long stamp1 = lock.writeLock(); try &#123; TimeUnit.SECONDS.sleep(40); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;// LockSupport.parkNanos(6100000000L); lock.unlockWrite(stamp1); System.out.println("123"); &#125;).start(); Thread.sleep(2000); for (int i = 0; i &lt; 3; ++i) &#123; holdCpuThreads[i] = new Thread(new HoldCPUReadThread()); holdCpuThreads[i].start(); &#125; System.out.println("0------------"); Thread.sleep(10000); System.out.println("1------------"); for (int i = 0; i &lt; 3; i++) &#123; holdCpuThreads[i].interrupt(); &#125; &#125; private static class HoldCPUReadThread implements Runnable &#123; public void run() &#123; long stamp2 = lock.readLock(); System.out.println(Thread.currentThread().getName() + " get read lock"); lock.unlockRead(stamp2); System.out.println("--------------------------"); &#125; &#125;&#125; 中断来之前的 CPU： 中断来了之后： 但是，但是，但是！网上的例子太不靠谱了，大家注意看，下面这个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142package 多线程.补充;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.LockSupport;import java.util.concurrent.locks.StampedLock;public class StampedLockDemo &#123; static Thread[] holdCpuThreads = new Thread[3]; static final StampedLock lock = new StampedLock(); public static void main(String[] args) throws InterruptedException &#123; new Thread(() -&gt; &#123; long stamp1 = lock.writeLock(); // 问题出在这 LockSupport.parkNanos(6100000000L); lock.unlockWrite(stamp1); System.out.println("123"); &#125;).start(); Thread.sleep(2000); for (int i = 0; i &lt; 3; ++i) &#123; holdCpuThreads[i] = new Thread(new HoldCPUReadThread()); holdCpuThreads[i].start(); &#125; System.out.println("0------------"); Thread.sleep(10000); System.out.println("1------------"); for (int i = 0; i &lt; 3; i++) &#123; holdCpuThreads[i].interrupt(); &#125; &#125; private static class HoldCPUReadThread implements Runnable &#123; public void run() &#123; long stamp2 = lock.readLock(); System.out.println(Thread.currentThread().getName() + " get read lock"); lock.unlockRead(stamp2); System.out.println("--------------------------"); &#125; &#125;&#125; 1234567890------------123Thread-3 get read lockThread-2 get read lock----------------------------------------------------Thread-1 get read lock--------------------------1------------ CPU 全程没有变化，维持在 2%，说明没有发生自旋。LockSupport.parkNanos(6100000000L);这一行，意思是该线程只会挂起 6 秒，等你10秒后中断来了的时候写锁早就释放锁了，此时被唤醒的读锁自旋能够直接拿到锁，不会死循环一直自旋，所以 CPU 压根不会发生变化…看了下网上说的例子，基本上全是错误的…难道真的全部都是抄来抄去嘛…好歹验证一下嘛… ConcurrentLinkedQueue List对应的线程安全的有 CopyOnWriteArrayList Map对应的线程安全的有 ConcurrentHashMap Queue对应的线程安全的则是 ConcurrentLinkedQueue https://www.ibm.com/developerworks/cn/java/j-lo-concurrent/index.html 「」 https://blog.csdn.net/qq_38293564/article/details/80798310 非阻塞算法 一个线程的失败和挂起不会引起其他些线程的失败和挂起，这样的算法称为非阻塞算法。非阻塞算法通过使用底层机器级别的原子指令来取代锁，从而保证数据在并发访问下的一致性。 ConcurrentLinkedQueue 的非阻塞算法简述本文接下来将在分析 ConcurrentLinkedQueue 源代码实现的过程中，穿插讲解非阻塞算法的具体实现。为了便于读者理解本文，首先让我们对它的实现机制做个全局性的简述。ConcurrentLinkedQueue 的非阻塞算法实现可概括为下面 5 点： 使用 CAS 原子指令来处理对数据的并发访问，这是非阻塞算法得以实现的基础。 head/tail 并非总是指向队列的头 / 尾节点，也就是说允许队列处于不一致状态。 这个特性把入队 / 出队时，原本需要一起原子化执行的两个步骤分离开来，从而缩小了入队 / 出队时需要原子化更新值的范围到唯一变量。这是非阻塞算法得以实现的关键。 由于队列有时会处于不一致状态。为此，ConcurrentLinkedQueue 使用三个不变式来维护非阻塞算法的正确性。 以批处理方式来更新 head/tail，从整体上减少入队 / 出队操作的开销。 为了有利于垃圾收集，队列使用特有的 head 更新机制；为了确保从已删除节点向后遍历，可到达所有的非删除节点，队列使用了特有的向后推进策略。 ConcurrentLinkedQueue 有机整合了上述 5 点来实现非阻塞算法。 不变式在后面的源代码分析中，我们将会看到队列有时会处于不一致状态。为此，ConcurrentLinkedQueue 使用三个不变式 ( 基本不变式，head 的不变式和 tail 的不变式 )，来约束队列中方法的执行。通过这三个不变式来维护非阻塞算法的正确性。 不变式：并发对象需要一直保持的特性。不变式是并发对象的各个方法之间必须遵守的“契约”，每个方法在调用前和调用后都必须保持不变式。采用不变式，就可以隔离的分析每个方法，而不用考虑它们之间所有可能的交互。 基本不变式在执行方法之前和之后，队列必须要保持的不变式： 当入队插入新节点之后，队列中有一个 next 域为 null 的（最后）节点。 从 head 开始遍历队列，可以访问所有 item 域不为 null 的节点。 head 的不变式和可变式在执行方法之前和之后，head 必须保持的不变式： 所有“活着”的节点（指未删除节点），都能从 head 通过调用 succ() 方法遍历可达。 head 不能为 null。 head 节点的 next 域不能引用到自身。 在执行方法之前和之后，head 的可变式： head 节点的 item 域可能为 null，也可能不为 null。 允许 tail 滞后（lag behind）于 head，也就是说：从 head 开始遍历队列，不一定能到达 tail。 tail 的不变式和可变式在执行方法之前和之后，tail 必须保持的不变式： 通过 tail 调用 succ() 方法，最后节点总是可达的。 tail 不能为 null。 在执行方法之前和之后，tail 的可变式： tail 节点的 item 域可能为 null，也可能不为 null。 允许 tail 滞后于 head，也就是说：从 head 开始遍历队列，不一定能到达 tail。 tail 节点的 next 域可以引用到自身。 补充Q1：head/tail 并非总是指向队列的头 / 尾节点，具体是怎样的呢？ A1： 不是每次出队时都更新head节点，当head节点里有元素时，直接弹出head节点里的元素，而不会更新head节点。只有当head节点里没有元素时，出队操作才会更新head节点。采用这种方式也是为了减少使用CAS更新head节点的消耗，从而提高出队效率。所以head元素可能是 null，此时并不是队列的头结点。 tail节点并不总是尾节点，所以每次入队都必须先通过tail节点来找到尾节点，尾节点可能就是tail节点，也可能是tail节点的next节点。doug lea使用hops变量来控制并减少tail节点的更新频率，并不是每次节点入队后都将 tail节点更新成尾节点，而是当tail节点和尾节点的距离大于等于常量HOPS的值（默认等于1）时才更新tail节点，tail和尾节点的距离越长使用CAS更新tail节点的次数就会越少，但是距离越长带来的负面效果就是每次入队时定位尾节点的时间就越长，因为循环体需要多循环一次来定位出尾节点，但是这样仍然能提高入队的效率，因为从本质上来看它通过增加对volatile变量的读操作来减少了对volatile变量的写操作，而对volatile变量的写操作开销要远远大于读操作，所以入队效率会有所提升。 所有的 BlockingQueue讲完了非阻塞算法，来讲讲阻塞队列换换口味。 阻塞队列与普通队列的区别在于，当队列是空的时，从队列中获取元素的操作将会被阻塞，或者队列是满时，往队列里添加元素的操作会被阻塞。试图从空的阻塞队列中获取元素的线程将会被阻塞，直到其他的线程往空的队列插入新的元素。同样，试图往已满的阻塞队列中添加新元素的线程同样也会被阻塞，直到其他的线程使队列重新变得空闲起来，如从队列中移除一个或者多个元素，或者完全清空队列. 实现 BlockingQueue 接口的有 ArrayBlockingQueue DelayQueue LinkedBlockingDeque LinkedBlockingQueue LinkedTransferQueue PriorityBlockingQueue SynchronousQueue ArrayBlockingQueueArrayBlockingQueue是由数组实现的有界阻塞队列。该队列命令元素 FIFO（先进先出）。因此，对头元素时队列中存在时间最长的数据元素，而对尾数据则是当前队列最新的数据元素。ArrayBlockingQueue 可作为“有界数据缓冲区”，生产者插入数据到队列容器中，并由消费者提取。ArrayBlockingQueue 一旦创建，容量不能改变。 当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。 ArrayBlockingQueue 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 ArrayBlockingQueue。而非公平性则是指访问 ArrayBlockingQueue 的顺序不是遵守严格的时间顺序，有可能存在，一旦 ArrayBlockingQueue 可以被访问时，长时间阻塞的线程依然无法访问到 ArrayBlockingQueue。如果保证公平性，通常会降低吞吐量。 DelayQueueDelayQueue 是一个存放实现 Delayed 接口的数据的无界阻塞队列，只有当数据对象的延时时间达到时才能插入到队列进行存储。如果当前所有的数据都还没有达到创建时所指定的延时期，则队列没有队头，并且线程通过 poll 等方法获取数据元素则返回 null。所谓数据延时期满时，则是通过 Delayed 接口的getDelay(TimeUnit.NANOSECONDS)来进行判定，如果该方法返回的是小于等于 0 则说明该数据元素的延时期已满。 LinkedBlockingDequeLinkedBlockingDeque 是基于链表数据结构的有界阻塞双端队列，如果在创建对象时为指定大小时，其默认大小为 Integer.MAX_VALUE。与 LinkedBlockingQueue 相比，主要的不同点在于，LinkedBlockingDeque 具有双端队列的特性。 LinkedBlockingQueue是用链表实现的有界阻塞队列，同样满足 FIFO 的特性，与 ArrayBlockingQueue 相比起来具有更高的吞吐量，为了防止 LinkedBlockingQueue 容量迅速增，损耗大量内存。通常在创建 LinkedBlockingQueue 对象时，会指定其大小，如果未指定，容量等于 Integer.MAX_VALUE。在上文多线程（七）中我们使用了该队列的思想完成了生产者-消费者分离两把锁，分别进行通信。 跟 ArrayBlockingQueue 一样提供了 put() 和 take() 方法可以进行阻塞插入和移除。 ArrayBlockingQueue 与 LinkedBlockingQueue 的比较 相同点：ArrayBlockingQueue 和 LinkedBlockingQueue 都是通过 condition 通知机制来实现可阻塞式插入和删除元素，并满足线程安全的特性； 不同点： ArrayBlockingQueue 底层是采用的数组进行实现，而 LinkedBlockingQueue 则是采用链表数据结构； ArrayBlockingQueue 插入和删除数据，只采用了一个 lock，而 LinkedBlockingQueue 则是在插入和删除分别采用了putLock和takeLock，这样可以降低线程由于线程无法获取到 lock 而进入 WAITING 状态的可能性，从而提高了线程并发执行的效率。 LinkedTransferQueueLinkedTransferQueue 是一个由链表数据结构构成的无界阻塞队列，由于该队列实现了 TransferQueue 接口，与其他阻塞队列相比主要有以下不同的方法： transfer(E e) 如果当前有线程（消费者）正在调用 take()方法或者可延时的 poll()方法进行消费数据时，生产者线程可以调用 transfer 方法将数据传递给消费者线程。如果当前没有消费者线程的话，生产者线程就会将数据插入到队尾，直到有消费者能够进行消费才能退出。 tryTransfer(E e) tryTransfer 方法如果当前有消费者线程（调用 take 方法或者具有超时特性的 poll 方法）正在消费数据的话，该方法可以将数据立即传送给消费者线程，如果当前没有消费者线程消费数据的话，就立即返回false。因此，与 transfer 方法相比，transfer 方法是必须等到有消费者线程消费数据时，生产者线程才能够返回。而 tryTransfer 方法能够立即返回结果退出。 tryTransfer(E e,long timeout,imeUnit unit) 与 transfer 基本功能一样，只是增加了超时特性，如果数据才规定的超时时间内没有消费者进行消费的话，就返回false。 PriorityBlockingQueuePriorityBlockingQueue 是一个支持优先级的无界阻塞队列。默认情况下元素采用自然顺序进行排序，也可以通过自定义类实现 compareTo()方法来指定元素排序规则，或者初始化时通过构造器参数 Comparator 来指定排序规则。 SynchronousQueueSynchronousQueue 是一个不存储元素的阻塞队列。每个插入操作必须等待另一个线程进行相应的删除操作，因此，SynchronousQueue 实际上没有存储任何数据元素，因为只有线程在删除数据时，其他线程才能插入数据，同样的，如果当前有线程在插入数据时，线程才能删除数据。SynchronousQueue 也可以通过构造器参数来为其指定公平性。两个线程只有互相等到了才能工作，有点类似于 Exchanger()。 Q: https://www.ibm.com/developerworks/cn/java/j-lo-concurrent/index.html 基于非阻塞算法实现的并发容器包括：ConcurrentLinkedQueue，SynchronousQueue，Exchanger 和 ConcurrentSkipListMap。ConcurrentLinkedQueue 是一个基于链接节点的无界线程安全队列。SynchronousQueue 是一个没有容量的阻塞队列，它使用双重数据结构 来实现非阻塞算法。Exchanger 是一个能对元素进行配对和交换的交换器。它使用 消除 技术来实现非阻塞算法 。ConcurrentSkipListMap 是一个可以根据 Key 进行排序的可伸缩的并发 Map。 这块我还有点疑问… 阻塞队列是用的非阻塞算法实现？可能这两个阻塞的意思不一样，前者是指队列满或者空阻塞的队列称为阻塞队列，后者非阻塞算法指的是 一个线程的挂起和失败不会影响另一个线程的挂起和失败。（也就是基于CAS实现的阻塞队列….） 也不知道理解的对不对……. Atomic 原子类全部是 CAS 操作的，具有原子性。 面试中好像没见到过…略过… Fork-Join 转自： https://www.liaoxuefeng.com/article/1146802219354112 当我们需要执行大量的小任务时，有经验的Java开发人员都会采用线程池来高效执行这些小任务。然而，有一种任务，例如，对超过1000万个元素的数组进行排序，这种任务本身可以并发执行，但如何拆解成小任务需要在任务执行的过程中动态拆分。这样，大任务可以拆成小任务，小任务还可以继续拆成更小的任务，最后把任务的结果汇总合并，得到最终结果，这种模型就是Fork/Join模型。 Java7引入了Fork/Join框架，我们通过RecursiveTask这个类就可以方便地实现Fork/Join模式。 例如，对一个大数组进行并行求和的RecursiveTask，就可以这样编写： 1234567891011121314151617181920212223242526272829303132333435363738394041class SumTask extends RecursiveTask&lt;Long&gt; &#123; static final int THRESHOLD = 100; long[] array; int start; int end; SumTask(long[] array, int start, int end) &#123; this.array = array; this.start = start; this.end = end; &#125; @Override protected Long compute() &#123; if (end - start &lt;= THRESHOLD) &#123; // 如果任务足够小,直接计算: long sum = 0; for (int i = start; i &lt; end; i++) &#123; sum += array[i]; &#125; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(String.format("compute %d~%d = %d", start, end, sum)); return sum; &#125; // 任务太大,一分为二: int middle = (end + start) / 2; System.out.println(String.format("split %d~%d ==&gt; %d~%d, %d~%d", start, end, start, middle, middle, end)); SumTask subtask1 = new SumTask(this.array, start, middle); SumTask subtask2 = new SumTask(this.array, middle, end); invokeAll(subtask1, subtask2); Long subresult1 = subtask1.join(); Long subresult2 = subtask2.join(); Long result = subresult1 + subresult2; System.out.println("result = " + subresult1 + " + " + subresult2 + " ==&gt; " + result); return result; &#125;&#125; 编写这个Fork/Join任务的关键在于，在执行任务的compute()方法内部，先判断任务是不是足够小，如果足够小，就直接计算并返回结果（注意模拟了1秒延时），否则，把自身任务一拆为二，分别计算两个子任务，再返回两个子任务的结果之和。 最后写一个main()方法测试： 123456789101112public static void main(String[] args) throws Exception &#123; // 创建随机数组成的数组: long[] array = new long[400]; fillRandom(array); // fork/join task: ForkJoinPool fjp = new ForkJoinPool(4); // 最大并发数4 ForkJoinTask&lt;Long&gt; task = new SumTask(array, 0, array.length); long startTime = System.currentTimeMillis(); Long result = fjp.invoke(task); long endTime = System.currentTimeMillis(); System.out.println("Fork/join sum: " + result + " in " + (endTime - startTime) + " ms.");&#125; 关键代码是fjp.invoke(task)来提交一个Fork/Join任务并发执行，然后获得异步执行的结果。 我们设置任务的最小阀值是100，当提交一个400大小的任务时，在4核CPU上执行，会一分为二，再二分为四，每个最小子任务的执行时间是1秒，由于是并发4个子任务执行，整个任务最终执行时间大约为1秒。 新手在编写Fork/Join任务时，往往用搜索引擎搜到一个例子，然后就照着例子写出了下面的代码： 123456789101112131415protected Long compute() &#123; if (任务足够小?) &#123; return computeDirect(); &#125; // 任务太大,一分为二: SumTask subtask1 = new SumTask(...); SumTask subtask2 = new SumTask(...); // 分别对子任务调用fork(): subtask1.fork(); subtask2.fork(); // 合并结果: Long subresult1 = subtask1.join(); Long subresult2 = subtask2.join(); return subresult1 + subresult2;&#125; 很遗憾，这种写法是错！误！的！这样写没有正确理解Fork/Join模型的任务执行逻辑。 JDK用来执行Fork/Join任务的工作线程池大小等于CPU核心数。在一个4核CPU上，最多可以同时执行4个子任务。对400个元素的数组求和，执行时间应该为1秒。但是，换成上面的代码，执行时间却是两秒。 这是因为执行compute()方法的线程本身也是一个Worker线程，当对两个子任务调用fork()时，这个Worker线程就会把任务分配给另外两个Worker，但是它自己却停下来等待不干活了！这样就白白浪费了Fork/Join线程池中的一个Worker线程，导致了4个子任务至少需要7个线程才能并发执行。 打个比方，假设一个酒店有400个房间，一共有4名清洁工，每个工人每天可以打扫100个房间，这样，4个工人满负荷工作时，400个房间全部打扫完正好需要1天。 Fork/Join的工作模式就像这样：首先，工人甲被分配了400个房间的任务，他一看任务太多了自己一个人不行，所以先把400个房间拆成两个200，然后叫来乙，把其中一个200分给乙。 紧接着，甲和乙再发现200也是个大任务，于是甲继续把200分成两个100，并把其中一个100分给丙，类似的，乙会把其中一个100分给丁，这样，最终4个人每人分到100个房间，并发执行正好是1天。 如果换一种写法： 123// 分别对子任务调用fork():subtask1.fork();subtask2.fork(); 这个任务就分！错！了！ 比如甲把400分成两个200后，这种写法相当于甲把一个200分给乙，把另一个200分给丙，然后，甲成了监工，不干活，等乙和丙干完了他直接汇报工作。乙和丙在把200分拆成两个100的过程中，他俩又成了监工，这样，本来只需要4个工人的活，现在需要7个工人才能1天内完成，其中有3个是不干活的。 其实，我们查看JDK的invokeAll()方法的源码就可以发现，invokeAll的N个任务中，其中N-1个任务会使用fork()交给其它线程执行，但是，它还会留一个任务自己执行，这样，就充分利用了线程池，保证没有空闲的不干活的线程。 部分 api 使用 demo这个咱以后慢慢补充… 未完待续…多线程告一段落…接下来进行 jvm 和 面试题复习 重点： jvm spring 算法复习 数据库 — mysql &amp; mongo ##]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>BlockingQueue</tag>
        <tag>StampedLock</tag>
        <tag>ConcurrentLinkedQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（七）--- 三种方式实现生产者-消费者]]></title>
    <url>%2F2020%2F02%2F29%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%83%EF%BC%89---%20%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85-%E6%B6%88%E8%B4%B9%E8%80%85.html</url>
    <content type="text"><![CDATA[生产者-消费者模式是一个十分经典的多线程并发协作的模式，弄懂生产者-消费者问题能够让我们对并发编程的理解加深。所谓生产者-消费者问题，实际上主要是包含了两类线程，一种是生产者线程用于生产数据，另一种是消费者线程用于消费数据，为了解耦生产者和消费者的关系，通常会采用共享的数据区域，就像是一个仓库，生产者生产数据之后直接放置在共享数据区中，并不需要关心消费者的行为；而消费者只需要从共享数据区中去获取数据，就不再需要关心生产者的行为。但是，这个共享数据区域中应该具备这样的线程间并发协作的功能： 如果共享数据区已满的话，阻塞生产者继续生产数据放置入内； 如果共享数据区为空的话，阻塞消费者继续消费数据； 在实现生产者消费者问题时，可以采用三种方式： 1.使用 Object 的 wait/notify 的消息通知机制； 2.使用 Lock 的 Condition 的 await/signal 的消息通知机制； 3.使用 BlockingQueue 实现。本文主要将这三种实现方式进行总结归纳。 1. wait/notify 机制123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package 多线程.Producer_Consumer;import java.util.LinkedList;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 我有三个问题： * 1. 为何每次都是 producer 和 consumer 连着一起？是 jvm 内置偏向锁的原因吗？ * 2. 为何 condition.await() 那块用 if 会报错，但是用 while 就可以？ * 3. 按道理来说，consumer 和 producer 不应该有资源竞争关系，所以合理来说应该是两把锁，如果 consumer 和 producer 分两把锁来操作又如何完成通信呢？ * */public class wait_notify &#123; private final static Object a = 1; private final static LinkedList&lt;Long&gt; linkedList = new LinkedList&lt;&gt;(); private final static int MAX_CAPACITY = 10; public static void main(String[] args) &#123; System.out.println("wait/notify"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; try &#123; wait_notify.producer(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; System.out.println("-----------------"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; try &#123; wait_notify.consumer(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; &#125; static void producer() throws InterruptedException &#123; synchronized (a)&#123; while(linkedList.size() == MAX_CAPACITY)&#123; a.wait(); &#125; long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + ":-------PRODUCER-------- " + value); linkedList.addLast(value); a.notifyAll(); &#125; &#125; static void consumer() throws InterruptedException &#123; synchronized (a)&#123; while (linkedList.size() == 0)&#123; a.wait(); &#125; long value = linkedList.pop(); System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); a.notifyAll(); &#125; &#125;&#125; 2. await/signal 机制这个我在之前有写过。不过其实我觉得，producer 和 consumer 应该两把锁会更加合适一些，生产者和消费者不应该有资源冲突。而这刚好就是 ArrayBlockingQueue 和 LinkedBlockingQueue 的区别，前者是生产者和消费者共一把独占锁，但是后者是分开，两把锁，这样并发的效率也就越高，线程等待的机会会更少。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package 多线程.Producer_Consumer;import java.util.LinkedList;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class await_signal &#123; private final static Lock lock = new ReentrantLock(); private final static Condition producer_condition = lock.newCondition(); private final static Condition consumer_condition = lock.newCondition(); private final static LinkedList&lt;Long&gt; linkedList = new LinkedList&lt;&gt;(); private final static int MAX_CAPACITY = 100; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; while (true) &#123; await_signal.producer(); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 14; i++) &#123; while (true) &#123; await_signal.consumer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); &#125; private static void producer() &#123; try &#123; lock.lock(); if (linkedList.size() &gt; MAX_CAPACITY) &#123; producer_condition.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + ":-------PRODUCER-------- " + value); linkedList.addLast(value); consumer_condition.signalAll(); lock.unlock(); &#125; &#125; private static void consumer() &#123; try &#123; lock.lock(); if (linkedList.size() == 0) &#123; consumer_condition.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; long value = linkedList.pop(); System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); producer_condition.signalAll(); lock.unlock(); &#125; &#125;&#125; 上面的代码写的有点问题，为了保证思考的过程呈现，我就不删除了： 首先是主程序 main 中创建线程写错了…这样只会创建两个线程…低级失误； 在 await 时，用 if 会报错，但是用 while 没问题。 然后写完之后我发现输出很奇怪，并且又产生了新的疑问： 为何每次都是 producer 和 consumer 连着一起？ 为何 await 那块用 if 会报错，但是用 while 就可以？ 按道理来说，consumer 和 producer 不应该有资源竞争关系，所以合理来说应该是两把锁，如果 consumer 和 producer 分两把锁来操作又如何完成呢？ 为了更好地解决以上几个疑问，我简化了一下例子，将 producer 的线程降为 1 个，将 consumer 的线程 降为 3 个。 Q1：为何每次都是 producer 和 consumer 连着一起？经过多次测试发现，不会总是 producer 和 consumer 连在一起出现，只是最开始的时候会连在一起出现，为什么呢？因为在最开始的时候，producer是先启动的，那么在同步队列中排队，producer是在前面的，所以率先抢到锁，consumer等producer抢完之后才开始抢锁消费，但是后期由于偏向锁和Condition的原因，就不会 producer连着出现的情况了。 Q2：为何 await 那块用 if 会报错，但是用 while 就可以？为了解决这个问题，我特意将 producer 的线程降为 1 个，将 consumer 的线程 降为 3 个，结果是依然会报错。 新的 demo 如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package 多线程.Producer_Consumer;import java.util.LinkedList;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 我有三个问题： * 1. 为何每次都是 producer 和 consumer 连着一起？是 jvm 内置偏向锁的原因吗？ * 2. 为何 condition.await() 那块用 if 会报错，但是用 while 就可以？ * 3. 按道理来说，consumer 和 producer 不应该有资源竞争关系，所以合理来说应该是两把锁，如果 consumer 和 producer 分两把锁来操作又如何完成通信呢？ * */public class await_signal &#123; private final static Lock lock = new ReentrantLock(); private final static Condition producer_condition = lock.newCondition(); private final static Condition consumer_condition = lock.newCondition(); private final static LinkedList&lt;Long&gt; linkedList = new LinkedList&lt;&gt;(); private final static int MAX_CAPACITY = 10; public static void main(String[] args) &#123; System.out.println("producer-consumer 同一把锁"); for (int i = 0; i &lt; 1; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; await_signal.producer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; System.out.println("-----------------"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; await_signal.consumer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; &#125; static void producer() &#123; try &#123; lock.lock(); // 为何这一块用 if 不行？ if (linkedList.size() == MAX_CAPACITY) &#123; producer_condition.await(); &#125; long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + ":-------PRODUCER-------- " + value); linkedList.addLast(value); consumer_condition.signalAll(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; static void consumer() &#123; try &#123; lock.lock(); // 为何这一块用 if 不行？ if (linkedList.size() == 0) &#123; consumer_condition.await(); &#125; long value = linkedList.pop(); System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); producer_condition.signalAll(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 如上，我们来分析一下为何要用 while，下面我把程序整个运行过程讲一遍。 首先我们清楚 Thread-0 是 producer，Thread 1 、2、3 都是 consumer； 在最开始， Thread-0 最先启动，所以它也理所当然的第一个抢到了锁 从图中我们也可以看出，此时抢到锁的 exclusiveOwnerThread 为 Thread 0，从队列中我们可以看出剩下的三个 Thread 已经全部进入到同步队列，且顺序是 Thread 3 - Thread 1 - Thread 2（后面那个 5 是指优先级，不用管），为什么不可能进入到条件队列呢？因为 nextWaiter = null，且从 waitStatus = -1可知，三个线程都在同步队列中等待被唤醒。所以我们预测下一个抢到锁的一定是 Thread 3,他会去消费 Thread 0 刚刚生产出的消息，让我们接着看； 正如我们所料，真的是 Thread 3抢到了锁然后去消费了。 1234producer-consumer 同一把锁-----------------Thread-0:-------PRODUCER-------- 1583068539426Thread-3::-------CONSUMER-------- 1583068539426 我们再通过 debug 的变量验证一下。 看，真的 Thread 3 抢到锁了。不信，继续看下面更详细的信息： 如图我们可以看到，Thread 3 消费掉了消息，并且注意到同步队列现在变为了 Thread 1 - Thread 2 - Thread 0，开始是 3 - 1 - 2,现在就变成了 1 - 2 - 0，你还别说，这种同步真的速度有点慢，Thread 0 都休息完了 3 秒了，前面那两老哥还在等着呢，不过这里还有一个要注意的地方，就是 Thread 0 的 waitStatus = 0，这是为啥呢，凭啥别人都是 waitStatus = -1呢，回顾一下，watiStatus = -1意思就是等待被唤醒，waitStatus = 0 意思就是下个节点是 null，当然也可能是下个节点还没初始化完，但是这里是 null。 这下总该肯定是 Thread 3 消费了吧，我们接着往下看。当然了，我们先预测一波，下面要抢锁的肯定是 1号老大哥，然后此时已经没有消息给他消费了，那他就只能掉入 Condition 的大坑了，只能从同步队列移到条件队列了，好苦逼啊，等了这么久，还得换个地方接着等！ 我们来看看，结果是不是如我们预料的一样。 果然，接着就是 1 号老大哥在运行，不信我们看 debug 详细的参数： 握草，我被打脸了…虽然此时的确是 Thread 1 在执行，但是！！！玛德条件队列竟然不是 1号老大哥，是刚刚消费消息的 3 号小老弟，它在搞什么？？？？？？仔细思考一下，我们得知道 jvm 默认开启了偏向锁，至于什么是偏向锁，那就回到多线程（一）去复习吧，3 号小老弟使用了外挂—偏向锁「错了！！！！纠正一下，压根不是用的偏向锁，这里早就升级到了重量级锁，哪里来的偏向锁，真正插队的原因是这里是非公平锁！！！！！」，于是它很恶心的插队了，本来按道理来说，应该是 1号老哥抢到锁，然后因为没有消息可以消费，所以它从同步队列移入到条件队列中等到被 signal，然后同步队列理应就剩下 2 - 0，然后可能 3 号执行完了接在后面。但是现实是什么呢？那就是 3 号小老弟使用外挂偏向锁，直接插队，率先获取锁，由于没有消息可以消费，所以它从同步队列移入到了条件队列，开始等待，我们也可以看到，图中 consumer_condition的队列中只有一个 Thread，那就是 3 号小老弟，3号小老弟在条件队列中等待之后，暂时放弃了锁，于是终于尼玛轮到我 1 号老大哥了，我们可以看到，此时的确是 1号开始运行，然后同步队列中是 2 - 0 - null，即两个队列，这的确是符合我们现在的想法。 虽然刚才预测错了，但是那是因为 3 号使用了外挂，所以我还是接着预测吧，毕竟我预测的其实还是挺准的。此时因为没有消息可以消费，所以 1 号老大哥很可怜，跟 3 号小弟一样的下场，从同步队列转移到条件队列队尾继续等待，按道理来说 1 号将要在条件队列中，也就是 3 - 1，然后他让出锁，等待被唤醒，此时同步队列中队首应该是 2 号老哥了，终于轮到人家了…当然它应该也是很不幸的，只能跟 3 - 1一样的下场，回到条件队列队尾继续待着…预测完了，我们来看看真实情况。 看结果的确是 2 号老哥在运行着： 我们来看一下具体的参数，验证一下： 完美，预测的一模一样。2 号抢到锁之后，同步队列就只剩下了 0 号，并且 waitStatus = 0，说明这是队尾了，再提醒一下哈，当前节点的 waitStatus 的值是表示下一节点的状态，比如说 waitStatus = -1说明的是该节点的下一个节点已经准备好被唤醒了。然后我们再看一下条件队列的情况，情况和我们思考的是一样的，条件队列中现在就 2 个 Thread，分别是 3 - 1，当然了，马上 2 就要加入他们了，因为此时还是没有消息可以消费，我们接着往下看。 啊啊啊啊，不知道为啥，可能是我点错了，突然就出现了这样的结果。 并且条件队列为空。 1234567producer-consumer 同一把锁-----------------Thread-0:-------PRODUCER-------- 1583068539426Thread-3::-------CONSUMER-------- 1583068539426Thread-0:-------PRODUCER-------- 1583068539427Thread-3::-------CONSUMER-------- 1583068539427Exception in thread "Thread-1" 不要着急，我们来分析一波，用理论战胜错误。我们刚才是这样的情况，输出框是 1234producer-consumer 同一把锁-----------------Thread-0:-------PRODUCER-------- 1583068539426Thread-3::-------CONSUMER-------- 1583068539426 说明这期间有一次生产，有一次消费，而此时我们的条件队列应该是 3 - 1 - 2，同步队列只剩下 0。所以此时只剩下 0 可以抢锁了，于是 0 拿到了锁，然后生产出了一条消息，这个可以说得通，此时生产完之后，它调用 consumer_condition.signalall()，把条件队列中的 3 个都给唤醒了，3 号小老弟由于是在条件队列队首，所以它第一个从条件队列中加入到同步队列，1 号 和 2 号 老大哥紧随其后加入到同步队列，于是他第一个抢到锁，继续执行自己的逻辑，注意，我们终于要解决第二个问题了！！！！他发现此时有消息可以消费，于是他进行了消费，此时已经没有消息可以消费了，但是 1号老大哥不知道啊，它刚刚被唤醒，兴高采烈的以为终于可以消费了，因为我们写的是 if，所以它不会再去判断是否还要消息可以消费，因为它以为肯定有消息啊对吧，不然你唤醒我干嘛！于是他去消费，poll()一个空list，这不是找报错嘛…所以，就真的报错了。 所以说，遇到问题不要慌，理论掌握的透透的，不会出现意外情况的，我们来总结一下为什么要用 while 而不能用 if： 被唤醒之后，条件队列中的 Thread 会加入到 同步队列中，如果此时同步队列中还有 Thread，那么条件队列中被唤醒的 Thread 必须跟在它们的后面，如果此时同步队列中的 Thread 把消息消费完了，我后面的刚才条件队列中被唤醒的 Thread，根本无法知道没有消息了，因为它们一拿到 lock 就会继续往下执行； 当然了，刚才是两批人的战斗，条件队列算一批人，同步队列算一批人，同步队列的把人家条件队列的人给害了还不告诉人家，恶心！当然最恶心的还是自家人的斗争，也就是条件队列中的 Thread 的自相残杀，条件队列后面的压根不知道前面的消费完了，所以也会报错。 因此，将 if 改为 while，我在被唤醒之后依旧可以首先判断一下是否还有消息，这是非常有必要的。 Q3：消费者-生产者使用两把锁，又如何完成呢？的确，消费者和生产者本就不应该放到一把锁中去控制，我消费我的，你生产你的，只是各有一种情况停止而已，所以使用两把锁才是非常合理的设计，我之前也有说，这就是 ArrayBlockingQueue 和 LinkedBlockingQueue 最大的区别。所以，接下来这个问题，我希望能借助于 LinkedBlockingQueue 的源码来解决这个问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package 多线程.Producer_Consumer;import java.util.LinkedList;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 我有三个问题： * 1. 为何每次都是 producer 和 consumer 连着一起？是 jvm 内置偏向锁的原因吗？ * 2. 为何 condition.await() 那块用 if 会报错，但是用 while 就可以？ * 3. 按道理来说，consumer 和 producer 不应该有资源竞争关系，所以合理来说应该是两把锁，如果 consumer 和 producer 分两把锁来操作又如何完成通信呢？ * */public class await_signal &#123; private final static Lock lock1 = new ReentrantLock(); private final static Lock lock2 = new ReentrantLock(); private final static Condition producer_condition = lock1.newCondition(); private final static Condition consumer_condition = lock2.newCondition(); private final static LinkedList&lt;Long&gt; linkedList = new LinkedList&lt;&gt;(); private final static int MAX_CAPACITY = 5; public static void main(String[] args) &#123; System.out.println("producer-consumer 两把锁"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; await_signal.producer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; System.out.println("-----------------"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; await_signal.consumer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; &#125; static void producer() &#123; int size = 0; try &#123; lock1.lock(); while (linkedList.size() == MAX_CAPACITY) &#123; producer_condition.await(); &#125; size = linkedList.size(); long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + ":-------PRODUCER-------- " + value); linkedList.addLast(value); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock1.unlock(); &#125; if(size == 0)&#123; try&#123; lock2.lock(); consumer_condition.signalAll(); &#125;finally &#123; lock2.unlock(); &#125; &#125; &#125; static void consumer() &#123; int size = 0; try &#123; lock2.lock(); while (linkedList.size() == 0) &#123; consumer_condition.await(); &#125; size = linkedList.size(); long value = linkedList.pop(); System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock2.unlock(); &#125; if(size == MAX_CAPACITY)&#123; try&#123; lock1.lock(); producer_condition.signalAll(); &#125;finally &#123; lock1.unlock(); &#125; &#125; &#125;&#125; 上述就是完全借鉴于 LinkedBlockingQueue，但是源码里面根本没有调用 signalAll()，而是利用两把锁的优势，只需要唤醒条件队列的第一个，然后一个接一个去唤醒就可以了，这个就很妙！！！！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110package 多线程.Producer_Consumer;import java.util.LinkedList;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * 我有三个问题： * 1. 为何每次都是 producer 和 consumer 连着一起？是 jvm 内置偏向锁的原因吗？ * 2. 为何 condition.await() 那块用 if 会报错，但是用 while 就可以？ * 3. 按道理来说，consumer 和 producer 不应该有资源竞争关系，所以合理来说应该是两把锁，如果 consumer 和 producer 分两把锁来操作又如何完成通信呢？ * */public class await_signal &#123; private final static Lock lock1 = new ReentrantLock(); private final static Lock lock2 = new ReentrantLock(); private final static Condition producer_condition = lock1.newCondition(); private final static Condition consumer_condition = lock2.newCondition(); private final static LinkedList&lt;Long&gt; linkedList = new LinkedList&lt;&gt;(); private final static int MAX_CAPACITY = 5; public static void main(String[] args) &#123; System.out.println("producer-consumer 两把锁"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; await_signal.producer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; System.out.println("-----------------"); for (int i = 0; i &lt; 3; i++) &#123; new Thread(() -&gt; &#123; while (true) &#123; await_signal.consumer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; &#125; static void producer() &#123; int size = 0; try &#123; lock1.lock(); while (linkedList.size() == MAX_CAPACITY) &#123; producer_condition.await(); &#125; size = linkedList.size(); long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + ":-------PRODUCER-------- " + value); linkedList.addLast(value); producer_condition.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock1.unlock(); &#125; if(size == 0)&#123; try&#123; lock2.lock(); consumer_condition.signal(); &#125;finally &#123; lock2.unlock(); &#125; &#125; &#125; static void consumer() &#123; int size = 0; try &#123; lock2.lock(); while (linkedList.size() == 0) &#123; consumer_condition.await(); &#125; size = linkedList.size(); long value = linkedList.pop(); System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); consumer_condition.signal(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock2.unlock(); &#125; if(size == MAX_CAPACITY)&#123; try&#123; lock1.lock(); producer_condition.signal(); &#125;finally &#123; lock1.unlock(); &#125; &#125; &#125;&#125; 感慨一句，源码写的真的精妙啊！读源码真的让自己学到了很多，就是有点可惜现在还不太懂设计模式，导致很多很好的思想还没有学会。。。 3. BlockingQueue 实现别人写的： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package 多线程.Producer_Consumer;import java.util.Random;import java.util.concurrent.BlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;public class BlockingQueue_product_consumer &#123;&#125;class ProductorConsumer &#123; private static LinkedBlockingQueue&lt;Integer&gt; queue = new LinkedBlockingQueue&lt;&gt;(); public static void main(String[] args) &#123; ExecutorService service = Executors.newFixedThreadPool(15); for (int i = 0; i &lt; 5; i++) &#123; service.submit(new Producer(queue)); &#125; for (int i = 0; i &lt; 10; i++) &#123; service.submit(new Consumer(queue)); &#125; &#125; static class Producer implements Runnable &#123; private BlockingQueue queue; public Producer(BlockingQueue queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; try &#123; while (true) &#123; Random random = new Random(); int i = random.nextInt(); System.out.println("生产者" + Thread.currentThread().getName() + "生产数据" + i); queue.put(i); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; static class Consumer implements Runnable &#123; private BlockingQueue queue; public Consumer(BlockingQueue queue) &#123; this.queue = queue; &#125; @Override public void run() &#123; try &#123; while (true) &#123; Integer element = (Integer) queue.take(); System.out.println("消费者" + Thread.currentThread().getName() + "正在消费数据" + element); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 自己无聊重写一个吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class BlockingQueue_product_consumer2 &#123; private static BlockingQueue&lt;Long&gt; blockingQueue = new LinkedBlockingQueue(5); public static void main(String[] args) &#123; System.out.println("BlockingQueue api 使用"); ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(() -&gt; &#123; while (true) &#123; long value = 0; try &#123; value = blockingQueue.take(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); &#125; &#125;); &#125; for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(() -&gt; &#123; while (true) &#123; long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + "::-------PRODUCER-------- " + value); try &#123; blockingQueue.put(value); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); &#125; &#125;&#125;]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>BlockingQueue</tag>
        <tag>生产者-消费者模式</tag>
        <tag>wait/notify</tag>
        <tag>await/signal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（六）--- 并发工具包]]></title>
    <url>%2F2020%2F02%2F28%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E5%85%AD%EF%BC%89---%20%E5%B9%B6%E5%8F%91%E5%B7%A5%E5%85%B7%E5%8C%85.html</url>
    <content type="text"><![CDATA[CountDownLatch(倒计时器)在多线程协作完成业务功能时，有时候需要等待其他多个线程完成任务之后，主线程才能继续往下执行业务功能，在这种的业务场景下，通常可以使用 Thread 类的 join 方法，让主线程等待被 join 的线程执行完之后，主线程才能继续往下执行。 使用线程间消息通信机制也可以完成。 使用Executor体系中的 FutureTask,主线程调用其他线程 get() 方法 其实，java 并发工具类中为我们提供了类似“倒计时”这样的工具类，可以十分方便的完成所说的这种业务场景。 方法 12345await() throws InterruptedException //调用该方法的线程等到构造方法传入的 N 减到 0 的时候，才能继续往下执行；await(long timeout, TimeUnit unit) //与上面的 await 方法功能一致，只不过这里有了时间限制，调用该方法的线程等到指定的 timeout 时间后，不管 N 是否减至为 0，都会继续往下执行；countDown() //使 CountDownLatch 初始值 N 减 1；long getCount() //获取当前 CountDownLatch 维护的值；CountDownLatch() //参数为倒计时数量 为了能够理解 CountDownLatch，举一个很通俗的例子，运动员进行跑步比赛时，假设有 6 个运动员参与比赛，裁判员在终点会为这 6 个运动员分别计时，可以想象每当一个运动员到达终点的时候，对于裁判员来说就少了一个计时任务。直到所有运动员都到达终点了，裁判员的任务也才完成。这 6 个运动员可以类比成 6 个线程，当线程调用 CountDownLatch.countDown 方法时就会对计数器的值减一，直到计数器的值为 0 的时候，裁判员（调用 await 方法的线程）才能继续往下执行。 Demo1234567891011121314151617181920212223242526272829303132333435package 多线程.并发工具包;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class CountDownLatchDemo &#123; private static CountDownLatch startSignal = new CountDownLatch(1); //用来表示裁判员需要维护的是6个运动员 private static CountDownLatch endSignal = new CountDownLatch(6); public static void main(String[] args) throws InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(6); for (int i = 0; i &lt; 6; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; System.out.println(Thread.currentThread().getName() + " 运动员等待裁判员响哨！！！"); startSignal.await(); System.out.println(Thread.currentThread().getName() + "正在全力冲刺"); System.out.println(Thread.currentThread().getName() + " 到达终点"); endSignal.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; TimeUnit.SECONDS.sleep(3); System.out.println("裁判员发号施令啦！！！"); startSignal.countDown(); endSignal.await(); System.out.println("所有运动员到达终点，比赛结束！"); executorService.shutdown(); &#125;&#125; CyclicBarrier(循环栅栏)CyclicBarrier 也是一种多线程并发控制的实用工具，和 CountDownLatch 一样具有等待计数的功能，但是相比于 CountDownLatch 功能更加强大。 循环栅栏，也就是有一个临界点，所有人到了这个临界点，才可以一起继续工作，然后再等下一次所有人都到这个临界点，继续工作…如此循环… Demo123456789101112131415161718192021222324252627282930package 多线程.并发工具包;import java.util.concurrent.*;public class CyclicBarrierDemo &#123; //指定必须有6个运动员到达才行 private static CyclicBarrier barrier = new CyclicBarrier(6, () -&gt; &#123; // 每次到了这个临界点，所有线程都必须停下。 System.out.println("裁判员发号施令啦！！！"); &#125;); public static void main(String[] args) &#123; System.out.println("运动员准备进场，全场欢呼............"); ExecutorService service = Executors.newFixedThreadPool(6); for (int i = 0; i &lt; 6; i++) &#123; service.execute(() -&gt;&#123; try &#123; System.out.println(Thread.currentThread().getName() + "运动员等待裁判员响哨！！！"); barrier.await(); System.out.println(Thread.currentThread().getName() + "运动员出发"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; service.shutdown(); &#125;&#125; CountDownLatch 与 CyclicBarrier 的比较CountDownLatch 与 CyclicBarrier 都是用于控制并发的工具类，都可以理解成维护的就是一个计数器，但是这两者还是各有不同侧重点的： CountDownLatch 一般用于某个线程 A 等待若干个其他线程执行完任务之后，它才执行，最重要的是有一个主心骨，主线程是核心，他等所有人干完；而 CyclicBarrier 一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行；CountDownLatch 强调一个线程等多个线程完成某件事情。CyclicBarrier 是多个线程互等，等大家都完成，再携手共进。所以两者 await 的地方不一样，前者是对主线程进行 await，而后者则是所有一起工作的进行 await。 CountDownLatch 方法比较少，操作比较简单，而 CyclicBarrier 提供的方法更多，比如能够通过 getNumberWaiting()，isBroken()这些方法获取当前多个线程的状态，并且 CyclicBarrier 的构造方法可以传入 barrierAction，指定当所有线程都到达时执行的业务功能。 Semaphore(信号量)信号量，看源码就是基于 AQS 实现的，类似于共享锁，只不过这个共享是有限制的共享，它能够控制共享的线程数量。Semaphore 就相当于一个许可证，线程需要先通过 acquire 方法获取该许可证，该线程才能继续往下执行，否则只能在该方法出阻塞等待。当执行完业务功能后，需要通过release()方法将许可证归还，以便其他线程能够获得许可证继续执行。 Semaphore 可以用于做流量控制，特别是公共资源有限的应用场景，比如数据库连接。假如有多个线程读取数据后，需要将数据保存在数据库中，而可用的最大数据库连接只有 10 个，这时候就需要使用 Semaphore 来控制能够并发访问到数据库连接资源的线程个数最多只有 10 个。在限制资源使用的应用场景下，Semaphore 是特别合适的。 这里的队列就是就是很简单的用 ArrayList 进行存储，存储那些等待许可证的线程，没有阻塞然后加入队列然后唤醒这么麻烦… 123456789101112131415161718192021222324252627282930package 多线程.并发工具包;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;public class SemaphoreDemo &#123; //表示老师只有5支笔 private static Semaphore semaphore = new Semaphore(5); public static void main(String[] args) &#123; //表示10个学生 ExecutorService service = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; 10; i++) &#123; service.execute(() -&gt; &#123; try &#123; semaphore.acquire(); System.out.println(Thread.currentThread().getName() + " 同学获取到笔，填表格"); TimeUnit.SECONDS.sleep(3); semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; service.shutdown(); &#125;&#125; Exchanger (交换器)可以直接在某个临界点，两个线程交换数据，不需要使用 wait/notify 或者是 condition 机制。 它提供了一个交换的同步点，在这个同步点两个线程能够交换数据。具体交换数据是通过 exchange 方法来实现的，如果一个线程先执行 exchange 方法，那么它会同步等待另一个线程也执行 exchange 方法，这个时候两个线程就都达到了同步点，两个线程就可以交换数据。 123456//当一个线程执行该方法的时候，会等待另一个线程也执行该方法，因此两个线程就都达到了同步点//将数据交换给另一个线程，同时返回获取的数据V exchange(V x) throws InterruptedException//同上一个方法功能基本一样，只不过这个方法同步等待的时候，增加了超时时间V exchange(V x, long timeout, TimeUnit unit)throws InterruptedException, TimeoutException 1234567891011121314151617181920212223242526272829303132333435363738package 多线程.并发工具包;import java.util.concurrent.Exchanger;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class ExchangerDemo &#123; private static Exchanger&lt;String&gt; exchanger = new Exchanger(); public static void main(String[] args) &#123; //代表男生和女生 ExecutorService service = Executors.newFixedThreadPool(2); service.execute(() -&gt; &#123; try &#123; //男生对女生说的话 String girl = exchanger.exchange("我其实暗恋你很久了......"); System.out.println("女孩儿说：" + girl); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); service.execute(() -&gt; &#123; try &#123; System.out.println("女生慢慢的从教室你走出来......"); TimeUnit.SECONDS.sleep(3); //男生对女生说的话 String boy = exchanger.exchange("我也很喜欢你......"); System.out.println("男孩儿说：" + boy); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); &#125;&#125; 123女生慢慢的从教室你走出来......男孩儿说：我其实暗恋你很久了......女孩儿说：我也很喜欢你...... 两句话交换了，这个真的牛逼！]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>CountDownLatch</tag>
        <tag>CyclicBarrier</tag>
        <tag>Semaphore</tag>
        <tag>Exchanger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（五）--- FutureTask]]></title>
    <url>%2F2020%2F02%2F28%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%BA%94%EF%BC%89---%20FutureTask.html</url>
    <content type="text"><![CDATA[转自：http://www.ideabuffer.cn/2017/04/06/FutureTask源码解析/ FutureTask介绍FutureTask是一种可以取消的异步的计算任务。它的计算是通过Callable实现的，可以把它理解为是可以返回结果的Runnable。 使用FutureTask的优势有： 可以获取线程执行后的返回结果； 提供了超时控制功能。 它实现了Runnable接口和Future接口： 什么是异步计算呢？也就是说，在让该任务执行时，不需要一直等待其运行结束返回结果，而是可以先去处理其他的事情，然后再获取返回结果。例如你想下载一个很大的文件，这时很耗时的操作，没必要一直等待着文件下载完，你可以先去吃个饭，然后再回来看下文件是否下载完成，如果下载完成就可以使用了，否则还需要继续等待。 FutureTask的实现FutureTask的状态FutureTask内部有这样几种状态： 1234567private static final int NEW = 0;private static final int COMPLETING = 1;private static final int NORMAL = 2;private static final int EXCEPTIONAL = 3;private static final int CANCELLED = 4;private static final int INTERRUPTING = 5;private static final int INTERRUPTED = 6; 看名字应该很好理解了，当创建一个FutureTask对象是，初始的状态是NEW，在运行时状态会转换，有4中状态的转换过程： NEW -&gt; COMPLETING -&gt; NORMAL：正常执行并返回； NEW -&gt; COMPLETING -&gt; EXCEPTIONAL：执行过程中出现了异常； NEW -&gt; CANCELLED；执行前被取消； NEW -&gt; INTERRUPTING -&gt; INTERRUPTED：取消时被中断。 使用FutureTask下面看一下具体的使用过程： 123456789101112131415161718192021public class FutureTaskTest &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; ExecutorService executor = Executors.newSingleThreadExecutor(); FutureTask&lt;Integer&gt; future = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; int result = 0; for (int i = 0; i &lt; 100; i++) &#123; result += i; &#125; return result; &#125; &#125;); executor.execute(future); System.out.println(future.get()); &#125;&#125; FutureTask内部结构12345678910111213141516171819202122232425262728293031323334353637383940public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; private volatile int state; private static final int NEW = 0; private static final int COMPLETING = 1; private static final int NORMAL = 2; private static final int EXCEPTIONAL = 3; private static final int CANCELLED = 4; private static final int INTERRUPTING = 5; private static final int INTERRUPTED = 6; /** The underlying callable; nulled out after running */ private Callable&lt;V&gt; callable; /** The result to return or exception to throw from get() */ private Object outcome; // non-volatile, protected by state reads/writes /** 执行callable的线程 **/ private volatile Thread runner; /** * Treiber stack of waiting threads * 使用Treiber算法实现的无阻塞的Stack， * 用于存放等待的线程 */ private volatile WaitNode waiters; static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125; &#125; public V get() throws InterruptedException, ExecutionException &#123; ... &#125; public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; ... &#125; ... 这里的waiters理解为一个stack，因为在调用get方法时任务可能还没有执行完，这时需要将调用get方法的线程放入waiters中。 最重要的两个get方法，用于获取返回结果，第二种提供了超时控制功能。 FutureTask构造方法FutureTask有两个构造方法： 1234567891011public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable&#125;public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable&#125; 第二种构造方法传入一个Runnable对象和一个返回值对象，因为Runnable是没有返回值的，所以要通过result参数在执行完之后返回结果。 run方法FutureTask实现了Runnable接口，所以需要实现run方法，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public void run() &#123; /* * 首先判断状态，如果不是初始状态，说明任务已经被执行或取消； * runner是FutureTask的一个属性，用于保存执行任务的线程， * 如果不为空则表示已经有线程正在执行，这里用CAS来设置，失败则返回。 */ if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; // 只有初始状态才会执行 if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; // 执行任务 result = c.call(); // 如果没出现异常，则说明执行成功了 ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; // 设置异常 setException(ex); &#125; // 如果执行成功，则设置返回结果 if (ran) set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() // 无论是否执行成功，把runner设置为null runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; // 如果被中断，则说明调用的cancel(true)， // 这里要保证在cancel方法中把state设置为INTERRUPTED // 否则可能在cancel方法中还没执行中断，造成中断的泄露 if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125;&#125; 总结一下run方法的执行过程 只有state为NEW的时候才执行任务； 执行前要设置runner为当前线程，使用CAS来设置是为了防止竞争； 如果任务执行成功，任务状态从NEW转换为COMPLETING，如果执行正常，设置最终状态为NORMAL；如果执行中出现了异常，设置最终状态为EXCEPTIONAL； 唤醒并删除Treiber Stack中的所有节点； 如果调用了cancel(true)方法进行了中断，要确保在run方法执行结束前的状态是INTERRUPTED。 这里涉及到3个比较重要的方法：setException，set和handlePossibleCancellationInterrupt。 setException方法1234567protected void setException(Throwable t) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; outcome = t; UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state finishCompletion(); &#125;&#125; 如果在执行过程中（也就是调用call方法时）出现了异常，则要把状态先设置为COMPLETING，如果成功，设置outcome = t，outcome对象是Object类型的，用来保存异常或者返回结果对象，也就是说，在正常的执行过程中（没有异常，没有调用cancel方法），outcome保存着返回结果对象，会被返回，如果出现了异常或者中断，则不会返回并抛出异常，这个在介绍report方法时会讲到。 接着设置状态为EXCEPTIONAL，这也是最终的状态。 finishCompletion方法稍后再分析。 set方法1234567protected void set(V v) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; outcome = v; UNSAFE.putOrderedInt(this, stateOffset, NORMAL); // final state finishCompletion(); &#125;&#125; 很简单，与setException类似，只不过这里的outcome是返回结果对象，状态先设置为COMPLETING，然后再设置为MORMAL。 handlePossibleCancellationInterrupt方法1234567891011121314151617private void handlePossibleCancellationInterrupt(int s) &#123; // It is possible for our interrupter to stall before getting a // chance to interrupt us. Let's spin-wait patiently. if (s == INTERRUPTING) while (state == INTERRUPTING) Thread.yield(); // wait out pending interrupt // assert state == INTERRUPTED; // We want to clear any interrupt we may have received from // cancel(true). However, it is permissible to use interrupts // as an independent mechanism for a task to communicate with // its caller, and there is no way to clear only the // cancellation interrupt. // // Thread.interrupted();&#125; handlePossibleCancellationInterrupt方法要确保cancel(true)产生的中断发生在run或runAndReset方法执行的过程中。这里会循环的调用Thread.yield()来确保状态在cancel方法中被设置为INTERRUPTED。 这里不能够清除中断标记，因为不能确定中断一定来自于cancel方法。 finishCompletion方法12345678910111213141516171819202122232425262728293031private void finishCompletion() &#123; // assert state &gt; COMPLETING; // 执行该方法时state必须大于COMPLETING // 逐个唤醒waiters中的线程 for (WaitNode q; (q = waiters) != null;) &#123; // 设置栈顶节点为null if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; for (;;) &#123; Thread t = q.thread; // 唤醒线程 if (t != null) &#123; q.thread = null; LockSupport.unpark(t); &#125; // 如果next为空，说明栈空了，跳出循环 WaitNode next = q.next; if (next == null) break; // 方便gc回收 q.next = null; // unlink to help gc // 重新设置栈顶node q = next; &#125; break; &#125; &#125; // 钩子方法 done(); callable = null; // to reduce footprint&#125; 在调用get方法时，如果任务还没有执行结束，则会阻塞调用的线程，然后把调用的线程放入waiters中，这时，如果任务执行完毕，也就是调用了finishCompletion方法，waiters会依次出栈并逐个唤醒对应的线程。 由此可以想到，WaitNode一定是在get方法中被添加到栈中的，下面来看下get方法的实现。 get方法1234567891011121314151617public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s);&#125;public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException &#123; if (unit == null) throw new NullPointerException(); int s = state; if (s &lt;= COMPLETING &amp;&amp; (s = awaitDone(true, unit.toNanos(timeout))) &lt;= COMPLETING) throw new TimeoutException(); return report(s);&#125; 这两个方法类似，首先判断状态，如果s &lt;= COMPLETING，说明还是未完成的状态「上文有讲只有当状态变为 normal/exceptional/interrupted/cancelled 才算结束」，这时需要将当前线程添加到waiters中并阻塞。 第二种get提供了超时功能，如果在规定时间内任务还未执行完毕或者状态还是COMPLETING，则获取结果超时，抛出TimeoutException。而第一种get会一直阻塞直到state &gt; COMPLETING。 awaitDone方法awaitDone方法的工作是根据状态来判断是否能够返回结果，如果任务还未设置为最终状态，要添加到waiters中并阻塞，否则返回状态。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; // 计算到期时间 final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; for (;;) &#123; // 如果被中断，删除节点，抛出异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; // 如果任务执行完毕并且设置了最终状态或者被取消，则返回 if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; // s == COMPLETING时通过Thread.yield();让步其他线程执行， // 主要是为了让状态改变 else if (s == COMPLETING) // cannot time out yet Thread.yield(); // 创建一个WaitNode else if (q == null) q = new WaitNode(); // CAS设置栈顶节点 else if (!queued) queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); // 如果设置了超时，则计算是否已经到了开始设置的到期时间 else if (timed) &#123; nanos = deadline - System.nanoTime(); // 如果已经到了到期时间，删除节点，返回状态 if (nanos &lt;= 0L) &#123; removeWaiter(q); return state; &#125; // 阻塞到到期时间 LockSupport.parkNanos(this, nanos); &#125; // 如果没有设置超时，会一直阻塞，直到被中断或者被唤醒 else LockSupport.park(this); &#125;&#125; removeWaiter方法12345678910111213141516171819202122232425262728293031private void removeWaiter(WaitNode node) &#123; if (node != null) &#123; // 将thread设置为null是因为下面要根据thread是否为null判断是否要把node移出 node.thread = null; // 这里自旋保证删除成功 retry: for (;;) &#123; // restart on removeWaiter race for (WaitNode pred = null, q = waiters, s; q != null; q = s) &#123; s = q.next; // q.thread != null说明该q节点不需要移除 if (q.thread != null) pred = q; // 如果q.thread == null，且pred != null，需要删除q节点 else if (pred != null) &#123; // 删除q节点 pred.next = s; // pred.thread == null时说明在并发情况下被其他线程修改了； // 返回第一个for循环重试 if (pred.thread == null) // check for race continue retry; &#125; // 如果q.thread != null且pred == null，说明q是栈顶节点 // 设置栈顶元素为s节点，如果失败则返回重试 else if (!UNSAFE.compareAndSwapObject(this, waitersOffset, q, s)) continue retry; &#125; break; &#125; &#125;&#125; cancel方法cancel方法用于取消任务，这里可能有两种情况，一种是任务已经执行了，另一种是还未执行，代码如下： 123456789101112131415161718192021222324public boolean cancel(boolean mayInterruptIfRunning) &#123; if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // in case call to interrupt throws exception // mayInterruptIfRunning参数表示是否要进行中断 if (mayInterruptIfRunning) &#123; try &#123; // runner保存着当前执行任务的线程 Thread t = runner; // 中断线程 if (t != null) t.interrupt(); &#125; finally &#123; // final state // 设置最终状态为INTERRUPTED UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; finishCompletion(); &#125; return true;&#125; 第一个if判断可能有些不好理解，其实等价于如下代码： 123if (!state == NEW || !UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED)) 如果状态不是NEW，或者设置状态为INTERRUPTING或CANCELLED失败，则取消失败，返回false。 简单来说有一下两种情况： 如果当前任务还没有执行，那么state == NEW，那么会尝试设置状态，如果设置状态失败会返回false，表示取消失败； 如果当前任务已经被执行了，那么state &gt; NEW，也就是!state == NEW为true，直接返回false。 也就是说，如果任务一旦（state != NEW），意味着其实任务已经执行完了，只是还在返回结果，那么就不能被取消。如果mayInterruptIfRunning为true，要中断当前执行任务的线程。 report方法get方法在调用awaitDone方法后，会调用report方法进行返回： 12345678private V report(int s) throws ExecutionException &#123; Object x = outcome; if (s == NORMAL) return (V)x; if (s &gt;= CANCELLED) throw new CancellationException(); throw new ExecutionException((Throwable)x);&#125; 很简单，可以看到有3中执行情况： 如果s == NORMAL为true，说明是正常执行结束，那么根据上述的分析，在正常执行结束时outcome存放的是返回结果，把outcome返回； 如果s &gt;= CANCELLED为true，说明是被取消了，抛出CancellationException； 如果s &lt; CANCELLED，那么状态只能是是EXCEPTIONAL，表示在执行过程中出现了异常，抛出ExecutionException。 runAndReset方法该方法和run方法类似，区别在于这个方法不会设置任务的执行结果值，所以在正常执行时，不会修改state，除非发生了异常或者中断，最后返回是否正确的执行并复位： 12345678910111213141516171819202122232425262728293031protected boolean runAndReset() &#123; if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return false; boolean ran = false; int s = state; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; s == NEW) &#123; try &#123; // 不获取和设置返回值 c.call(); // don't set result ran = true; &#125; catch (Throwable ex) &#123; setException(ex); &#125; &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; // 是否正确的执行并复位 return ran &amp;&amp; s == NEW;&#125; 总结本文分析了FutureTask的执行过程和获取返回值的过程，要注意以下几个地方： FutureTask是线程安全的，在多线程下任务也只会被执行一次； 注意在执行时各种状态的切换； get方法调用时，如果任务没有结束，要阻塞当前线程，被阻塞的线程会保存在一个Treiber Stack中； get方法超时功能如果超时未获取成功，会抛出TimeoutException； 注意在取消时的线程中断，在run方法中一定要保证结束时的状态是INTERRUPTED，否则在cancel方法中可能没有执行interrupt，造成中断的泄露。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>FutureTask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（四）--- 线程池]]></title>
    <url>%2F2020%2F02%2F28%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E5%9B%9B%EF%BC%89---%20%E7%BA%BF%E7%A8%8B%E6%B1%A0.html</url>
    <content type="text"><![CDATA[深入理解Java线程池：ThreadPoolExecutor ps:不过说实话，写的败笔就是线程状态转换那一块~ 简略介绍线程池 转：http://www.ideabuffer.cn/2017/04/04/深入理解Java线程池：ThreadPoolExecutor/#addWorker方法 线程池（一）：ThreadPoolExecutor线程池介绍在web开发中，服务器需要接受并处理请求，所以会为一个请求来分配一个线程来进行处理。如果每次请求都新创建一个线程的话实现起来非常简便，但是存在一个问题： 如果并发的请求数量非常多，但每个线程执行的时间很短，这样就会频繁的创建和销毁线程，如此一来会大大降低系统的效率。可能出现服务器在为每个请求创建新线程和销毁线程上花费的时间和消耗的系统资源要比处理实际的用户请求的时间和资源更多。 那么有没有一种办法使执行完一个任务，并不被销毁，而是可以继续执行其他的任务呢？ 这就是线程池的目的了。线程池为线程生命周期的开销和资源不足问题提供了解决方案。通过对多个任务重用线程，线程创建的开销被分摊到了多个任务上。 什么时候使用线程池？ 单个任务处理时间比较短 需要处理的任务数量很大 使用线程池的好处 引用自 http://ifeve.com/java-threadpool/ 的说明： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 Java中的线程池是用ThreadPoolExecutor类来实现的. 本文就结合JDK 1.8对该类的源码来分析一下这个类内部对于线程的创建, 管理以及后台任务的调度等方面的执行原理。 先看一下线程池的类图： Executor框架接口Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架，目的是提供一种将”任务提交”与”任务如何运行”分离开来的机制。 J.U.C中有三个Executor接口： Executor：一个运行新任务的简单接口； ExecutorService：扩展了Executor接口。添加了一些用来管理执行器生命周期和任务生命周期的方法； ScheduledExecutorService：扩展了ExecutorService。支持Future和定期执行任务。 Executor接口123public interface Executor &#123; void execute(Runnable command);&#125; Executor接口只有一个execute方法，用来替代通常创建或启动线程的方法。例如，使用Thread来创建并启动线程的代码如下： 12Thread t = new Thread();t.start(); 使用Executor来启动线程执行任务的代码如下： 12Thread t = new Thread();executor.execute(t); 对于不同的Executor实现，execute()方法可能是创建一个新线程并立即启动，也有可能是使用已有的工作线程来运行传入的任务，也可能是根据设置线程池的容量或者阻塞队列的容量来决定是否要将传入的线程放入阻塞队列中或者拒绝接收传入的线程。 ExecutorService接口ExecutorService接口继承自Executor接口，提供了管理终止的方法，以及可为跟踪一个或多个异步任务执行状况而生成 Future 的方法。增加了shutDown()，shutDownNow()，invokeAll()，invokeAny()和submit()等方法。如果需要支持即时关闭，也就是shutDownNow()方法，则任务需要正确处理中断。 ScheduledExecutorService接口ScheduledExecutorService扩展ExecutorService接口并增加了schedule方法。调用schedule方法可以在指定的延时后执行一个Runnable或者Callable任务。ScheduledExecutorService接口还定义了按照指定时间间隔定期执行任务的scheduleAtFixedRate()方法和scheduleWithFixedDelay()方法。 ThreadPoolExecutor分析ThreadPoolExecutor继承自AbstractExecutorService，也是实现了ExecutorService接口。 几个重要的字段12345678910private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; ctl是对线程池的运行状态和线程池中有效线程的数量进行控制的一个字段， 它包含两部分的信息: 线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，这里可以看到，使用了Integer类型来保存，高3位保存runState，低29位保存workerCount。COUNT_BITS 就是29，CAPACITY就是1左移29位减1（29个1），这个常量表示workerCount的上限值，大约是5亿。 下面再介绍下线程池的运行状态. 线程池一共有五种状态, 分别是: RUNNING ：能接受新提交的任务，并且也能处理阻塞队列中的任务； SHUTDOWN：关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务。在线程池处于 RUNNING 状态时，调用 shutdown()方法会使线程池进入到该状态。（finalize() 方法在执行过程中也会调用shutdown()方法进入该状态）；「注意shutdown可能会出现有线程正在等阻塞队列中的任务过来，但是突然shutdown，不会有任务再来了，那线程就会一直阻塞，导致无法回收，所以 shutdown()方法中有调用interruptIdleWorkers()。」 STOP：不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态； TIDYING：如果所有的任务都已终止了，workerCount (有效线程数) 为0，线程池进入该状态后会调用 terminated() 方法进入TERMINATED 状态。 TERMINATED：在terminated() 方法执行完后进入该状态，默认terminated()方法中什么也没有做。 线程池不是RUNNING状态； 线程池状态不是TIDYING状态或TERMINATED状态； 如果线程池状态是SHUTDOWN并且workerQueue为空； workerCount为0； 设置TIDYING状态成功。 下图为线程池的状态转换过程： ctl相关方法这里还有几个对ctl进行计算的方法： 123private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; runStateOf：获取运行状态； workerCountOf：获取活动线程数； ctlOf：获取运行状态和活动线程数的值。 ThreadPoolExecutor构造方法123456789101112131415161718192021public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 构造方法中的字段含义如下： corePoolSize：核心线程数量，当有新任务在execute()方法提交时，会执行以下判断： 如果运行的线程少于 corePoolSize，则创建新线程来处理任务，即使线程池中的其他线程是空闲的； 如果线程池中的线程数量大于等于 corePoolSize 且小于 maximumPoolSize，则只有当workQueue满时才创建新的线程去处理任务； 如果设置的corePoolSize 和 maximumPoolSize相同，则创建的线程池的大小是固定的，这时如果有新任务提交，若workQueue未满，则将请求放入workQueue中，等待有空闲的线程去从workQueue中取任务并处理； 如果运行的线程数量大于等于maximumPoolSize，这时如果workQueue已经满了，则通过handler所指定的策略来处理任务； 所以，任务提交时，判断的顺序为 corePoolSize –&gt; workQueue –&gt; maximumPoolSize。 maximumPoolSize：最大线程数量； workQueue：等待队列，当任务提交时，如果线程池中的线程数量大于等于corePoolSize的时候，把该任务封装成一个Worker对象放入等待队列，当提交一个新的任务到线程池以后, 线程池会根据当前线程池中正在运行着的线程的数量来决定对该任务的处理方式，也就是 BlockingQueue 的选择，主要有以下几种处理方式: 直接切换：这种方式常用的队列是SynchronousQueue，但现在还没有研究过该队列，这里暂时还没法介绍； 使用无界队列：一般使用基于链表的阻塞队列LinkedBlockingQueue。如果使用这种方式，那么线程池中能够创建的最大线程数就是corePoolSize，而maximumPoolSize就不会起作用了「因为队列永远不会满…」（后面也会说到）。当线程池中所有的核心线程都是RUNNING状态时，这时一个新的任务提交就会放入等待队列中。 使用有界队列：一般使用ArrayBlockingQueue。使用该方式可以将线程池的最大线程数量限制为maximumPoolSize，这样能够降低资源的消耗，但同时这种方式也使得线程池对线程的调度变得更困难，因为线程池和队列的容量都是有限的值，所以要想使线程池处理任务的吞吐率达到一个相对合理的范围，又想使线程调度相对简单，并且还要尽可能的降低线程池对资源的消耗，就需要合理的设置这两个数量。 如果要想降低系统资源的消耗（包括CPU的使用率，操作系统资源的消耗，上下文环境切换的开销等）, 可以设置较大的队列容量和较小的线程池容量, 但这样也会降低线程处理任务的吞吐量。 如果提交的任务经常发生阻塞，那么可以考虑通过调用 setMaximumPoolSize() 方法来重新设定线程池的容量。 如果队列的容量设置的较小，通常需要将线程池的容量设置大一点，这样CPU的使用率会相对的高一些。但如果线程池的容量设置的过大，则在提交的任务数量太多的情况下，并发量会增加，那么线程之间的调度就是一个要考虑的问题，因为这样反而有可能降低处理任务的吞吐量。 keepAliveTime：线程池维护线程所允许的空闲时间。当线程池中的线程数量大于corePoolSize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了keepAliveTime； threadFactory：它是ThreadFactory类型的变量，用来创建新线程。默认使用Executors.defaultThreadFactory() 来创建线程。使用默认的ThreadFactory来创建线程时，会使新创建的线程具有相同的NORM_PRIORITY优先级并且是非守护线程，同时也设置了线程的名称。 handler：它是RejectedExecutionHandler类型的变量，表示线程池的饱和策略。如果阻塞队列满了并且没有空闲的线程，这时如果继续提交任务，就需要采取一种策略处理该任务。线程池提供了4种策略： AbortPolicy：直接抛出异常，这是默认策略； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； execute方法execute()方法用来提交任务，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); /* * clt记录着runState和workerCount */ int c = ctl.get(); /* * workerCountOf方法取出低29位的值，表示当前活动的线程数； * 如果当前活动线程数小于corePoolSize，则新建一个线程放入线程池中； * 并把任务添加到该线程中。 */ if (workerCountOf(c) &lt; corePoolSize) &#123; /* * addWorker中的第二个参数表示限制添加线程的数量是根据corePoolSize来判断还是maximumPoolSize来判断； * 如果为true，根据corePoolSize来判断； * 如果为false，则根据maximumPoolSize来判断 */ if (addWorker(command, true)) return; /* * 如果添加失败，则重新获取ctl值 */ c = ctl.get(); &#125; /* * 如果当前线程池是运行状态并且任务添加到队列成功 */ if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; // 重新获取ctl值 int recheck = ctl.get(); // 再次判断线程池的运行状态，如果不是运行状态，由于之前已经把command添加到workQueue中了， // 这时需要移除该command // 执行过后通过handler使用拒绝策略对该任务进行处理，整个方法返回 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); /* * 获取线程池中的有效线程数，如果数量是0，则执行addWorker方法 * 这里传入的参数表示： * 1. 第一个参数为null，表示在线程池中创建一个线程，但不去启动； * 2. 第二个参数为false，将线程池的有限线程数量的上限设置为maximumPoolSize，添加线程时根据maximumPoolSize来判断； * 如果判断workerCount大于0，则直接返回，在workQueue中新增的command会在将来的某个时刻被执行。 */ else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; /* * 如果执行到这里，有两种情况： * 1. 线程池已经不是RUNNING状态； * 2. 线程池是RUNNING状态，但workerCount &gt;= corePoolSize并且workQueue已满。 * 这时，再次调用addWorker方法，但第二个参数传入为false，将线程池的有限线程数量的上限设置为maximumPoolSize； * 如果失败则拒绝该任务 */ else if (!addWorker(command, false)) reject(command);&#125; 简单来说，在执行execute()方法时如果状态一直是RUNNING时，的执行过程如下： 如果workerCount &lt; corePoolSize，则创建并启动一个线程来执行新提交的任务； 如果workerCount &gt;= corePoolSize，且线程池内的阻塞队列未满，则将任务添加到该阻塞队列中； 如果workerCount &gt;= corePoolSize &amp;&amp; workerCount &lt; maximumPoolSize，且线程池内的阻塞队列已满，则创建并启动一个线程来执行新提交的任务； 如果workerCount &gt;= maximumPoolSize，并且线程池内的阻塞队列已满, 则根据拒绝策略来处理该任务, 默认的处理方式是直接抛异常。 这里要注意一下addWorker(null, false);，也就是创建一个线程，但并没有传入任务，因为任务已经被添加到workQueue中了，所以worker在执行的时候，会直接从workQueue中获取任务。所以，在workerCountOf(recheck) == 0时执行addWorker(null, false);也是为了保证线程池在RUNNING状态下必须要有一个线程来执行任务。 execute方法执行流程如下： addWorker方法addWorker方法的主要工作是在线程池中创建一个新的线程并执行，firstTask参数 用于指定新增的线程执行的第一个任务，core参数为true表示在新增线程时会判断当前活动线程数是否少于corePoolSize，false表示新增线程前需要判断当前活动线程数是否少于maximumPoolSize，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); // 获取运行状态 int rs = runStateOf(c); /* * 这个if判断 * 如果rs &gt;= SHUTDOWN，则表示此时不再接收新任务； * 接着判断以下3个条件，只要有1个不满足，则返回false： * 1. rs == SHUTDOWN，这时表示关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务 * 2. firsTask为空 * 3. 阻塞队列不为空 * * 首先考虑rs == SHUTDOWN的情况 * 这种情况下不会接受新提交的任务，所以在firstTask不为空的时候会返回false； * 然后，如果firstTask为空，并且workQueue也为空，则返回false， * 因为队列中已经没有任务了，不需要再添加线程了 */ // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; // 获取线程数 int wc = workerCountOf(c); // 如果wc超过CAPACITY，也就是ctl的低29位的最大值（二进制是29个1），返回false； // 这里的core是addWorker方法的第二个参数，如果为true表示根据corePoolSize来比较， // 如果为false则根据maximumPoolSize来比较。 // if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; // 尝试增加workerCount，如果成功，则跳出第一个for循环 if (compareAndIncrementWorkerCount(c)) break retry; // 如果增加workerCount失败，则重新获取ctl的值 c = ctl.get(); // Re-read ctl // 如果当前的运行状态不等于rs，说明状态已被改变，返回第一个for循环继续执行 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; // 根据firstTask来创建Worker对象 w = new Worker(firstTask); // 每一个Worker对象都会创建一个线程 final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // rs &lt; SHUTDOWN表示是RUNNING状态； // 如果rs是RUNNING状态或者rs是SHUTDOWN状态并且firstTask为null，向线程池中添加线程。 // 因为在SHUTDOWN时不会在添加新的任务，但还是会执行workQueue中的任务 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // workers是一个HashSet workers.add(w); int s = workers.size(); // largestPoolSize记录着线程池中出现过的最大线程数量 if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; // 启动线程 t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; 注意一下这里的t.start()这个语句，启动时会调用Worker类中的run方法，Worker本身实现了Runnable接口，所以一个Worker类型的对象也是一个线程。 Worker类线程池中的每一个线程被封装成一个Worker对象，ThreadPool维护的其实就是一组Worker对象，看一下Worker的定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; /** * This class will never be serialized, but we provide a * serialVersionUID to suppress a javac warning. */ private static final long serialVersionUID = 6138294804551838833L; /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; /** * Creates with given first task and thread from ThreadFactory. * @param firstTask the first task (null if none) */ Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); &#125; /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this); &#125; // Lock methods // // The value 0 represents the unlocked state. // The value 1 represents the locked state. protected boolean isHeldExclusively() &#123; return getState() != 0; &#125; protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; protected boolean tryRelease(int unused) &#123; setExclusiveOwnerThread(null); setState(0); return true; &#125; public void lock() &#123; acquire(1); &#125; public boolean tryLock() &#123; return tryAcquire(1); &#125; public void unlock() &#123; release(1); &#125; public boolean isLocked() &#123; return isHeldExclusively(); &#125; void interruptIfStarted() &#123; Thread t; if (getState() &gt;= 0 &amp;&amp; (t = thread) != null &amp;&amp; !t.isInterrupted()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; &#125; &#125;&#125; Worker类继承了AQS，并实现了Runnable接口，注意其中的firstTask和thread属性：firstTask用它来保存传入的任务；thread是在调用构造方法时通过ThreadFactory来创建的线程，是用来处理任务的线程。 在调用构造方法时，需要把任务传入，这里通过getThreadFactory().newThread(this);来新建一个线程，newThread方法传入的参数是this，因为Worker本身继承了Runnable接口，也就是一个线程，所以一个Worker对象在启动的时候会调用Worker类中的run方法。 Worker继承了AQS，使用AQS来实现独占锁的功能。为什么不使用ReentrantLock来实现呢？可以看到tryAcquire方法，它是不允许重入的，而ReentrantLock是允许重入的： lock方法一旦获取了独占锁，表示当前线程正在执行任务中； 如果正在执行任务，则不应该中断线程； 如果该线程现在不是独占锁的状态，也就是空闲的状态，说明它没有在处理任务，这时可以对该线程进行中断； 线程池在执行shutdown方法或tryTerminate方法时会调用interruptIdleWorkers方法来中断空闲的线程，interruptIdleWorkers方法会使用tryLock方法来判断线程池中的线程是否是空闲状态； 之所以设置为不可重入，是因为我们不希望任务在调用像setCorePoolSize这样的线程池控制方法时重新获取锁。如果使用ReentrantLock，它是可重入的，这样如果在任务中调用了如setCorePoolSize这类线程池控制的方法，会中断正在运行的线程。 所以，Worker继承自AQS，用于判断线程是否空闲以及是否可以被中断。 此外，在构造方法中执行了setState(-1);，把state变量设置为-1，为什么这么做呢？是因为AQS中默认的state是0，如果刚创建了一个Worker对象，还没有执行任务时，这时就不应该被中断，看一下tryAquire方法： 1234567protected boolean tryAcquire(int unused) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false;&#125; tryAcquire方法是根据state是否是0来判断的，所以，setState(-1);将state设置为-1是为了禁止在执行任务前对线程进行中断。 正因为如此，在runWorker方法中会先调用Worker对象的unlock方法将state设置为0. runWorker方法在Worker类中的run方法调用了runWorker方法来执行任务，runWorker方法的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); // 获取第一个任务 Runnable task = w.firstTask; w.firstTask = null; // 允许中断 w.unlock(); // allow interrupts // 是否因为异常退出循环 boolean completedAbruptly = true; try &#123; // 如果task为空，则通过getTask来获取任务 while (task != null || (task = getTask()) != null) &#123; w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; 这里说明一下第一个if判断，目的是： 如果线程池正在停止，那么要保证当前线程是中断状态； 如果不是的话，则要保证当前线程不是中断状态； 这里要考虑在执行该if语句期间可能也执行了shutdownNow方法，shutdownNow方法会把状态设置为STOP，回顾一下STOP状态： 不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态。 STOP状态要中断线程池中的所有线程，而这里使用Thread.interrupted()来判断是否中断是为了确保在RUNNING或者SHUTDOWN状态时线程是非中断状态的，因为Thread.interrupted()方法会复位中断的状态。 总结一下runWorker方法的执行过程： while循环不断地通过getTask()方法获取任务； getTask()方法从阻塞队列中取任务； 如果线程池正在停止，那么要保证当前线程是中断状态，否则要保证当前线程不是中断状态； 调用task.run()执行任务； 如果task为null则跳出循环，执行processWorkerExit()方法； runWorker方法执行完毕，也代表着Worker中的run方法执行完毕，销毁线程。 这里的beforeExecute方法和afterExecute方法在ThreadPoolExecutor类中是空的，留给子类来实现。 completedAbruptly变量来表示在执行任务过程中是否出现了异常，在processWorkerExit方法中会对该变量的值进行判断。 getTask方法getTask方法用来从阻塞队列中取任务，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263private Runnable getTask() &#123; // timeOut变量的值表示上次从阻塞队列中取任务时是否超时 boolean timedOut = false; // Did the last poll() time out? for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. /* * 如果线程池状态rs &gt;= SHUTDOWN，也就是非RUNNING状态，再进行以下判断： * 1. rs &gt;= STOP，线程池是否正在stop； * 2. 阻塞队列是否为空。 * 如果以上条件满足，则将workerCount减1并返回null。 * 因为如果当前线程池状态的值是SHUTDOWN或以上时，不允许再向阻塞队列中添加任务。 */ if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; int wc = workerCountOf(c); // Are workers subject to culling? // timed变量用于判断是否需要进行超时控制。 // allowCoreThreadTimeOut默认是false，也就是核心线程不允许进行超时； // wc &gt; corePoolSize，表示当前线程池中的线程数量大于核心线程数量； // 对于超过核心线程数量的这些线程，需要进行超时控制 boolean timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; /* * wc &gt; maximumPoolSize的情况是因为可能在此方法执行阶段同时执行了setMaximumPoolSize方法； * timed &amp;&amp; timedOut 如果为true，表示当前操作需要进行超时控制，并且上次从阻塞队列中获取任务发生了超时 * 接下来判断，如果有效线程数量大于1，或者阻塞队列是空的，那么尝试将workerCount减1； * 如果减1失败，则返回重试。 * 如果wc == 1时，也就说明当前线程是线程池中唯一的一个线程了。 */ if ((wc &gt; maximumPoolSize || (timed &amp;&amp; timedOut)) &amp;&amp; (wc &gt; 1 || workQueue.isEmpty())) &#123; if (compareAndDecrementWorkerCount(c)) return null; continue; &#125; try &#123; /* * 根据timed来判断，如果为true，则通过阻塞队列的poll方法进行超时控制，如果在keepAliveTime时间内没有获取到任务，则返回null； * 否则通过take方法，如果这时队列为空，则take方法会阻塞直到队列不为空。 * */ Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; // 如果 r == null，说明已经超时，timedOut设置为true timedOut = true; &#125; catch (InterruptedException retry) &#123; // 如果获取任务时当前线程发生了中断，则设置timedOut为false并返回循环重试 timedOut = false; &#125; &#125;&#125; 这里重要的地方是第二个if判断，目的是控制线程池的有效线程数量。由上文中的分析可以知道，在执行execute方法时，如果当前线程池的线程数量超过了corePoolSize且小于maximumPoolSize，并且workQueue已满时，则可以增加工作线程，但这时如果超时没有获取到任务，也就是timedOut为true的情况，说明workQueue已经为空了，也就说明了当前线程池中不需要那么多线程来执行任务了，可以把多于corePoolSize数量的线程销毁掉，保持线程数量在corePoolSize即可。 什么时候会销毁？当然是runWorker方法执行完之后，也就是Worker中的run方法执行完，由JVM自动回收。 getTask方法返回null时，在runWorker方法中会跳出while循环，然后会执行processWorkerExit方法。 processWorkerExit方法12345678910111213141516171819202122232425262728293031323334353637private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; // 如果completedAbruptly值为true，则说明线程执行时出现了异常，需要将workerCount减1； // 如果线程执行时没有出现异常，说明在getTask()方法中已经已经对workerCount进行了减1操作，这里就不必再减了。 if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //统计完成的任务数 completedTaskCount += w.completedTasks; // 从workers中移除，也就表示着从线程池中移除了一个工作线程 workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; // 根据线程池状态进行判断是否结束线程池 tryTerminate(); int c = ctl.get(); /* * 当线程池是RUNNING或SHUTDOWN状态时，如果worker是异常结束，那么会直接addWorker； * 如果allowCoreThreadTimeOut=true，并且等待队列有任务，至少保留一个worker； * 如果allowCoreThreadTimeOut=false，workerCount不少于corePoolSize。 */ if (runStateLessThan(c, STOP)) &#123; if (!completedAbruptly) &#123; int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; addWorker(null, false); &#125;&#125; 至此，processWorkerExit执行完之后，工作线程被销毁，以上就是整个工作线程的生命周期，从execute方法开始，Worker使用ThreadFactory创建新的工作线程，runWorker通过getTask获取任务，然后执行任务，如果getTask返回null，进入processWorkerExit方法，整个线程结束，如图所示： tryTerminate方法tryTerminate方法根据线程池状态进行判断是否结束线程池，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); /* * 当前线程池的状态为以下几种情况时，直接返回： * 1. RUNNING，因为还在运行中，不能停止； * 2. TIDYING或TERMINATED，因为线程池中已经没有正在运行的线程了； * 3. SHUTDOWN并且等待队列非空，这时要执行完workQueue中的task； */ if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; // 如果线程数量不为0，则中断一个空闲的工作线程，并返回 if (workerCountOf(c) != 0) &#123; // Eligible to terminate interruptIdleWorkers(ONLY_ONE); return; &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 这里尝试设置状态为TIDYING，如果设置成功，则调用terminated方法 if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; // terminated方法默认什么都不做，留给子类实现 terminated(); &#125; finally &#123; // 设置状态为TERMINATED ctl.set(ctlOf(TERMINATED, 0)); termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125; interruptIdleWorkers(ONLY_ONE)的作用是因为在getTask方法中执行workQueue.take()时，如果不执行中断会一直阻塞。在下面介绍的shutdown方法中，会中断所有空闲的工作线程，如果在执行shutdown时工作线程没有空闲，然后又去调用了getTask方法，这时如果workQueue中没有任务了，调用workQueue.take()时就会一直阻塞。所以每次在工作线程结束时调用tryTerminate方法来尝试中断一个空闲工作线程，避免在队列为空时取任务一直阻塞的情况。 shutdown方法shutdown方法要将线程池切换到SHUTDOWN状态，并调用interruptIdleWorkers方法请求中断所有空闲的worker，最后调用tryTerminate尝试结束线程池。 1234567891011121314151617public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // 安全策略判断 checkShutdownAccess(); // 切换状态为SHUTDOWN advanceRunState(SHUTDOWN); // 中断空闲线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; // 尝试结束线程池 tryTerminate();&#125; 这里思考一个问题：在runWorker方法中，执行任务时对Worker对象w进行了lock操作，为什么要在执行任务的时候对每个工作线程都加锁呢？ 下面仔细分析一下： 在getTask方法中，如果这时线程池的状态是SHUTDOWN并且workQueue为空，那么就应该返回null来结束这个工作线程，而使线程池进入SHUTDOWN状态需要调用shutdown方法； shutdown方法会调用interruptIdleWorkers来中断空闲的线程，interruptIdleWorkers持有mainLock，会遍历workers来逐个判断工作线程是否空闲。但getTask方法中没有mainLock； 在getTask中，如果判断当前线程池状态是RUNNING，并且阻塞队列为空，那么会调用workQueue.take()进行阻塞； 如果在判断当前线程池状态是RUNNING后，这时调用了shutdown方法把状态改为了SHUTDOWN，这时如果不进行中断，那么当前的工作线程在调用了workQueue.take()后会一直阻塞而不会被销毁，因为在SHUTDOWN状态下不允许再有新的任务添加到workQueue中，这样一来线程池永远都关闭不了了； 由上可知，shutdown方法与getTask方法（从队列中获取任务时）存在竞态条件； 解决这一问题就需要用到线程的中断，也就是为什么要用interruptIdleWorkers方法。在调用workQueue.take()时，如果发现当前线程在执行之前或者执行期间是中断状态，则会抛出InterruptedException，解除阻塞的状态； 但是要中断工作线程，还要判断工作线程是否是空闲的，如果工作线程正在处理任务，就不应该发生中断； 所以Worker继承自AQS，在工作线程处理任务时会进行lock，interruptIdleWorkers在进行中断时会使用tryLock来判断该工作线程是否正在处理任务，如果tryLock返回true，说明该工作线程当前未执行任务，这时才可以被中断。 下面就来分析一下interruptIdleWorkers方法。 interruptIdleWorkers方法12345678910111213141516171819202122232425private void interruptIdleWorkers() &#123; interruptIdleWorkers(false);&#125;private void interruptIdleWorkers(boolean onlyOne) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; for (Worker w : workers) &#123; Thread t = w.thread; if (!t.isInterrupted() &amp;&amp; w.tryLock()) &#123; try &#123; t.interrupt(); &#125; catch (SecurityException ignore) &#123; &#125; finally &#123; w.unlock(); &#125; &#125; if (onlyOne) break; &#125; &#125; finally &#123; mainLock.unlock(); &#125;&#125; interruptIdleWorkers遍历workers中所有的工作线程，若线程没有被中断tryLock成功，就中断该线程。 为什么需要持有mainLock？因为workers是HashSet类型的，不能保证线程安全。 shutdownNow方法1234567891011121314151617public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); advanceRunState(STOP); // 中断所有工作线程，无论是否空闲 interruptWorkers(); // 取出队列中没有被执行的任务 tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks;&#125; shutdownNow方法与shutdown方法类似，不同的地方在于： 设置状态为STOP； 中断所有工作线程，无论是否是空闲的； 取出阻塞队列中没有被执行的任务并返回。 shutdownNow方法执行完之后调用tryTerminate方法，该方法在上文已经分析过了，目的就是使线程池的状态设置为TERMINATED。 线程池的监控通过线程池提供的参数进行监控。线程池里有一些属性在监控线程池的时候可以使用 getTaskCount：线程池已经执行的和未执行的任务总数； getCompletedTaskCount：线程池已完成的任务数量，该值小于等于taskCount； getLargestPoolSize：线程池曾经创建过的最大线程数量。通过这个数据可以知道线程池是否满过，也就是达到了maximumPoolSize； getPoolSize：线程池当前的线程数量； getActiveCount：当前线程池中正在执行任务的线程数量。 通过这些方法，可以对线程池进行监控，在ThreadPoolExecutor类中提供了几个空方法，如beforeExecute方法，afterExecute方法和terminated方法，可以扩展这些方法在执行前或执行后增加一些新的操作，例如统计线程池的执行任务的时间等，可以继承自ThreadPoolExecutor来进行扩展。 总结本文比较详细的分析了线程池的工作流程，总体来说有如下几个内容： 分析了线程的创建，任务的提交，状态的转换以及线程池的关闭； 这里通过execute方法来展开线程池的工作流程，execute方法通过corePoolSize，maximumPoolSize以及阻塞队列的大小来判断决定传入的任务应该被立即执行，还是应该添加到阻塞队列中，还是应该拒绝任务。 介绍了线程池关闭时的过程，也分析了shutdown方法与getTask方法存在竞态条件； 在获取任务时，要通过线程池的状态来判断应该结束工作线程还是阻塞线程等待新的任务，也解释了为什么关闭线程池时要中断工作线程以及为什么每一个worker都需要lock。 在向线程池提交任务时，除了execute方法，还有一个submit方法，submit方法会返回一个Future对象用于获取返回值，具体可以见 多线程（一）中的 最开始的 demo。 遇到的问题最近好久没复习了，突然有点对线程池晕头转向，阻塞队列到底是存储什么的呢？里面是怎么一个存储形式呢？阻塞队列又是如何实现的呢？阻塞队列和 AQS 中的 CLH 变体队列又有什么关系和联系呢？线程池中的线程又是如何管理的呢？他们是如何存储的呢？线程池中的线程又是怎么去运作的呢？ 来，这些问题一个一个来解决！ 阻塞队列到底存储的是什么？ 存储的是 Runnable 对象，也就是我们常说的任务 里面是如何存储以及如何实现的？ ArrayBlockingQueue 是利用数组存储的，LinkedBlockingQueue 是利用的链表存储的，这里比普通的队列多了一个使用了 ReentrantLock 保证线程安全并且通过使用 Condition 队列保证消费者/生产者模式。 阻塞队列和 AQS 的 CLH变体队列有啥关系和联系 没有任何关系，CLH 变体队列是同步队列，存储的是线程包装后的 Node，我们这里的阻塞队列仅仅是任务包装后的节点，所以并没有 CLH 变体队列那么复杂，不需要进行一个线程唤醒阻塞的过程「park、unpark」，也无需去考虑共享锁和独占锁之类的，更不用考虑同步队列和条件队列的切换过程。 线程池中的线程是如何管理的呢？又是如何存储的呢？ 线程池其实管理的并不是线程，而是 Worker，Worker 包括的属性有线程、任务，每一个 Worker 都是用 HashSet 存储的。 线程池中的线程如何运作的？ 最开始的 Worker 是直接任务来了就直接 new thread 并接受任务，因为此时还没到核心线程数，当到了核心线程数之后，Worker 就会去队列里 take 相应的任务，如果队列也满了的话，就去 new 新的 Worker 接受任务，如果到了最大线程数，就直接默认抛异常「当然还有另外三种措施：1. 丢弃阻塞队列中最前面的 2. 直接不管这个任务 3. 使用调用的这个线程去执行」。 这里 Worker 去拿任务需要注意一下，由于 阻塞队列是线程安全的，所以不会存在一个任务可能被多个线程取走，这是不可能的，其次，得注意一下，Worker 有去继承 AQS，这是我认为线程池中写的最好的一块，主要是解决了两个问题「支持中断以及标记线程是否空闲」，我下面也有详细说这个。 这个也讲的还行：https://www.cnblogs.com/micrari/p/7429364.html 如何保证核心线程不被销毁 timedOut超时标记默认设置为false； 计算timed的值，该值决定了线程的生死大权，(timed &amp;&amp; timedOut) 即是线程超时回收的条件之一，需要注意的是第一次(timed &amp;&amp; timedOut) 为false，因为timedOut默认值为false，此时还没到poll超时获取的操作； 根据timed值来决定是用阻塞超时获取任务还是阻塞获取任务，如果用阻塞超时获取任务，超时后timedOut会被设置为true，接着继续循环，若(timed &amp;&amp; timedOut) 为true，满足线程超时回收。 没有组织的一些话，留在这吧，当时想到啥就说啥了 「问题的核心是区分开 Runnable 和 Thread的区别，一个是任务，一个是线程」 「阻塞队列中存储的是Runnable，阻塞队列底层就是链表实现，然后通过 ReentrantLock 达到线程安全，并且通过 Condition 完成生产者/消费者模式，然后线程池中的线程使用 hashSet 存储，核心线程池中的线程由于 timeout等于无穷所以不会被销毁。阻塞队列和 AQS 没有关系，只是用到了 Condition 和 ReentrantLock 」 「线程池的线程，new 出来后，会被封装成 Worker 对象，然后里面可以运行任务，所有的 Worker 是放在 HashSet 中的。」 「线程池中我觉得最精妙的地方是让 worker 去继承 AQS，这是非常巧妙的设计，这个设计主要是为了在更改 shutdown 和 stop 的时候做出的设计， 我们知道，shutdown就是不再接受新的任务，但是不能把正在运行的任务停掉，同时要把处于空闲的状态的线程停掉，在这里如何区分线程是否空闲是一个问题，源码的解决方案是让 worker 包装成一个互斥锁，如果在运行就给自己加锁，这样我判断线程是否空闲就可以直接用 tryAcquire() 去判断了； 原因之二就是 stop 的时候，我们是需要强制停掉正在运行的任务的，而AQS刚好就提供响应中断，所以这又是worker继承AQS的一大原因。」 http://objcoding.com/2019/04/25/threadpool-running/#worker 这个对 ”如何保证核心线程不被销毁“讲的挺好的 线程池（二）：ScheduledThreadPoolExecutor 转：http://www.ideabuffer.cn/2017/04/14/深入理解Java线程池：ScheduledThreadPoolExecutor/ 介绍自JDK1.5开始，JDK提供了ScheduledThreadPoolExecutor类来支持周期性任务的调度。在这之前的实现需要依靠Timer和TimerTask或者其它第三方工具来完成。但Timer有不少的缺陷： Timer是单线程模式； 如果在执行任务期间某个TimerTask耗时较久，那么就会影响其它任务的调度； Timer的任务调度是基于绝对时间的，对系统时间敏感； Timer不会捕获执行TimerTask时所抛出的异常，由于Timer是单线程，所以一旦出现异常，则线程就会终止，其他任务也得不到执行。 ScheduledThreadPoolExecutor继承ThreadPoolExecutor来重用线程池的功能，它的实现方式如下： 将任务封装成ScheduledFutureTask对象，ScheduledFutureTask基于相对时间，不受系统时间的改变所影响； ScheduledFutureTask实现了java.lang.Comparable接口和java.util.concurrent.Delayed接口，所以有两个重要的方法：compareTo和getDelay。compareTo方法用于比较任务之间的优先级关系，如果距离下次执行的时间间隔较短，则优先级高；getDelay方法用于返回距离下次任务执行时间的时间间隔； ScheduledThreadPoolExecutor定义了一个DelayedWorkQueue，它是一个有序队列，会通过每个任务按照距离下次执行时间间隔的大小来排序； ScheduledFutureTask继承自FutureTask，可以通过返回Future对象来获取执行的结果。 通过如上的介绍，可以对比一下Timer和ScheduledThreadPoolExecutor： Timer ScheduledThreadPoolExecutor 单线程 多线程 单个任务执行时间影响其他任务调度 多线程，不会影响 基于绝对时间 基于相对时间 一旦执行任务出现异常不会捕获，其他任务得不到执行 多线程，单个任务的执行不会影响其他线程 所以，在JDK1.5之后，应该没什么理由继续使用Timer进行任务调度了。 ScheduledThreadPoolExecutor的使用下面用一个具体的例子来说明ScheduledThreadPoolExecutor的使用： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ScheduledThreadPoolTest &#123; public static void main(String[] args) throws InterruptedException &#123; // 创建大小为5的线程池 ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); for (int i = 0; i &lt; 3; i++) &#123; Task worker = new Task("task-" + i); // 只执行一次// scheduledThreadPool.schedule(worker, 5, TimeUnit.SECONDS); // 周期性执行，每5秒执行一次 scheduledThreadPool.scheduleAtFixedRate(worker, 0,5, TimeUnit.SECONDS); &#125; Thread.sleep(10000); System.out.println("Shutting down executor..."); // 关闭线程池 scheduledThreadPool.shutdown(); boolean isDone; // 等待线程池终止 do &#123; isDone = scheduledThreadPool.awaitTermination(1, TimeUnit.DAYS); System.out.println("awaitTermination..."); &#125; while(!isDone); System.out.println("Finished all threads"); &#125;&#125;class Task implements Runnable &#123; private String name; public Task(String name) &#123; this.name = name; &#125; @Override public void run() &#123; System.out.println("name = " + name + ", startTime = " + new Date()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("name = " + name + ", endTime = " + new Date()); &#125;&#125; 下面就来具体分析一下ScheduledThreadPoolExecutor的实现过程。 ScheduledThreadPoolExecutor的实现ScheduledThreadPoolExecutor的类结构看下ScheduledThreadPoolExecutor内部的类图： 不要被这么多类吓到，这里只不过是为了更清楚的了解ScheduledThreadPoolExecutor有关调度和队列的接口。 ScheduledThreadPoolExecutor继承自ThreadPoolExecutor，实现了ScheduledExecutorService接口，该接口定义了schedule等任务调度的方法。 同时ScheduledThreadPoolExecutor有两个重要的内部类：DelayedWorkQueue和ScheduledFutureTask。可以看到，DelayeddWorkQueue是一个阻塞队列，而ScheduledFutureTask继承自FutureTask，并且实现了Delayed接口。有关FutureTask的介绍请参考另一篇文章：FutureTask源码解析。 ScheduledThreadPoolExecutor的构造方法ScheduledThreadPoolExecutor有3中构造方法： 123456789101112131415161718public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue(), threadFactory);&#125;public ScheduledThreadPoolExecutor(int corePoolSize, RejectedExecutionHandler handler) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue(), handler);&#125;public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue(), threadFactory, handler);&#125; 因为ScheduledThreadPoolExecutor继承自ThreadPoolExecutor，所以这里都是调用的ThreadPoolExecutor类的构造方法。有关ThreadPoolExecutor可以参考深入理解Java线程池：ThreadPoolExecutor。 这里注意传入的阻塞队列是DelayedWorkQueue类型的对象。后面会详细介绍。 schedule方法在上文的例子中，使用了schedule方法来进行任务调度，schedule方法的代码如下： 123456789101112131415161718192021222324public ScheduledFuture&lt;?&gt; schedule(Runnable command, long delay, TimeUnit unit) &#123; if (command == null || unit == null) throw new NullPointerException(); RunnableScheduledFuture&lt;?&gt; t = decorateTask(command, new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(delay, unit))); delayedExecute(t); return t;&#125;public &lt;V&gt; ScheduledFuture&lt;V&gt; schedule(Callable&lt;V&gt; callable, long delay, TimeUnit unit) &#123; if (callable == null || unit == null) throw new NullPointerException(); RunnableScheduledFuture&lt;V&gt; t = decorateTask(callable, new ScheduledFutureTask&lt;V&gt;(callable, triggerTime(delay, unit))); delayedExecute(t); return t;&#125; 首先，这里的两个重载的schedule方法只是传入的第一个参数不同，可以是Runnable对象或者Callable对象。会把传入的任务封装成一个RunnableScheduledFuture对象，其实也就是ScheduledFutureTask对象，decorateTask默认什么功能都没有做，子类可以重写该方法： 123456789101112131415/** * 修改或替换用于执行 runnable 的任务。此方法可重写用于管理内部任务的具体类。默认实现只返回给定任务。 */protected &lt;V&gt; RunnableScheduledFuture&lt;V&gt; decorateTask( Runnable runnable, RunnableScheduledFuture&lt;V&gt; task) &#123; return task;&#125;/** * 修改或替换用于执行 callable 的任务。此方法可重写用于管理内部任务的具体类。默认实现只返回给定任务。 */protected &lt;V&gt; RunnableScheduledFuture&lt;V&gt; decorateTask( Callable&lt;V&gt; callable, RunnableScheduledFuture&lt;V&gt; task) &#123; return task;&#125; 然后，通过调用delayedExecute方法来延时执行任务。最后，返回一个ScheduledFuture对象。 scheduleAtFixedRate方法该方法设置了执行周期，下一次执行时间相当于是上一次的执行时间加上period，它是采用已固定的频率来执行任务： 123456789101112131415161718public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) &#123; if (command == null || unit == null) throw new NullPointerException(); if (period &lt;= 0) throw new IllegalArgumentException(); ScheduledFutureTask&lt;Void&gt; sft = new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(period)); RunnableScheduledFuture&lt;Void&gt; t = decorateTask(command, sft); sft.outerTask = t; delayedExecute(t); return t;&#125; scheduleWithFixedDelay方法该方法设置了执行周期，与scheduleAtFixedRate方法不同的是，下一次执行时间是上一次任务执行完的系统时间加上period，因而具体执行时间不是固定的，但周期是固定的，是采用相对固定的延迟来执行任务： 123456789101112131415161718public ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) &#123; if (command == null || unit == null) throw new NullPointerException(); if (delay &lt;= 0) throw new IllegalArgumentException(); ScheduledFutureTask&lt;Void&gt; sft = new ScheduledFutureTask&lt;Void&gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(-delay)); RunnableScheduledFuture&lt;Void&gt; t = decorateTask(command, sft); sft.outerTask = t; delayedExecute(t); return t;&#125; 注意这里的unit.toNanos(-delay));，这里把周期设置为负数来表示是相对固定的延迟执行。 scheduleAtFixedRate和scheduleWithFixedDelay的区别在setNextRunTime方法中就可以看出来： 123456789private void setNextRunTime() &#123; long p = period; // 固定频率，上次执行时间加上周期时间 if (p &gt; 0) time += p; // 相对固定延迟执行，使用当前系统时间加上周期时间 else time = triggerTime(-p);&#125; setNextRunTime方法会在run方法中执行完任务后调用。 triggerTime方法triggerTime方法用于获取下一次执行的具体时间： 123456789private long triggerTime(long delay, TimeUnit unit) &#123; return triggerTime(unit.toNanos((delay &lt; 0) ? 0 : delay));&#125;long triggerTime(long delay) &#123; return now() + ((delay &lt; (Long.MAX_VALUE &gt;&gt; 1)) ? delay : overflowFree(delay));&#125; 这里的delay &lt; (Long.MAX_VALUE &gt;&gt; 1是为了判断是否要防止Long类型溢出，如果delay的值小于Long类型最大值的一半，则直接返回delay，否则需要进行防止溢出处理。 overflowFree方法该方法的作用是限制队列中所有节点的延迟时间在Long.MAX_VALUE之内，防止在compareTo方法中溢出。 12345678910111213private long overflowFree(long delay) &#123; // 获取队列中的第一个节点 Delayed head = (Delayed) super.getQueue().peek(); if (head != null) &#123; // 获取延迟时间 long headDelay = head.getDelay(NANOSECONDS); // 如果延迟时间小于0，并且 delay - headDelay 超过了Long.MAX_VALUE // 将delay设置为 Long.MAX_VALUE + headDelay 保证delay小于Long.MAX_VALUE if (headDelay &lt; 0 &amp;&amp; (delay - headDelay &lt; 0)) delay = Long.MAX_VALUE + headDelay; &#125; return delay;&#125; 当一个任务已经可以执行出队操作，但还没有执行，可能由于线程池中的工作线程不是空闲的。具体分析一下这种情况： 为了方便说明，假设Long.MAX_VALUE=1023，也就是11位，并且当前的时间是100，调用triggerTime时并没有对delay进行判断，而是直接返回了now() + delay，也就是相当于100 + 1023，这肯定是溢出了，那么返回的时间是-925； 如果头节点已经可以出队但是还没有执行出队，那么头节点的执行时间应该是小于当前时间的，假设是95； 这时调用offer方法向队列中添加任务，在offer方法中会调用siftUp方法来排序，在siftUp方法执行时又会调用ScheduledFutureTask中的compareTo方法来比较执行时间； 这时如果执行到了compareTo方法中的long diff = time - x.time;时，那么计算后的结果就是-925 - 95 = -1020，那么将返回-1，而正常情况应该是返回1，因为新加入的任务的执行时间要比头结点的执行时间要晚，这就不是我们想要的结果了，这会导致队列中的顺序不正确。 同理也可以算一下在执行compareTo方法中的long diff = getDelay(NANOSECONDS) - other.getDelay(NANOSECONDS);时也会有这种情况； 所以在triggerTime方法中对delay的大小做了判断，就是为了防止这种情况发生。 如果执行了overflowFree方法呢，这时headDelay = 95 - 100 = -5，然后执行delay = 1023 + (-5) = 1018，那么triggerTime会返回100 + 1018 = -930，再执行compareTo方法中的long diff = time - x.time;时，diff = -930 - 95 = -930 - 100 + 5 = 1018 + 5 = 1023，没有溢出，符合正常的预期。 所以，overflowFree方法中把已经超时的部分时间给减去，就是为了避免在compareTo方法中出现溢出情况。 （说实话，这段代码看的很痛苦，一般情况下也不会发生这种情况，谁会传一个Long.MAX_VALUE呢。要知道Long.MAX_VALUE的纳秒数换算成年的话是292年，谁会这么无聊。。。） ScheduledFutureTask的getDelay方法1234public long getDelay(TimeUnit unit) &#123; // 执行时间减去当前系统时间 return unit.convert(time - now(), NANOSECONDS);&#125; ScheduledFutureTask的构造方法ScheduledFutureTask继承自FutureTask并实现了RunnableScheduledFuture接口，具体可以参考上文的类图，构造方法如下： 1234567891011121314151617181920212223242526ScheduledFutureTask(Runnable r, V result, long ns) &#123; super(r, result); this.time = ns; this.period = 0; this.sequenceNumber = sequencer.getAndIncrement();&#125;/** * Creates a periodic action with given nano time and period. */ScheduledFutureTask(Runnable r, V result, long ns, long period) &#123; super(r, result); this.time = ns; this.period = period; this.sequenceNumber = sequencer.getAndIncrement();&#125;/** * Creates a one-shot action with given nanoTime-based trigger time. */ScheduledFutureTask(Callable&lt;V&gt; callable, long ns) &#123; super(callable); this.time = ns; this.period = 0; this.sequenceNumber = sequencer.getAndIncrement();&#125; 这里面有几个重要的属性，下面来解释一下： time：下次任务执行时的时间； period：执行周期； sequenceNumber：保存任务被添加到ScheduledThreadPoolExecutor中的序号。 在schedule方法中，创建完ScheduledFutureTask对象之后，会执行delayedExecute方法来执行任务。 delayedExecute方法1234567891011121314151617private void delayedExecute(RunnableScheduledFuture&lt;?&gt; task) &#123; // 如果线程池已经关闭，使用拒绝策略拒绝任务 if (isShutdown()) reject(task); else &#123; // 添加到阻塞队列中 super.getQueue().add(task); if (isShutdown() &amp;&amp; !canRunInCurrentRunState(task.isPeriodic()) &amp;&amp; remove(task)) task.cancel(false); else // 确保线程池中至少有一个线程启动，即使corePoolSize为0 // 该方法在ThreadPoolExecutor中实现 ensurePrestart(); &#125;&#125; 说一下这里的第二个if判断： 如果不是SHUTDOWN状态，执行else，否则执行步骤2； 如果在当前线程池运行状态下可以执行任务，执行else，否则执行步骤3； 从阻塞队列中删除任务，如果失败，执行else，否则执行步骤4； 取消任务，但不中断执行中的任务。 对于步骤2，可以通过setContinueExistingPeriodicTasksAfterShutdownPolicy方法设置在线程池关闭时，周期任务继续执行，默认为false，也就是线程池关闭时，不再执行周期任务。 ensurePrestart方法在ThreadPoolExecutor中定义： 1234567void ensurePrestart() &#123; int wc = workerCountOf(ctl.get()); if (wc &lt; corePoolSize) addWorker(null, true); else if (wc == 0) addWorker(null, false);&#125; 调用了addWorker方法，可以在深入理解Java线程池：ThreadPoolExecutor中查看addWorker方法的介绍，线程池中的工作线程是通过该方法来启动并执行任务的。 ScheduledFutureTask的run方法回顾一下线程池的执行过程：当线程池中的工作线程启动时，不断地从阻塞队列中取出任务并执行，当然，取出的任务实现了Runnable接口，所以是通过调用任务的run方法来执行任务的。 这里的任务类型是ScheduledFutureTask，所以下面看一下ScheduledFutureTask的run方法： 123456789101112131415161718public void run() &#123; // 是否是周期性任务 boolean periodic = isPeriodic(); // 当前线程池运行状态下如果不可以执行任务，取消该任务 if (!canRunInCurrentRunState(periodic)) cancel(false); // 如果不是周期性任务，调用FutureTask中的run方法执行 else if (!periodic) ScheduledFutureTask.super.run(); // 如果是周期性任务，调用FutureTask中的runAndReset方法执行 // runAndReset方法不会设置执行结果，所以可以重复执行任务 else if (ScheduledFutureTask.super.runAndReset()) &#123; // 计算下次执行该任务的时间 setNextRunTime(); // 重复执行任务 reExecutePeriodic(outerTask); &#125;&#125; 有关FutureTask的run方法和runAndReset方法，可以参考FutureTask源码解析。 分析一下执行过程： 如果当前线程池运行状态不可以执行任务，取消该任务，然后直接返回，否则执行步骤2； 如果不是周期性任务，调用FutureTask中的run方法执行，会设置执行结果，然后直接返回，否则执行步骤3； 如果是周期性任务，调用FutureTask中的runAndReset方法执行，不会设置执行结果，然后直接返回，否则执行步骤4和步骤5； 计算下次执行该任务的具体时间； 重复执行任务。 ScheduledFutureTask的reExecutePeriodic方法123456789void reExecutePeriodic(RunnableScheduledFuture&lt;?&gt; task) &#123; if (canRunInCurrentRunState(true)) &#123; super.getQueue().add(task); if (!canRunInCurrentRunState(true) &amp;&amp; remove(task)) task.cancel(false); else ensurePrestart(); &#125;&#125; 该方法和delayedExecute方法类似，不同的是： 由于调用reExecutePeriodic方法时已经执行过一次周期性任务了，所以不会reject当前任务； 传入的任务一定是周期性任务。 onShutdown方法onShutdown方法是ThreadPoolExecutor中的钩子方法，在ThreadPoolExecutor中什么都没有做，参考深入理解Java线程池：ThreadPoolExecutor，该方法是在执行shutdown方法时被调用： 123456789101112131415161718192021222324252627282930313233@Override void onShutdown() &#123; BlockingQueue&lt;Runnable&gt; q = super.getQueue(); // 获取在线程池已 shutdown 的情况下是否继续执行现有延迟任务 boolean keepDelayed = getExecuteExistingDelayedTasksAfterShutdownPolicy(); // 获取在线程池已 shutdown 的情况下是否继续执行现有定期任务 boolean keepPeriodic = getContinueExistingPeriodicTasksAfterShutdownPolicy(); // 如果在线程池已 shutdown 的情况下不继续执行延迟任务和定期任务 // 则依次取消任务，否则则根据取消状态来判断 if (!keepDelayed &amp;&amp; !keepPeriodic) &#123; for (Object e : q.toArray()) if (e instanceof RunnableScheduledFuture&lt;?&gt;) ((RunnableScheduledFuture&lt;?&gt;) e).cancel(false); q.clear(); &#125; else &#123; // Traverse snapshot to avoid iterator exceptions for (Object e : q.toArray()) &#123; if (e instanceof RunnableScheduledFuture) &#123; RunnableScheduledFuture&lt;?&gt; t = (RunnableScheduledFuture&lt;?&gt;)e; // 如果有在 shutdown 后不继续的延迟任务或周期任务，则从队列中删除并取消任务 if ((t.isPeriodic() ? !keepPeriodic : !keepDelayed) || t.isCancelled()) &#123; // also remove if already cancelled if (q.remove(t)) t.cancel(false); &#125; &#125; &#125; &#125; tryTerminate();&#125; DelayedWorkQueueScheduledThreadPoolExecutor之所以要自己实现阻塞的工作队列，是因为ScheduledThreadPoolExecutor要求的工作队列有些特殊。 DelayedWorkQueue是一个基于堆的数据结构，类似于DelayQueue和PriorityQueue。在执行定时任务的时候，每个任务的执行时间都不同，所以DelayedWorkQueue的工作就是按照执行时间的升序来排列，执行时间距离当前时间越近的任务在队列的前面（注意：这里的顺序并不是绝对的，堆中的排序只保证了子节点的下次执行时间要比父节点的下次执行时间要大，而叶子节点之间并不一定是顺序的，下文中会说明）。 堆结构如下图所示： 可见，DelayedWorkQueue是一个基于最小堆结构的队列。堆结构可以使用数组表示，可以转换成如下的数组： 在这种结构中，可以发现有如下特性： 假设，索引值从0开始，子节点的索引值为k，父节点的索引值为p，则： 一个节点的左子节点的索引为：k = p * 2 + 1； 一个节点的右子节点的索引为：k = (p + 1) * 2； 一个节点的父节点的索引为：p = (k - 1) / 2。 为什么要使用DelayedWorkQueue呢？ 定时任务执行时需要取出最近要执行的任务，所以任务在队列中每次出队时一定要是当前队列中执行时间最靠前的，所以自然要使用优先级队列。 DelayedWorkQueue是一个优先级队列，它可以保证每次出队的任务都是当前队列中执行时间最靠前的，由于它是基于堆结构的队列，堆结构在执行插入和删除操作时的最坏时间复杂度是 O(logN)。 DelayedWorkQueue的属性123456789101112// 队列初始容量private static final int INITIAL_CAPACITY = 16;// 根据初始容量创建RunnableScheduledFuture类型的数组private RunnableScheduledFuture&lt;?&gt;[] queue = new RunnableScheduledFuture&lt;?&gt;[INITIAL_CAPACITY];private final ReentrantLock lock = new ReentrantLock();private int size = 0;// leader线程private Thread leader = null;// 当较新的任务在队列的头部可用时，或者新线程可能需要成为leader，则通过该条件发出信号private final Condition available = lock.newCondition(); 注意这里的leader，它是Leader-Follower模式的变体，用于减少不必要的定时等待。什么意思呢？对于多线程的网络模型来说： 所有线程会有三种身份中的一种：leader和follower，以及一个干活中的状态：proccesser。它的基本原则就是，永远最多只有一个leader。而所有follower都在等待成为leader。线程池启动时会自动产生一个Leader负责等待网络IO事件，当有一个事件产生时，Leader线程首先通知一个Follower线程将其提拔为新的Leader，然后自己就去干活了，去处理这个网络事件，处理完毕后加入Follower线程等待队列，等待下次成为Leader。这种方法可以增强CPU高速缓存相似性，及消除动态内存分配和线程间的数据交换。 参考自：http://blog.csdn.net/goldlevi/article/details/7705180 具体leader的作用在分析take方法时再详细介绍。 offer方法既然是阻塞队列，入队的操作如add和put方法都调用了offer方法，下面查看一下offer方法： 12345678910111213141516171819202122232425262728293031public boolean offer(Runnable x) &#123; if (x == null) throw new NullPointerException(); RunnableScheduledFuture&lt;?&gt; e = (RunnableScheduledFuture&lt;?&gt;)x; final ReentrantLock lock = this.lock; lock.lock(); try &#123; int i = size; // queue是一个RunnableScheduledFuture类型的数组，如果容量不够需要扩容 if (i &gt;= queue.length) grow(); size = i + 1; // i == 0 说明堆中还没有数据 if (i == 0) &#123; queue[0] = e; setIndex(e, 0); &#125; else &#123; // i != 0 时，需要对堆进行重新排序 siftUp(i, e); &#125; // 如果传入的任务已经是队列的第一个节点了，这时available需要发出信号 if (queue[0] == e) &#123; // leader设置为null为了使在take方法中的线程在通过available.signal();后会执行available.awaitNanos(delay); leader = null; available.signal(); &#125; &#125; finally &#123; lock.unlock(); &#125; return true;&#125; 有关Condition的介绍请参考深入理解AbstractQueuedSynchronizer（三） 这里的重点是siftUp方法。 siftUp方法12345678910111213141516171819private void siftUp(int k, RunnableScheduledFuture&lt;?&gt; key) &#123; while (k &gt; 0) &#123; // 找到父节点的索引 int parent = (k - 1) &gt;&gt;&gt; 1; // 获取父节点 RunnableScheduledFuture&lt;?&gt; e = queue[parent]; // 如果key节点的执行时间大于父节点的执行时间，不需要再排序了 if (key.compareTo(e) &gt;= 0) break; // 如果key.compareTo(e) &lt; 0，说明key节点的执行时间小于父节点的执行时间，需要把父节点移到后面 queue[k] = e; // 设置索引为k setIndex(e, k); k = parent; &#125; // key设置为排序后的位置中 queue[k] = key; setIndex(key, k);&#125; 代码很好理解，就是循环的根据key节点与它的父节点来判断，如果key节点的执行时间小于父节点，则将两个节点交换，使执行时间靠前的节点排列在队列的前面。 假设新入队的节点的延迟时间（调用getDelay()方法获得）是5，执行过程如下： 先将新的节点添加到数组的尾部，这时新节点的索引k为7： 计算新父节点的索引：parent = (k - 1) &gt;&gt;&gt; 1，parent = 3，那么queue[3]的时间间隔值为8，因为 5 &lt; 8 ，将执行queue[7] = queue[3]：： 这时将k设置为3，继续循环，再次计算parent为1，queue[1]的时间间隔为3，因为 5 &gt; 3 ，这时退出循环，最终k为3： 可见，每次新增节点时，只是根据父节点来判断，而不会影响兄弟节点。 另外，setIndex方法只是设置了ScheduledFutureTask中的heapIndex属性： 1234private void setIndex(RunnableScheduledFuture&lt;?&gt; f, int idx) &#123; if (f instanceof ScheduledFutureTask) ((ScheduledFutureTask)f).heapIndex = idx;&#125; take方法12345678910111213141516171819202122232425262728293031323334353637383940public RunnableScheduledFuture&lt;?&gt; take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; for (;;) &#123; RunnableScheduledFuture&lt;?&gt; first = queue[0]; if (first == null) available.await(); else &#123; // 计算当前时间到执行时间的时间间隔 long delay = first.getDelay(NANOSECONDS); if (delay &lt;= 0) return finishPoll(first); first = null; // don't retain ref while waiting // leader不为空，阻塞线程 if (leader != null) available.await(); else &#123; // leader为空，则把leader设置为当前线程， Thread thisThread = Thread.currentThread(); leader = thisThread; try &#123; // 阻塞到执行时间 available.awaitNanos(delay); &#125; finally &#123; // 设置leader = null，让其他线程执行available.awaitNanos(delay); if (leader == thisThread) leader = null; &#125; &#125; &#125; &#125; &#125; finally &#123; // 如果leader不为空，则说明leader的线程正在执行available.awaitNanos(delay); // 如果queue[0] == null，说明队列为空 if (leader == null &amp;&amp; queue[0] != null) available.signal(); lock.unlock(); &#125;&#125; take方法是什么时候调用的呢？在深入理解Java线程池：ThreadPoolExecutor中，介绍了getTask方法，工作线程会循环地从workQueue中取任务。但定时任务却不同，因为如果一旦getTask方法取出了任务就开始执行了，而这时可能还没有到执行的时间，所以在take方法中，要保证只有在到指定的执行时间的时候任务才可以被取走。 再来说一下leader的作用，这里的leader是为了减少不必要的定时等待，当一个线程成为leader时，它只等待下一个节点的时间间隔，但其它线程无限期等待。 leader线程必须在从take（）或poll（）返回之前signal其它线程，除非其他线程成为了leader。 举例来说，如果没有leader，那么在执行take时，都要执行available.awaitNanos(delay)，假设当前线程执行了该段代码，这时还没有signal，第二个线程也执行了该段代码，则第二个线程也要被阻塞。多个这时执行该段代码是没有作用的，因为只能有一个线程会从take中返回queue[0]（因为有lock），其他线程这时再返回for循环执行时取的queue[0]，已经不是之前的queue[0]了，然后又要继续阻塞。 所以，为了不让多个线程频繁的做无用的定时等待，这里增加了leader，如果leader不为空，则说明队列中第一个节点已经在等待出队，这时其它的线程会一直阻塞，减少了无用的阻塞（注意，在finally中调用了signal()来唤醒一个线程，而不是signalAll()）。 poll方法下面看下poll方法，与take类似，但这里要提供超时功能： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public RunnableScheduledFuture&lt;?&gt; poll(long timeout, TimeUnit unit) throws InterruptedException &#123; long nanos = unit.toNanos(timeout); final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; for (;;) &#123; RunnableScheduledFuture&lt;?&gt; first = queue[0]; if (first == null) &#123; if (nanos &lt;= 0) return null; else nanos = available.awaitNanos(nanos); &#125; else &#123; long delay = first.getDelay(NANOSECONDS); // 如果delay &lt;= 0，说明已经到了任务执行的时间，返回。 if (delay &lt;= 0) return finishPoll(first); // 如果nanos &lt;= 0，说明已经超时，返回null if (nanos &lt;= 0) return null; first = null; // don't retain ref while waiting // nanos &lt; delay 说明需要等待的时间小于任务要执行的延迟时间 // leader != null 说明有其它线程正在对任务进行阻塞 // 这时阻塞当前线程nanos纳秒 if (nanos &lt; delay || leader != null) nanos = available.awaitNanos(nanos); else &#123; Thread thisThread = Thread.currentThread(); leader = thisThread; try &#123; // 这里的timeLeft表示delay减去实际的等待时间 long timeLeft = available.awaitNanos(delay); // 计算剩余的等待时间 nanos -= delay - timeLeft; &#125; finally &#123; if (leader == thisThread) leader = null; &#125; &#125; &#125; &#125; &#125; finally &#123; if (leader == null &amp;&amp; queue[0] != null) available.signal(); lock.unlock(); &#125;&#125; finishPoll方法当调用了take或者poll方法能够获取到任务时，会调用该方法进行返回： 123456789101112private RunnableScheduledFuture&lt;?&gt; finishPoll(RunnableScheduledFuture&lt;?&gt; f) &#123; // 数组长度-1 int s = --size; // 取出最后一个节点 RunnableScheduledFuture&lt;?&gt; x = queue[s]; queue[s] = null; // 长度不为0，则从第一个元素开始排序，目的是要把最后一个节点放到合适的位置上 if (s != 0) siftDown(0, x); setIndex(f, -1); return f;&#125; siftDown方法siftDown方法使堆从k开始向下调整： 12345678910111213141516171819202122232425private void siftDown(int k, RunnableScheduledFuture&lt;?&gt; key) &#123; // 根据二叉树的特性，数组长度除以2，表示取有子节点的索引 int half = size &gt;&gt;&gt; 1; // 判断索引为k的节点是否有子节点 while (k &lt; half) &#123; // 左子节点的索引 int child = (k &lt;&lt; 1) + 1; RunnableScheduledFuture&lt;?&gt; c = queue[child]; // 右子节点的索引 int right = child + 1; // 如果有右子节点并且左子节点的时间间隔大于右子节点，取时间间隔最小的节点 if (right &lt; size &amp;&amp; c.compareTo(queue[right]) &gt; 0) c = queue[child = right]; // 如果key的时间间隔小于等于c的时间间隔，跳出循环 if (key.compareTo(c) &lt;= 0) break; // 设置要移除索引的节点为其子节点 queue[k] = c; setIndex(c, k); k = child; &#125; // 将key放入索引为k的位置 queue[k] = key; setIndex(key, k);&#125; siftDown方法执行时包含两种情况，一种是没有子节点，一种是有子节点（根据half判断）。例如： 没有子节点的情况： 假设初始的堆如下： 假设 k = 3 ，那么 k = half ，没有子节点，在执行siftDown方法时直接把索引为3的节点设置为数组的最后一个节点： 有子节点的情况： 假设 k = 0 ，那么执行以下步骤： 获取左子节点，child = 1 ，获取右子节点， right = 2 ： 由于 right &lt; size ，这时比较左子节点和右子节点时间间隔的大小，这里 3 &lt; 7 ，所以 c = queue[child] ； 比较key的时间间隔是否小于c的时间间隔，这里不满足，继续执行，把索引为k的节点设置为c，然后将k设置为child，； 因为 half = 3 ，k = 1 ，继续执行循环，这时的索引变为： 这时再经过如上判断后，将k的值为3，最终的结果如下： 最后，如果在finishPoll方法中调用的话，会把索引为0的节点的索引设置为-1，表示已经删除了该节点，并且size也减了1，最后的结果如下： 可见，siftdown方法在执行完并不是有序的，但可以发现，子节点的下次执行时间一定比父节点的下次执行时间要大，由于每次都会取左子节点和右子节点中下次执行时间最小的节点，所以还是可以保证在take和poll时出队是有序的。 remove方法1234567891011121314151617181920212223242526public boolean remove(Object x) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; int i = indexOf(x); if (i &lt; 0) return false; setIndex(queue[i], -1); int s = --size; RunnableScheduledFuture&lt;?&gt; replacement = queue[s]; queue[s] = null; if (s != i) &#123; // 从i开始向下调整 siftDown(i, replacement); // 如果queue[i] == replacement，说明i是叶子节点 // 如果是这种情况，不能保证子节点的下次执行时间比父节点的大 // 这时需要进行一次向上调整 if (queue[i] == replacement) siftUp(i, replacement); &#125; return true; &#125; finally &#123; lock.unlock(); &#125;&#125; 假设初始的堆结构如下： 这时要删除8的节点，那么这时 k = 1，key为最后一个节点： 这时通过上文对siftDown方法的分析，siftDown方法执行后的结果如下： 这时会发现，最后一个节点的值比父节点还要小，所以这里要执行一次siftUp方法来保证子节点的下次执行时间要比父节点的大，所以最终结果如下： 总结本文详细分析了ScheduedThreadPoolExecutor的实现，主要介绍了以下方面： 与Timer执行定时任务的比较，相比Timer，ScheduedThreadPoolExecutor有什么优点； ScheduledThreadPoolExecutor继承自ThreadPoolExecutor，所以它也是一个线程池，也有coorPoolSize和workQueue，ScheduledThreadPoolExecutor特殊的地方在于，自己实现了优先工作队列DelayedWorkQueue； ScheduedThreadPoolExecutor实现了ScheduledExecutorService，所以就有了任务调度的方法，如schedule，scheduleAtFixedRate和scheduleWithFixedDelay，同时注意他们之间的区别； 内部类ScheduledFutureTask继承自FutureTask，实现了任务的异步执行并且可以获取返回结果。同时也实现了Delayed接口，可以通过getDelay方法获取将要执行的时间间隔； 周期任务的执行其实是调用了FutureTask类中的runAndReset方法，每次执行完不设置结果和状态。参考FutureTask源码解析； 详细分析了DelayedWorkQueue的数据结构，它是一个基于最小堆结构的优先队列，并且每次出队时能够保证取出的任务是当前队列中下次执行时间最小的任务。同时注意一下优先队列中堆的顺序，堆中的顺序并不是绝对的，但要保证子节点的值要比父节点的值要大，这样就不会影响出队的顺序。 总体来说，ScheduedThreadPoolExecutor的重点是要理解下次执行时间的计算，以及优先队列的出队、入队和删除的过程，这两个是理解ScheduedThreadPoolExecutor的关键。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>线程池</tag>
        <tag>ThreadPoolExecutor</tag>
        <tag>ScheduledThreadPoolExecutor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（三）--- 并发容器]]></title>
    <url>%2F2020%2F02%2F27%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%89%EF%BC%89----%20%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8.html</url>
    <content type="text"><![CDATA[ConcurrentHashMap这个我在 Map &amp; Set 源码分析 中已经非常详细的讲过了，这里就直接略过了… CopyOnWriteArrayList https://juejin.im/post/5aeeae756fb9a07ab11112af 吖！这个更简单哈哈哈，其实也就是读不加锁，写加个锁，写的时候由于是先复制一遍，然后再写回到原数组，所以存在读写的弱一致性，也就是说读到的数据可能不会是最新的。 BlockingQueue阻塞队列（BlockingQueue）被广泛使用在“生产者-消费者”问题中，其原因是 BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。其实就是封装了我在 Condition 那一块举的例子。相比于平常的 Queue()，就是在入队出队封装了下 Condition，也就是 put() 和 take() 「入队和出队」。 https://juejin.im/post/5aeebd02518825672f19c546 ArrayBlockingQueue &amp; LinkedBlockingQueue https://juejin.im/post/5aeebdb26fb9a07aa83ea17e 挺简单的…没啥讲的。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>并发容器</tag>
        <tag>ConcurrentHashMap</tag>
        <tag>CopyOnWriteArrayList</tag>
        <tag>BlockingQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（二）--- ThreadLocal]]></title>
    <url>%2F2020%2F02%2F27%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89----%20ThreadLocal.html</url>
    <content type="text"><![CDATA[ThreadLocal这个需要好好写一下，感觉是比较麻烦的。难点主要就在 Thread 和 ThreadLocal 之间的关系。 主要参考了ThreadLocal的原理部分，就是这篇文章终于让我明白我想错了 初始化的时候，注意不要保存对象的引用，这样的话拷贝的副本也必然是对象的引用，最终都会改变对象，这样就不对了 ThreadLocal 简述 ThreadLocal 内存泄漏问题 ThreadLocal相关问题！非常有深度！面试之前一定要再看看！！！！！强烈推荐！！！ ThreadLocal 简介 ThreadLocal 与线程同步无关。 ThreadLocal虽然提供了一种解决多线程环境下成员变量的问题，但是它并不是解决多线程共享变量的问题。对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 正如上面所言，每个线程都会复制一份 ThreadLocal 的副本，然后线程内部进行使用，所谓的 ThreadLocal就是指 “线程局部变量”。 ThreadLocal 原理看源码的时候，被 ThreadLocal、ThreadLocalMap、threadLocals 绕晕了，最后终于绕明白了…其实还是自己先入为主的去思考了 ThreadLocal 的实现，最重要的是，每一个线程都是拥有一个 ThreadLocalMap 的，为什么要用 Map 而不使用 Pair 呢？因为每个线程可能拥有非常多个 ThreadLocal 变量。再重复一遍，ThreadLocalMap 是从属于每个线程的，每个线程自己管理自己的 map，既然说到这，我就再详细的说一下。 我的想法—-ThreadLocal 维护线程与实例的映射因为我看 ThreadLocalMap 是 ThreadLocal 的 内部类，所以以为从属关系是 Map 从属于该变量，那一个可能的方案就是每一个 ThreadLocal 变量都维护一个 ThreadLocalMap，我们都知道，每个访问 ThreadLocal 变量的线程都有自己的一个“本地”实例副本。线程通过该 ThreadLocal 的 get() 方案获取实例时，只需要以线程为键，从 Map 中找出对应的实例即可。该方案如下图所示： 该方案可满足上文提到的每个线程内一个独立备份的要求。每个新线程访问该 ThreadLocal 时，需要向 Map 中添加一个映射，而每个线程结束时，应该清除该映射。这里就有两个问题： 增加线程与减少线程均需要写 Map，故需保证该 Map 线程安全。虽然有几种实现线程安全 Map 的方式，但它或多或少都需要锁来保证线程的安全性。 线程结束时，需要保证它所访问的所有 ThreadLocal 中对应的映射均删除，否则可能会引起内存泄漏。（后文会介绍避免内存泄漏的方法） 其中锁的问题，应该是 JDK 未采用该方案的一个原因。 真正的实现—-Thread 维护ThreadLocal与实例的映射上述方案中，出现锁的问题，原因在于多线程访问同一个 Map。如果该 Map 由 Thread 维护，从而使得每个 Thread 只访问自己的 Map，那就不存在多线程写的问题，也就不需要锁。该方案如下图所示。 该方案虽然没有锁的问题，但是由于每个线程访问某 ThreadLocal 变量后，都会在自己的 Map 内维护该 ThreadLocal 变量与具体实例的映射，如果不删除这些引用（映射），则这些 ThreadLocal 不能被回收，可能会造成内存泄漏。后文会介绍 JDK 如何解决该问题。 ThreadLocal 使用 Demo在这里，我们先来使用一下 ThreadLocal，再来分析其源码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344package 多线程;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ThreadLocalTest &#123;&#125;class ThreadLocalDemo &#123; private static ThreadLocal&lt;SimpleDateFormat&gt; sdf = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); for (int i = 0; i &lt; 100; i++) &#123; executorService.submit(new DateUtil("2019-11-25 09:00:" + i % 60)); &#125; &#125; static class DateUtil implements Runnable &#123; private String date; public DateUtil(String date) &#123; this.date = date; &#125; @Override public void run() &#123; if (sdf.get() == null) &#123; sdf.set(new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")); &#125; else &#123; try &#123; Date date = sdf.get().parse(this.date); System.out.println(date); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 注意我们在这里，用的是 private static 修饰的 ThreadLocal，下文有说为何官方推荐这样修饰。 注意到我们这里使用的方法是 sdf.get()，sdf.set(obj)。 ThreadLocal 源码分析分析之前，我说一下重点，最核心的当然是 ThreadLocal 中的内部类 ThreadLocalMap，整个底层的实现基本全部依托于该内部类，同时别忘了为了 Thread 和 ThreadLocalMap 一一对应，Thread 了类中有一个成员变量—-threadLocals。 123/* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ThreadLocal.ThreadLocalMap threadLocals = null; 接下来正式进入源码分析接阶段： ThreadLocal 部分方法set()12345678910111213141516 public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125;void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125; 逻辑如下： 通过 getMap() 拿到当前线程对应的 ThreadLocals，注意这里可能有多个 ThreadLocal，所以返回的是一个map； 如果 map 为空，说明该线程还没有使用过 ThreadLocal 变量，此时就调用 createMap() 新建一个 ThreadLocalMap，这里 map 又出现了，相信大家已经感受到了重要性了。 如果 map 不为空，我们就把 key = 当前的 ThreadLocal，value 即为当前线程的 ThreadLocal 副本 对应的值。 get()1234567891011121314151617181920212223public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; &#125; 逻辑如下： 通过当前线程拿到 map，然后拿到对应的 key 的 entry，然后返回对应的 value 值。 如果 map 为空，则 调用 setInitialValue()，调用初始化 ThreadLocal 时的值。 remove()12345public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this);&#125; 懒得解释了…还是调用的 ThreadLocalMap 中的 remove() 函数。 ThreadLocalMap（一）前面的方法已经铺垫很久了，所有的方法基本都是基于这个类实现的。ThreadLocal 700多行源代码，有500多行都是内部类 ThreadLocalMap。 数据结构必须要说明的是，这里的 map 不同于我们之前介绍的 HashMap、TreeMap等，这里的 map 底层采用的就是只有数组，没有用到红黑树和链表，因为这里解决冲突的方法并不是链地址法，而是开放地址法中的线性探测。所以这里数组的基本单元是 Entry。 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; Tip: 这里有一个比较特殊的类 WeakReference，也就是弱引用，我们后面会详细介绍这个，因为这个跟内存泄漏有很大的关系。 然后初始值跟 HashMap 一样，都是 16，但是其装载因子变成了 2/3，装载因子越大，越容易出现冲突的现象，但是装载因子越小，就越浪费空间，我们可以看到，这里的装载因子是比 HashMap 要小的，所以这里更不容易产生冲突，这也是采用开放地址法的原因吧。 现在来分析一下主要的函数： set()123456789101112131415161718192021222324252627282930313233private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; //这里有一段英文注释，意思是不会像 get() 一样，直接去找，成功了就返回，没成功就再调用另外一个函数 // 因为 set()大概率是失败的多，所以就直接写在一个函数内了。 Entry[] tab = table; int len = tab.length; //根据threadLocal的hashCode确定Entry应该存放的位置 int i = key.threadLocalHashCode &amp; (len-1); //采用开放地址法，hash冲突的时候使用线性探测 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); //覆盖旧Entry if (k == key) &#123; e.value = value; return; &#125; //当key为null时，说明threadLocal强引用已经被释放掉，那么就无法 //再通过这个key获取threadLocalMap中对应的entry，这里就存在内存泄漏的可能性 if (k == null) &#123; //用当前插入的值替换掉这个key为null的“脏”entry replaceStaleEntry(key, value, i); return; &#125; &#125; //新建entry并插入table中i处 tab[i] = new Entry(key, value); int sz = ++size; //插入后再次清除一些key为null的“脏”entry,如果大于阈值就需要扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; 逻辑如下： 先确定 ThreadLocal 所在的位置； 然后采用线性探测法，确定位置； 如果存在该 ThreadLocal，那么直接覆盖就好了；如果为 null，说明该 ThreadLocal的强引用已经消失，此时直接在该位置插入即可；如果不存在，那么就新建一个 Entry，插入，最后再次清楚 脏entry，并判断是否需要扩容。「后面会详细讲专门处理脏 entry 的这几个方法」 getEntry()123456789101112private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; //1. 确定在散列数组中的位置 int i = key.threadLocalHashCode &amp; (table.length - 1); //2. 根据索引i获取entry Entry e = table[i]; //3. 满足条件则返回该entry if (e != null &amp;&amp; e.get() == key) return e; else //4. 未查找到满足条件的entry，额外在做的处理 return getEntryAfterMiss(key, i, e);&#125; 12345678910111213141516171819private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) //找到和查询的key相同的entry则返回 return e; if (k == null) //解决脏entry的问题 expungeStaleEntry(i); else //继续向后环形查找 i = nextIndex(i, len); e = tab[i]; &#125; return null;&#125; resize()123456789101112131415161718192021222324252627282930313233343536/** * Double the capacity of the table. */private void resize() &#123; Entry[] oldTab = table; int oldLen = oldTab.length; //新数组为原数组的2倍 int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j &lt; oldLen; ++j) &#123; Entry e = oldTab[j]; if (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); //遍历过程中如果遇到脏entry的话直接另value为null,有助于value能够被回收 if (k == null) &#123; e.value = null; // Help the GC &#125; else &#123; //重新确定entry在新数组的位置，然后进行插入 int h = k.threadLocalHashCode &amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; //设置新哈希表的threshHold和size属性 setThreshold(newLen); size = count; table = newTab;&#125; private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0);&#125; 扩容还是非常简单的，因为不涉及多线程，而且只用到数组，整体逻辑如下： new 一个原来容量两倍的数组； 遇到了脏 entry，就将其 value 置为 null，方便 GC，如果 key != null，就重新计算 entry 在新数组的位置； 然后插入，如果有冲突就线性探测就好了； 最后更新 threshold、size、table就好啦。 remove()12345678910111213141516private void remove(ThreadLocal&lt;?&gt; key) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; if (e.get() == key) &#123; //将entry的key置为null e.clear(); //将该entry的value也置为null expungeStaleEntry(i); return; &#125; &#125;&#125; 这里也很简单，就不说了，就跟上面的一样，在这疯狂用 expungeStaleEntry()！！！ ThreadLocalMap（二）这里单独开一小节，讲 ThreadLocalMap中是如何处理脏 entry 的，也就是 key == null 的键值对，这样会导致内存泄漏，所以必须处理。 cleanSomeSlots()可以看到，核心依旧是 expungeStaleEntry()。 123456789101112131415161718192021// 清除部分 脏entry// 启发式的扫描过期数据并擦除，启发式是这样的：// 如果实在没有过期数据，那么这个算法的时间复杂度就是O(log s)// 如果有过期数据，那么这个算法的时间复杂度就是O(log n)private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; i = nextIndex(i, len); Entry e = tab[i]; // 如果 entry 不为空，且entry中的key指向的是 null // 说明产生了 脏entry if (e != null &amp;&amp; e.get() == null) &#123; n = len; removed = true; i = expungeStaleEntry(i); &#125; &#125; while ( (n &gt;&gt;&gt;= 1) != 0); return removed;&#125; 整体逻辑如下： 从插入的entry索引的后一位开始检测是否有 脏entry； 先n == size，log n 去检测，如果发现有脏entry，立即加大n = length，然后从下一个哈希桶为 null 的索引位置 i 继续 log n 检测； 最后返回是否有修改。 expungeStaleEntry()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Expunge a stale entry by rehashing any possibly colliding entries * lying between staleSlot and the next null slot. This also expunges * any other stale entries encountered before the trailing null. See * Knuth, Section 6.4 * * @param staleSlot index of slot known to have null key * @return the index of the next null slot after staleSlot * (all between staleSlot and this slot will have been checked * for expunging). */private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; //清除当前脏entry // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; //2.往后环形继续查找,直到遇到table[i]==null时结束 for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); //3. 如果在向后搜索过程中再次遇到脏entry，同样将其清理掉 if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; //处理rehash的情况 int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125; 该方法逻辑请看注释（第1,2,3步），主要做了这么几件事情： 清理当前脏entry，即将其value引用置为null，并且将table[staleSlot]也置为null。value置为null后该value域变为不可达，在下一次gc的时候就会被回收掉，同时table[staleSlot]为null后以便于存放新的entry; 从当前staleSlot位置向后环形（nextIndex）继续搜索，直到遇到哈希桶（tab[i]）为null的时候退出； 若在搜索过程再次遇到脏entry，继续将其清除。 也就是说该方法，清理掉当前脏entry后，并没有闲下来继续向后搜索，若再次遇到脏entry继续将其清理，直到哈希桶（table[i]）为null时退出。因此方法执行完的结果为 从当前脏entry（staleSlot）位到返回的i位，这中间所有的entry不是脏entry。为什么是遇到null退出呢？原因是存在脏entry的前提条件是 当前哈希桶（table[i]）不为null,只是该entry的key域为null。如果遇到哈希桶为null,很显然它连成为脏entry的前提条件都不具备。 cleanSomeSlot方法主要有这样几点： 从当前位置i处（位于i处的entry一定不是脏entry）为起点在初始小范围（log2(n)，n为哈希表已插入entry的个数size）开始向后搜索脏entry，若在整个搜索过程没有脏entry，方法结束退出 如果在搜索过程中遇到脏entryt通过expungeStaleEntry方法清理掉当前脏entry，并且该方法会返回下一个哈希桶(table[i])为null的索引位置为i。这时重新令搜索起点为索引位置i，n为哈希表的长度len，再次扩大搜索范围为log2(n)继续搜索。 下面，以一个例子更清晰的来说一下，假设当前table数组的情况如下图。 如图当前n等于hash表的size即n=10，i=1,在第一趟搜索过程中通过nextIndex,i指向了索引为2的位置，此时table[2]为null，说明第一趟未发现脏entry,则第一趟结束进行第二趟的搜索。 第二趟所搜先通过nextIndex方法，索引由2的位置变成了i=3,当前table[3]!=null但是该entry的key为null，说明找到了一个脏entry，先将n置为哈希表的长度len,然后继续调用expungeStaleEntry方法，该方法会将当前索引为3的脏entry给清除掉（令value为null，并且table[3]也为null）,但是该方法可不想偷懒，它会继续往后环形搜索，往后会发现索引为4,5的位置的entry同样为脏entry，索引为6的位置的entry不是脏entry保持不变，直至i=7的时候此处table[7]位null，该方法就以i=7返回。至此，第二趟搜索结束； 由于在第二趟搜索中发现脏entry，n增大为数组的长度len，因此扩大搜索范围（增大循环次数）继续向后环形搜索； 直到在整个搜索范围里都未发现脏entry，cleanSomeSlot方法执行结束退出。 replaceStaleEntry()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/* * @param key the key * @param value the value to be associated with key * @param staleSlot index of the first stale entry encountered while * searching for key. */private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; Entry[] tab = table; int len = tab.length; Entry e; // Back up to check for prior stale entry in current run. // We clean out whole runs at a time to avoid continual // incremental rehashing due to garbage collector freeing // up refs in bunches (i.e., whenever the collector runs). //向前找到第一个脏entry int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null)1. slotToExpunge = i; // Find either the key or trailing null slot of run, whichever // occurs first for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); // If we find key, then we need to swap it // with the stale entry to maintain hash table order. // The newly stale slot, or any other stale slot // encountered above it, can then be sent to expungeStaleEntry // to remove or rehash all of the other entries in run. if (k == key) &#123; //如果在向后环形查找过程中发现key相同的entry就覆盖并且和脏entry进行交换2. e.value = value;3. tab[i] = tab[staleSlot];4. tab[staleSlot] = e; // Start expunge at preceding stale entry if it exists //如果在查找过程中还未发现脏entry，那么就以当前位置作为cleanSomeSlots //的起点 if (slotToExpunge == staleSlot)5. slotToExpunge = i; //搜索脏entry并进行清理6. cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; // If we didn't find stale entry on backward scan, the // first stale entry seen while scanning for key is the // first still present in the run. //如果向前未搜索到脏entry，则在查找过程遇到脏entry的话，后面就以此时这个位置 //作为起点执行cleanSomeSlots if (k == null &amp;&amp; slotToExpunge == staleSlot)7. slotToExpunge = i; &#125; // If key not found, put new entry in stale slot //如果在查找过程中没有找到可以覆盖的entry，则将新的entry插入在脏entry8. tab[staleSlot].value = null;9. tab[staleSlot] = new Entry(key, value); // If there are any other stale entries in run, expunge them10. if (slotToExpunge != staleSlot) //执行cleanSomeSlots11. cleanSomeSlots(expungeStaleEntry(slotToExpunge), len);&#125; 这个有点复杂，暂时就不去掌握了…只需要知道这个函数用当前插入的值替换掉这个key为null的“脏”entry，并且并不局限于如此，还把附近所有可能是脏entry都清理了，应该算是整个 ThreadLocal中最为复杂的函数了。 ThreadLocalMap 与 HashMap 的部分区别 解决冲突的方式不一样。一个使用的是开放地址法，一个是链地址法； 默认装载因子不同。ThreadLocalMap 2/3，HashMap 0.75； ThreadLocalMap 没有扰动函数，直接就是相与(也就是取模的过程)，而 HashMap 有高16位与低16位相与进行扰动； 扩容和初始容量都是一样的，初始16，然后扩容都是两倍。 弱引用 四大引用的概念 Q：弱引用的概念 A：四大引用。强引用、弱引用、软引用、幻想引用。弱引用通过WeakReference类实现。 弱引用的生命周期比软引用短。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。由于垃圾回收器是一个优先级很低的线程，因此不一定会很快回收弱引用的对象。弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 Q：这里的key对ThreadLocal为弱引用的作用，如果使用强引用呢？ A：这里的 key 对 ThreadLocal 采用弱引用，是为了减少内存泄漏的发生。当 ThreadLocal 强引用全部消失时，Entry 中的 key 会变为 null，但此时 value 还是被 map 持有强引用，此时 entry 就是可能会造成内存泄漏，不可用但是可达，无法 gc，针对这个问题，THreadLocalMap中可以调用 set()、get()、remove()等方法都会触发 expungeStaleEntry机制，将脏 entry清除掉防止内存泄漏。如果像以前使用了强引用，那么当对象使用完 ThreadLocal 变量，ThreadLocal由于一直被 key 持有强引用，无法回收，并且该 Entry 也过期了，导致更大的内存泄漏，并且此时无法通过 expungeStaleEntry机制解决该问题。至于造成内存泄漏的原因，根本在于 ThreadLocalMap 和 Thread 拥有同样长的生命周期。 重要的事情说三遍！用完ThreadLocal最后一定要 remove()、remove()、remove()。 Q：Entry、value 使用弱引用，是什么效果？ A：Entry定义为弱引用：当GC回收后，无法区分是原本就没有写入还是被回收了，后续线性探测的修补也无法完成。 value定义为弱引用：似乎也是个不错的方法，为啥没这么做？因为这么做和将key定义为弱引用基本没区别，仍然可以依赖弱引用机制清理，但通常在我们的使用中不会持有value的强引用，只会持有key即ThreadLocal对象的强引用，而value没有强引用的情况下会被GC回收，与我们期望的功能不符。 内存泄漏 主要参考网址：https://juejin.im/post/5d75ebf451882576c478a102 这应该是 ThreadLocal 最难的部分了，网上查阅了几百篇文章，基本上把所有有关的 ThreadLocal 内存泄漏的文章都看了一遍…只有这一篇比较欣赏，剩下的基本上全部是在互相抄…好多地方解释的也不通…接下来希望自己能把这个讲通吧… 什么是内存泄漏对象已经没有被应用程序使用，但是垃圾回收器没办法移除它们，因为还在被引用着。 在Java中，内存泄漏就是存在一些被分配的对象，这些对象有下面两个特点，首先，这些对象是可达的，即在有向图中，存在通路可以与其相连；其次，这些对象是无用的，即程序以后不会再使用这些对象。如果对象满足这两个条件，这些对象就可以判定为Java中的内存泄漏，这些对象不会被GC所回收，然而它却占用内存。 ThreadLocal 什么时候会发生内存泄漏我个人倾向于将其分为两类： 当 ThreadLocal 变量不使用 static 修饰当 ThreadLocal 变量不使用 static 修饰，那么 ThreadLocal 的强引用就很有可能全部消失「什么时候会消失？？ 这个翻了整个互联网都没见到…」，而 ThreadLocalMap 中的 key 对 ThreadLocal 的引用是弱引用，根据 GC 规则，ThreadLocal 变量会被回收，此时 ThreadLocaMap 对应的 key 会自动变为 null，而 此时的 Entry 和 value 又被 ThreadLocalMap 强引用，即不可能会被GC，所以此时就造成了内存泄漏，解决的方法就是显示的调用 set()、get()、remove()「当然调用 set()、get() 也不一定能解决内存泄漏问题，因为这两个方法可能在没碰到脏 entry就结束循环了」等方法，因为其可以调用 expungeStaleEntry() 将这些 key 为 null 的键值对回收。当然了，如果线程会结束，那最终也不会发生内存泄漏啦，线程死了那肯定就自动清理 map 了。 Q：回到上面那个问题，什么时候强引用会消失？ A：我参考的网址终于有了一个例子。强引用是对应的子线程或主线程中某个对象持有的，对象生命周期结束或对象替换指向这个key的引用后，key的强引用也就断了。对，就是对象生命周期结束，强引用就消失了。 eg：假如我现在有一个主线程，然后我调用了 A，创建了 A 的实例对象 a1，a1 调用了 ThreadLocal 变量，此时对象 a1 是有对 ThreadLocal 的强引用的，此时 ThreadLocalMap 中的 key还不会为 null ，当主线程调用了 a = null，该对象生命周期结束，此时强引用消失了，但是 ThreadLocalMap 中的 value 还是强引用，无法 GC，也就是上面的情况。 12345678910111213141516public class A &#123; private ThreadLocal&lt;Context&gt; local = new ThreadLocal&lt;&gt;(); public void doSth() &#123; // Context ctx = ... local.set(ctx); &#125;&#125;public static void main(String[] args)&#123; A a = new A(); a.doSth(); a = null; // 或者直接 finalize(); // ... doSometing();&#125; 当 ThreadLocal 变量使用了 static 修饰当 ThreadLocal 变量使用了 static 修饰，说明该变量的生命周期和类一样的，只要引用不改变，那么该变量会一直持有强引用（强引用上面已经讲了），那么此时 key 对 ThreadLocal 持有弱引用我个人觉得就没有任何意义了，因为是不可能触发 expungeStaleEntry() 的，因为 key 不会变为 null。那么此时是不是就不会出现内存泄漏了呢？不，依旧会出现。其实此时如果我们正常使用，是不会产生内存泄漏的，但是一旦我们使用的是线程池，线程使用完之后不会回收，而我们之前使用的 ThreadLocalMap 又存在很多过期的 value 需要清理，此时就算 key 是弱引用，也没有办法，因为 ThreadLocal 一直持有强引用根本不会触发 GC，此时就产生了内存泄漏。 举个例子： 线程1和线程2都用了threadLocal1和threadLocal2，且设置了value 线程1使用完毕归还线程池，但没有调用threadLocal1.remove() 之后线程1不再使用threadLocal1了，仅使用threadLocal2 线程1的threadLocalMap中仍然保存了obj1 由于静态变量threadLocal1引用仍然可达，不会被回收，线程1无法触发expungeStaleEntry机制，threadLocal1对应的entry和value无法回收，造成了内存泄漏 所以用private static修饰之后，好处就是仅使用有限的ThreadLocal对象以节约创建对象和后续自动回收的开销，坏处是需要我们手动调用remove方法清理使用完的slot，否则会有内存泄漏问题。 问题 https://juejin.im/post/5a0e985df265da430e4ebb92#heading-2 https://zhuanlan.zhihu.com/p/91756169 Q1：ThreadLocal 跟普通局部变量有何区别，在其内部使用局部变量不是一样的吗？ A1：网上有两种说法。 该实例需要在多个方法中共享，但不希望被多线程共享，此时可以使用 ThreadLocal，这样使得代码耦合度更低，无需在每个方法中都去声明一个局部变量，简而言之，就是做到线程间隔离，线程内共享。我个人觉得这种说法有些过于牵强，但是貌似网上基本都是这种说法… 我觉得 ThreadLocal 相比局部变量而言，其实也就是 全局变量 和 局部变量 的区别。虽然每个线程的 ThreadLocal 副本互不影响，但是我可以通过 ThreadLocal 进行通信，这是局部变量所不具备的能力。举个例子，假设要设计这么一个程序：三个线程同时从0数到10，一旦有一个线程数到了10，三个线程都从0重新开始数，一直这么循环。用run创建局部变量的方式就没法做，因为每一个线程之间是完全独立的。 ThreadLocal保存共享变量的副本，用于特定的使用场景。看源码就知道ThreadLocal类是基于Thread类来实现的，如果直接用继承Thread来实现上面的功能也是可以的，只不过JDK都已经给封装好了，自己能想到的思路估计跟ThreadLocal的实现思路差不多。如果场景适合使用ThreadLocal，为何不直接用呢。 所以，线程间可通信且可以做到互不影响，我觉得这才是 ThreadLocal 真正存在的意义吧。 Q2：既然 ThreadMap 从属于每个线程，那为何不把 ThreadLocalMap 放到 Thread 类中，反而是成为了 ThreadLocal 的内部类？ A2：其实我觉得都是可以的，不管放到 Thread 类中，还是放到 ThreadLocal 中，都是可以解决问题的，但是我觉得作者这样设计，主要是因为在之前 ThreadLocalMap 是从属于 ThreadLocal 的，可能后来考虑到我上文提到的需要加锁还有线程结束带来的 ThreadLocal 的内存泄漏问题，改成了 ThreadLocalMap 从属于 Thread，为了最大限度的不改动代码，只在 Thread 中添加了一个成员变量 ThreadLocal.ThreadLocalMap threadLocals，并且这样设计的话，用户层面去调用 api 的时候根本不会发现 ThreadLocalMap 的存在，设计的还是非常优美的！ Q3：为何官方建议定义 ThreadLocal 时 采用 private static 修饰？ A3：可以避免重复创建TSO(Thread Specific Object，即与线程相关的变量)，如果把 ThreadLocal 声明为某个类的实例变量（而不是静态变量），那么每创建一个该类的实例就会导致一个新的TSO实例被创建。显然，这些被创建的TSO实例是没有任何意义的。既然所有的线程用到的 ThreadLocal 都是一样的，那么很明显设计成 static 是很正常的，不以对象为出发点，而以整个类为出发点就好了。 所以用private static修饰之后，好处就是仅使用有限的ThreadLocal对象以节约创建对象和后续自动回收的开销，坏处是需要我们手动调用remove方法清理使用完的slot，否则会有内存泄漏问题。 Q4：ThreadLocal 的用途？ A4： 线程间隔离，线程内通信，解决相同变量在多线程环境下的冲突问题，例如： hibernate中的session使用； spring中单例bean的多线程访问。我们知道在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全状态采用ThreadLocal进行处理，让它们也成为线程安全的状态，有状态的Bean就可以在多线程中共享了； 以及 spring 如何保证数据库事务在同一个连接下执行的。DataSourceTransactionManager 是spring的数据源事务管理器， 它会在你调用getConnection()的时候从数据库连接池中获取一个connection， 然后将其与ThreadLocal绑定， 事务完成后解除绑定。这样就保证了事务在同一连接下完成。 可以用于线程间通信，比如三个线程都用到了 ThreadLocal，只要有一个达到了目标值，我就全部设为初值，这是可以通过 ThreadLocal直接完成的。 Q5：ThreadLocal 和 Synchronized 区别？ A5：ThreadLocal和synchronized关键字都用于处理多线程并发访问变量的问题，只是二者处理问题的角度和思路不同。 ThreadLocal是一个Java类,通过对当前线程中的局部变量的操作来解决不同线程的变量访问的冲突问题。所以，ThreadLocal提供了线程安全的共享对象机制，每个线程都拥有其副本。 Java中的synchronized是一个保留字，它依靠JVM的锁机制来实现临界区的函数或者变量的访问中的原子性。在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。此时，被用作“锁机制”的变量时多个线程共享的。 同步机制(synchronized关键字)采用了以“时间换空间”的方式，提供一份变量，让不同的线程排队访问。而ThreadLocal采用了“以空间换时间”的方式，为每一个线程都提供一份变量的副本，从而实现同时访问而互不影响。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>ThreadLocal</tag>
        <tag>内存泄漏</tag>
        <tag>弱引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F02%2F25%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F.html</url>
    <content type="text"><![CDATA[创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 整挺好：https://blog.csdn.net/carson_ho/article/details/54910518 单例模式简介介绍单例模式（Singleton），也叫单子模式，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。 实现思路一个类能返回对象一个引用(永远是同一个)和一个获得该实例的方法（必须是静态方法，通常使用getInstance这个名 称）；当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用；同时我们 还将该类的构造函数定义为私有方法，这样其他处的代码就无法通过调用该类的构造函数来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例。 注意点单例模式在多线程的应用场合下必须小心使用。如果当唯一实例尚未创建时，有两个线程同时调用创建方法，那么它们同时没有检测到唯一实例的存在，从而同时各自创建了一个实例， 这样就有两个实例被构造出来，从而违反了单例模式中实例唯一的原则。 解决这个问题的办法是为指示类是否已经实例化的变量提供一个互斥锁(虽然这样会降低效率)。 优缺点优点： 在单例模式中，活动的单例只有一个实例，对单例类的所有实例化得到的都是相同的一个实例。这样就防止其它对象对自己的实例化，确保所有的对象都访问一个实例。 单例模式具有一定的伸缩性，类自己来控制实例化进程，类就在改变实例化进程上有相应的伸缩性。 提供了对唯一实例的受控访问。 由于在系统内存中只存在一个对象，因此可以 节约系统资源，当 需要频繁创建和销毁的对象时单例模式无疑可以提高系统的性能。 允许可变数目的实例。 避免对共享资源的多重占用。 缺点： 不适用于变化的对象，如果同一类型的对象总是要在不同的用例场景发生变化，单例就会引起数据的错误，不能保存彼此的状态。 由于单例模式中没有抽象层，因此单例类的扩展有很大的困难。 单例类的职责过重，在一定程度上违背了“单一职责原则”。 滥用单例将带来一些负面问题，如为了节省资源将数据库连接池对象设计为的单例类，可能会导致共享连接池对象的程序过多而出现连接池溢出；如果实例化的对象长时间不被利用，系统会认为是垃圾而被回收，这将导致对象状态的丢失。 适用场景单例模式只允许创建一个对象，因此节省内存，加快对象访问速度，因此对象需要被公用的场合适合使用，如多个模块使用同一个数据源连接对象等等。如： 需要频繁实例化然后销毁的对象。 创建对象时耗时过多或者耗资源过多，但又经常用到的对象。 有状态的工具类对象。 频繁访问数据库或文件的对象。 以下都是单例模式的经典使用场景： 资源共享的情况下，避免由于资源操作时导致的性能或损耗等。如上述中的日志文件，应用配置。 控制资源的情况下，方便资源之间的互相通信。如线程池等。 应用场景举例： 外部资源：每台计算机有若干个打印机，但只能有一个PrinterSpooler，以避免两个打印作业同时输出到打印机。内部资源：大多数软件都有一个（或多个）属性文件存放系统配置，这样的系统应该有一个对象管理这些属性文件。 Windows的Task Manager（任务管理器）就是很典型的单例模式（这个很熟悉吧），想想看，是不是呢，你能打开两个windows task manager吗？ 不信你自己试试看哦~ windows的Recycle Bin（回收站）也是典型的单例应用。在整个系统运行过程中，回收站一直维护着仅有的一个实例。 网站的计数器，一般也是采用单例模式实现，否则难以同步。 应用程序的日志应用，一般都何用单例模式实现，这一般是由于共享的日志文件一直处于打开状态，因为只能有一个实例去操作，否则内容不好追加。 Web应用的配置对象的读取，一般也应用单例模式，这个是由于配置文件是共享的资源。 数据库连接池的设计一般也是采用单例模式，因为数据库连接是一种数据库资源。数据库软件系统中使用数据库连接池，主要是节省打开或者关闭数据库连接所引起的效率损耗，这种效率上的损耗还是非常昂贵的，因为何用单例模式来维护，就可以大大降低这种损耗。 多线程的线程池的设计一般也是采用单例模式，这是由于线程池要方便对池中的线程进行控制。 操作系统的文件系统，也是大的单例模式实现的具体例子，一个操作系统只能有一个文件系。 HttpApplication 也是单例的典型应用。 以上来自：https://www.cnblogs.com/damsoft/p/6105122.html 实现方式饿汉式这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在第一次加载类到内存中时就会初始化，所以创建实例本身是线程安全的 12345678public class Singleton &#123; // 类加载时就初始化 private static final Singleton instance = new Singleton(); private Singleton() &#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 缺点是它不是一种懒加载模式（lazy initialization），单例会在加载类后一开始就被初始化，即使客户端没有调用 getInstance()方法。饿汉式的创建方式在一些场景中将无法使用：譬如 Singleton 实例的创建是依赖参数或者配置文件的，在 getInstance() 之前必须调用某个方法设置参数给它，那样这种单例写法就无法使用 懒汉式，线程不安全12345678910public class Singleton &#123; private static Singleton instance; private Singleton () &#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这段代码简单明了，而且使用了懒加载模式，但是却存在致命的问题。当有多个线程并行调用 getInstance() 的时候，就会创建多个实例 懒汉式，线程安全为了解决上面的问题，最简单的方法是将整个 getInstance() 方法设为同步（synchronized） 123456public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance;&#125; 虽然做到了线程安全，并且解决了多实例的问题，但是它并不高效。因为在任何时候只能有一个线程调用 getInstance() 方法。但是同步操作只需要在第一次调用时才被需要，即第一次创建单例实例对象时。这就引出了双重检验锁 双重检验锁1234567891011121314public class Singleton &#123; private volatile static Singleton instance; private Singleton () &#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 称其为双重检查锁，是因为会有两次检查 instance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了 而 instance = new Singleton() 这句，并非是一个原子操作，事实上在 JVM 中这句话做了下面 3 件事： 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将 instance 对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错 所以需要将 instance 变量声明成 volatile 「禁止重排序」 静态内部类123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton () &#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 静态内部类与外部类没有什么关系，外部类加载的时候，内部类不会被加载，静态内部类只是调用的时候用了外部类的名字而已，所以即使 Singleton 类被加载也不会创建单例对象 静态内部类 SingletonHolder 只有在 getInstance() 方法第一次被调用时，才会被加载，从而初始化它的静态域（创建 Singleton 的实例），因此该种方式实现了懒汉式的单例模式 不仅如此，根据 JVM 本身机制，由于是静态的域，因此只会在虚拟机装载类的时候初始化一次，并由虚拟机来保证它的线程安全性 同时不用 synchronized，所以没有性能缺陷 1234567public class Singleton &#123; private static final Singleton INSTANCE = new Singleton(); private Singleton() &#123;&#125; public static Singleton getInstance() &#123; return INSTANCE; &#125;&#125; 这个写法也是利用类的静态变量的唯一性，不过和上面的写法相比，不能实现懒加载 Enum用枚举写单例最大的优点是简单 1234567891011121314151617public enum Singleton &#123; INSTANCE &#123; @Override protected void read() &#123; System.out.println("read"); &#125; @Override protected void write() &#123; System.out.println("write"); &#125; &#125;; protected abstract void read(); protected abstract void write();&#125; 以上是一个单例枚举的例子，要获取该实例只需要 Singleton.INSTANCE，并且此种方式可以保证该单例线程安全、防反射攻击、防止序列化生成新的实例 反编译后的类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public abstract class Singleton extends Enum &#123; private Singleton(String s, int i) &#123; super(s, i); &#125; protected abstract void read(); protected abstract void write(); public static Singleton[] values() &#123; Singleton asingleton[]; int i; Singleton asingleton1[]; System.arraycopy(asingleton = ENUM$VALUES, 0, asingleton1 = new Singleton[i = asingleton.length], 0, i); return asingleton1; &#125; public static Singleton valueOf(String s) &#123; return (Singleton)Enum.valueOf(singleton/Singleton, s); &#125; Singleton(String s, int i, Singleton singleton) &#123; this(s, i); &#125; public static final Singleton INSTANCE; private static final Singleton ENUM$VALUES[]; static &#123; INSTANCE = new Singleton("INSTANCE", 0) &#123; protected void read() &#123; System.out.println("read"); &#125; protected void write() &#123; System.out.println("write"); &#125; &#125;; ENUM$VALUES = (new Singleton[] &#123; INSTANCE &#125;); &#125;&#125; 看了这个类的真身后，可以知道： 枚举类实现其实省略了 private 类型的构造函数 枚举类的域其实是相应的 enum 类型的一个实例对象 枚举类的域会在 static 方法块中被实例化，也就是说在 enum 被类加载器加载时被实例化，并非懒加载 enum 是 abstract 类，所以没法实例化，反射也无能为力 关于线程安全的保证，其实是通过类加载机制来保证的，INSTANCE 是在 static 块中实例化的，JVM 加载类的过程显然是线程安全的 而枚举可以反序列化是因为 Enum 实现了 readResolve 方法 对于一个标准的 enum 单例模式，最优秀的写法还是实现接口的形式: 1234567891011121314151617// 定义单例模式中需要完成的代码逻辑public interface MySingleton &#123; void doSomething();&#125;public enum Singleton implements MySingleton &#123; INSTANCE &#123; @Override public void doSomething() &#123; System.out.println("complete singleton"); &#125; &#125;; public static MySingleton getInstance() &#123; return Singleton.INSTANCE; &#125;&#125; 破坏单例模式的三种方式 反射 序列化 克隆 当单例类被多个类加载器加载，如何还能保持单例如果单例由不同的类装载器装入，那便有可能存在多个单例类的实例。假定不是远端存取，例如一些 servlet 容器对每个 servlet 使用完全不同的类装载器，这样的话如果有两个 servlet 访问一个单例类，它们就都会有各自的实例 基于同样的原因，分布式系统和集群系统也都可能出现单例失效的情况 解决方法：用多个类加载器的父类来加载单例类 123456789private static Class getClass(String classname) throws ClassNotFoundException &#123; // 线程上下文类加载器，未设置的话默认是应用程序类加载器 ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); if (classLoader == null) classLoader = Singleton.class.getClassLoader(); return classLoader.loadClass(classname);&#125; 单例类防止反序列化123456public class Singleton implements java.io.Serializable &#123; // ... private Object readResolve() &#123; return INSTANCE; &#125;&#125; readResolve() 方法可以理解反序列化过程的出口，就是在反序列化完成得到对象前，把这个对象换成我们确定好的那个。 反射破坏单例原则在单例模式中，只对外提供工厂方法（获取单例），并私有化构造函数，来防止外面多余的创建。对于一般的外部调用来说，私有构造函数已经很安全了。但是一些特权用户可以通过反射来访问私有构造函数，然后打开访问权限 setAccessible(true)，就可以访问私有构造函数了，这样破坏了单例的私有构造函数保护，创建了一个新的实例。如果要防御这样的反射侵入，可以修改构造函数，加上第二次实例化的检查，当发生创建第二个单例的请求时会抛出异常。 1234567private static int cntInstance = 0; private Singleton() throws Exception &#123; if (cntInstance++ &gt; 1) &#123; throw new Exception("can't create another singleton instance."); &#125;&#125; 作者：杰哥长得帅链接：https://www.jianshu.com/p/ee110d76c42b来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 模板方式模式自己根据 AQS 重写了一个可重入，一个不可重入的显示锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164/** * 非重入 */class MutexDemo &#123;// private static Mutex mutex = new Mutex(); private static aa mutex = new aa(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 2 ; i++)&#123; Thread thread = new Thread(() -&gt; &#123; mutex.lock(); try &#123; System.out.println("Current Thread:" + Thread.currentThread().getName()); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; finally &#123; mutex.unlock(); &#125; &#125;); thread.start(); &#125; &#125;&#125;class Mutex implements Lock, java.io.Serializable &#123; // Our internal helper class private static class Sync extends AbstractQueuedSynchronizer &#123; // Reports whether in locked state protected boolean isHeldExclusively() &#123; return getState() == 1; &#125; protected boolean tryAcquire(int acquires) &#123; assert acquires == 1; // Otherwise unused if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; protected boolean tryRelease(int releases) &#123; assert releases == 1; // Otherwise unused if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; &#125; // Provides a Condition Condition newCondition() &#123; return new ConditionObject(); &#125; &#125; // The sync object does all the hard work. We just forward to it. private final Sync sync = new Sync(); public void lock() &#123; sync.acquire(1); &#125; public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; public void unlock() &#123; sync.release(1); &#125; public Condition newCondition() &#123; return sync.newCondition(); &#125; public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(timeout)); &#125;&#125;/** * 可重入 */class aa implements Lock,java.io.Serializable&#123; private static class Sync extends AbstractQueuedSynchronizer&#123; protected boolean isHeldExclusively()&#123; return getExclusiveOwnerThread() == Thread.currentThread(); &#125; protected boolean tryAcquire(int acquires)&#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int next = c + acquires; if (next &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(next); return true; &#125; return false; &#125; protected final boolean tryRelease(int releases) &#123; int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; free = true; setExclusiveOwnerThread(null); &#125; setState(c); return free; &#125; // Provides a Condition Condition newCondition() &#123; return new ConditionObject(); &#125; &#125; private final Sync sync = new Sync(); @Override public void lock() &#123; sync.acquire(1); &#125; @Override public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; @Override public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(time)); &#125; @Override public void unlock() &#123; sync.release(1); &#125; @Override public Condition newCondition() &#123; return sync.newCondition(); &#125;&#125; 代理设计模式动态代理为何动态代理只能代理接口其实我觉得完全可以代理类，如果你说 ”是因为Proxy$0已经继承了Proxy，java里面单继承所以导致只能代理接口“，那我就得否认你了，因为其实完全可以做到直接让 ”Proxy去代理“，所以，我觉得人家这么设计的原因只有一个： 看了这么多分析的，我觉得这样设计的初衷就是为了套用代理模式模板和基于接口编程规范的 静态代理中就是实现接口，再在代理类set进实现类的引用，调用真正代理的方法 就算他不继承Proxy，也不会继承被代理的实现类 ，而是必须有一个代理类的接口去实现它，这是设计的初衷 作者：Forward链接：https://www.zhihu.com/question/62201338/answer/904553916来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 动态代理的过程是什么代理的过程在 Proxy.newProxyInstance() 方法中，程序先是进行了验证、优化、缓存、同步、生成字节码和显式类加载等操作，最后调用的 sum.misc.ProxyGenerator.generateProxyClass() 来完成生成字节码的动作，这个方法可以在运行时产生一个描述代理类的字节码数组byte[]。 代理类的实现代码也很简单，它为每一个传入接口中的每一个方法都进行了相应的实现，调用的方法都是在执行 InvocationHandler.invoke() 中的代理逻辑。 动态代理的好处是什么实现了可以在原始类和接口还未知的情况下，就确定代理类的代理行为，当代理类与原始类脱离直接联系后，就可以灵活的重用于不同的应用场景中了。 什么叫静态代理静态代理，在编译期间就已经确定了要代理的类和接口，是写死了的。 123456789101112131415161718192021nterface Person&#123; public void play();&#125;class 静态代理&#123; public static void main(String[] args) &#123; Person person = new StaticProxy(); person.play(); &#125;&#125;class StaticProxy implements Person&#123; @Override public void play() &#123; Student student = new Student("jerome",10); student.play(); System.out.println("静态代理"); &#125;&#125; Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package 设计模式;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class 代理 &#123; public static void main(String[] args) &#123; Person person = new Student("jerome",18); InvocationHandler dynamic = new Dynamic(person); Person person_proxy = (Person) Proxy.newProxyInstance(person.getClass().getClassLoader(),person.getClass().getInterfaces(),dynamic); person_proxy.play();// System.getProperties().put("sun.misc.ProxyGenerator.saveGeneratedFiles","true"); &#125;&#125;class Dynamic implements InvocationHandler&#123; // 要代理的对象得传进来 private Object object; public Dynamic(Object object)&#123; this.object = object; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //do something System.out.println("let's begin!"); Object result = method.invoke(object,args); System.out.println("It's over!"); return result; &#125;&#125;class Student implements Person &#123; private String name; private int age; public Student(String name,int age)&#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public void play()&#123; System.out.println("I just want to play with " + this.name + ", and her age is " + this.age); &#125;&#125;interface Person&#123; public void play();&#125; 相关的字节码生成技术—-cglib相关的 Aop &amp; IOC]]></content>
      <tags>
        <tag>单例模式</tag>
        <tag>模板方法模式</tag>
        <tag>代理模式</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 二刷复习]]></title>
    <url>%2F2020%2F02%2F10%2Fleetcode%26%E5%89%91%E6%8C%87%20%E4%BA%8C%E5%88%B7(%E6%8C%89tag).html</url>
    <content type="text"><![CDATA[dp背包九讲问题 01 背包问题 完全背包问题 多重背包问题 混合背包问题 二维费用的背包问题 分组背包问题 背包问题求方案数 背包问题求方案路径 有依赖的背包问题 01背包问题背景：有 n 件物品，每一件物品 i 对应一个体积 v[i] 和一个价值 w[i]，设有一个背包，体积为 V，在这个背包下能装下的价值的最大值是多少？ 模板 最原始方法： dp[i][j]定义：表示前 i 个物品，总体积是 j 的情况下，总价值最大是什么？ 状态转移方程：dp[i][j] = Math.max(dp[i-1][j], dp[i-1][j -v[i]] + w[i]) Max = dp[n][V] 优化 dp[i][j] 只跟 上一层的外循环 有关，所以根本没有必要使用二维数组，只需要使用一维数组即可，注意，使用一维数组的时候，内层循环需要从后向前遍历； 将 dp[0][j] 全部初始化为 0 ，则 dp[i][j] 就代表前 i 个物品，总体积小于等于 j 的最大总价值，故上文的Max = dp[V]，代表总体积小于等于 V 的最大总价值； 如果要求的总价值的前提是背包容量恰好是 V，则 只需要将初始化的值改为 dp[0][0] = 0 ，dp[0][j] = - INF 即可。 最终模板 1234567int[] dp = new int[V+1];for(int i = 1;i &lt;= n;i++)&#123; for(int j = V;j &gt;= v[i];j--)&#123; dp[j] = Math.max(dp[j],dp[j - v[i]] + w[i]); &#125;&#125;return dp[V]; ​ 例题 leetcode第416题：分割等和子集 leetcode第494题 目标和 416 题代码 12345678910111213141516171819public class Solution &#123; public boolean canPartition(int[] nums) &#123; int sum = 0; for(int i = 0;i &lt; nums.length;i++)&#123; sum = sum + nums[i]; &#125; if(sum % 2 != 0 ) return false; int target = sum / 2; boolean[] dp = new boolean[target+1]; dp[0] = true; for(int i = 1;i &lt;= nums.length;i++)&#123; for(int j = target;j &gt;= nums[i-1];j--)&#123; if(dp[target]) return true; dp[j] = dp[j] || dp[j-nums[i-1]]; &#125; &#125; return false; &#125;&#125; 494 题代码 12345678910111213141516171819202122232425public class Solution &#123; public int findTargetSumWays(int[] nums, int S) &#123; // 正数集合 P，负数集合 T // P - T = target， P + T = sum // P = (sum + target) / 2 // 即题目就是找 子集和的方法数 // 典型的01背包问题求解方案数，只需要把初始化的值改动一下，还要把状态转移方程中的 max ---&gt; sum int count = 0; for(int i = 0;i &lt; nums.length;i++)&#123; count = count + nums[i]; &#125; if(count &lt; S) return 0; if((S + count) % 2 != 0) return 0; int P = (S + count)/2; int[] dp = new int[P+1]; dp[0] = 1; // 这里是第一个改动，因为例如二维数组中的dp[1][1] = dp[0][0] + dp[0][1],dp[0][1] = 0 //此时要使得dp[1][1] = 1,则dp[0][0] = 1,这也符合常理，即0个数能找到和为0的方案为一种 for(int i = 1;i &lt;= nums.length;i++)&#123; for(int j = P;j &gt;= nums[i-1];j--)&#123; dp[j] = dp[j] + dp[j - nums[i-1]]; &#125; &#125; return dp[P]; &#125;&#125; 完全背包相比 01 背包问题来说，这里的一件物品可以选不止一次 所以，这里的做法跟 01 背包只有一个不同，01背包的一维数组的方法中的内层循环是从后向前遍历，原因是其在循环第 i 个物品时要用到第 i-1个物品的对应的值，且要用到的值都没有被本次循环更新过，而这里恰恰相反，因为一次物品可以用多次，所以完全背包问题允许在循环第 i 个物品时提前更新要用到的值，具体˙证明可以使用 数学归纳法。 模板f[j] 表示 总体积是 j 的情况下，最大价值是什么 12345678int[] dp = new int[V+1];// 注意内层循环是从前往后for(int i = 0;i &lt;= n;i++)&#123; for(int j = v[i];j &lt;= V;j--)&#123; dp[j] = Math.max(dp[j],dp[j - v[i]] + w[i]); &#125;&#125;return dp[V]; 多重背包问题 I相比 完全背包问题 相比，就是每个物品多了一个数量的限制，类似于01背包，只不过那里的物品的数量为1，这里的数量是 s。 模板123456789101112// 初始化，f[i] = 0,i =0...nint[] dp = new int[V+1];for(int i = 1;i &lt;= n;i++)&#123; for(int j = V;j &gt;= 0;j--)&#123; // 和01背包唯一的区别 for(int k = 1;k &lt;= s &amp;&amp; j &gt;= k * v[i];k++)&#123; dp[j] = Math.max(dp[j],dp[j - v[i]*k] + w[i]*k); &#125; &#125;&#125;return dp[V]; 多重背包问题 II其实就是对上面这个模板的优化，是对多重背包的二进制优化方法 。最内层循环的意思其实就是把背包的每一次重复都当成一次新的物品，类似于打包的形式，比如第一个物品体积是 v，价值为 w，第二个物品就是 2v，价值为 2w，以此类推，但是这样分解的时间复杂度太高了，可以通过二进制优化方法，为什么呢？因为他们之间是互斥的，每次都会只选出一个来，是后面的分组背包的特殊情况，所以我们只需要找到能表示所有方案的物品就行，无需全部罗列都当成01背包的物品。比如现在的 s 为7，按照上面的方法，我们需要比较0、1、2、3、4、5、6、7这8个方案的最大值，但是其实我们可以用三位数就表示0、1、2、3、4、5、6、7，因为我们只需要从这8个方案中选取一个即可，所以三位数就可以表示所有，然后从中选取自己符合要求的就行，在这里我们可以选择1、2、4，就能表示 0 ~ 7 所有元素，如果 s 为 10，那么我们可以选取 1、2、4、3 ，3是如何来的呢，是 10 - （2^ 3 -1） 得来的，故就将 线性复杂度 变为 log2 复杂度了。 模板123456789101112int[] dp = new int[V+1];for(int i = 1;i &lt;= n;i++)&#123; for(int j = V;j &gt;= 0;j--)&#123; for(int k = 1;k &lt;= s;k = k * 2)&#123; s = s - k; // 把每个 k 对应的都变成 一种物品 if(j &gt;= k * v[i]) dp[j] = Math.max(dp[j],dp[j - k * v[i]] + k * w[i]); &#125; //不等于0，说明有余数 if(s != 0 &amp;&amp; j &gt;= s * v[i]) dp[j] = Math.max(dp[j],dp[j - s * v[i]] + s * w[i]); &#125;&#125; 多重背包问题 III这是对多重背包的再次优化，上文已经将最内层循环从 O(n) 降为了 O(logn)，这里可以运用 单调队列 将最内层循环的 O(n) 降成 O(1)。 完全背包问题转移图和多重背包一样。不过没有了物品个数限制，所以不是滑动窗口情况，不用单调队列。只保存一个最大值就可以。所以完全背包问题可以直接从前向后遍历，因为其只需要保存一个最大值。所以这里引入了单调队列之后，也维护了相同余数的最大值，故 j 也可以从前向后遍历。 思路：https://blog.csdn.net/qq_40679299/article/details/81978770 为何引入单调队列分析：https://www.cnblogs.com/DeepJay/p/12025225.html 代码和思路精简版：https://www.acwing.com/solution/acwing/content/6500/（主推这个） 模板1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.Arrays;import java.util.LinkedList;import java.util.Scanner;public class Main &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); // 表示物品种类 int n = scan.nextInt(); // 表示 最大的容积 int V = scan.nextInt(); int[] dp = new int[V + 1]; for (int i = 0; i &lt; n; i++) &#123; int v = scan.nextInt(); int w = scan.nextInt(); int s = scan.nextInt(); int[] pre = Arrays.copyOf(dp,dp.length); // 遍历所有余数 for (int j = 0; j &lt; v; j++) &#123; // 每个余数对应一个双端队列，实现单调队列 LinkedList&lt;Integer&gt; queue = new LinkedList(); for (int k = j; k &lt;= V; k = k + v) &#123; // 1. 去除队尾不符合要求的，保证从大到小，如果前面数小则需要依次弹出 while (!queue.isEmpty() &amp;&amp; pre[queue.peekLast()] - (queue.peekLast() - j) / v * w &lt;= pre[k] - (k - j) / v * w) &#123; queue.pollLast(); &#125; // 2.判断当前队列中队首的值是否有效 if (!queue.isEmpty() &amp;&amp; queue.peek() &lt; k - s * v) &#123; queue.poll(); &#125; // 3.添加当前值对应的数组下标 queue.addLast(k); // 4.拿到队首最大值 dp[k] = Math.max(dp[k], pre[queue.peek()] + (k - queue.peek()) / v * w); &#125; &#125; &#125; System.out.println(dp[V]); &#125;&#125; 例题LeetCode 第 239 题：滑动窗口最大值 该题也是同样采用了单调队列的方式，官方题解的解法二 就是采用双端队列实现单调队列。 1234567891011121314151617181920212223242526272829class Solution &#123; public int[] maxSlidingWindow(int[] nums, int k) &#123; if(nums == null || nums.length &lt; 2) return nums; // 双向队列 保存当前窗口最大值的数组位置 保证队列中数组位置的数值按从大到小排序 LinkedList&lt;Integer&gt; queue = new LinkedList(); // 结果数组 int[] result = new int[nums.length-k+1]; // 遍历nums数组 for(int i = 0;i &lt; nums.length;i++)&#123; // 保证从大到小 如果前面数小则需要依次弹出，直至满足要求 while(!queue.isEmpty() &amp;&amp; nums[queue.peekLast()] &lt;= nums[i])&#123; queue.pollLast(); &#125; // 11 10 9 8 7 6 5 // 判断当前队列中队首的值是否有效 if(!queue.isEmpty() &amp;&amp; queue.peek() &lt;= i-k)&#123; queue.poll(); &#125; // 添加当前值对应的数组下标 queue.addLast(i); // 当窗口长度第一次达到k后，开始保存当前窗口中最大值 // 以后每走一步，就会保存一个最大值 if(i+1 &gt;= k)&#123; result[i+1-k] = nums[queue.peek()]; &#125; &#125; return result; &#125;&#125; 故总结一下，单调队列的模板为： 1234// 1.存入双端队列的是值的索引，第一步是先把队尾不符合要求的全部弹栈// 2.然后判断队首是不是有不符合要求的，即不在窗口之中// 3.将该值加入到队列中// 4.获取到题目需要的东西，比如上文要取每个窗口的最大值，则将每个窗口的最大值存入数组即可 混合背包问题混合背包，就是01背包，完全背包，多重背包结合在一起，方法就是分类判断，分类处理即可，非常简单，就不做过多解释了。 二维费用的背包问题其实跟一维费用的背包问题如出一辙，就是多了一层循环，dp 由一维变成了二维，其他的跟一维的都是一样的。 分组背包问题其实，分组背包前面已经讲过了，就是多重背包的一般化，就是因为多重背包的特殊，所以才会有两个优化的方法——-二进制优化 &amp;&amp; 单调队列优化，所以对于分组背包问题，就只能采用 多重背包问题 I 的方法，三层循环。 背包问题求方案数求方案数的关键就在于确定：在每一次循环中，是不是选了该数。求最优方案的方案数是通过判断该体积对应的价值的最大值有没有改变，从而判断是否选了该数，选了的话就把以前的方案数替换或者累加即可；求固定体积的方案数，就是直接dp[j] = dp[j] + dp[j - v[i]]，体积为 j 的就是等于选了和没选的总方案数（这个初始化是关键）。 求最优方案的方案数这里只有一个比较大的改动，那就是添加了一个 cnt 数组来统计方案数。 这里初始化的时候，有两个方案，一个是，dp[0] = 0，dp[i] = 0; 另外一个是 dp[0] = 0，dp[i] = -INF。 方法一和方法二区别不大，跟 01背包一样，由于初始化的不同，导致最后的结果，一个直接取容量最大值即可，一个需要遍历，这里还是推荐方案一！ 方法一 输入 1234567总物品数：4 最大容积：5物品体积 价值1 22 43 44 6 若初始化的值都为 0 0 1 2 3 4 5 0 0 0 0 0 0 0 1 0 2 2 2 2 2 2 0 2 2 6 6 6 3 0 2 2 6 6 8 4 0 2 2 6 6 8 模板 定义两个数组，dp[j] 用来存储背包容积是 j 时的最佳方案的总价值，cnt[j] 用来存储容积为 j 时总价值为最佳的方案数； 先初始化 dp[0] = 0，dp[j] = -INF，很容易理解，容积为 0，自然价值就为 0 了，而要使得容积为 1、2…且没有数，则自然价值就不存在了，记为 -INF，再初始化所有的 cnt[j] = 1，因为背包里什么都不装对任何容积来说都是一种方案，在没有数的情况下自然就是最好的情况了； 在每次循环新物品时，先求得装新物品的总价值，然后与不装新物品对比，如果装了新物品的价值更大，那么就需要用 dp[j - v[i]] + w 更新 dp[j]，同时用 cnt[j - v[i]] 来更新 cnt[j]； 如果总价值相等，那么 cnt[j] = cnt[j] + cnt[j-v[i]]； 当然如果装了新物品还不如不装呢，那就啥也不用做了。 1234567891011121314151617181920212223242526272829303132import java.util.Scanner;public class Test &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); // 表示物品种类 int n = scan.nextInt(); // 表示 最大的容积 int V = scan.nextInt(); int[] dp = new int[V + 1]; int[] cnt = new int[V + 1]; for (int j = 0; j &lt;= V; j++) &#123; cnt[j] = 1; &#125; for (int i = 0; i &lt; n; i++) &#123; int v = scan.nextInt(); int w = scan.nextInt(); for (int j = V; j &gt;= v; j--) &#123; int value = dp[j - v] + w; if (value &gt; dp[j]) &#123; dp[j] = value; cnt[j] = cnt[j - v]; &#125; else if (value == dp[j]) &#123; cnt[j] = cnt[j] + cnt[j - v]; &#125; &#125; &#125; System.out.println(cnt[V]); &#125;&#125; 回看表格(括号内为 cnt 的值) 1234567总物品数：4 最大容积：5物品体积 价值1 22 43 44 6 0 1 2 3 4 5 0 0(1) 0(1) 0(1) 0(1) 0(1) 0(1) 1 0(1) 2(1) 2(1) 2(1) 2(1) 2(1) 2 0(1) 2(1) 4(1) 6(1) 6(1) 6(1) 3 0(1) 2(1) 4(1) 6(1) 6(2) 8(1) 4 0(1) 2(1) 4(1) 6(1) 6(3) 8(2) 再来一个示例 1234567总物品数：4 最大容积：5物品体积 价值2 42 43 44 6 0 1 2 3 4 5 0 0(1) 0(1) 0(1) 0(1) 0(1) 0(1) 1 0(1) 0(1) 4(1) 4(1) 4(1) 4(1) 2 0(1) 0(1) 4(2) 4(2) 8(1) 8(1) 3 0(1) 0(1) 4(2) 4(3) 8(1) 8(3) 4 0(1) 0(1) 4(2) 4(3) 8(1) 8(3) 套用上述模板，实现 LeetCode 494题 1234567891011121314151617181920212223242526272829public class Solution &#123; public int findTargetSumWays(int[] nums, int S) &#123; int count = 0; for(int i = 0;i &lt; nums.length;i++)&#123; count = count + nums[i]; &#125; if(count &lt; S) return 0; if((S + count) % 2 != 0) return 0; int V = (S + count)/2; int[] dp = new int[V + 1]; int[] cnt = new int[V + 1]; for (int j = 0; j &lt;= V; j++) &#123; cnt[j] = 1; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int v = nums[i]; for (int j = V; j &gt;= v; j--) &#123; int value = dp[j - v] + v; if (value &gt; dp[j]) &#123; dp[j] = value; cnt[j] = cnt[j - v]; &#125; else if (value == dp[j]) &#123; cnt[j] = cnt[j] + cnt[j - v]; &#125; &#125; &#125; return cnt[V]; &#125;&#125; 方法二 若初始化的值不都为 0,用上面的示例 1234567总物品数：4 最大容积：5物品体积 价值2 42 43 44 6 0 1 2 3 4 5 0 0(1) -INF(1) -INF(1) -INF(1) -INF(1) -INF(1) 1 0(1) -INF(1) 4(1) -INF(1) -INF(1) -INF(1) 2 0(1) -INF(1) 4(2) -INF(2) 8(1) -INF(1) 3 0(1) -INF(1) 4(2) 4(1) 8(1) 8(2) 4 0(1) -INF(1) 4(2) 4(1) 8(1) 8(2) 模板 1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.Scanner;public class Test &#123; public static void main(String[] args) &#123; Scanner scan = new Scanner(System.in); int ma = (int) (Math.pow(10,9) + 7); // 表示物品种类 int n = scan.nextInt(); // 表示 最大的容积 int V = scan.nextInt(); int[] dp = new int[V + 1]; int[] cnt = new int[V + 1]; for (int j = 0; j &lt;= V; j++) &#123; cnt[j] = 1; dp[j] = Integer.MIN_VALUE; &#125; dp[0] = 0; for (int i = 0; i &lt; n; i++) &#123; int v = scan.nextInt(); int w = scan.nextInt(); for (int j = V; j &gt;= v; j--) &#123; int value = dp[j - v] + w; if (value &gt; dp[j]) &#123; dp[j] = value; cnt[j] = cnt[j - v]; &#125; else if (value == dp[j]) &#123; cnt[j] = cnt[j] + cnt[j - v]; &#125; &#125; &#125; int max = Integer.MIN_VALUE; int count = 0; for(int i = 0;i &lt;= V;i++)&#123; max = Math.max(dp[i],max); &#125; for(int i = 0;i &lt;= V;i++)&#123; if(dp[i] == max)&#123; count+=cnt[i]; &#125; &#125; System.out.println(count); &#125;&#125; 思路一模一样…就是头和尾复杂了一下，所以感觉没啥子必要… 求背包装至指定容量的方案数这个非常简单了，就是上面的特殊化例子，也就是把 循环到的该数的容量 作为价值，然后计算指定价值的方案数，这是一般化的做法，但是既然谈到特殊化，也就是容量和价值相等，那自然可以采用更简单的方法去处理了。 初始化 dp[0] = 1，代表 0 容量的背包的方案为1，其他都为0； dp[i][j] = dp[i-1][j] + dp[i-1][j-v[i]]，很好理解了，就不做解释了。 示例就是 LeetCode 494 题。 背包问题求方案路径如果没有要求得到 按字典顺序最小的 方案路径的话，就可以直接按01背包的方法去做(由于要记录方案路径，所以不能使用滚动数组，需要用到每一轮的方案，所以需要改为二维数组)，然后在遍历的时候，判断 f[i][j]是否和 f[i-1][j-v[i]] + w[i] 相等，如果相等说明用到了该个物品，加入方案路径中即可。 如果要求是得到最小的 方案路径的话，则需要将方案从末尾遍历到开始，得到最佳方案后，再从前向后寻找方案路径。 模板123456789101112131415int[][] dp = new int[n+1][V+1];for(int i = n;i &gt;= 1;i--)&#123; // 二维的话内层循环就无所谓是从前往后还是从后往前了 for(int j = V;j &gt;= v[i];j--)&#123; dp[i][j] = Math.max(dp[i+1][j],dp[i+1][j - v[i]] + w[i]); &#125;&#125;int vol = m;for(int i = 1;i &lt;= n;i++)&#123; if(f[i][vol] == f[i+1][vol-v[i]] + w[i])&#123; // 将该方案加入 System.out.println(i); vol = vol - v[i]; &#125;&#125; 有依赖的背包问题（ 我暂时没复习！）这应该是背包问题里面最难的一类了，是用 树形 dp + 分组背包问题 的思路去解决。 https://www.acwing.com/solution/acwing/content/8316/ 更多见视频：https://www.bilibili.com/video/av33930433/?p=2、https://www.bilibili.com/video/av34467850/?p=2（这个视频是从第 12 分钟开始讲背包 9 讲后面的 3 讲） 训练地址：https://www.acwing.com/problem/content/2/ 更详细的背包九讲问题：已经下载到本地了，崔添翼的 pdf——&gt; 本地文件名字叫 背包问题九讲 二叉树链表 热题 100： 2. 两数相加 19. 删除链表的倒数第N个节点 21. 合并两个有序链表 23. 合并 K 个排序链表 141. 环形链表 142. 环形链表 II 148. 排序链表 160. 相交链表 206. 反转链表 234. 回文链表 剑指 offer： 18. 删除链表的节点 22. 删除倒数第 k 个节点 24. 反转链表 35. 复杂链表的复制 52. 两个链表的第一个公共节点 相关题目（热题100） 两数相加1234567891011121314151617181920212223class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; // 首先比较位数，两个同时向后遍历，当 cur.next == null 时说明有一个已经到了最高位 // 按照逆序的方式存储，其实是将题目变简单了。因为我们在计算两数相加时，是先算低位，再进位给高位 // 逆序的话刚好就符合这种运算规则 // 头结点 ListNode res = new ListNode(-1); ListNode cur = res; // 定义一个进位 int quo = 0; while(l1 != null || l2 != null || quo != 0)&#123; int t = (l1 == null ? 0 : l1.val) + (l2 == null ? 0 : l2.val) + quo; quo = t/10; t = t % 10; cur.next = new ListNode(t); cur = cur.next; l1 = l1 == null ? l1 : l1.next; l2 = l2 == null ? l2 : l2.next; &#125; return res.next; &#125;&#125; 拓展，链表正序存储数字，如何计算？ 双端队列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class addTwoNumbers &#123; public static void main(String[] args) &#123; addTwoNumbers addTwoNumbers = new addTwoNumbers(); ListNode list1 = new ListNode(9);// list1.next = new ListNode(4);// list1.next.next = new ListNode(3); ListNode list2 = new ListNode(5); list2.next = new ListNode(6); list2.next.next = new ListNode(4); // 9 + 564 = 573 // 243 + 564 = 807 ListNode listNode = addTwoNumbers.addTwoNumbers(list1,list2); System.out.println(listNode.val + "-----&gt;" + listNode.next.val + "------&gt;" + listNode.next.next.val); &#125; // 方法一：用双端队列实现 public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; // 想法是使用两个双端队列，先将数都从队尾进队，当某个数 位数不够时，用0加到队首 // 计算是都从队尾出 // 出队后再用栈接收一下 LinkedList&lt;Integer&gt; deque_l1 = new LinkedList&lt;&gt;(); LinkedList&lt;Integer&gt; deque_l2 = new LinkedList&lt;&gt;(); while(l1 != null)&#123; deque_l1.push(l1.val); l1 = l1.next; &#125; while (l2 != null)&#123; deque_l2.push(l2.val); l2 = l2.next; &#125; int deq1_len = deque_l1.size(); int deq2_len = deque_l2.size(); if(deq1_len &gt; deq2_len)&#123; for(int i = 1;i &lt;= deq1_len - deq2_len;i++)&#123; deque_l2.addLast(0); &#125; &#125; else if(deq2_len &gt; deq1_len)&#123; for(int i = 1;i &lt;= deq2_len - deq1_len;i++)&#123; deque_l1.addLast(0); &#125; &#125; Stack&lt;Integer&gt; stack = new Stack(); int quo = 0; while (deque_l1.size() != 0)&#123; int sum = deque_l1.pop() + deque_l2.pop() + quo; stack.push(sum % 10); quo = sum/10; &#125; ListNode res = new ListNode(-1); ListNode cur = res; while(!stack.isEmpty())&#123; cur.next = new ListNode(stack.pop()); cur = cur.next; &#125; return res.next; &#125;&#125; 将链表反转，然后计算，最后再反转回来 12345678910111213141516171819202122232425262728293031323334353637public ListNode addTwoNumbers2(ListNode l1, ListNode l2)&#123; ListNode reverse_l1 = reverseList(l1); ListNode reverse_l2 = reverseList(l2); ListNode tmp = addTwoNumbers(reverse_l1,reverse_l2); ListNode listNode = reverseList(tmp); return listNode; &#125; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; // 头结点 ListNode res = new ListNode(-1); ListNode cur = res; // 定义一个进位 int quo = 0; while(l1 != null || l2 != null || quo != 0)&#123; int t = (l1 == null ? 0 : l1.val) + (l2 == null ? 0 : l2.val) + quo; quo = t/10; t = t % 10; cur.next = new ListNode(t); cur = cur.next; l1 = l1 == null ? l1 : l1.next; l2 = l2 == null ? l2 : l2.next; &#125; return res.next; &#125; public ListNode reverseList(ListNode head) &#123; // 如果当前要反转的节点为 null 或者反转链表为 null // head.next 为 null，即反转链表的尾结点不存在，即反转链表不存在 if (head == null || head.next == null) return head; // 节点 p 其实就是反转链表的头节点 ListNode p = reverseList(head.next); // 我们将反转链表的尾结点（head.next）的 next 指向当前即将反转的节点 head.next.next = head; // 然后让当前节点变成反转链表的尾结点 head.next = null; // 返回反转链表的头结点 return p; &#125; 删除链表的倒数第 N 个节点利用先后指针，常用的双指针技巧之一：快慢指针、先后指针、滑动窗口。 1234567891011121314151617181920class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; if(head == null) return null; //可能会把头结点给删了，所以可以建个哑巴节点 ListNode dummy = new ListNode(-1); dummy.next = head; ListNode front = dummy; ListNode back = dummy; for(int i = 1;i &lt;= n;i++)&#123; front = front.next; &#125; while(front.next != null)&#123; front = front.next; back = back.next; &#125; back.next = back.next.next; return dummy.next; &#125;&#125; 合并两个有序链表这个貌似没什么难度，稍微注意一点细节就行了 迭代 123456789101112131415161718192021class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; ListNode res = new ListNode(-1); ListNode dummy = res; while(l1 != null &amp;&amp; l2 != null)&#123; if(l1.val &gt; l2.val)&#123; res.next = new ListNode(l2.val); l2 = l2.next; &#125; else&#123; res.next = new ListNode(l1.val); l1 = l1.next; &#125; res = res.next; &#125; if(l1 == null) res.next = l2; else res.next = l1; return dummy.next; &#125;&#125; 递归 12345678910111213141516class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) &#123; return l2; &#125; if (l2 == null) &#123; return l1; &#125; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125;&#125; 合并 K 个排序链表 优先级队列 1234567891011121314151617181920class Solution &#123; // 方法一：直接使用优先级队列 public ListNode mergeKLists(ListNode[] lists) &#123; PriorityQueue&lt;ListNode&gt; priorityQueue = new PriorityQueue((Comparator&lt;ListNode&gt;) (o1, o2) -&gt; o1.val - o2.val); ListNode dummy = new ListNode(-1); ListNode cur = dummy; if(lists.length == 0) return null; for(ListNode list : lists)&#123; if(list == null) continue; priorityQueue.add(list); &#125; while(!priorityQueue.isEmpty())&#123; ListNode min = priorityQueue.poll(); cur.next = min; if(min.next != null) priorityQueue.add(min.next); cur = cur.next; &#125; return dummy.next; &#125;&#125; 归并 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; /** * 方法二 归并排序 * 要注意跟数组的区别 * 数组可以直接通过下标索引，所以在归并时无需返回值，但链表必须在每次归并时提供链表头结点，否则无法归并 * @param lists * @return */ public ListNode mergeKLists(ListNode[] lists) &#123; if(lists.length == 0) return null; return merge(lists,0,lists.length-1); &#125; private ListNode merge(ListNode[] lists, int left, int right) &#123; if(left == right) return lists[left]; int mid = left + (right - left)/2; ListNode l1 = merge(lists,left,mid); ListNode l2 = merge(lists,mid+1,right); return mergeTwoLists(l1,l2); &#125; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) &#123; return l2; &#125; if (l2 == null) &#123; return l1; &#125; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125;&#125; 暴力法 123456789101112131415161718192021222324class Solution &#123; public ListNode mergeKLists(ListNode[] lists) &#123; if(lists.length == 0) return null; for(int i = 1;i &lt; lists.length;i++)&#123; lists[i] = mergeTwoLists(lists[i],lists[i-1]); &#125; return lists[lists.length-1]; &#125; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) &#123; return l2; &#125; if (l2 == null) &#123; return l1; &#125; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125;&#125; 环形链表 Set集合存储 123456789101112131415public class Solution &#123; //用 Set 来存储，如果出现了，则说明有环，否则就无环 public boolean hasCycle(ListNode head) &#123; HashSet&lt;ListNode&gt; set = new HashSet&lt;&gt;(); while(head != null)&#123; if(set.add(head))&#123; head = head.next; &#125; else &#123; return true; &#125; &#125; return false; &#125;&#125; 快慢指针 1234567891011121314public class Solution &#123; //用 快慢指针 public boolean hasCycle(ListNode head) &#123; if (head == null || head.next == null) return false; ListNode slow = head; ListNode fast = head.next; while (fast != null &amp;&amp; fast.next != null) &#123; if (slow == fast) return true; slow = slow.next; fast = fast.next.next; &#125; return false; &#125;&#125; 环形链表 II典型的快慢指针解决此问题 起点是哑巴节点 12345678910111213141516171819202122232425public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; if(head == null || head.next == null) return null; // 起点很重要，这个跟环形链表 I 不一样 // 一定要注意这个细节 // 因为第二次相遇得 fast 和 slow 走的距离一样 ListNode slow = head; ListNode fast = head.next; while (true) &#123; if(fast == null || fast.next == null) return null; if (slow == fast) &#123; break; &#125;; slow = slow.next; fast = fast.next.next; &#125; slow = head; fast = fast.next; while(slow != fast)&#123; slow = slow.next; fast = fast.next; &#125; return slow; &#125;&#125; 起点是头结点 123456789101112131415161718192021222324public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; if(head == null) return null; // 起点很重要，这个跟环形链表 I 不一样 // 一定要注意这个细节 // 因为第二次相遇得 fast 和 slow 走的距离一样 ListNode slow = head; ListNode fast = head; while (true) &#123; if(fast == null || fast.next == null) return null; slow = slow.next; fast = fast.next.next; // 注意这里判断的地方跟上面略有不同 if (slow == fast) &#123; break; &#125;; &#125; slow = head; while(slow != fast)&#123; slow = slow.next; fast = fast.next; &#125; return slow; &#125; 排序链表【难点哈！】 不符合空间复杂度，直接利用列表 12345678910111213141516171819class Solution &#123; public ListNode sortList(ListNode head) &#123; List&lt;Integer&gt; arrayList = new ArrayList(); while(head != null)&#123; arrayList.add(head.val); head = head.next; &#125; Collections.sort(arrayList); return ListToLinked(arrayList); &#125; public ListNode ListToLinked(List&lt;Integer&gt; arrayList)&#123; if(arrayList.size() == 0) return null; ListNode node = new ListNode(arrayList.get(0)); // 注意这个 arrayList.subList()的返回值是 List // 并且方法中的参数是[fromIndex,toIndex)，左闭右开。 node.next = ListToLinked(arrayList.subList(1,arrayList.size())); return node; &#125;&#125; 归并排序，并且在合并时采用递归，空间复杂度降为 O(logn)，这个就跟合并 K 个有序链表是一样的思路，只不过这里就相当于 K 个 链表长度为 1 的链表进行归并，并且还不能通过数组索引到 mid，只能是通过快慢指针找到 mid。 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) return head; ListNode fast = head.next, slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; ListNode tmp = slow.next; slow.next = null; ListNode left = sortList(head); ListNode right = sortList(tmp); ListNode h = new ListNode(0); ListNode res = h; while (left != null &amp;&amp; right != null) &#123; if (left.val &lt; right.val) &#123; h.next = left; left = left.next; &#125; else &#123; h.next = right; right = right.next; &#125; h = h.next; &#125; h.next = left != null ? left : right; return res.next; &#125;&#125; 归并排序，合并时不采用递归，空间复杂度为常数 还未研读！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution &#123; public ListNode sortList(ListNode head) &#123; ListNode h, h1, h2, pre, res; h = head; int length = 0, intv = 1; while (h != null) &#123; h = h.next; length++; &#125; res = new ListNode(0); res.next = head; while (intv &lt; length) &#123; pre = res; h = res.next; while (h != null) &#123; int i = intv; h1 = h; while (i &gt; 0 &amp;&amp; h != null) &#123; h = h.next; i--; &#125; if (i &gt; 0) break; i = intv; h2 = h; while (i &gt; 0 &amp;&amp; h != null) &#123; h = h.next; i--; &#125; int c1 = intv, c2 = intv - i; while (c1 &gt; 0 &amp;&amp; c2 &gt; 0) &#123; if (h1.val &lt; h2.val) &#123; pre.next = h1; h1 = h1.next; c1--; &#125; else &#123; pre.next = h2; h2 = h2.next; c2--; &#125; pre = pre.next; &#125; pre.next = c1 == 0 ? h2 : h1; while (c1 &gt; 0 || c2 &gt; 0) &#123; pre = pre.next; c1--; c2--; &#125; pre.next = h; &#125; intv *= 2; &#125; return res.next; &#125;&#125; 相交链表走相同距离的路，必定在相交点相遇。 123456789101112public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if(headA == null || headB == null) return null; ListNode p1 = headA; ListNode p2 = headB; while(p1 != p2)&#123; p1 = (p1 == null ? headB : p1.next); p2 = (p2 == null ? headA : p2.next); &#125; return p1; &#125;&#125; 反转链表 递归(这 5 步得牢记于心) 123456789class Solution &#123; public ListNode reverseList(ListNode head) &#123; if(head == null || head.next == null) return head; ListNode p = reverseList(head.next); head.next.next = head; head.next = null; return p; &#125;&#125; 非递归(每一步就只是把下一次该反转的节点记住即可，然后把curr.next换成pre就行了) 12345678910111213141516class Solution &#123; public ListNode reverseList(ListNode head) &#123; // 利用三个指针，分别存储遍历的该节点，前节点和后节点 ListNode pre = null; ListNode cur = head; while(cur != null)&#123; // 只需要保存下一次反转的节点就行了 ListNode nextTemp = cur.next; cur.next = pre; // 下一次反转 pre = cur; cur = nextTemp; &#125; return pre; &#125;&#125; 回文链表 先利用快慢指针找到中点，然后反转前半部分，然后比对即可 123456789101112131415161718192021222324252627282930313233class Solution &#123; // 反转前半部分 public boolean isPalindrome(ListNode head) &#123; ListNode slow = head; ListNode fast = head; while(fast != null &amp;&amp; fast.next != null)&#123; slow = slow.next; fast = fast.next.next; &#125; // 1 2 3 4 此时 fast 会为 null ，slow 会为 3 // 1 2 3 4 5 此时 fast.next 会为 null，slow 会为 3 // 也就是 不论是奇数还是偶数，slow 都会是后半部分的第一个数 // 反转前半部分 ListNode pre = null; ListNode cur = head; while(cur != slow)&#123; ListNode nextTemp = cur.next; cur.next = pre; pre = cur; cur = nextTemp; &#125; if(fast != null)&#123; slow = slow.next; &#125; while(pre != null)&#123; if(pre.val != slow.val) return false; pre = pre.next; slow = slow.next; &#125; return true; &#125;&#125; 可以优化一下，直接在快慢指针找中点的同时进行翻转 1234567891011121314151617181920212223242526class Solution &#123; // 反转前半部分 public boolean isPalindrome(ListNode head) &#123; if(head == null) return true; ListNode fast = head; ListNode cur = head, pre = null; while(fast != null &amp;&amp; fast.next != null) &#123; // 这里的 cur 指针就是 slow 指针，充当了两个作用 // 一个是快慢指针找中点，一个是反转链表，因为 cur 也是每轮循环走一步 fast = fast.next.next; ListNode nextTemp = cur.next; cur.next = pre; pre = cur; cur = nextTemp; &#125; if(fast != null) &#123; cur = cur.next; &#125; while(pre != null) &#123; if(pre.val != cur.val) return false; pre = pre.next; cur = cur.next; &#125; return true; &#125;&#125; 链表转列表，然后比对 123456789101112131415161718192021class Solution &#123; public boolean isPalindrome(ListNode head) &#123; List&lt;Integer&gt; vals = new ArrayList&lt;&gt;(); ListNode currentNode = head; while (currentNode != null) &#123; vals.add(currentNode.val); currentNode = currentNode.next; &#125; int front = 0; int back = vals.size() - 1; while (front &lt; back) &#123; if (!vals.get(front).equals(vals.get(back))) &#123; return false; &#125; front++; back--; &#125; return true; &#125;&#125; (剑指offer) 删除链表中的节点 找一个牺牲者 12345678910111213141516171819202122class Solution &#123; public ListNode deleteNode(ListNode head, int val) &#123; // 这个方法叫找一个牺牲者 ListNode dummy = new ListNode(-1); dummy.next = head; ListNode cur = head; ListNode pre = dummy; while(cur != null &amp;&amp; cur.next != null)&#123; if(cur.val == val)&#123; cur.val = cur.next.val; cur.next = cur.next.next; return dummy.next; &#125; pre = cur; cur = cur.next; &#125; if(cur.next == null)&#123; pre.next = null; &#125; return dummy.next; &#125;&#125; 直接判断下一个是不是 123456789101112131415class Solution &#123; public ListNode deleteNode(ListNode head, int val) &#123; ListNode dummy = new ListNode(-1); ListNode pre = dummy; dummy.next = head; while(pre.next != null)&#123; if(pre.next.val == val)&#123; pre.next = pre.next.next; return dummy.next; &#125; pre = pre.next; &#125; return dummy.next; &#125;&#125; 链表中的倒数第K个节点123456789101112131415class Solution &#123; public ListNode getKthFromEnd(ListNode head, int k) &#123; //使用前后指针 ListNode back = head; ListNode front = head; for(int i = 0;i &lt; k;i++)&#123; front = front.next; &#125; while(front != null)&#123; back = back.next; front = front.next; &#125; return back; &#125;&#125; 复制带随即指针的链表【难点哈】 HashMap 复制 1234567891011121314151617181920212223242526272829303132/*// Definition for a Node.class Node &#123; int val; Node next; Node random; public Node(int val) &#123; this.val = val; this.next = null; this.random = null; &#125;&#125;*/class Solution &#123; public Node copyRandomList(Node head) &#123; if (head == null) return null; Map&lt;Node, Node&gt; lookup = new HashMap&lt;&gt;(); Node node = head; while (node != null)&#123; lookup.put(node, new Node(node.val, null, null)); node = node.next; &#125; node = head; while (node != null)&#123; lookup.get(node).next = lookup.get(node.next); lookup.get(node).random = lookup.get(node.random); node = node.next; &#125; return lookup.get(head); &#125;&#125; O(1) 空间复杂度 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public Node copyRandomList(Node head) &#123; if(head == null) return null; Node cur = head; // 复制 while(cur != null)&#123; Node tmp = cur.next; cur.next = new Node(cur.val,null,null); cur.next.next = tmp; cur = tmp; &#125; // 置随机指针 cur = head; while(cur != null)&#123; // 易错点，别忘了判断 if(cur.random != null) cur.next.random = cur.random.next; cur = cur.next.next; &#125; // 拆分 cur = head; Node copy_head = head.next; Node copy_cur = copy_head; while(copy_cur.next != null)&#123; cur.next = cur.next.next; cur = cur.next; // 注意哦，这里while循环必须是 copy_cur.next != null // 原因是下面这行代码，如果是 copy_cur != null，那下面就会空指针异常 copy_cur.next = copy_cur.next.next; copy_cur = copy_cur.next; &#125; //别忘了给 cur 最后 加 null cur.next = null; return copy_head; &#125;&#125;]]></content>
      <tags>
        <tag>链表</tag>
        <tag>dp</tag>
        <tag>背包九讲</tag>
        <tag>二叉树</tag>
        <tag>LeetCode 相关题型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零碎知识点总结]]></title>
    <url>%2F2020%2F02%2F07%2F%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9.html</url>
    <content type="text"><![CDATA[静态方法 &amp; 非静态方法的区别 本质区别：静态方法与非静态方法的本质区别：静态方法在程序初始化后会一直贮存在内存中，不会被垃圾回收器回收，非静态方法只在该类初始化后贮存在内存中，当该类调用完毕后会被垃圾回收器收集释放。 调用方式区别：在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 访问限制不同：静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。如果想要在静态方法中访问本类的非静态方法，则需要先实例化类，再实例化类.实例方法。 之所以不允许静态方法直接访问实例成员变量，是因为实例成员变量是属于某个对象的，而静态方法在执行时，并不一定存在对象。同样，因为实例方法可以访问实例成员变量，如果允许静态方法调用实例方法，将间接地允许它使用实例成员变量，所以它也不能调用实例方法。基于同样的道理，静态方法中也不能使用关键字this。 声明方式不同：静态方法声明必须有static，调用时使用类名+.+静态方法名的方式。非静态方法不能使用static关键字，调用时需初始化该方法所属类。 零拷贝 https://www.jianshu.com/p/275602182f39 https://blog.csdn.net/CringKong/article/details/80274148 简而言之，就是减少了拷贝次数，直接使用 DMA（Direct Memory Access，直接内存存取），并且减少了上下文切换的次数，所谓的零拷贝，意思应该是指不需要 CPU 参与拷贝就能完成数据的拷贝过程。 Tip: kafka之所以这么快，就是用了零拷贝技术 传统的读写(这里假设是将文件中的数据读取到网络上)： 首先是从磁盘通过 DMA 加载到内存，第一次拷贝，然后内存将其放到内核缓冲区，同时完成了一次从 用户态到 内核态的上下文切换； 然后是内核缓冲区拷贝到用户缓存区，第二次拷贝，同时又将内核态切回到用户态； 然后是用户缓冲区拷贝至 Socket 缓冲区，第三次拷贝，同时将用户态切回内核态； 然后 Socket 缓冲区通过 DMA 将数据拷贝至网络引擎，然后结束 wirte，此时第四次拷贝，并最终将内核态切回用户态。 历经了 4 次拷贝，4 次上下文切换，其中包括2次需要 CPU 参与工作(第二步和第三步)。 mmap 对内核缓冲区和用户缓冲区做地址映射，这样的话上述第二步和第三步可以直接归为一步，也就是减少了一次拷贝，但是并没有减少上下文切换的次数。 sendfile 这是真正的零拷贝技术， Java 中的 NIO 就是通过这个底层实现的，也就是第二步中直接将内核缓冲区通过 DMA 发送至网络引擎，将内核态切换为用户态，此时全程不需要 CPU 参与工作，且拷贝次数降为2次，上下文切换次数也同时降为2次。 Spring AOP原理Spring MVC 过程Spring IocSpring BeanSpring Bean 的生命周期抽象类和接口的区别抽象类：一个包含抽象方法的类，抽象方法是指用 abstract 修饰，没有实现的方法； 接口：一个方法的集合； 区别： 抽象类具有类的特性，可以 public、private、protected 修饰类，而接口只能使用 public，且抽象类具有构造函数、成员变量，接口都不具备； 抽象类和接口都不能实例化，但是抽象类中可以有实现好的方法，可以被继承，继承者可以选择实现部分抽象类的方法，然后交由子类继续去完成抽象类中未完成的抽象方法的重写，但是接口必须把所有定义的方法全部实现； 抽象类可以被继承，所以必须满足单继承，但是接口不一样，接口没有限制。 Java中public，protected，private以及默认的访问权限作用域 内部类 https://www.cnblogs.com/dolphin0520/p/3811445.html 写的真的太好了… 内部类基础内部类就是一个类可以放到另外一个类或者方法里面，总共分为四种情况： 成员内部类；「放在类中」 局部内部类；「放在方法中」 匿名内部类； 静态内部类。 成员内部类123456789101112131415161718192021222324252627282930313233343536public class InnerClassDemo &#123; public int age = 18; public int sex = 1; // 外部类只能是 public or default // 跟外部类不一样，这里可以有private、public、protected、不加(default) class Inner &#123; public int age = 20; // 当内部类的变量和外部类变量名字一致，注意对象，如果要使用外部对象 // 则是 外部类名.this public int inner_sex = 1; public void showAge() &#123; int age = 25; System.out.println(sex); System.out.println(age);//25 System.out.println(this.age);//20 System.out.println(InnerClassDemo.this.age);//18 内部类访问外部类的对象，使用 类名.this &#125; &#125; // 虽然成员内部类可以无条件地访问外部类的成员， // 而外部类想访问成员内部类的成员却不是这么随心所欲了。 // 在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问： public void get_inner_sex()&#123; Inner inner = new Inner(); inner.showAge(); System.out.println(inner.inner_sex); &#125;&#125;class Test &#123; public static void main(String[] args) &#123; // 注意写法：new 类名().new 内部类名() InnerClassDemo.Inner innerClassDemo = new InnerClassDemo().new Inner(); innerClassDemo.showAge(); &#125;&#125; 成员内部类有四点需要注意： 内部类可以拥有private访问权限、protected访问权限、public访问权限及包访问权限。比如上面的例子，如果成员内部类Inner用private修饰，则只能在外部类的内部访问，如果用public修饰，则任何地方都能访问；如果用protected修饰，则只能在同一个包下或者继承外部类的情况下访问；如果是默认访问权限，则只能在同一个包下访问。这一点和外部类有一点不一样，外部类只能被public和包访问两种权限修饰。我个人是这么理解的，由于成员内部类看起来像是外部类的一个成员，所以可以像类的成员一样拥有多种权限修饰。 当内部类和外部类有着一样的变量时，需要注意的是，如果用 this.变量名 获得的是内部类的成员变量，如果想要获得外部类的成员变量或者成员方法需要使用 外部类名.this.变量名 和 外部类名.this.成员方法名； 虽然成员内部类可以无条件地访问外部类的成员，而外部类想访问成员内部类的成员却不是这么随心所欲了。在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问； 成员内部类是依附外部类而存在的，也就是说，如果要创建成员内部类的对象，前提是必须存在一个外部类的对象。也就是 new 类名().new 内部类名()。 局部内部类12345678910111213141516171819202122232425262728293031package 内部类;/** * 与成员内部类基本一致，但是有2处不一样 * 1.局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内。 * 2. 内部类此时不能有修饰符 */public class InnerClassDemo2 &#123; public int age = 18; public void example()&#123; int age = 20; // 不能有修饰！ class Inner2&#123; public int age = 25; public void inner_method()&#123; System.out.println(age); // 25 System.out.println(this.age); // 25 System.out.println(InnerClassDemo2.this.age); //18 &#125; &#125; new Inner2().inner_method(); &#125;&#125;class Test2 &#123; public static void main(String[] args) &#123; InnerClassDemo2 innerClassDemo2 = new InnerClassDemo2(); innerClassDemo2.example(); &#125;&#125; 局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内。 注意，局部内部类就像是方法里面的一个局部变量一样，是不能有public、protected、private以及static修饰符的。 匿名内部类1234567891011121314151617181920212223242526package 内部类;public class InnerClassDemo3 implements Runnable&#123; @Override public void run() &#123; System.out.println("不使用内部类，得先声明一个类并实现接口"); &#125;&#125;class Test3&#123; public static void main(String[] args) &#123; // 使用内部类，可以加持 lambda 表达式 new Thread(() -&gt; System.out.println("内部类真好用")).start(); // 使用内部类，但是没用 lambda 表达式 new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("内部类真好用"); &#125; &#125;).start(); // 不使用内部类，又臭又长 InnerClassDemo3 innerClassDemo3 = new InnerClassDemo3(); new Thread(innerClassDemo3).start(); &#125;&#125; 很明显，内部类 + lambda表达式 使得代码非常的干净。使用匿名内部类能够在实现父类或者接口中的方法情况下同时产生一个相应的对象，但是前提是这个父类或者接口必须先存在才能这样使用。 当然，需要注意的是： 匿名内部类是唯一一种没有构造器的类。正因为其没有构造器，所以匿名内部类的使用范围非常有限，大部分匿名内部类用于接口回调。匿名内部类在编译的时候由系统自动起名为Outter$1.class。一般来说，匿名内部类用于继承其他类或是实现接口，并不需要增加额外的方法，只是对继承方法的实现或是重写。 静态内部类 静态内部类也是定义在另一个类里面的类，只不过在类的前面多了一个关键字 static； 静态内部类是不需要依赖于外部类的实例对象的，所以在别的类中调用内部类时，不需要先去获取外部类的对象，这点和类的静态成员属性有点类似； 并且它不能使用外部类的非static成员变量或者方法，这点很好理解，因为在没有外部类的对象的情况下，可以创建静态内部类的对象，如果允许访问外部类的非static成员就会产生矛盾，因为外部类的非static成员必须依附于具体的对象。 为了对比第一种区别，我直接在第一种代码上做了部分改动： 1234567891011121314151617181920212223242526272829303132333435363738394041package 内部类;public class InnerClassDemo4 &#123; public int age = 18; public int sex = 1; public static int static_age = 188; // 外部类只能是 public or default // 跟外部类不一样，这里可以有private、public、protected、不加(default) static class Inner &#123; public int age = 20; // 当内部类的变量和外部类变量名字一致，注意对象，如果要使用外部对象 // 则是 外部类名.this public int inner_sex = 1; public void showAge() &#123; int age = 25;// System.out.println(sex); // 此时就不能使用了，因为是 非 static System.out.println(age);//25 System.out.println(this.age);//20// System.out.println(InnerClassDemo4.this.age);//18 内部类访问外部类的对象，使用 类名.this ---&gt;此时会报错 System.out.println(InnerClassDemo4.static_age); //188 &#125; &#125; // 虽然成员内部类可以无条件地访问外部类的成员， // 而外部类想访问成员内部类的成员却不是这么随心所欲了。 // 在外部类中如果要访问成员内部类的成员，必须先创建一个成员内部类的对象，再通过指向这个对象的引用来访问： public void get_inner_sex()&#123; Inner inner = new Inner(); inner.showAge(); System.out.println(inner.inner_sex); &#125;&#125;class Test4 &#123; public static void main(String[] args) &#123; // 注意写法：new 类名().new 内部类名()// InnerClassDemo4.Inner innerClassDemo = new InnerClassDemo4().new Inner();// 此时会报错 InnerClassDemo4.Inner innerClassDemo = new InnerClassDemo4.Inner(); innerClassDemo.showAge(); &#125;&#125; 从上述代码可以看到，有两个地方和第一处不一样： 内部类中无法调用外部类非 static 变量； 其他类调用内部类时，无需先实例化外部类了，可以直接外部类名.内部类名()。 深入理解内部类1.为何在成员内部类中，内部类可以随意访问外部类？ 虽然我们在定义的内部类的构造器是无参构造器，编译器还是会默认添加一个参数，该参数的类型为指向外部类对象的一个引用，所以成员内部类中的Outter this&amp;0 指针便指向了外部类对象，因此可以在成员内部类中随意访问外部类的成员。从这里也间接说明了成员内部类是依赖于外部类的，如果没有创建外部类的对象，则无法对Outter this&amp;0引用进行初始化赋值，也就无法创建成员内部类的对象了。 2.静态内部类有特殊的地方吗？ 从前面可以知道，静态内部类是不依赖于外部类的，也就说可以在不创建外部类对象的情况下创建内部类的对象。另外，静态内部类是不持有指向外部类对象的引用的，这个读者可以自己尝试反编译class文件看一下就知道了，是没有Outter this&amp;0引用的。 内部类的使用场景为什么在Java中需要内部类？总结一下主要有以下四点： 每个内部类都能独立的继承一个接口的实现，所以无论外部类是否已经继承了某个(接口的)实现，对于内部类都没有影响，内部类使得多继承的解决方案变得完整； 方便将存在一定逻辑关系的类组织在一起，又可以对外界隐藏；「ThreadLocal &amp; ThreadLocalMap」 方便编写线程代码。 最后补充一点知识：关于成员内部类的继承问题。一般来说，内部类是很少用来作为继承用的。但是当用来继承的话，要注意两点： 1）成员内部类的引用方式必须为 外部类名.内部类名； 2）构造器中必须有指向外部类对象的引用，并通过这个引用调用super()。这段代码摘自《Java编程思想》 1234567891011121314151617class WithInner &#123; class Inner&#123; &#125;&#125;class InheritInner extends WithInner.Inner &#123; // InheritInner() 是不能通过编译的，一定要加上形参 InheritInner(WithInner wi) &#123; wi.super(); //必须有这句调用 &#125; public static void main(String[] args) &#123; WithInner wi = new WithInner(); InheritInner obj = new InheritInner(wi); &#125;&#125; 反射机制 反射的基本用法：https://blog.csdn.net/a745233700/article/details/82893076 获取类（第三个方法最常用） // 方法一：通过实例对象获取，不过说实话，都能拿到对象了，还要类对象干嘛..// 不过有种用途，就是比如可以越过泛型检查，添加数据// 比如一个对象是 ArrayList\，可通过反射添加一个 Integer的数// 因为java的泛型是在编译之后擦除的…存在漏洞哈哈哈// 例子见： https://blog.csdn.net/sinat_38259539/article/details/71799078 123&gt; InnerClassDemo1 innerClassDemo1 = new InnerClassDemo1();&gt; Class innerClass = innerClassDemo1.getClass();&gt; 方法二：类名.class，这个需要导包… 12&gt; Class innerClass = InnerClassDemo1.class;&gt; 方法三：最常用，使用类名（相对路径） 12&gt; Class innerClass = Class.forName("内部类.InnerClassDemo1");&gt; 获得实例对象 1234&gt; Class.forname("").getConstructor(String.class).newinstance() // 可用于生成有参的对象&gt; &gt; Class.forname("").newInstance() // 可生成无参的对象&gt; 拿到实例属性、实例方法 123456789&gt; // 拿到对象的属性&gt; Field field = innerClass.getField("sex");&gt; // 如果参数是private，可以擦除&gt; // field.setAccessible(true);&gt; int a = (int) field.get(innerClassDemo1);&gt; // 拿到方法&gt; Method showAge = innerClass.getMethod("get_inner_sex");&gt; showAge.invoke(innerClassDemo1);&gt; 拿到静态变量或者静态对象 123456&gt; // 拿到类的属性&gt; Field ff = innerClass.getField("haha");&gt; int b = (int) ff.get(innerClass);&gt; // 或者 int b = (int) ff.get(innerClassDemo1);&gt; System.out.println("b: " + b);&gt; 动态代理 demo 测试类 12345678910111213141516171819202122232425262728package Proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Proxy;public class test &#123; public static void main(String[] args) &#123; // 相当于做生意的过程，比如我去医院整容，首先我要确保我有钱(也就是有接口) Universities person = new Universities(); // 其次，我得去医院找到相应的医生，也就是把我想代理的对象，即我本人，告知给医生 // 医生肯定得有能整容的技术，那就是得继承 InvocationHandler InvocationHandler dynamic_person = new Dynamic(person); // 进行交易的过程，一手交钱一手交换，医生拿到钱，会返回一个有钱的处理好的美女，也就是代理完成了 // 这里必须强调代理返回的是 接口对象，也就是医生只会对有钱人进行代理，没钱的代理就失败了 // 如果最开始我没钱，我就去找医生了，那在这一步交易的过程就会出错，因为医生只会处理有钱人，并且返回有钱人的代理好的对象 // 交易的三个参数，也就是本人、钱、还有医生。本人就是实例对象，钱就是前文说的接口，医生就是上面的动态代理对象 // 有钱人得到代理后的对象，就可以为所欲为了 Person pp = (Person) Proxy.newProxyInstance(person.getClass().getClassLoader(),person.getClass().getInterfaces(),dynamic_person); // pp 就是 被代理对象 被 代理对象 代理后的对象 pp.dance(); System.out.println("-----------"); pp.play(); &#125;&#125; Person 接口 1234567package Proxy;public interface Person &#123; void play(); void dance();&#125; Universities 实现类 1234567891011121314package Proxy;public class Universities implements Person &#123; @Override public void play() &#123; System.out.println("I like play computer"); &#125; @Override public void dance() &#123; System.out.println("I like dance"); &#125;&#125; 动态代理类 1234567891011121314151617181920package Proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;public class Dynamic implements InvocationHandler &#123; // 整容的人是谁，我得知道 public Object obj; public Dynamic(Object obj)&#123; this.obj = obj; &#125; // 整容的过程就是这的 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("开始为" + method.getName() + "方法进行代理"); Object result = method.invoke(obj,args); System.out.println("结束" + method.getName() + "方法的代理"); return result; &#125;&#125; Integer 和 int 的区别基本对比 Integer是int的包装类；int是基本数据类型； Integer变量必须实例化后才能使用；int变量不需要； Integer实际是对象的引用，指向此new的Integer对象；int是直接存储数据值； Integer的默认值是null；int的默认值是0。 深入对比 demo 由于Integer变量实际上是对一个Integer对象的引用，所以两个通过new生成的Integer变量永远是不相等的（因为new生成的是两个对象，其内存地址不同）。 123Integer i = new Integer(100);Integer j = new Integer(100);System.out.print(i == j); //false Integer变量和int变量比较时，只要两个变量的值是向等的，则结果为true（因为包装类Integer和基本数据类型int比较时，java会自动拆包装为int，然后进行比较，实际上就变为两个int变量的比较） 123Integer i = new Integer(100);int j = 100；System.out.print(i == j); //true 非new生成的Integer变量和new Integer()生成的变量比较时，结果为false。（因为非new生成的Integer变量指向的是java常量池中的对象「注意是 -128 ~ 127 才有缓存，其他的数也是会重新开辟空间」，而new Integer()生成的变量指向堆中新建的对象，两者在内存中的地址不同） 123Integer i = new Integer(100);Integer j = 100;System.out.print(i == j); //false 对于两个非new生成的Integer对象，进行比较时，如果两个变量的值在区间-128到127之间，则比较结果为true，如果两个变量的值不在此区间，则比较结果为false。 1234567Integer i = 100;Integer j = 100;System.out.print(i == j); //true Integer i = 128;Integer j = 128;System.out.print(i == j); //false 缓存部分可以看到源代码的写法： 1234567public static Integer valueOf(int i)&#123; assert IntegerCache.high &gt;= 127; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)&#123; return IntegerCache.cache[i + (-IntegerCache.low)]; &#125; return new Integer(i);&#125; “== “ 和 equals() 在 Integer 中的不同 12345678910111213141516171819202122/** * Integer vs Int */class Test3&#123; public static void main(String[] args) &#123; Integer a1 = 200; Integer a11 = 200; Integer b1 = 200; Integer b11 = 200; System.out.println(a1.equals(b1)); // true，equals 会自动拆箱 System.out.println(a1 == b1); // false，== 只有遇到了算术运算才会拆箱，此时由于不在缓存范围内，所以是不同的对象 System.out.println(a1 + a11 == b1 + b11); // true，算术运算，所以拆箱了 // 不用 new 得到的 Integer 对象，实际上跟 String.inner 一样 // 常量池没有，就创建一个扔常量池中，但是这里的 Integer 有限制，是 -128 ~ 127 // 所以这里的 127 都是常量池中的数字，也就是所谓的缓存机制，所以是同一个对象 Integer a2 = 127; Integer b2 = 127; System.out.println(a2.equals(b2)); // true，拆箱了 System.out.println(a2 == b2); // true，虽然未拆箱，但是在缓存范围，是一个对象 &#125;&#125; Kafka 的相关问题 为何使用 kafka 解耦，规定各模块输入输出格式就可以了，各模块耦合性下降； 异步，消息来了，不用立即接收，可以慢慢消费，也达到了一个削峰的作用。 为何不用其他的 MQ？ Kafka 吞吐量最大； 稳定性好 为什么 kafka 很快？ 和Producer与Consum收发消息快的原因是用了 NIO的socket有关的功能，也就是 I/O 复用技术「底层使用 epoll」，用selector去轮询，不需要在连接producer和consumer之后一直等待，等到producer准备好数据之后才去new一个线程跟他传输消息； 然后消息传输到服务器上之后使用NIO中的File有关的功能，也就是零拷贝技术中的 mmap，内存地址和磁盘地址映射，读写内存就相当于直接读写磁盘，所以很快。 其还提供分区，这样的话可以多个broker同时写磁盘，所以就很快 然后在将broker中数据直接发给consumer中是采用了sendfile这个零拷贝技术，直接通过DMA将内核缓冲区中的数据传送到socket缓冲区「FileChannel.transferTo/transferFrom」 https://www.jianshu.com/p/7863667d5fa7 写的真的好 kakfa会丢消息吗 会的，当leader还在进行同步复制的时候，副本复制给其他follower，还没复制完成，leader就挂了，此时再次选举之前的部分数据就丢失了。 kafka如何保证不丢消息 Producer 端是采用了 ack 机制，发送消息之后会有一个 ack的反馈； Consumer端是采用了offset进行一个控制，保证不丢消息，每次都记录最后一次读取的lastoffset kafka消息有序吗 同一个partition是有序的，但是不同partition之间是无序的。 如何防止数据丢失 多副本冗余的高可用机制。。。。 Leader会跟踪与其保持同步的Replica列表，该列表称为ISR（即in-sync Replica），保持同步的副本。Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。而Kafka的这种使用ISR的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了Follower与Leader的差距。 //批量复制数据方式，防止数据丢失1 一条消息只有被ISR里的所有Follower都从Leader复制过去才会被认为已提交。这样就避免了部分数据被写进了Leader，还没来得及被任何Follower复制就宕机了，而造成数据丢失（Consumer无法消费这些数据）。 //防止数据丢失2 如何判断 kafka 中的broker挂掉 zookeeper 的心跳机制 ISR机制，每个leader会关注follow的同步状况，如果不能及时的更新leader的写操作，延时太久，leader就会将其移除 Kafka 的消息传递语义 At most once - 消息传递过程中有可能丢失，丢失的消息也不会重新传递，其实就是保证消息不会重复发送或者重复消费 At least once - 消息在传递的过程中不可能会丢失，丢失的消息会重新传递，其实就是保证消息不会丢失，但是消息有可能重复发送或者重新被消费 「默认」 Exactly once - 这个是大多数场景需要的语义，其实就是保证消息不会丢失，也不会重复被消费，消息只传递一次 在 Producer 端实现 At least once： 在0.11.0.0版本，Kafka的Producer开始支持了幂等发送消息了，其实说白了，就是重复发送一条消息不会导致broker server中存储两条一样的消息了。这样的话Kafka在消息的发送的语义就可以达到Exactly once了 为了实现幂等发送消息，在0.11.0.0版本，Kafka给每一个Producer一个唯一的ID，以及给发送的每一条消息一个唯一的sequence number，利用这两个信息就可以在broker server段对消息进行去重了。 如果想要实现 Exactly once： 协调好消费者消费的消息offset的保存和处理消息结果的保存之间的关系就可以达到Exactly once的语义，就是说，当消费的消息offset的保存成功以及处理消息结果的保存成功，则算成功，如果两者有一个失败的话，那么就需要回滚两个保存了(和事务有点像)。 kafka中的ack机制 第一种选择是把acks参数设置为0，意思就是我的KafkaProducer在客户端，只要把消息发送出去，不管那条数据有没有在哪怕Partition Leader上落到磁盘，我就不管他了，直接就认为这个消息发送成功了。 如果你采用这种设置的话，那么你必须注意的一点是，可能你发送出去的消息还在半路。结果呢，Partition Leader所在Broker就直接挂了，然后结果你的客户端还认为消息发送成功了，此时就会导致这条消息就丢失了。 第二种选择是设置 acks = 1，意思就是说只要Partition Leader接收到消息而且写入本地磁盘了，就认为成功了，不管他其他的Follower有没有同步过去这条消息了。 这种设置其实是kafka默认的设置，大家请注意，划重点！这是默认的设置 也就是说，默认情况下，你要是不管acks这个参数，只要Partition Leader写成功就算成功。 但是这里有一个问题，万一Partition Leader刚刚接收到消息，Follower还没来得及同步过去，结果Leader所在的broker宕机了，此时也会导致这条消息丢失，因为人家客户端已经认为发送成功了。 最后一种情况，就是设置acks=all，这个意思就是说，Partition Leader接收到消息之后，还必须要求ISR列表里跟Leader保持同步的那些Follower都要把消息同步过去，才能认为这条消息是写入成功了。 如果说Partition Leader刚接收到了消息，但是结果Follower没有收到消息，此时Leader宕机了，那么客户端会感知到这个消息没发送成功，他会重试再次发送消息过去。 此时可能Partition 2的Follower变成Leader了，此时ISR列表里只有最新的这个Follower转变成的Leader了，那么只要这个新的Leader接收消息就算成功了。 ack = all 就能保证数据不丢失吗？ 当然不是，如果你的Partition只有一个副本，也就是一个Leader，任何Follower都没有，你认为acks=all有用吗？ 当然没用了，因为ISR里就一个Leader，他接收完消息后宕机，也会导致数据丢失。 所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。 最后总结一下 kafka 如何保证生产者和消费者不丢消息，如何保证生产者和消费者不生产和消费重复消息，如何保证消息有序？ 先回答第一个问题。保证生产者和消费者不丢消息，首先由于 kafka 的默认的消息传递语义是 at least once，就是确保生产者消息不丢，确保的手段就是 ack = all 机制，加上设置尽量多的 broker，达到高可用，这样可以使得就算 leader 挂了，还有其他的 follower 能够顶上去。确保消费者不丢消息，需要我们程序员自己去确保，因为 kafka 并没有提供这样的消息语义，因为它是允许丢消息的，当消费者消息丢了之后，我们无需去改动消费的offset，这样就不会去导致消费不到消息，我们通过这种手动控制 offset 的方式也同样能够确保不会去重复消费，因为我们在客户端已经做了一个事务的原子操作，绑定了更改 offset 和消费消息这两件事。 第二个问题，如何保证生产者不重复生产同样的消息， kafka 的机制是每个Producer都有一个 id，然后每一条消息都有一个 id，只要服务器收到了，就会记录下该id，然后就算重复发送了也会幂等，所以就解决了该问题，至于消费者如何解决消费重复问题，我在上面已经讲过了，手动控制 offset 就不会重复消费了。 第三个问题，如何保证有序，Apache Kafka官方保证了partition内部的数据有效性（追加写、offset读）；为了提高Topic的并发吞吐能力，可以提高Topic的partition数，并通过设置partition的replica来保证数据高可靠； 但是在多个Partition时，不能保证Topic级别的数据有序性。 为何使用 MongoDB 需求总是变化，no-sql数据库改动小 速度快，mmap内存映射 文档多，社区资源多 支持范围查询，python和node支持的也很好，mongoengine]]></content>
      <categories>
        <category>零碎知识点</category>
      </categories>
      <tags>
        <tag>动态代理</tag>
        <tag>零拷贝</tag>
        <tag>Spring AOP</tag>
        <tag>Spring IOC</tag>
        <tag>Spring Bean</tag>
        <tag>内部类</tag>
        <tag>反射</tag>
        <tag>静态方法</tag>
        <tag>Integer &amp; Int</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（一）--- 锁 & 关键字]]></title>
    <url>%2F2020%2F01%2F29%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89----%20%E9%94%81%20%26%20%E5%85%B3%E9%94%AE%E5%AD%97%20.html</url>
    <content type="text"><![CDATA[线程的基本操作及状态转换创建线程的三种方式继承 Thread 类方法一 继承 Thread 类并重写 run 方法 创建线程对象 调用该线程对象的 start() 方法来启动线程 demo 12345678910111213141516class ThreadDemo&#123; public static void main(String[] args) &#123; new ThreadTest().start(); &#125;&#125;class ThreadTest extends Thread&#123; private int i; @Override public void run() &#123; for (; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getName() + " is running: " + i); &#125; &#125;&#125; 方法二 当然也可以直接实现内部类，然后重写 run() 方法，然后适当的可以用 lamda 表达式 demo 123456789101112 public static void main(String[] args) &#123;// new ThreadTest().start(); // 2. 直接继承 Thread 类 Thread threadC = new Thread()&#123; @Override public void run()&#123; System.out.println("3.1 get resource c"); super.run(); &#125; &#125;; threadC.start(); &#125; 实现Runnable接口方法一 定义一个类实现 Runnable 接口，并重写该接口的 run() 方法 创建 Runnable 实现类的对象，作为创建 Thread 对象的 target 参数，此 Thread 对象才是真正的线程对象，这个跟继承 Thread 不一样，第一种方式是直接 Thread 的实现类就是真正的线程对象，而实现 Runnable 接口得对象在这里并不是真正的线程对象。 调用线程对象的 start() 方法来启动线程 demo 12345678910111213141516class RunnableDemo &#123; public static void main(String[] args) &#123; RunnableTest runnableTest = new RunnableTest(); new Thread(runnableTest, "线程1").start(); new Thread(runnableTest, "线程2").start(); &#125;&#125;class RunnableTest implements Runnable&#123; private int i = 0; @Override public void run() &#123; for (; i &lt; 100; i++) &#123; System.out.println(Thread.currentThread().getName() + " is running: " + i); &#125; &#125;&#125; 方法二 直接实现匿名内部类就行了，跟方式一中的方法二一样 demo 1234567891011121314class RunnableDemo &#123; public static void main(String[] args) &#123;// RunnableTest runnableTest = new RunnableTest();// new Thread(runnableTest, "线程1").start();// new Thread(runnableTest, "线程2").start(); Thread thread = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("这里是实现 Runnable 接口的线程"); &#125; &#125;); thread.start(); &#125;&#125; 实现 Callable 接口上述两种方式都有一个缺陷就是：在执行完任务之后无法获取执行结果。于是java后面又增加了 Callable 和 Future，通过它们可以在任务执行完毕之后得到任务执行结果。 Runnable 和 Callable Runnable 只是一个接口，里面只是声明了一个run方法，可以看到方法的返回值是void，所以线程执行完了没有任何的返回值。 Callable 也是一个接口，它里面声明了一个call方法，可以看到它是一个泛型接口，call()函数返回的类型就是传递进来的V类型。线程的执行是异步的，一个线程和另外一个线程的执行是互不干扰的，所以你不可能从别的线程中获得返回值，所以要想获得Callable的返回值就需要用到Future这个接口，Futrue可以监视目标线程调用call的情况，当你调用Future的get()方法以获得结果时，当前线程就开始阻塞，直到call方法结束返回结果。 总而言之，就是三点： Runnable 调用 run方法，Callable 调用 call 方法； Runnable不能有返回值，但是 Callable可以返回一个Future对象； Runnable不可以抛出异常，但是 Callable 可以抛出异常。 FutureFuture 就是对于具体的 Runnable 或者 Callable 任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。 12345678public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 下面我们讲解下这五个方法的作用： cancel方法：用来取消任务，如果取消任务成功则返回true，如果取消任务失败则返回false。参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务，如果设置true，则表示可以取消正在执行过程中的任务。如果任务已经完成，则无论mayInterruptIfRunning为true还是false，此方法肯定返回false，即如果取消已经完成的任务会返回false；如果任务正在执行，若mayInterruptIfRunning设置为true，则返回true，若mayInterruptIfRunning设置为false，则返回false；如果任务还没有执行，则无论mayInterruptIfRunning为true还是false，肯定返回true。 isCancelled方法：表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true isDone方法：表示任务是否已经完成，若任务完成，则返回true get()方法：用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回。这里的阻塞需要解释一下，阻塞的是当前调用get方法的线程，直到get方法返回结果才能继续向下执行，如果get方法一直没有返回值，那么当前线程会一直阻塞下去 get(long timeout, TimeUnit unit)方法：获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null，这个就避免了一直获取不到结果使得当前线程一直阻塞的情况发生 也就是说Future提供了三种功能： 1）判断任务是否完成； 2）能够中断任务； 3）能够获取任务执行结果。 因为Future只是一个接口，所以是无法直接用来创建对象使用的，因此就有了下面的 FutureTask。 FutureTask我们先来看一下 FutureTask 的类图： 可以看出 RunnableFuture 继承了 Runnable 接口和 Future 接口，而 FutureTask 实现了 RunnableFuture 接口。所以它既可以作为 Runnable 被线程执行，又可以作为 Future 得到 Callable 的返回值。 实现 ExecutorService &amp; Future &amp; Callable 第一种方式是使用继承了ExecutorService的线程池ThreadPoolExecutor中的submit方法，将Callable直接提交创建Future。 demo 123456789101112131415161718192021class CallableDemo&#123; public static void main(String[] args) &#123; // ExecutorService &amp; Callable ExecutorService service = Executors.newSingleThreadExecutor(); // 或者用这个// ExecutorService service = Executors.newCachedThreadPool(); Future&lt;String&gt; future = service.submit((Callable) () -&gt; "通过实现Callable接口"); System.out.println(future.isDone()); service.shutdown(); try &#123; String result = future.get(); System.out.println(result); System.out.println(future.isDone()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125; ExecutorService &amp; futureTask &amp; Callable 第二种方式就是使用 futureTask，具体看 demo 吧 demo 12345678910111213141516171819202122232425262728293031323334class CallableDemo &#123; public static void main(String[] args) &#123; // FutureTask &amp; Callable CallableTest callableTest = new CallableTest(); FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(callableTest); // futureTask 可以采用两种方式运行线程 // 1. 直接new Thread// new Thread(futureTask).start(); // 2. 将FutureTask 对象放入 executorService 中 ExecutorService service = Executors.newCachedThreadPool(); service.submit(futureTask); service.shutdown(); try &#123; System.out.println("子线程的返回值: " + futureTask.get(3, TimeUnit.SECONDS)); &#125; catch (InterruptedException | TimeoutException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class CallableTest implements Callable &#123; @Override public Integer call() throws Exception &#123; int sum = 0; for (int i = 1; i &lt; 101; i++) &#123; sum += i; &#125; Thread.sleep(2000); System.out.println(Thread.currentThread().getName() + " is running: " + sum); return sum; &#125;&#125; futureTask &amp; Callable 因为 futureTask 实现了 run() 方法，所以可以直接 new Thread 启动。 demo 12345678910111213141516171819202122232425262728293031323334class CallableDemo &#123; public static void main(String[] args) &#123; // FutureTask &amp; Callable CallableTest callableTest = new CallableTest(); FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(callableTest); // futureTask 可以采用两种方式运行线程 // 1. 直接new Thread new Thread(futureTask).start(); // 2. 将FutureTask 对象放入 executorService 中// ExecutorService service = Executors.newCachedThreadPool();// service.submit(futureTask);// service.shutdown(); try &#123; System.out.println("子线程的返回值: " + futureTask.get(3, TimeUnit.SECONDS)); &#125; catch (InterruptedException | TimeoutException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class CallableTest implements Callable &#123; @Override public Integer call() throws Exception &#123; int sum = 0; for (int i = 1; i &lt; 101; i++) &#123; sum += i; &#125; Thread.sleep(2000); System.out.println(Thread.currentThread().getName() + " is running: " + sum); return sum; &#125;&#125; 三种方式的比较1.实现Runnable/Callable接口相比继承Thread类的优势 （1）适合多个线程进行资源共享 （2）可以避免java中单继承的限制 （3）增加程序的健壮性，代码和数据独立 （4）线程池只能放入 Runable 或 Callable 接口实现类，不能直接放入继承 Thread 的类 2.Callable和Runnable的区别 ​ (1) Callable 重写的是 call() 方法， Runnable 重写的方法是 run() 方法 ​ (2) call() 方法执行后可以有返回值，run() 方法没有返回值 ​ (3) call() 方法可以抛出异常，run() 方法不可以 ​ (4) 运行 Callable 任务可以拿到一个 Future 对象，表示异步计算的结果 。通过 Future 对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果 https://zhuanlan.zhihu.com/p/88933756 https://juejin.im/post/5ae6cf7a518825670960fcc2 Synchronized 超详细例子讲述 Synchronized 形象讲述对象锁和类锁区别 Synchronized 主要是类锁和对象锁的区别 synchronized特点：保证内存可见性、操作原子性 synchronized影响性能的原因： 1、加锁解锁操作需要额外操作； 2、互斥同步对性能最大的影响是阻塞的实现，因为阻塞涉及到的挂起线程和恢复线程的操作都需要转入内核态中完成（用户态与内核态的切换的性能代价是比较大的） synchronized锁：对象头中的Mark Word根据锁标志位的不同而被复用 偏向锁：在只有一个线程执行同步块时提高性能。Mark Word存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单比较ThreadID。特点：只有等到线程竞争出现才释放偏向锁，持有偏向锁的线程不会主动释放偏向锁。之后的线程竞争偏向锁，会先检查持有偏向锁的线程是否存活，如果不存货，则对象变为无锁状态，重新偏向；如果仍存活，则偏向锁升级为轻量级锁，此时轻量级锁由原持有偏向锁的线程持有，继续执行其同步代码，而正在竞争的线程会进入自旋等待获得该轻量级锁 轻量级锁：在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，尝试拷贝锁对象目前的Mark Word到栈帧的Lock Record，若拷贝成功：虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向对象的Mark Word。若拷贝失败：若当前只有一个等待线程，则可通过自旋稍微等待一下，可能持有轻量级锁的线程很快就会释放锁。 但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁膨胀为重量级锁 重量级锁：指向互斥量（mutex），底层通过操作系统的mutex lock实现。等待锁的线程会被阻塞，由于Linux下Java线程与操作系统内核态线程一一映射，所以涉及到用户态和内核态的切换、操作系统内核态中的线程的阻塞和恢复。 Volatile volatile可见性原理底层分析（视频） https://juejin.im/post/5ae9b41b518825670b33e6c4 https://www.infoq.cn/article/java-memory-model-4/ 是否能重排序 第二个操作 第一个操作 普通读 / 写 volatile 读 volatile 写 普通读 / 写 NO volatile 读 NO NO( NO volatile 写 NO NO( 从上表我们可以看出： 当第二个操作是 volatile 写时，不管第一个操作是什么，都不能重排序。这个规则确保 volatile 写之前的操作不会被编译器重排序到 volatile 写之后。 当第一个操作是 volatile 读时，不管第二个操作是什么，都不能重排序。这个规则确保 volatile 读之后的操作不会被编译器重排序到 volatile 读之前。 当第一个操作是 volatile 写，第二个操作是 volatile 读时，不能重排序。 为了实现 volatile 的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能，为此，JMM 采取保守策略。下面是基于保守策略的 JMM 内存屏障插入策略： 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。(可以允许普通) 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的 volatile 内存语义。 下面是保守策略下，volatile 写插入内存屏障后生成的指令序列示意图： 上图中的 StoreStore 屏障可以保证在 volatile 写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为 StoreStore 屏障将保障上面所有的普通写在 volatile 写之前刷新到主内存。 这里比较有意思的是 volatile 写后面的 StoreLoad 屏障。这个屏障的作用是避免 volatile 写与后面可能有的 volatile 读 / 写操作重排序。因为编译器常常无法准确判断在一个 volatile 写的后面，是否需要插入一个 StoreLoad 屏障（比如，一个 volatile 写之后方法立即 return）。为了保证能正确实现 volatile 的内存语义，JMM 在这里采取了保守策略：在每个 volatile 写的后面或在每个 volatile 读的前面插入一个 StoreLoad 屏障。从整体执行效率的角度考虑，JMM 选择了在每个 volatile 写的后面插入一个 StoreLoad 屏障。因为 volatile 写 - 读内存语义的常见使用模式是：一个写线程写 volatile 变量，多个读线程读同一个 volatile 变量。当读线程的数量大大超过写线程时，选择在 volatile 写之后插入 StoreLoad 屏障将带来可观的执行效率的提升。从这里我们可以看到 JMM 在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 下面是在保守策略下，volatile 读插入内存屏障后生成的指令序列示意图： 上图中的 LoadLoad 屏障用来禁止处理器把上面的 volatile 读与下面的普通读重排序。LoadStore 屏障用来禁止处理器把上面的 volatile 读与下面的普通写重排序。 上述 volatile 写和 volatile 读的内存屏障插入策略非常保守。在实际执行时，只要不改变 volatile 写 - 读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面我们通过具体的示例代码来说明： 123456789101112131415class VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; // 第一个 volatile 读 int j = v2; // 第二个 volatile 读 a = i + j; // 普通写 v1 = i + 1; // 第一个 volatile 写 v2 = j * 2; // 第二个 volatile 写 &#125; … // 其他方法 &#125; 针对 readAndWrite() 方法，编译器在生成字节码时可以做如下的优化： 注意，最后的 StoreLoad 屏障不能省略。因为第二个 volatile 写之后，方法立即 return。此时编译器可能无法准确断定后面是否会有 volatile 读或写，为了安全起见，编译器常常会在这里插入一个 StoreLoad 屏障。 上面的优化是针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以 x86 处理器为例，上图中除最后的 StoreLoad 屏障外，其它的屏障都会被省略。 前面保守策略下的 volatile 读和写，在 x86 处理器平台可以优化成： 前文提到过，x86 处理器仅会对写 - 读操作做重排序。X86 不会对读 - 读，读 - 写和写 - 写操作做重排序，因此在 x86 处理器中会省略掉这三种操作类型对应的内存屏障。在 x86 中，JMM 仅需在 volatile 写后面插入一个 StoreLoad 屏障即可正确实现 volatile 写 - 读的内存语义。这意味着在 x86 处理器中，volatile 写的开销比 volatile 读的开销会大很多（因为执行 StoreLoad 屏障开销会比较大）。 AQS AQS源码分析—独占模式&amp;共享模式 AQS分析一、AQS分析二 美团团队：从ReentrantLock的实现看AQS的原理及应用 全称是 AbstractQueuedSynchronizer（抽象队列同步器），是通过一个先进先出的队列（存储等待的线程）来实现同步器的一个框架，Lock、ReentrantLock、Semaphore 等等都是基于 AQS 实现的。 整体框架 上图中有颜色的为Method，无颜色的为Attribution。 总的来说，AQS框架共分为五层，自上而下由浅入深，从AQS对外暴露的API到底层基础数据。 当有自定义同步器接入时，只需重写第一层所需要的部分方法即可，不需要关注底层具体的实现流程。当自定义同步器进行加锁或者解锁操作时，先经过第一层的API进入AQS内部方法，然后经过第二层进行锁的获取，接着对于获取锁失败的流程，进入第三层和第四层的等待队列处理，而这些处理方式均依赖于第五层的基础数据提供层。 下面我们会从整体到细节，从流程到方法逐一剖析AQS框架，主要分析过程如下： 原理概述AQS核心思想是，如果被请求的共享资源空闲，那么就将当前请求资源的线程设置为有效的工作线程，将共享资源设置为锁定状态；如果共享资源被占用，就需要一定的阻塞等待唤醒机制来保证锁分配。这个机制主要用的是CLH队列的变体实现的，将暂时获取不到锁的线程加入到队列中。 CLH：Craig、Landin and Hagersten队列，是单向链表，AQS中的队列是CLH变体的虚拟双向队列（FIFO），AQS是通过将每条请求共享资源的线程封装成一个节点来实现锁的分配。 AQS使用一个Volatile的int类型的成员变量来表示同步状态，通过内置的FIFO队列来完成资源获取的排队工作，通过CAS完成对State值的修改。 AQS 的结构大概可总结为以下 3 部分： 用 volatile 修饰的整数类型的 state 状态，用于表示同步状态，提供 getState 和 setState 来操作同步状态； 提供了一个 FIFO 等待队列，实现线程间的竞争和等待，这是 AQS 的核心； AQS 内部提供了各种基于 CAS 原子操作方法，如 compareAndSetState 方法，并且提供了锁操作的acquire和release方法。 数据结构 &amp; 重要变量和类先来看下AQS中最基本的数据结构——Node，Node即为上面CLH变体队列中的节点。 123456//AQS等待队列的头结点，AQS的等待队列是基于一个双向链表来实现的，这个头结点并不包含具体的线程是一个空结点（注意不是null）private transient volatile Node head;//AQS等待队列的尾部结点private transient volatile Node tail;//AQS同步器状态，也可以说是锁的状态，注意volatile修饰证明这个变量状态要对多线程可见private volatile int state; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static final class Node &#123; //下面两个属性都是说明这个结点是共享模式还是独占模式 static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; //下面这四个属性就是说明结点的状态 static final int CANCELLED = 1;//由于超时或中断，节点已被取消 static final int SIGNAL = -1;//表示下一个节点是通过park阻塞的，需要通过unpark唤醒 static final int CONDITION = -2;//表示线程在等待条件变量（先获取锁，加入到条件等待队列，然后释放锁，等待条件变量满足条件；只有重新获取锁之 后才能返回） static final int PROPAGATE = -3;//表示后续结点会传播唤醒的操作，共享模式下起作用 //当前节点在队列中的状态，就是上述几个数值 volatile int waitStatus; //前驱结点（双链表） volatile Node prev; //后继结点（双链表） volatile Node next; // 结点所包装的线程 volatile Thread thread; // 对于Condtion表示下一个等待条件变量的节点 // 其它情况下用于是区分共享模式和独占模式 Node nextWaiter; final boolean isShared() &#123; return nextWaiter == SHARED; &#125; //取得前驱结点 final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) //null的时候抛出异常 throw new NullPointerException(); else return p; &#125; Node() &#123; &#125; Node(Thread thread, Node mode) &#123; this.nextWaiter = mode; this.thread = thread; &#125; Node(Thread thread, int waitStatus) &#123; this.waitStatus = waitStatus; this.thread = thread; &#125; &#125; 两种锁模式AQS支持两种锁一种是独占锁（独占模式），一种是共享锁（共享模式） 独占锁：比如像ReentrantLock就是一种独占锁模式，多个线程去同时抢一个锁，只有一个线程能抢到这个锁，其他线程就只能阻塞等待锁被释放后重新竞争锁。 共享锁：比如像读写锁里面的读锁，一个锁可以同时被多个线程拥有（多个线程可以同时拥有读锁），再比如Semaphore 设置一个资源数目（可以理解为一个锁能同时被多少个线程拥有）。 共享锁跟独占锁可以同时存在，比如读写锁，读锁、写锁分别对应共享锁和独占锁 队列节点状态队列中的 Node，有一个 waitStatus，用来表示该节点对应的线程状态。 CANCELLED = 1；取消状态，如果当前线程的 前置节点 状态为 CANCELLED，则表明前置节点已经等待超时或者已经被中断了，这时需要将其从等待队列中删除。 SIGNAL = -1；等待触发状态，如果当前线程的 前置节点 状态为 SIGNAL，则表明当前线程需要阻塞。 CONDITION = -2；用于 condition，也就是线程间通信，类似于 Synchronized 中的 wait/notify 机制，在 Lock 中是用 condition 来完成的， waitStatus = -2 表示该线程在进队列之前就已经获取到了锁，然后再加入到条件等待队列中，然后释放锁资源，等到条件满足了，就再次获取锁，然后进行线程相应的操作。 PROPAGATE = -3；状态需要向后传播，表示 releaseShared 需要被传播给后续节点，仅在共享锁模式下使用。 可以这么理解：head 节点可以表示成当前持有锁的线程的节点，其余线程竞争锁失败后，会加入到队尾，tail 始终指向队列的最后一个节点。 独占模式独占锁的原理是如果有线程获取到锁，那么其它线程只能是获取锁失败，然后进入等待队列中等待被唤醒。 12345678// 获取锁方法protected boolean tryAcquire(int arg) &#123; throw new UnsupportedOperationException();&#125;// 释放锁方法protected boolean tryRelease(int arg) &#123; throw new UnsupportedOperationException();&#125; 可以看到，在 AQS 中并没有提供获取锁和释放锁的方法，需要实现类自己去实现这两个方法，在这里只是抛出了异常而已。 获取锁12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 源码解读： 通过 tryAcquire(arg) 方法尝试获取锁，这个方法需要实现类自己实现获取锁的逻辑，获取锁成功后则不执行后面加入等待队列的逻辑了； 如果尝试获取锁失败后，则执行 addWaiter(Node.EXCLUSIVE) 方法将当前线程封装成一个 Node 节点对象，并加入队列尾部； 把当前线程执行封装成 Node 节点后，继续执行 acquireQueued 的逻辑，该逻辑主要是判断当前节点的前置节点是否是头节点，来尝试获取锁，如果获取锁成功，则当前节点就会成为新的头节点，这也是获取锁的核心逻辑。 继续看 addWaiter(Node.EXCLUSIVE)： 12345678910111213141516171819private Node addWaiter(Node mode) &#123; // 创建一个基于当前线程的节点，该节点是 Node.EXCLUSIVE 独占式类型 // 注意这里的构造函数，第二个参数就是 NextWaiter，用来区分是共享模式还是独占模式 Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; // 这里先判断队尾是否为空，如果不为空则直接将节点加入队尾 if (pred != null) &#123; node.prev = pred; // 这里必须采用 CAS，因为可能有多个线程加入同时加入到队尾 // 采取 CAS 操作，将当前节点设置为队尾节点，由于采用了 CAS 原子操作，无论并发怎么修改，都有且只有一条线程可以修改成功，其余都将执行后面的enq方法 if (compareAndSetTail(pred, node)) &#123; pred.next = node; return node; &#125; &#125; enq(node); return node;&#125; 简单来说 addWaiter(Node mode) 方法做了以下事情： 创建基于当前线程的独占式类型的节点； 利用 CAS 原子操作，将节点加入队尾。 我们继续看 enq(Node node) 方法： 12345678910111213141516171819private Node enq(final Node node) &#123; // 自旋操作 for (;;) &#123; Node t = tail; // 如果队尾节点为空，那么进行CAS操作初始化队列 if (t == null) &#123; // 这里很关键，即如果队列为空，那么此时必须初始化队列，初始化一个空的节点表示队列头，用于表示当前正在执行的节点，头节点即表示当前正在运行的节点 if (compareAndSetHead(new Node())) tail = head; &#125; else &#123; node.prev = t; // 这一步也是采取CAS操作，将当前节点加入队尾，如果失败的话，自旋继续修改直到成功为止 if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; enq(Node node)主要做了三件事： 采用了自旋机制，没成功就一直循环进行 若队尾节点为空，就先初始化队列，然后因为自旋，再在 head 节点后将当前节点加入队尾 若对尾结点不为空，则采取 CAS ，将当前节点加入队尾 注意返回值是当前节点的 pred 节点 123456789101112131415161718192021222324252627final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; // 线程中断标记字段 boolean interrupted = false; for (;;) &#123; // 获取当前节点的 pred 节点 final Node p = node.predecessor(); // 如果 pred 节点为 head 节点，那么再次尝试获取锁 if (p == head &amp;&amp; tryAcquire(arg)) &#123; // 获取锁之后，那么当前节点也就成为了 head 节点 setHead(node); p.next = null; // help GC failed = false; // 不需要挂起，返回 false return interrupted; &#125; // 获取锁失败，则进入挂起逻辑 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; acquireQueued 主要做了两件事： 判断当前节点的 pred 节点是否是 head 节点，如果是，说明下一个执行的线程就是该线程，所以就可以去尝试获取锁（自旋的过程），如果获取了锁，就将当前节点置为 head； 获取锁如果失败了，就得进入挂起逻辑，即进入 shouldParkAfterFailedAcquire(p, node)。 接下来我们继续看挂起逻辑： 123456789101112131415161718private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; if (ws == Node.SIGNAL) // 如果 pred 节点为 SIGNAL 状态，返回true，说明当前节点需要挂起 return true; // 如果ws &gt; 0,说明节点状态为CANCELLED，需要从队列中删除 if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; // 如果是其它状态，则操作CAS统一改成SIGNAL状态 // 由于这里waitStatus的值只能是0或者PROPAGATE，所以我们将节点设置为SIGNAL，重新循环一次判断 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; shouldParkAfterFailedAcquire(Node pred, Node node) 主要做了三件事： 判断当前节点的 pred 的 waitStatus 是否为 SIGNAL，如果是，则说明当前节点可以挂起； 如果不是 SIGNAL 状态，若是 CANCELLED 状态，则需要将该节点从队列中删除； 否则，需要将该节点的前置节点置为 SIGNAL，再从 acquireQueued 方法自旋操作循环一次判断。 通俗来说就是：根据 pred 节点状态来判断当前节点是否可以挂起，如果该方法返回 false，那么挂起条件还没准备好，就会重新进入 acquireQueued(final Node node, int arg) 的自旋体，重新进行判断。如果返回 true，那就说明当前线程可以进行挂起操作了，那么就会继续执行挂起。 继续看挂起逻辑： 1234private final boolean parkAndCheckInterrupt() &#123; LockSupport.park(this); return Thread.interrupted();&#125; LockSupport 是用来创建锁和其他同步类的基本线程阻塞原语。LockSupport 提供 park() 和 unpark() 方法实现阻塞线程和解除线程阻塞。release 释放锁方法逻辑会调用 LockSupport.unPark 方法来唤醒后继节点。 最后看一下 cancelAcquire()： 12345678910111213141516171819202122232425262728293031323334353637// java.util.concurrent.locks.AbstractQueuedSynchronizerprivate void cancelAcquire(Node node) &#123; // 将无效节点过滤 if (node == null) return; // 设置该节点不关联任何线程，也就是虚节点 node.thread = null; Node pred = node.prev; // 通过前驱节点，跳过取消状态的node while (pred.waitStatus &gt; 0) node.prev = pred = pred.prev; // 获取过滤后的前驱节点的后继节点 Node predNext = pred.next; // 把当前node的状态设置为CANCELLED node.waitStatus = Node.CANCELLED; // 如果当前节点是尾节点，将从后往前的第一个非取消状态的节点设置为尾节点 // 更新失败的话，则进入else，如果更新成功，将tail的后继节点设置为null if (node == tail &amp;&amp; compareAndSetTail(node, pred)) &#123; compareAndSetNext(pred, predNext, null); &#125; else &#123; int ws; // 如果当前节点不是head的后继节点 //1:判断当前节点前驱节点的是否为SIGNAL，2:如果不是，则把前驱节点设置为SINGAL看是否成功 // 如果1和2中有一个为true，再判断当前节点的线程是否为null // 如果上述条件都满足，把当前节点的前驱节点的后继指针指向当前节点的后继节点 if (pred != head &amp;&amp; ((ws = pred.waitStatus) == Node.SIGNAL || (ws &lt;= 0 &amp;&amp; compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) &amp;&amp; pred.thread != null) &#123; Node next = node.next; if (next != null &amp;&amp; next.waitStatus &lt;= 0) compareAndSetNext(pred, predNext, next); &#125; else &#123; // 如果当前节点是head的后继节点，或者上述条件不满足，那就唤醒当前节点的后继节点 unparkSuccessor(node); &#125; node.next = node; // help GC &#125;&#125; 具体可参考： https://tech.meituan.com/2019/12/05/aqs-theory-and-apply.html 整体逻辑图如下： 释放锁123456789public final boolean release(int arg) &#123; if (tryRelease(arg)) &#123; Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) unparkSuccessor(h); return true; &#125; return false;&#125; 思路还是很好理解的，先尝试释放锁，这个 tryRelease() 需要自己实现，如果头结点不为空，且 waitStatus != 0 （因为 addWaiter 方法默认的节点状态为 0，为 0 说明此时节点还没有进入就绪状态），就可以执行唤醒下个节点的操作。 123456789101112131415161718192021private void unparkSuccessor(Node node) &#123; int ws = node.waitStatus; if (ws &lt; 0) // 将头节点的状态设置为0 // 这里会尝试清除头节点的状态，改为初始状态 compareAndSetWaitStatus(node, ws, 0); // 后继节点 Node s = node.next; // 如果后继节点为null，或者已经被取消了 if (s == null || s.waitStatus &gt; 0) &#123; s = null; // for循环从队列尾部一直往前找可以唤醒的节点 for (Node t = tail; t != null &amp;&amp; t != node; t = t.prev) if (t.waitStatus &lt;= 0) s = t; &#125; if (s != null) // 唤醒后继节点 LockSupport.unpark(s.thread);&#125; unparkSuccessor(Node node) 主要做了二件事： 将头结点的 waitStatus 置为初始状态 0 ； 找后继节点，如果后继节点是 null 或者 waitStatus = Node.Cancelled，就直接从队尾开始向前遍历，找到最靠近头结点的下一个符合条件的节点，并将其唤醒。 共享模式获取锁123456public final void acquireShared(int arg) &#123; // 尝试获取共享锁，小于0表示获取失败 if (tryAcquireShared(arg) &lt; 0) // 执行获取锁失败的逻辑 doAcquireShared(arg);&#125; 跟独占模式差不多， tryAcquireShared() 需要实现类自己去实现，这里只要 &gt;=0 说明共享锁的资源还有，就说明获取锁成功，否则就执行获取锁失败的逻辑。 1234567891011121314151617181920212223242526272829303132private void doAcquireShared(int arg) &#123; // 添加共享锁类型节点到队列中 final Node node = addWaiter(Node.SHARED); boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); if (p == head) &#123; // 再次尝试获取共享锁 int r = tryAcquireShared(arg); // 如果在这里成功获取共享锁，会进入共享锁唤醒逻辑 if (r &gt;= 0) &#123; // 共享锁唤醒逻辑 setHeadAndPropagate(node, r); p.next = null; // help GC if (interrupted) selfInterrupt(); failed = false; return; &#125; &#125; // 与独占锁相同的挂起逻辑 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; 跟独占锁基本一样的逻辑，也是先将该线程包装成 Node，加入队尾，然后不断自旋，判断当前节点的前置节点是否为 head，如果是 head，就尝试获取锁，当然这里有一个不同的地方，那就是原来的 setHead() 变成了 setHeadAndPropagate()，因为这里不仅要将获取到锁的节点置为头结点，同时只要共享锁还拥有资源，就需要去唤醒后续共享锁节点。下面我们具体看看 setHeadAndPropagate(node, r)。 12345678910111213141516private void setHeadAndPropagate(Node node, int propagate) &#123; // 头节点 Node h = head; // 设置当前节点为新的头节点 // 这里不需要加锁操作，因为获取共享锁后，会从FIFO队列中依次唤醒队列，并不会产生并发安全问题 setHead(node); if (propagate &gt; 0 || h == null || h.waitStatus &lt; 0 || (h = head) == null || h.waitStatus &lt; 0) &#123; // 后继节点 Node s = node.next; // 如果后继节点为空或者后继节点为共享类型，则进行唤醒后继节点 // 这里后继节点为空意思是只剩下当前头节点了 if (s == null || s.isShared()) doReleaseShared(); &#125;&#125; 主要做了两件事： 将当前节点置为 head，这个跟独占锁是一样的； 寻找后续的共享锁线程，然后将其唤醒。 我们注意到，这里唤醒调用的是 doReleaseShared()，也就是说释放共享锁也是调用的这个方法，那么是怎么做到唤醒多个拥有共享锁线程呢？我们注意到首先是将当前获取到锁的线程置为 head，然后只要还有资源，就唤醒该线程， 释放锁123456789public final boolean releaseShared(int arg) &#123; // 由用户自行实现释放锁条件 if (tryReleaseShared(arg)) &#123; // 执行释放锁 doReleaseShared(); return true; &#125; return false;&#125; 1234567891011121314151617181920212223242526272829303132private void doReleaseShared() &#123; for (;;) &#123; // 从头节点开始执行唤醒操作 // 这里需要注意，如果从setHeadAndPropagate方法调用该方法，那么这里的head是新的头节点 Node h = head; if (h != null &amp;&amp; h != tail) &#123; int ws = h.waitStatus; //表示后继节点需要被唤醒 if (ws == Node.SIGNAL) &#123; // 初始化节点状态 //这里需要CAS原子操作，因为setHeadAndPropagate和releaseShared这两个方法都会顶用doReleaseShared，避免两次unpark唤醒操作 if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0)) // 如果初始化节点状态失败，继续循环执行 continue; // loop to recheck cases // 执行唤醒操作 // 注意这里是唤醒头结点的下一个节点 // 唤醒了下一个节点之后，下一个节点就会去尝试获取锁，形成一个循环 unparkSuccessor(h); &#125; // 这里个人认为是为 setHeadAndPropagate()中 s == null准备的，当只有头结点的时候 // 后继节点没有所以不需要唤醒，但是我们必须把这种可传递的状态显示表现出来 // 确保后续可以传递给后继节点 else if (ws == 0 &amp;&amp; !compareAndSetWaitStatus(h, 0, Node.PROPAGATE)) continue; // loop on failed CAS &#125; // 如果在唤醒的过程中头节点没有更改，退出循环 // 这里防止其它线程又设置了头节点，说明其它线程获取了共享锁，会继续循环操作 if (h == head) // loop if head changed break; &#125;&#125; 注：上面的setHeadAndPropagate()方法表示等待队列中的线程成功获取到共享锁，这时候它需要调用doReleaseShared() 唤醒它后面的共享节点（如果有），但是当通过 releaseShared() 方法去调用doReleaseShared() 释放一个共享锁的时候，接下来等待独占锁跟共享锁的线程都可以被唤醒进行尝试获取。 总结逻辑图 获取锁的过程： 当线程调用acquireShared()申请获取锁资源时，如果成功，则进入临界区。 当获取锁失败时，则创建一个共享类型的节点并进入一个FIFO等待队列，然后被挂起等待唤醒。 当队列中的等待线程被唤醒以后就重新尝试获取锁资源，如果成功则唤醒后面还在等待的共享节点并把该唤醒事件传递下去，即会依次唤醒在该节点后面的所有共享节点，然后进入临界区，否则继续挂起等待。 释放锁过程： 当线程调用releaseShared()进行锁资源释放时，如果释放成功，则唤醒队列中等待的节点，如果有的话。 比较跟独占锁相比，共享锁的主要特征在于当一个在等待队列中的共享节点成功获取到锁以后（它获取到的是共享锁），既然是共享，那它必须要依次唤醒后面所有可以跟它一起共享当前锁资源的节点，毫无疑问，这些节点必须也是在等待共享锁（这是大前提，如果等待的是独占锁，那前面已经有一个共享节点获取锁了，它肯定是获取不到的）。当共享锁被释放的时候，可以用读写锁为例进行思考，当一个读锁被释放，此时不论是读锁还是写锁都是可以竞争资源的。 https://juejin.im/post/5cd2b58f6fb9a032332b45aa https://www.cnblogs.com/lfls/p/7599863.html 面试题目Q：某个线程获取锁失败的后续流程是什么呢？ A：存在某种排队等候机制，线程继续等待，仍然保留获取锁的可能，获取锁流程仍在继续。 Q：既然说到了排队等候机制，那么就一定会有某种队列形成，这样的队列是什么数据结构呢？ A：是CLH变体的FIFO双端队列。 Q：处于排队等候机制中的线程，什么时候可以有机会获取锁呢？ A：当前节点的前置节点为 head，可以尝试获取锁。 Q：如果处于排队等候机制中的线程一直无法获取锁，需要一直等待么？还是有别的策略来解决这一问题？ A：线程所在节点的状态会变成取消状态，取消状态的节点会从队列中释放。 Q：Lock函数通过Acquire方法进行加锁，但是具体是如何加锁的呢？ A：AQS的Acquire会调用tryAcquire方法，tryAcquire由各个自定义同步器实现，通过tryAcquire完成加锁过程。 Tip: 自我实现一个锁，利用 AQS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110package 多线程.Concurrent包理解;import java.io.IOException;import java.io.ObjectInputStream;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.AbstractQueuedSynchronizer;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;public class AQS &#123; public static void main(String[] args) &#123; return; &#125;&#125;class MutexDemo &#123; private static Mutex mutex = new Mutex(); public static void main(String[] args) &#123; for (int i = 0; i &lt; 10 ; i++)&#123; Thread thread = new Thread(() -&gt; &#123; mutex.lock(); try &#123; System.out.println("Current Thread:" + Thread.currentThread().getName()); Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; mutex.unlock(); &#125; &#125;); thread.start(); &#125; &#125;&#125;class Mutex implements Lock, java.io.Serializable &#123; // Our internal helper class private static class Sync extends AbstractQueuedSynchronizer &#123; // Reports whether in locked state protected boolean isHeldExclusively() &#123; return getState() == 1; &#125; public boolean tryAcquire(int acquires) &#123; assert acquires == 1; // Otherwise unused if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; protected boolean tryRelease(int releases) &#123; assert releases == 1; // Otherwise unused if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; &#125; // Provides a Condition Condition newCondition() &#123; return new ConditionObject(); &#125; // Deserializes properly 反序列化 private void readObject(ObjectInputStream s) throws IOException, ClassNotFoundException &#123; s.defaultReadObject(); setState(0); // reset to unlocked state &#125; &#125; // The sync object does all the hard work. We just forward to it. private final Sync sync = new Sync(); public void lock() &#123; sync.acquire(1); &#125; public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; public void unlock() &#123; sync.release(1); &#125; public Condition newCondition() &#123; return sync.newCondition(); &#125; public boolean isLocked() &#123; return sync.isHeldExclusively(); &#125; public boolean hasQueuedThreads() &#123; return sync.hasQueuedThreads(); &#125; public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireNanos(1, unit.toNanos(timeout)); &#125;&#125; ReentrantLock 可重入性、公平与非公平 ReentranLock的特点 重点还是： 可重入性的实现 公平锁和非公平锁的区别 其他的基本上就是 AQS 的独占锁实现而已 可重入性的实现在可重入性方面，个人觉得还是针对于同一把锁而言的，不同锁去谈重入性没有意义，但是有个特例，就是 ReentrantReadWriteLock，因为其 ReadLock 和 WriteLock 是属于同一把大锁下的两把小锁，它们之间的重入性有些特殊，也就是同一把写锁肯定是可以重入的，同一把读锁肯定也是可以重入的，但是注意的是读锁下不可重入写锁，因为读-写互斥，但是有个锁降级，也就是 写锁可以重入读锁，将写锁降级为读锁（先获取写锁，然后获取读锁，然后释放写锁，然后再释放读锁）。 言归正传，不同锁的 ReentrantLock，之间不会有影响， 类似于下面的 demo： 123456789101112131415161718192021222324252627282930package 多线程.Concurrent包理解;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.ReentrantLock;public class ReentrantLockDemo &#123; public static final ReentrantLock lock1 = new ReentrantLock(); public static final ReentrantLock lock2 = new ReentrantLock(); public static void main(String[] args) throws InterruptedException &#123; for(int i = 0;i &lt; 10;i++)&#123; Thread thread = new Thread(() -&gt; &#123; lock1.lock(); lock2.lock(); try&#123; System.out.println("current:" + Thread.currentThread().getName() ); TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock2.unlock(); lock1.unlock(); &#125; &#125;); thread.start(); &#125; &#125;&#125; 我们以非公平锁为例，看看可重入的代码实现： 123456789101112131415161718192021final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //1. 如果该锁未被任何线程占有，该锁能被当前线程获取 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //2.若被占有，检查占有线程是否是当前线程 else if (current == getExclusiveOwnerThread()) &#123; // 3. 再次获取，计数加一 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; 123456789101112131415protected final boolean tryRelease(int releases) &#123; //1. 同步状态减1 int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; //2. 只有当同步状态为0时，锁成功被释放，返回true free = true; setExclusiveOwnerThread(null); &#125; // 3. 锁未被完全释放，返回false setState(c); return free;&#125; 公平锁与非公平锁ReentrantLock支持两种锁：公平锁和非公平锁。何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO。ReentrantLock的构造方法无参时是构造非公平锁，源码为： 123public ReentrantLock() &#123; sync = new NonfairSync();&#125; 另外还提供了另外一种方式，可传入一个boolean值，true时为公平锁，false时为非公平锁，源码为： 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 在上面非公平锁获取时（nonfairTryAcquire方法）只是简单的获取了一下当前状态做了一些逻辑处理，并没有考虑到当前同步队列中线程等待的情况。我们来看看公平锁的处理逻辑是怎样的，核心方法为： 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false; &#125;&#125; 其实就是多了一个方法 hasQueuedPredecessors()，方法名就可知道该方法用来判断当前节点在同步队列中是否有前驱节点的判断，如果有前驱节点说明有线程比当前线程更早的请求资源，根据公平性，当前线程请求资源失败。如果当前节点没有前驱节点的话，再才有做后面的逻辑判断的必要性。公平锁每次都是从同步队列中的第一个节点获取到锁，而非公平性锁则不一定，有可能刚释放锁的线程能再次获取到锁。 公平锁 VS 非公平锁 公平锁每次获取到锁为同步队列中的第一个节点，保证请求资源时间上的绝对顺序，而非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，造成“饥饿”现象。 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。因此，ReentrantLock默认选择的是非公平锁，则是为了减少一部分上下文切换，保证了系统更大的吞吐量。 与 Synchronized 的区别可重入性从名字上理解，ReenTrantLock的字面意思就是再进入的锁，其实synchronized关键字所使用的锁也是可重入的，两者关于这个的区别不大。两者都是同一个线程没进入一次，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。 锁的实现Synchronized是依赖于JVM实现的，而ReenTrantLock是JDK实现的，有什么区别，说白了就类似于操作系统来控制实现和用户自己敲代码实现的区别。前者的实现是比较难见到的，后者有直接的源码可供阅读。 性能的区别在Synchronized优化以前，synchronized的性能是比ReenTrantLock差很多的，但是自从Synchronized引入了偏向锁，轻量级锁（自旋锁）后，两者的性能就差不多了，在两种方法都可用的情况下，官方甚至建议使用synchronized，其实synchronized的优化我感觉就借鉴了ReenTrantLock中的CAS技术。都是试图在用户态就把加锁问题解决，避免进入内核态的线程阻塞。 功能区别便利性：很明显Synchronized的使用比较方便简洁，并且由编译器去保证锁的加锁和释放，而ReenTrantLock需要手工声明来加锁和释放锁，为了避免忘记手工释放锁造成死锁，所以最好在finally中声明释放锁。 锁的细粒度和灵活度：很明显ReenTrantLock优于Synchronized ReenTrantLock独有的能力 ReenTrantLock可以指定是公平锁还是非公平锁。而synchronized只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。 ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。 ReenTrantLock提供了一种能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现这个机制。 ReenTrantLock实现的原理简单来说，ReenTrantLock的实现是一种自旋锁，通过循环调用CAS操作来实现加锁。它的性能比较好也是因为避免了使线程进入内核态的阻塞状态。想尽办法避免线程进入内核的阻塞状态是我们去分析和理解锁设计的关键钥匙。 什么情况下使用ReenTrantLock答案是，如果你需要实现ReenTrantLock的三个独有功能时。 ————————————————版权声明：本文为CSDN博主「qq838642798」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq838642798/article/details/65441415 ReentrantReadWriteLock 写锁—-真的是写的超棒！ 读锁—-不说了太赞了！！关注这个作者 锁降级问题 这个才是完整继承了 AQS 基本所有方法的实现类，因为它不像 ReentrantLock 只有独占锁，它可以同时拥有独占锁和共享锁，并且这两个锁还有一些额外的限制，也就是 写-读互斥(当然锁降级除外)，写-写互斥，读-读共享。所以这个很有必要详细说一下。 主要还是三个特点： 公平性选择：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平； 重入性：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁； 锁降级：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁 要弄清楚上述三个特点，就得解决以下几个问题： 写锁和读锁是如何记录读写状态的； 写锁是如何获取和释放的； 读锁是如何获取和释放的； 锁降级的规则是什么。 读写锁如何记录状态之前都是用 state 这个变量来记录锁的获取情况，这里也是一样，但是这里比较特殊，使用了(Integer 32位)低 16 位来存储写锁的获取情况，高 16 位用来存储读锁的获取情况。 写锁的获取和释放写锁由于是独占锁，并且不能锁升级（也就是不允许读锁内嵌套写锁），所以也就比较简单了。 tryAcquire()写锁独占锁，所以需要实现的就是 AQS 中的 tryAcquire() 1234567891011121314151617181920212223242526272829303132// 如果有读锁，此时是获取不到写锁的。当有写锁时，判断重入次数。// 当写锁空闲，读锁空闲，公平模式下，如果队列中有等待的，不会抢锁。非公平模式下，必抢锁。protected final boolean tryAcquire(int acquires) &#123; // 写 Thread current = Thread.currentThread(); int c = getState(); // 用 state &amp; 65535 得到低 16 位的值。 int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) // 如果 state 不是0，且低16位是0，说明了什么？说明写锁是空闲的，读锁被霸占了。那么也不能拿锁，返回 fasle。 // 如果低 16 位不是0，说明写锁被霸占了，并且，如果持有锁的不是当前线程，那么这次拿锁是失败的。返回 fasle。 // 总之，当只有读锁时，就不能获取写锁。当有写锁，可能是写锁降级，也可能是正常的拥有写锁，此时就 // 必须是重入锁。 if (w == 0 || current != getExclusiveOwnerThread()) return false; // 到这一步了，只会是写重入锁。如果写重入次数超过最大值 65535，就会溢出。 if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error("Maximum lock count exceeded"); // Reentrant acquire // 将 state + 1 setState(c + acquires); return true; &#125; // 当 state 是 0 的时候，那么就可以获取锁了。 // writerShouldBlock 判断是否需要锁。非公平情况下，返回 false。公平情况下，根据 hasQueuedPredecessors 结果判断。 // 当队列中有锁等待了，就返回 false 了。 // 当是非公平锁的时候，或者队列中没有等待节点的时候，尝试用 CAS 修改 state。 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; // 修改成功 state 后，修改锁的持有线程。 setExclusiveOwnerThread(current); 最难理解的就是if (w == 0 || current != getExclusiveOwnerThread()) 这一步，具体的流程分析如下图： tryRelease()123456789101112131415protected final boolean tryRelease(int releases) &#123; // 是否持有当前锁 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); // 计算 state 值 int nextc = getState() - releases; // 计算写锁的状态，如果是0，说明是否成功。 boolean free = exclusiveCount(nextc) == 0; // 释放成功，设置持有锁的线程为 null。 if (free) setExclusiveOwnerThread(null); // 设置 state setState(nextc); return free;&#125; 这里还是很简单的，只是有一个地方需要注意： 12// 计算写锁的状态，如果是0，说明是否成功。boolean free = exclusiveCount(nextc) == 0; 这里计算的只是 state 变量的低 16 的值，而不是整个 state 的值。虽然写的时候，必然是串行的，但这里计算的仍然是低 16 位的。 读锁的获取和释放获取读锁的过程是获取共享锁的过程。 tryAcquireShared()123456789101112131415161718192021222324252627282930313233343536373839404142434445protected final int11 tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); // exclusiveCount(c) != 0 ---》 用 state &amp; 65535 得到低 16 位的值。如果不是0，说明写锁别持有了。 // getExclusiveOwnerThread() != current----&gt; 不是当前线程 // 如果写锁被霸占了，且持有线程不是当前线程，返回 false，加入队列。获取写锁失败。 // 反之，如果持有写锁的是当前线程，就可以继续获取读锁了。 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) // 获取锁失败 return -1; // 如果写锁没有被霸占，则将高16位移到低16位。 int r = sharedCount(c);// c &gt;&gt;&gt; 16 // !readerShouldBlock() 和写锁的逻辑一样（根据公平与否策略和队列是否含有等待节点） // 不能大于 65535，且 CAS 修改成功 if (!readerShouldBlock() &amp;&amp; r &lt; 65535 &amp;&amp; compareAndSetState(c, c + 65536)) &#123; // 如果读锁是空闲的， 获取锁成功。 if (r == 0) &#123; // 将当前线程设置为第一个读锁线程 firstReader = current; // 计数器为1 firstReaderHoldCount = 1; &#125;// 如果读锁不是空闲的，且第一个读线程是当前线程。获取锁成功。 else if (firstReader == current) &#123;// // 将计数器加一 firstReaderHoldCount++; &#125; else &#123;// 如果不是第一个线程，获取锁成功。 // cachedHoldCounter 代表的是最后一个获取读锁的线程的计数器。 HoldCounter rh = cachedHoldCounter; // 如果最后一个线程计数器是 null 或者不是当前线程，那么就新建一个 HoldCounter 对象 if (rh == null || rh.tid != getThreadId(current)) // 给当前线程新建一个 HoldCounter cachedHoldCounter = rh = readHolds.get(); // 如果不是 null，且 count 是 0，就将上个线程的 HoldCounter 覆盖本地的。 else if (rh.count == 0) readHolds.set(rh); // 对 count 加一 rh.count++; &#125; return 1; &#125; // 死循环获取读锁。包含锁降级策略。 return fullTryAcquireShared(current);&#125; 总结一下上面代码的逻辑吧！ 判断写锁是否空闲。 如果不是空闲，且当前线程不是持有写锁的线程，则返回 -1 ，表示抢锁失败。如果是空闲的，进入第三步。如果是当前线程，进入第三步。 判断持有读锁的数量是否超过 65535，然后使用 CAS 设置 int 高 16 位的值，也就是加一。 如果设置成功，且是第一次获取读锁，就设置 firstReader 相关的属性（为了性能提升）。 如果不是第一次，当当前线程就是第一次获取读锁的线程，对 “第一次获取读锁线程计数器” 加 1. 如果都不是，则获取最后一个读锁的线程计数器，判断这个计数器是不是当前线程的。如果是，加一，如果不是，自己创建一个新计数器，并更新 “最后读取的线程计数器”（也是为了性能考虑）。最后加一。返回成功。 如果上面的判断失败了（CAS 设置失败，或者队列有等待的线程（公平情况下））。就调用 fullTryAcquireShared 方法死循环执行上面的步骤。 步骤还是有点多哈，画个图吧，更清晰一点。 可以看到读锁对 AQS 的共享锁做了两个优化： 引进了 firstReaderHoldCount 和 cachedHoldCounter ，可以加快计数的性能； 引入锁降级机制，对读多写少且读更重要的场景很适用。 继续看 fullTryAcquireShared()： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364final int fullTryAcquireShared(Thread current) &#123; /* * 这段代码与tryAcquireShared中的代码有部分重复，但整体更简单。 */ HoldCounter rh = null; // 死循环 for (;;) &#123; int c = getState(); // 如果存在写锁 if (exclusiveCount(c) != 0) &#123; // 并且不是当前线程，获取锁失败，反之，如果持有写锁的是当前线程，那么就会进入下面的逻辑。 // 反之，如果存在写锁，但持有写锁的是当前线程。那么就继续尝试获取读锁。 if (getExclusiveOwnerThread() != current) return -1; // 如果写锁空闲，且可以获取读锁。 &#125; else if (readerShouldBlock()) &#123; // 第一个读线程是当前线程 if (firstReader == current) &#123; // 如果不是当前线程 &#125; else &#123; if (rh == null) &#123; rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) &#123; // 从 ThreadLocal 中取出计数器 rh = readHolds.get(); if (rh.count == 0) readHolds.remove(); &#125; &#125; if (rh.count == 0) return -1; &#125; &#125; // 如果读锁次数达到 65535 ，抛出异常 if (sharedCount(c) == MAX_COUNT) throw new Error("Maximum lock count exceeded"); // 尝试对 state 加 65536, 也就是设置读锁，实际就是对高16位加一。 if (compareAndSetState(c, c + SHARED_UNIT)) &#123; // 如果读锁是空闲的 if (sharedCount(c) == 0) &#123; // 设置第一个读锁 firstReader = current; // 计数器为 1 firstReaderHoldCount = 1; // 如果不是空闲的，查看第一个线程是否是当前线程。 &#125; else if (firstReader == current) &#123; firstReaderHoldCount++;// 更新计数器 &#125; else &#123;// 如果不是当前线程 if (rh == null) rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current))// 如果最后一个读计数器所属线程不是当前线程。 // 自己创建一个。 rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); // 对计数器 ++ rh.count++; // 更新缓存计数器。 cachedHoldCounter = rh; // cache for release &#125; return 1; &#125; &#125;&#125; 基本上和 tryAcquireShared() 差不多，只不过这里有一个自旋的过程。 firstReader 是获取读锁的第一个线程。如果只有一个线程获取读锁，很明显，使用这样一个变量速度更快。 firstReaderHoldCount是 firstReader的计数器。同上。 cachedHoldCounter是最后一个获取到读锁的线程计数器，每当有新的线程获取到读锁，这个变量都会更新。这个变量的目的是：当最后一个获取读锁的线程重复获取读锁，或者释放读锁，就会直接使用这个变量，速度更快，相当于缓存。 tryReleaseShared()1234567891011121314151617181920212223242526272829303132333435363738protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); // 如果是第一个线程 if (firstReader == current) &#123; // 如果是 1，将第一个线程设置成 null。结束。 if (firstReaderHoldCount == 1) firstReader = null; // 如果不是 1，减一操作 else firstReaderHoldCount--; &#125; else &#123;//如果不是当前线程 HoldCounter rh = cachedHoldCounter; // 如果缓存是 null 或者缓存所属线程不是当前线程，则当前线程不是最后一个读锁。 if (rh == null || rh.tid != getThreadId(current)) // 获取当前线程的计数器 rh = readHolds.get(); int count = rh.count; // 如果计数器小于等于一，就直接删除计数器 if (count &lt;= 1) &#123; readHolds.remove(); // 如果计数器的值小于等于0，说明有问题了，抛出异常 if (count &lt;= 0) throw unmatchedUnlockException(); &#125; // 对计数器减一 --rh.count; &#125; for (;;) &#123;// 死循环使用 CAS 修改状态 int c = getState(); // c - 65536, 其实就是减去一个读锁。对高16位减一。 int nextc = c - SHARED_UNIT; // 修改 state 状态。 if (compareAndSetState(c, nextc)) // 修改成功后，如果是 0，表示读锁和写锁都空闲，则可以唤醒后面的等待线程 return nextc == 0; &#125;&#125; 释放还是很简单的，步骤如下： 如果当前线程是第一个持有读锁的线程，则只需要操作 firstReaderHoldCount 减一。如果不是，进入第二步。 获取到缓存计数器（最后一个线程的计数器），如果匹配到当前线程，就减一。如果不匹配，进入第三步。 获取当前线程自己的计数器（由于每个线程都会多次获取到锁，所以，每个线程必须保存自己的计数器。）。 做减一操作。 死循环修改 state 变量。 锁降级重入还允许从写入锁降级为读取锁，其实现方式是：先获取写入锁，然后获取读取锁，最后释放写入锁。但是，从读取锁升级到写入锁是不可能的。 锁降级的必要性主要有两个观点： 「个人还是倾向于第一种观点」 是为了保证数据的可见性，类似于volatile，只有写锁降级才能保证我读到的数据是我这次写数据后的数据，否则就可能发生其他线程先抢写锁，导致读到的数据并不是我这个线程写后的数据，当前线程无法感知数据更新。 特殊的写锁重入机制，JDK 使用 先获取写入锁，然后获取读取锁，最后释放写入锁 这个步骤，是为了提高获取锁的效率。尤其是读锁的效率，不需要等到写锁释放，也无需跟写锁竞争共享资源。 Condition 源码部分讲的很好，分析的很透彻 简介部分写的还行，但是源码分析的非常糟糕 简介任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、wait(long timeout)、notify()以及notifyAll()方法，这些方法与synchronized同步关键字配合，可以实现等待/通知模式。Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式，但是这两者在使用方式以及功能特性上还是有差别的。 Q：为什么 wait/notify 等方法定义在 Object 方法内？ A：Java中所有的类和对象逻辑上都对应有一个锁和监视器，也就是说在Java中一切对象都可以用来线程的同步、所以这些管程（监视器）的“过程”方法定义在Object中一点也不奇怪。 一句话归纳，就是 Lock 搭配 Condition(await/signal)，Synchronized(监视器) 搭配 wait/notify。 从整体上来看Object的wait和notify/notify是与对象监视器配合完成线程间的等待/通知机制，而Condition与Lock配合完成等待通知机制，前者是java底层级别的，后者是语言级别的，具有更高的可控制性和扩展性。两者除了在使用方式上不同外，在功能特性上还是有很多的不同： 前置条件不同，Objec方式的对象监视器需要获得锁，而 Condition 的前提是要拿到 Lock； Condition能够支持不响应中断，而通过使用Object方式不支持； Condition能够支持多个等待队列（new 多个Condition对象），而Object方式只能支持一个； Condition能够支持超时时间的设置，而Object不支持。 参照Object的wait和notify/notifyAll方法，Condition也提供了同样的方法： 针对Object的wait方法 void await() throws InterruptedException:当前线程进入等待状态，如果其他线程调用condition的signal或者signalAll方法并且当前线程获取Lock从await方法返回，如果在等待状态中被中断会抛出被中断异常； long awaitNanos(long nanosTimeout)：当前线程进入等待状态直到被通知，中断或者超时； boolean await(long time, TimeUnit unit)throws InterruptedException：同第二种，支持自定义时间单位 boolean awaitUntil(Date deadline) throws InterruptedException：当前线程进入等待状态直到被通知，中断或者到了某个时间 针对Object的notify/notifyAll方法 void signal()：唤醒一个等待在condition上的线程，将该线程从等待队列中转移到同步队列中，如果在同步队列中能够竞争到Lock则可以从等待方法中返回。 void signalAll()：与1的区别在于能够唤醒所有等待在condition上的线程 源码分析创建一个 Condition 对象，是通过 lock.newCondition()，new出来一个 ConditionObject 对象，而这个恰好又是 AQS 的内部类，前面我们说过，condition是要和lock配合使用的，也就是condition和lock是绑定在一起的，而lock的实现原理又依赖于AQS，自然而然 ConditionObject 作为AQS的一个内部类无可厚非。 我们在 AQS 讲过，nextWaiter 既可以用来标志 Condition 队列中的下一个，又可以标志是独占锁/共享锁，很明显从这里就可以看出等待队列是一个单向队列，不同于同步队列是一个双向队列，并且等待队列并没有头结点，不像同步队列是有一个头结点的。还有一点需要再强调一次，对象Object对象监视器上只能拥有一个同步队列和一个等待队列，而并发包中的Lock拥有一个同步队列和多个等待队列。 注： 图中的条件队列即等待队列。条件队列(等待队列)的属性如下，用头尾指针控制整个队列。 12345678910private static final long serialVersionUID = 1173984872572414699L;/** First node of condition queue. */private transient Node firstWaiter; // 条件队列的头节点/** Last node of condition queue. */ private transient Node lastWaiter; // 条件队列的尾节点 /** * Creates a new &#123;@code ConditionObject&#125; instance. */public ConditionObject() &#123; &#125; Condition的实现主要包括：条件队列、等待和通知。其中条件队列放的是AQS里的Node数据结构，使用nextWaiter来维护条件队列。等待和通知共有7个方法。 12345678signal() //唤醒该条件队列的头节点。signalAll() //唤醒该条件队列的所有节点。awaitUninterruptibly() //等待，此方法无法被中断，必须通过唤醒才能解除阻塞。await() //当前线程进入等待。awaitNanos(long) //当前线程进入等待，有超时时间，入参的单位为纳秒。awaitUntil(Date) //当先线程进入等待，直到当前时间超过入参的时间。await(long, TimeUnit) //当前线程进入等待，有超时时间，入参可以自己设置时间单位。 这些方法其实大同小异，因此本文只对常用的signal()、signalAll()和await()方法展开详解。搞懂了这3个方法，搞懂其他几个方法也基本没什么阻碍。 await()123456789101112131415161718public final void await() throws InterruptedException &#123; // 阻塞当前线程，直接被唤醒或被中断 if (Thread.interrupted()) // 如果当前线程被中断过，则抛出中断异常 throw new InterruptedException(); Node node = addConditionWaiter(); // 添加一个waitStatus为CONDITION的节点到条件队列尾部 int savedState = fullyRelease(node); // 释放操作。我们知道只有在拥有锁（acquire成功）的时候才能调用await()方法，因此，调用await()方法的线程的节点必然是同步队列的头节点。所以，当调用await()方法时，相当于同步队列的首节点（获取了锁的节点）移动到Condition的条件队列中。 int interruptMode = 0; // 0为正常，被中断值为THROW_IE或REINTERRUPT while (!isOnSyncQueue(node)) &#123; // isOnSyncQueue：判断node是否在同步队列（注意和条件队列区分。调用signal方法会将节点从条件队列移动到同步队列，因此这边就可以跳出while循环） LockSupport.park(this); // node如果不在同步队列则进行park（阻塞当前线程） if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) // 检查线程被唤醒是否是因为被中断，如果是则跳出循环，否则会进行下一次循环，因为被唤醒前提是进入同步队列，所以下一次循环也必然会跳出循环 break; &#125; if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) // acquireQueued返回true代表被中断过，如果中断模式不是THROW_IE，则必然为REINTERRUPT（见上面的checkInterruptWhileWaiting方法） interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); // 移除waitStatus为CANCELLED的节点 if (interruptMode != 0) // 如果跳出while循环是因为被中断 reportInterruptAfterWait(interruptMode); // 则根据interruptMode，选择抛出InterruptedException 或 重新中断当前线程&#125; 总结一下整体的流程： 判断线程是否被中断，是的话就抛出异常； 然后将给线程包装成节点，且 WaitStatus == Condition，这个由 addConditionWaiter() 完成，下面会讲； 然后将同步队列中的节点删除，其实就相当于是将同步队列的头结点加入到条件队列中的尾结点，这个是由fullRelease() 完成的； 然后判断线程是否在同步队列中，这个使用的是 isOnSyncQueue()，第一次肯定是已经不在了，因为调用了 fullRelease() 已经将其从同步队列中删除了，进入循环后，就会阻塞该线程，如何退出呢，那就需要 signal了； 当调用了 signal 后，程序会跳出循环，此时 signal 函数已经将该节点从条件队列加回至同步队列中了，我们只需要调用 acquireQueued() 尝试获取锁就可以了。 接下来，讲一下刚才用到的几个方法： addConditionWaiter()123456789101112131415private Node addConditionWaiter() &#123; // 添加一个waitStatus为CONDITION的节点到条件队列尾部 Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); // 移除waitStatus不为CONDITION的节点（条件队列里的节点waitStatus都为CONDITION） t = lastWaiter; // 将t赋值为移除了waitStatus不为CONDITION后的尾节点（上面进行了移除操作，因此尾节点可能会发生变化） &#125; Node node = new Node(Thread.currentThread(), Node.CONDITION); // 以当前线程新建一个waitStatus为CONDITION的节点 if (t == null) // t为空，代表条件队列为空 firstWaiter = node; // 将头节点赋值为node else t.nextWaiter = node; // 否则，队列不为空。将t（原尾节点）的后继节点赋值为node lastWaiter = node; // 将node赋值给尾节点，即将node放到条件队列的尾部。这里没有用CAS来保证原子性，原因在于调用await()方法的线程必定是获取了锁的线程，也就是说该过程是由锁来保证线程安全的 return node;&#125; 这个函数的主体功能就是添加一个waitStatus为CONDITION的节点到条件队列尾部，具体实现步骤是： 如果条件队列的尾节点不为null并且waitStatus不为CONDITION，则调用unlinkCancelledWaiters方法（详解见下文unlinkCancelledWaiters方法）移除waitStatus不为CONDITION的节点（条件队列里的节点waitStatus都为CONDITION），并将t赋值为移除了waitStatus不为CONDITION后的尾节点（上面进行了移除操作，因此尾节点可能会发生变化）； 新建一个节点，存储当前线程； 添加到队尾，注意这里不需要使用 CAS。 12345678910111213141516171819private void unlinkCancelledWaiters() &#123; // 从条件队列移除所有waitStatus不为CONDITION的节点 Node t = firstWaiter; // t赋值为条件队列的尾节点 Node trail = null; while (t != null) &#123; Node next = t.nextWaiter; // 向下遍历 if (t.waitStatus != Node.CONDITION) &#123; // 如果t的waitStatus不为CONDITION t.nextWaiter = null; // 断开t与t后继节点的关联 if (trail == null) // 如果trail为null，则将firstWaiter赋值为next节点，此时还没有遍历到waitStatus为CONDITION的节点，因此直接移动firstWaiter的指针即可移除前面的节点 firstWaiter = next; else trail.nextWaiter = next; // 否则将trail的后继节点设为next节点。此时，trail节点到next节点中的所有节点被移除（包括t节点，但可能不止t节点。因为，trail始终指向遍历过的最后一个waitStatus为CONDITION，因此只需要将trail的后继节点设置为next，即可将trail之后到next之前的所有节点移除） if (next == null) lastWaiter = trail; &#125; else trail = t; // 如果t的waitStatus为CONDITION，则将trail赋值为t，trail始终指向遍历过的最后一个waitStatus为CONDITION t = next; // t指向下一个节点 &#125;&#125; fullRelease()主体也就是 AQS 的 release 方法，将其从同步队列中删除。 123456789101112131415final int fullyRelease(Node node) &#123; // 释放锁 boolean failed = true; try &#123; int savedState = getState(); // 当前的同步状态 if (release(savedState)) &#123; // 独占模式下release（一般指释放锁） failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; // 如果release失败则将该节点的waitStatus设置为CANCELLED &#125;&#125; isOnSyncQueue()12345678910111213141516171819final boolean isOnSyncQueue(Node node) &#123; // 判断node是否再同步队列中 if (node.waitStatus == Node.CONDITION || node.prev == null) // 如果waitStatus为CONDITION 或 node没有前驱节点，则必然不在同步队列，直接返回false return false; if (node.next != null) // 如果有后继节点，必然是在同步队列中，返回true return true; return findNodeFromTail(node); // 返回node是否为同步队列节点，如果是返回true，否则返回false&#125;// 从同步队列的尾节点开始向前遍历，如果node为同步队列节点则返回true，否则返回falseprivate boolean findNodeFromTail(Node node) &#123; Node t = tail; for (;;) &#123; if (t == node) return true; if (t == null) return false; t = t.prev; &#125; signal()1234567public final void signal() &#123; if (!isHeldExclusively()) // 检查当前线程是否为独占模式同步器的所有者 throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignal(first); // 唤醒条件队列的头节点&#125; 因为条件队列，即等待队列也是 FIFO，头结点必然是等的最久的一个，所以每次唤醒都是唤醒头结点。 具体步骤： 检查当前线程是否为独占模式同步器的所有者，在ReentrantLock中即检查当前线程是否为拥有锁的线程。如果不是，则抛IllegalMonitorStateException。 拿到条件队列的头节点，如果不为null，则调用doSignal方法（详解见下文doSignal方法）唤醒头节点。 12345678910111213141516171819202122232425private void doSignal(Node first) &#123; // 将条件队列的头节点移到同步队列 do &#123; if ( (firstWaiter = first.nextWaiter) == null) // 将first节点赋值为first节点的后继节点（相当于移除first节点），如果first节点的后继节点为空，则将lastWaiter赋值为null lastWaiter = null; first.nextWaiter = null; // 断开first节点对first节点后继节点的关联 &#125; while (!transferForSignal(first) &amp;&amp; // transferForSignal：将first节点从条件队列移动到同步队列 (first = firstWaiter) != null); // 如果transferForSignal失败，并且first节点不为null，则向下遍历条件队列的节点，直到节点成功移动到同步队列 或者 firstWaiter为null&#125;// 将node节点从条件队列移动到同步队列，如果成功则返回true。final boolean transferForSignal(Node node) &#123; if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) // 如果不能更改节点的waitStatus，则表示该节点已被取消，返回false return false; // 否则，调用enq方法将node添加到同步队列，注意：enq方法返回的节点是node的前驱节点 Node p = enq(node); int ws = p.waitStatus; // 将ws赋值为node前驱节点的等待状态 if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) // 如果node前驱节点的状态为CANCELLED（ws&gt;0） //或 使用CAS将waitStatus修改成SIGNAL失败，则代表node的前驱节点无法来唤醒node节点，因此直接调用LockSupport.unpark方法唤醒node节点 // 注意，unparkSuccessor(node)是唤醒该节点的下一个节点，而LockSupport.lock则是直接唤醒该节点 LockSupport.unpark(node.thread); return true;&#125; signalAll()1234567public final void signalAll() &#123; if (!isHeldExclusively()) // 检查当前线程是否为独占模式同步器的所有者 throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) doSignalAll(first); // 唤醒条件队列的所有节点&#125; 123456789private void doSignalAll(Node first) &#123; // 将条件队列的所有节点移到同步队列 lastWaiter = firstWaiter = null; // 因为要移除条件队列的所有节点到同步队列，因此这边直接将firstWaiter和lastWaiter赋值为null do &#123; Node next = first.nextWaiter; // next赋值为first节点的后继节点 first.nextWaiter = null; // 断开first节点对first节点后继节点的关联 transferForSignal(first); // transferForSignal：将first节点从条件队列移动到同步队列 first = next; // first赋值为next节点 &#125; while (first != null); // 循环遍历，将条件队列的所有节点移动到同步队列&#125; 至此，Condition 就分析完了，我们可以用 Condition 写一个 生产者-消费者的demo： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package 多线程.Producer_Consumer;import java.util.LinkedList;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class await_signal &#123; private final static Lock lock = new ReentrantLock(); private final static Condition producer_condition = lock.newCondition(); private final static Condition consumer_condition = lock.newCondition(); private final static LinkedList&lt;Long&gt; linkedList = new LinkedList&lt;&gt;(); private final static int MAX_CAPACITY = 100; public static void main(String[] args) &#123; new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; while (true) &#123; await_signal.producer(); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 14; i++) &#123; while (true) &#123; await_signal.consumer(); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;).start(); &#125; private static void producer() &#123; try &#123; lock.lock(); if (linkedList.size() &gt; MAX_CAPACITY) &#123; producer_condition.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; long value = System.currentTimeMillis(); System.out.println(Thread.currentThread().getName() + ":-------PRODUCER-------- " + value); linkedList.addLast(value); consumer_condition.signalAll(); lock.unlock(); &#125; &#125; private static void consumer() &#123; try &#123; lock.lock(); if (linkedList.size() == 0) &#123; consumer_condition.await(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; long value = linkedList.pop(); System.out.println(Thread.currentThread().getName() + "::-------CONSUMER-------- " + value); producer_condition.signalAll(); lock.unlock(); &#125; &#125;&#125; LockSupport &amp; UnsafeLockSupport.park() vs Object.wait() vs Thread.sleep() 从释放资源看，三者在阻塞线程后，LockSupport.park() 会阻塞线程并且不会释放锁资源，注意，本身 park() 是不会释放资源的，我们在同步队列中中就是用 park() 来使得线程挂起（wait状态），等待资源。Object.wait() （也是wait状态）会阻塞当前线程，并且让出时间片和锁资源，等待被 notify 才获得 monitor，拿到锁资源。Thread.sleep() 会让线程sleep（sleep状态），同样阻塞当前线程，但是不会让出锁资源； 使用的前提条件看。Object.wait() 必须在 Synchronized 里用，LockSupport.park() 可以在任何地方用，Thread.sleep() 当然也是地方都能用； 从中断异常看。LockSupport.park() 不需要捕获中断异常，注意，它不需要捕获，并且它会从中断中醒来，将线程由 interrupted —&gt; running，所以外部调用时需要谨慎，这里提醒一下，尤其是在 StampedLock 中使用的时候，如果 writeLock 一直在使用锁资源，而readLock在等待锁资源后开始挂起，突然将readLock线程中断的话，因为 StampedLock.readLock() 不处理中断，所以直接将线程由 wait –&gt; running，此时线程会一直自旋获取锁，但是由于锁一直被 writeLock，此时 CPU就会爆炸。Object.wait() 声明抛出了中断异常，调用者需要捕获或者再抛出，Thread.sleep()方法声明上也是抛出了InterruptedException中断异常，所以调用者需要捕获这个异常或者再抛出； 唤醒方式看。LockSupport.park() 可以通过 LockSupport.unpark() 或者中断唤醒，Object.wait() 则需要调用 另一个线程执行 notify() 进行唤醒，Thread.sleep() 只能自己醒过来； 补充：如果在wait()之前执行了notify()会怎样？抛出IllegalMonitorStateException异常； 如果在park()之前执行了unpark()会怎样？线程不会被阻塞，直接跳过park()，继续执行后续内容。 简介LockSupport下的很多方法，例如我们之前常常提及的 LockSupport.park()、LockSupport.unpark() 都是用了 Unsafe 类下的 park() 和 unpark() 方法。 123456public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); &#125; 1234private static void setBlocker(Thread t, Object arg) &#123; // Even though volatile, hotspot doesn't need a write barrier here. UNSAFE.putObject(t, parkBlockerOffset, arg);&#125; 12345public static Object getBlocker(Thread t) &#123; if (t == null) throw new NullPointerException(); return UNSAFE.getObjectVolatile(t, parkBlockerOffset);&#125; 可以看到，基本上每个方法的底层都是使用了 Unsafe 类的方法，所以接下来主要分析一下 Unsafe。 Unsafe简介 https://tech.meituan.com/2019/02/14/talk-about-java-magic-class-unsafe.html 属于 sun.misc.Unsafe。 采用饿汉式的单例模式，但Unsafe类做了限制，如果是普通的调用的话，它会抛出一个SecurityException异常；只有由主类加载器加载的类才能调用这个方法。 1234567891011121314151617private static native void registerNatives();static &#123; registerNatives(); sun.reflect.Reflection.registerMethodsToFilter(Unsafe.class, "getUnsafe");&#125;private Unsafe() &#123;&#125;private static final Unsafe theUnsafe = new Unsafe();@CallerSensitivepublic static Unsafe getUnsafe() &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); if (!VM.isSystemDomainLoader(caller.getClassLoader())) throw new SecurityException("Unsafe"); return theUnsafe;&#125; 功能介绍 如上图所示，Unsafe提供的API大致可分为内存操作、CAS、Class相关、对象操作、线程调度、系统信息获取、内存屏障、数组操作等几类，下面将对其相关方法和应用场景进行详细介绍。 内存操作主要是管控堆外的一些内存操作。 123456789101112131415161718//分配内存, 相当于C++的malloc函数public native long allocateMemory(long bytes);//扩充内存public native long reallocateMemory(long address, long bytes);//释放内存public native void freeMemory(long address);//在给定的内存块中设置值public native void setMemory(Object o, long offset, long bytes, byte value);//内存拷贝public native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);//获取给定地址值，忽略修饰限定符的访问限制。与此类似操作还有: getInt，getDouble，getLong，getChar等public native Object getObject(Object o, long offset);//为给定地址设置值，忽略修饰限定符的访问限制，与此类似操作还有: putInt,putDouble，putLong，putChar等public native void putObject(Object o, long offset, Object x);//获取给定地址的byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果为确定的）public native byte getByte(long address);//为给定地址设置byte类型的值（当且仅当该内存地址为allocateMemory分配时，此方法结果才是确定的）public native void putByte(long address, byte x); 通常，我们在Java中创建的对象都处于堆内内存（heap）中，堆内内存是由JVM所管控的Java进程内存，并且它们遵循JVM的内存管理机制，JVM会采用垃圾回收机制统一管理堆内存。与之相对的是堆外内存，存在于JVM管控之外的内存区域，Java中对堆外内存的操作，依赖于Unsafe提供的操作堆外内存的native方法。 使用堆外内存的原因 对垃圾回收停顿的改善。由于堆外内存是直接受操作系统管理而不是JVM，所以当我们使用堆外内存时，即可保持较小的堆内内存规模。从而在GC时减少回收停顿对于应用的影响。 提升程序I/O操作的性能。通常在I/O通信过程中，会存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间数据拷贝且生命周期较短的暂存数据，都建议存储到堆外内存。 典型应用DirectByteBuffer是Java用于实现堆外内存的一个重要类，通常用在通信过程中做缓冲池，如在Netty、MINA等NIO框架中应用广泛。DirectByteBuffer对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现。 下图为DirectByteBuffer构造函数，创建DirectByteBuffer的时候，通过Unsafe.allocateMemory分配内存、Unsafe.setMemory进行内存初始化，而后构建Cleaner对象用于跟踪DirectByteBuffer对象的垃圾回收，以实现当DirectByteBuffer被垃圾回收时，分配的堆外内存一起被释放。 那么如何通过构建垃圾回收追踪对象Cleaner实现堆外内存释放呢？ Cleaner继承自Java四大引用类型之一的虚引用PhantomReference（众所周知，无法通过虚引用获取与之关联的对象实例，且当对象仅被虚引用引用时，在任何发生GC的时候，其均可被回收），通常PhantomReference与引用队列ReferenceQueue结合使用，可以实现虚引用关联对象被垃圾回收时能够进行系统通知、资源清理等功能。如下图所示，当某个被Cleaner引用的对象将被回收时，JVM垃圾收集器会将此对象的引用放入到对象引用中的pending链表中，等待Reference-Handler进行相关处理。其中，Reference-Handler为一个拥有最高优先级的守护线程，会循环不断的处理pending链表中的对象引用，执行Cleaner的clean方法进行相关清理工作。 Tip: 这块基本不懂啊……等复习完多线程得恶补一下 JVM 的相关知识了… CAS 相关12345678910111213/** * CAS * @param o 包含要修改field的对象 * @param offset 对象中某field的偏移量 * @param expected 期望值 * @param update 更新值 * @return true | false */public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object update);public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update); public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update); 什么是CAS? 即比较并替换，实现并发算法时常用到的一种技术。CAS操作包含三个操作数——内存位置、预期原值及新值。执行CAS操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。我们都知道，CAS是一条CPU的原子指令（cmpxchg指令），不会造成所谓的数据不一致问题，Unsafe提供的CAS方法（如compareAndSwapXXX）底层实现即为CPU指令cmpxchg。 线程调度1234567891011121314//取消阻塞线程public native void unpark(Object thread);//阻塞线程public native void park(boolean isAbsolute, long time);//获得对象锁（可重入锁）@Deprecatedpublic native void monitorEnter(Object o);//释放对象锁@Deprecatedpublic native void monitorExit(Object o);//尝试获取对象锁@Deprecatedpublic native boolean tryMonitorEnter(Object o); 如上源码说明中，方法park、unpark即可实现线程的挂起与恢复，将一个线程进行挂起是通过park方法实现的，调用park方法后，线程将一直阻塞直到超时或者中断等条件出现；unpark可以终止一个挂起的线程，使其恢复正常。 Java锁和同步器框架的核心类AbstractQueuedSynchronizer，就是通过调用LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒的，而LockSupport的park、unpark方法实际是调用Unsafe的park、unpark方式来实现。 Class相关此部分主要提供Class和它的静态字段的操作相关方法，包含静态字段内存定位、定义类、定义匿名类、检验&amp;确保初始化等。 123456789101112//获取给定静态字段的内存地址偏移量，这个值对于给定的字段是唯一且固定不变的public native long staticFieldOffset(Field f);//获取一个静态类中给定字段的对象指针public native Object staticFieldBase(Field f);//判断是否需要初始化一个类，通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。 当且仅当ensureClassInitialized方法不生效时返回false。public native boolean shouldBeInitialized(Class&lt;?&gt; c);//检测给定的类是否已经初始化。通常在获取一个类的静态属性的时候（因为一个类如果没初始化，它的静态属性也不会初始化）使用。public native void ensureClassInitialized(Class&lt;?&gt; c);//定义一个类，此方法会跳过JVM的所有安全检查，默认情况下，ClassLoader（类加载器）和ProtectionDomain（保护域）实例来源于调用者public native Class&lt;?&gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain);//定义一个匿名类public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; hostClass, byte[] data, Object[] cpPatches); 对象操作此部分主要包含对象成员属性相关操作及非常规的对象实例化方式等相关方法。 1234567891011121314//返回对象成员属性在内存地址相对于此对象的内存地址的偏移量public native long objectFieldOffset(Field f);//获得给定对象的指定地址偏移量的值，与此类似操作还有：getInt，getDouble，getLong，getChar等public native Object getObject(Object o, long offset);//给定对象的指定地址偏移量设值，与此类似操作还有：putInt，putDouble，putLong，putChar等public native void putObject(Object o, long offset, Object x);//从对象的指定偏移量处获取变量的引用，使用volatile的加载语义public native Object getObjectVolatile(Object o, long offset);//存储变量的引用到对象的指定的偏移量处，使用volatile的存储语义public native void putObjectVolatile(Object o, long offset, Object x);//有序、延迟版本的putObjectVolatile方法，不保证值的改变被其他线程立即看到。只有在field被volatile修饰符修饰时有效public native void putOrderedObject(Object o, long offset, Object x);//绕过构造方法、初始化代码来创建对象public native Object allocateInstance(Class&lt;?&gt; cls) throws InstantiationException; 常规对象实例化方式：我们通常所用到的创建对象的方式，从本质上来讲，都是通过new机制来实现对象的创建。但是，new机制有个特点就是当类只提供有参的构造函数且无显示声明无参构造函数时，则必须使用有参构造函数进行对象构造，而使用有参构造函数时，必须传递相应个数的参数才能完成对象实例化。 非常规的实例化方式：而Unsafe中提供allocateInstance方法，仅通过Class对象就可以创建此类的实例对象，而且不需要调用其构造函数、初始化代码、JVM安全检查等。它抑制修饰符检测，也就是即使构造器是private修饰的也能通过此方法实例化，只需提类对象即可创建相应的对象。由于这种特性，allocateInstance在java.lang.invoke、Objenesis（提供绕过类构造器的对象生成方式）、Gson（反序列化时用到）中都有相应的应用。 数组相关这部分主要介绍与数据操作相关的arrayBaseOffset与arrayIndexScale这两个方法，两者配合起来使用，即可定位数组中每个元素在内存中的位置。 1234//返回数组中第一个元素的偏移地址public native int arrayBaseOffset(Class&lt;?&gt; arrayClass);//返回数组中一个元素占用的大小public native int arrayIndexScale(Class&lt;?&gt; arrayClass); 这两个与数据操作相关的方法，在java.util.concurrent.atomic 包下的AtomicIntegerArray（可以实现对Integer数组中每个元素的原子性操作）中有典型的应用，如下图AtomicIntegerArray源码所示，通过Unsafe的arrayBaseOffset、arrayIndexScale分别获取数组首元素的偏移地址base及单个元素大小因子scale。 内存屏障在Java 8中引入，用于定义内存屏障（也称内存栅栏，内存栅障，屏障指令等，是一类同步屏障指令，是CPU或编译器在对内存随机访问的操作中的一个同步点，使得此点之前的所有读写操作都执行后才可以开始执行此点之后的操作），避免代码重排序。 在Java 8中引入了一种锁的新机制——StampedLock，它可以看成是读写锁的一个改进版本。StampedLock提供了一种乐观读锁的实现，这种乐观读锁类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少时写线程“饥饿”现象。由于StampedLock提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存load到线程工作内存时，会存在数据不一致问题，所以当使用StampedLock的乐观读锁时，需要遵从如下图用例中使用的模式来确保数据的一致性。 系统相关这部分包含两个获取系统相关信息的方法。 1234//返回系统指针的大小。返回值为4（32位系统）或 8（64位系统）。public native int addressSize(); //内存页的大小，此值为2的幂次方。public native int pageSize(); 各种锁复习 各种锁的对比 偏向锁、轻量级锁、自旋锁、重量级锁 并发编程之锁优化]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
        <tag>Synchronized</tag>
        <tag>Volatile</tag>
        <tag>AQS</tag>
        <tag>ReentrantLock</tag>
        <tag>ReentrantReadWriteLock</tag>
        <tag>Condition</tag>
        <tag>LockSupport</tag>
        <tag>Unsafe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法题总结]]></title>
    <url>%2F2020%2F01%2F26%2F%E9%83%A8%E5%88%86%E5%81%9A%E9%A2%98%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[在线 oj 输入 12345678910import java.util.Scanner; public class Main&#123; public static void main(String[] args)&#123; Scanner in = new Scanner(System.in); String s =in.nextLine(); String[] str = s.spilt(" "); for(String data : str) System.out.println(data); &#125;&#125; 更具体一点的：https://blog.csdn.net/da_kao_la/article/details/80225003?utm_source=blogxgwz2 数组都是arr.length，字符串是str.length()，列表是list.size() 字符串 ——&gt; 数组，可以有 str.toCharArray()，然后数组 —–&gt; 字符串 是 new String(arr)，注意如果想要直接打印数组内容则是 Arrays.toString(arr)，打印多维数组是 Arrays.deeptoString() String 不能直接变化，但是可以用StringBuilder 123StringBuilder sb = new StringBuilder(str);sb.replace(1,2,"aa");return sb.toString(); 不管是数组转换成集合，还是集合转换成数组，都要注意转换类型的一致性，String[]数组转String类型的集合，当需要使用int，double等集合的时候，需要使用对应的对象 如：数组int[]用Integer[]，double[]用Double[] ,因为List集合是对象的集合，而int、double等不是对象，所以需要用字段的对应对象类 数组直接转成列表 1234// 注意这里必须是 Integer对象的数组，否则转不了Integer[] nums = &#123;1,2,3&#125;;ArrayList arr= new ArrayList&lt;&gt;(nums.length);Collections.addAll(arr,nums); 12// 好像也可以，但是跟上面一样，得是 String、Integer类型的数组Arrays.asList(arr); 如果是int，需要装箱再转才可以 123int[] a = &#123;1,2,3,4&#125;;List arr = Arrays.stream(a).boxed().collect(Collectors.toList());System.out.println(arr); 列表转数组 采用集合的toArray()方法直接把List集合转换成数组，这里需要注意，不能这样写： 1String[] array = (String[]) mlist.toArray(); 这样写的话，编译运行时会报类型无法转换java.lang.ClassCastException的错误，这是为何呢，这样写看起来没有问题啊 因为java中的强制类型转换是针对单个对象才有效果的，而List是多对象的集合，所以将整个List强制转换是不行的 。正确的写法应该是这样的： 1String[] array = mlist.toArray(new String[0]); 12345List&lt;int[]&gt; res = new ArrayList();res.add(new int[]&#123;1,2&#125;);res.add(new int[]&#123;3,4&#125;);System.out.println(res);int[][] a = res.toArray(new int[0][]); 123456List&lt;Integer&gt; res = new ArrayList();res.add(1);res.add(2);System.out.println(res);Integer[] a = res.toArray(new Integer[0]);System.out.println(Arrays.toString(a)); 哈希表中统计数出现的次数 123456789class Solution &#123; public void majorityElement(int[] nums) &#123; Map&lt;Integer, Integer&gt; numCount = new HashMap&lt;&gt;(); for (int num : nums) &#123; int count = numCount.getOrDefault(num, 0) + 1; numCount.put(num, count); &#125; &#125;&#125; 对哈希map中的 value 值升序排列 123456789List&lt;Map.Entry&lt;Integer, Integer&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;Integer, Integer&gt;&gt;(hashMap.entrySet());Collections.sort(list, (o1, o2) -&gt; &#123; int compare = (o1.getValue()).compareTo(o2.getValue()); return compare;&#125;);Map&lt;Integer, Integer&gt; returnMap = new LinkedHashMap&lt;Integer, Integer&gt;();for (Map.Entry&lt;Integer, Integer&gt; entry : list) &#123; returnMap.put(entry.getKey(), entry.getValue());&#125; 数字转字符，字符转数字 12345678910// 数字转字符 方式一String str = "1230";int d = Integer.parseInt(str); //静态函数直接通过类名调用// 数字转字符 方式二int d3 = Integer.valueOf("1230");System.out.println("digit3: " + d3);// 字符转数字String s = String.valueOf(d);]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>小tricks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树]]></title>
    <url>%2F2020%2F01%2F16%2F%E7%BA%A2%E9%BB%91%E6%A0%91.html</url>
    <content type="text"><![CDATA[红黑树特点 每个节点或者是红色的，或者是黑色的； 根节点是黑色的； 每个叶子结点（最后的空节点）是黑色的； 如果一个节点是红色的，那么他的孩子都是黑色的 ；[红色不能连着] 从任意一个节点到叶子节点经过的黑色节点是一样的。 增加操作 图中 C 为当前需要插入的节点，P 为当前插入节点的双亲结点，U 为当前插入节点的叔叔节点，G 为当前插入节点的祖父节点！增加节点就是这四种情况了，我们只需要考虑三代节点，还是非常简单的… 这里推荐一个网站，可以试验一下红黑树的增加删除，感觉贼爽啊！！！ (其实是我不想画图找例子了…自己去试验吧..这四种情况的方法屡试不爽) cs.usfca.edu/~galles/visualization/RedBlack.html 话说，咱中文的博客真的好多人都不太懂这个红黑树，各种错误的例子就往上贴，其实本来是很简单的知识点，把我硬是整迷糊了…真是误人子弟啊…好气啊！ 删除操作删除是红黑树中最难的部分！但是其实跟拧魔方是一样哒！只要掌握了技巧，管你是什么，套公式就完事儿了！ [维基百科写的啥玩意儿，我的智商受到了压制…] 想要删除一个节点，首先就要考虑它有几个孩子： 若想要删除的节点，我这里记为 N，若 N 有两个孩子，则我们可以找 N 的左子树最大值 或者是 N 的右子树最小值，将其值复制到 N ,然后 不删除 N 这个节点，转而去删除被复制的那个节点(N 的左子树最大值 或者是 N 的右子树最小值所在的节点)，为什么要这样转换呢？因为我们知道，N 的左子树最大值 或者是 N 的右子树最小值最多也就一个儿子(我这里说的儿子是不包括叶子结点[叶子结点全部是黑色空节点哦，这是红黑树的性质3]的哈~) 所以我们现在只需要考虑删除 只有一个儿子或者没有儿子的节点 看上图，其实也就剩下3种情况，分别是 需删除节点为黑色，有一个颜色为红色的孩子 需删除节点为红色，且没有儿子 需删除节点为黑色，且没有儿子 前面两种情况是很好处理的，最难的就是第三种情况，需删除节点为黑色，且没有儿子，接下来我们就只需要考虑这种情况！！！ 这种情况下，只需要考虑四个节点，分别是 父亲节点 P、兄弟节点 S、S 的左孩子 S1、S 的右孩子 S2，按理说4个节点，颜色排列组合，可以排成16种，但是有些是不符合红黑树性质的组合，现在我们来一一排列一下。 粗略看一下，总共是有9种情况的！在文末会给出我自己总结的这9种分别对应的操作方式，大家可以直接看文末！！！ 一道例题 要删除的节点是黑色节点，且儿子为叶子结点 如题，要删除的节点是40，其父亲是55，黑色节点，记为P；其兄弟是65，红色节点，记为S，其侄子是60，75，黑色节点。 如图，这是删除了40之后的图，其儿子—-叶子结点变为x，现在不满足性质5，需要进行变换。 Case I：S 为 红色，P 为黑色 将 S 和 P 的颜色互换，即 S 变为黑色，P 变为红色 对父亲节点 P 进行 左旋，注意哦，因为是左旋，所以圆心是 兄弟节点 S Case II：此时又是一个新的可能出现的场景，删除节点后，P 为红色节点，而兄弟节点 S 为黑色节点，但是兄弟的左孩子为 红色，右孩子(叶子结点，当然其也只能是叶子节点，否则就违反性质5了)为黑色。 此时，将 S 变为 红色，S 的左孩子变为 黑色 然后，违法了性质 4，将 S 右旋 Case III：此时第三种情况出现了，P 为红色， S 为黑色，S 的左孩子为黑色叶子结点，S 的右孩子为红色节点 将 S 的右孩子(红色)、P(红色)、S(黑色)，颜色互换 然后将 x 现在的父亲进行左旋 维基百科部分情形1: N是新的根。在这种情形下，我们就做完了。我们从所有路径去除了一个黑色节点，而新根是黑色的，所以性质都保持着。 注意：在情形2、5和6下，我们假定N是它父亲的左儿子。如果它是右儿子，则在这些情形下的左和右应当对调。 情形2： S是红色。在这种情形下我们在N的父亲上做左旋转，把红色兄弟转换成N的祖父，我们接着对调N的父亲和祖父的颜色。完成这两个操作后，尽管所有路径上黑色节点的数目没有改变，但现在N有了一个黑色的兄弟和一个红色的父亲（它的新兄弟是黑色因为它是红色S的一个儿子），所以我们可以接下去按情形4、情形5或情形6来处理。（注意：这里的图中没有显示出来，N是删除了黑色节点后替换上来的子节点，所以这个过程中由P-&gt;X-&gt;N变成了P-&gt;N，实际上是少了一个黑色节点，也可以理解为Parent(Black)和Silbing(Red)那么他们的孩子黑色节点的数目肯定不等，让他们做新兄弟肯定是不平衡的，还需后面继续处理。这里看英文版本的[1]比较的明了） 情形3： N的父亲、S和S的儿子都是黑色的。在这种情形下，我们简单的重绘S为红色。结果是通过S的所有路径，它们就是以前不通过N的那些路径，都少了一个黑色节点。因为删除N的初始的父亲使通过N的所有路径少了一个黑色节点，这使事情都平衡了起来。但是，通过P的所有路径现在比不通过P的路径少了一个黑色节点，所以仍然违反性质5。要修正这个问题，我们要从情形1开始，在P上做重新平衡处理。 情形4： S和S的儿子都是黑色，但是N的父亲是红色。在这种情形下，我们简单的交换N的兄弟和父亲的颜色。这不影响不通过N的路径的黑色节点的数目，但是它在通过N的路径上对黑色节点数目增加了一，添补了在这些路径上删除的黑色节点。 情形5： S是黑色，S的左儿子是红色，S的右儿子是黑色，而N是它父亲的左儿子。在这种情形下我们在S上做右旋转，这样S的左儿子成为S的父亲和N的新兄弟。我们接着交换S和它的新父亲的颜色。所有路径仍有同样数目的黑色节点，但是现在N有了一个黑色兄弟，他的右儿子是红色的，所以我们进入了情形6。N和它的父亲都不受这个变换的影响。 情形6： S是黑色，S的右儿子是红色，而N是它父亲的左儿子。在这种情形下我们在N的父亲上做左旋转，这样S成为N的父亲（P）和S的右儿子的父亲。我们接着交换N的父亲和S的颜色，并使S的右儿子为黑色。子树在它的根上的仍是同样的颜色，所以性质3没有被违反。但是，N现在增加了一个黑色祖先：要么N的父亲变成黑色，要么它是黑色而S被增加为一个黑色祖父。所以，通过N的路径都增加了一个黑色节点。此时，如果一个路径不通过N，则有两种可能性：它通过N的新兄弟。那么它以前和现在都必定通过S和N的父亲，而它们只是交换了颜色。所以路径保持了同样数目的黑色节点。它通过N的新叔父，S的右儿子。那么它以前通过S、S的父亲和S的右儿子，但是现在只通过S，它被假定为它以前的父亲的颜色，和S的右儿子，它被从红色改变为黑色。合成效果是这个路径通过了同样数目的黑色节点。在任何情况下，在这些路径上的黑色节点数目都没有改变。所以我们恢复了性质4。在示意图中的白色节点可以是红色或黑色，但是在变换前后都必须指定相同的颜色。 自己总结的操作方式 总结至此，红黑树告一段落了…学了两天才真正搞明白每一步干嘛的…真是太麻烦了 建议学红黑树，先熟悉它的5个性质，然后将增加和删除的操作背一下就行…自己可以多上我推荐的那个网站，用我总结的操作方式多操作几遍，就应该没啥大问题了！至于手写红黑树代码…我选择放弃了！]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>红黑树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Map]]></title>
    <url>%2F2020%2F01%2F15%2FMap%20%26%20Set%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[HashMap参考 集合番@HashMap一文通（1.7版） 集合番@HashMap一文通（1.8版） HashMap源码分析—掘金博主 美团技术讲解HashMap 相关面试问题 问题引导由于知识点太多，我就以自己搜集到的面试常问的有关于HashMap的问题进行解决，然后再进一步深挖！ HashMap的底层数据结构？ HashMap的主要方法？ HashMap 是如何确定元素存储位置的以及如何处理哈希冲突的？ HashMap 扩容机制是怎样的？ JDK 1.8 在扩容和解决哈希冲突上对 HashMap 源码做了哪些改动？有什么好处? 什么时候会使用HashMap？他有什么特点？ 你知道HashMap的工作原理吗？ 你知道get和put的原理吗？equals()和hashCode()都有什么作用？ 你知道hash的实现吗？为什么要这样实现？ 如果HashMap的大小超过了负载因子（load factor）定义的容量，怎么办？ 你了解重新调整HashMap大小存在什么问题吗？ 为什么使用String，Interger这样的wrapper类适合作为键？ 我们可以使用自定义的对象作为键吗？ 如何对HashMap进行排序？ HashMap的删除陷阱？ 为什么只允许通过iterator进行删除操作？ 如果是遍历过程中增加或修改数据呢？ hashmap为什么初始长度为16？ HashMap中的扰动函数是什么，有什么作用，TreeMap中有吗？ HashMap为何可以插入空值？为什么它是线程不安全的？ 这20个问题的参考想法会在文章末尾给出！先一起来学习一哈HashMap的源码！ 概述 HashMap的特性 HashMap的存储结构 HashMap的重要方法—增删改查 HashMap的扩容机制 HashMap的迭代器 HashMap的Fail-Fast机制 HashMap与大家庭中其他Map的区别 接下来从这些方面拿下HashMap！！！ HashMap的特性 它根据键的 hashCode值 存储数据，大多数情况下可以直接定位到它的值，因而具有很快的访问速度，但遍历顺序却是不确定的。[只是说跟hashCode的值有关系哦！并不是直接按照hashCode()返回值存取的] HashMap最多只允许一条记录的键为null，允许多条记录的值为null。 HashMap 非线程安全，即任一时刻可以有多个线程同时写HashMap，可能会导致数据的不一致。如果需要满足线程安全，可以用 Collections 的 synchronizedMap 方法使 HashMap 具有线程安全的能力，或者使用ConcurrentHashMap。 键唯一，如果键相同，则值会被覆盖。同时若键为自定义的数据类型，则需要保证已经重写了equals() 和 hashCode()。 ——&gt; equals() 和 hashCode()详解 HashMap的存储结构JDK 1.7 版本的HashMap是以 数组 + 链表 存储的，也就是我们常说的按 链地址法 存储 常见的处理哈希冲突的方法就两种： 开放地址法 开放地址法又可以分为线性探测法、平方探测法、再散列法 拉链法 JDK 1.8 版本的HashMap是以 数组 + 链表 + 红黑树 实现的，红黑树 可能大家接触的不是很多，所以我赶紧学习了一波 传送门)，具体的实现如下图所示： 源码中的相关概念重要参数 哈希桶（buckets）：在 HashMap 的注释里使用哈希桶来形象的表示数组中每个地址位置。注意这里并不是数组本身，数组是装哈希桶的，他可以被称为哈希表。 初始容量(initial capacity) : 这个很容易理解，就是哈希表中哈希桶初始的数量。如果我们没有通过构造方法修改这个容量值默认为DEFAULT_INITIAL_CAPACITY = 1&lt;&lt;4 即16。值得注意的是为了保证 HashMap 添加和查找的高效性，HashMap 的容量总是 2^n 的形式，下文会讲为什么 HashMap 的容量总是 2^n次方。 加载因子(load factor)：加载因子是哈希表（散列表）在其容量自动增加之前被允许获得的最大数量的度量。当哈希表中的条目数量超过负载因子和当前容量的乘积时，散列表就会被重新映射（即重建内部数据结构），重新创建的散列表容量大约是之前散列表哈系统桶数量的两倍。默认加载因子（0.75）在时间和空间成本之间提供了良好的折衷。加载因子过大会导致很容易链表过长，加载因子很小又容易导致频繁的扩容。所以不要轻易试着去改变这个默认值。 扩容阈值（threshold）：其实在说加载因子的时候已经提到了扩容阈值了，扩容阈值 = 哈希表容量 * 加载因子。哈希表的键值对总数 = 所有哈希桶中所有链表节点数的加和，扩容阈值比较的是是键值对的个数而不是哈希表的数组中有多少个位置被占了。 树化阀值(TREEIFY_THRESHOLD) ：这个参数概念是在 JDK1.8后加入的，它的含义代表一个哈希桶中的节点个数大于该值（默认为8）的时候将会被转为红黑树行存储结构。 非树化阀值(UNTREEIFY_THRESHOLD)： 与树化阈值相对应，表示当一个已经转化为数形存储结构的哈希桶中节点数量小于该值（默认为 6）的时候将再次改为单链表的格式存储。导致这种操作的原因可能有删除节点或者扩容。 最小树化容量(MIN_TREEIFY_CAPACITY): 经过上边的介绍我们只知道，当链表的节点数超过8的时候就会转化为树化存储，其实对于转化还有一个要求就是哈希表的数量超过最小树化容量的要求（默认要求是 64）,且为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLD);在达到该要求之前优先选择扩容。扩容因为因为容量的变化可能会使单链表的长度改变。 与这个几个概念对应的在 HashMap 中几个常亮量，由于上边的介绍比较详细了，下边仅列出几个变量的声明： 1234567891011121314151617/*默认初始容量*/static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/*最大存储容量*/static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/*默认加载因子*/static final float DEFAULT_LOAD_FACTOR = 0.75f;/*默认树化阈值*/static final int TREEIFY_THRESHOLD = 8;/*默认非树化阈值*/static final int UNTREEIFY_THRESHOLD = 6;/*默认最小树化容量*/static final int MIN_TREEIFY_CAPACITY = 64; 对应的还有几个全局变量： 1234567891011121314// 扩容阈值 = 容量 x 加载因子int threshold;//存储哈希桶的数组，哈希桶中装的是一个单链表或一颗红黑树，长度一定是 2^ntransient Node&lt;K,V&gt;[] table; // HashMap中存储的键值对的数量注意这里是键值对的个数而不是数组的长度transient int size; //所有键值对的Set集合 区分于 table 可以调用 entrySet(）得到该集合transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; //操作数记录 为了多线程操作时 Fast-fail 机制transient int modCount; 作者：像一只狗链接：https://juejin.im/post/5ac83fa35188255c5668afd0来源：掘金 基本存储单元Node12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) * 1.8中将Entry改成Node（内部结构不变） * 虽然只是改了名字，但名字的变更体现出HashMap对于节点概念的重视 */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;//哈希值,新增final属性，表明hash值也不可变了，更加严谨 final K key;//key V value;//value Node&lt;K,V&gt; next;//链表后置节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; //每一个节点的hash值，是将 key 的 hashCode 和 value 的 hashCode 异或得到的。 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; //设置新的value 同时返回旧value public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; TreeNode12345678910111213141516171819202122232425262728293031/** * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn extends Node) * so can be used as extension of either regular or linked node. * 红黑树节点 相比于TreeMap， * 1.增加pre来记录前一个节点 * 2.继承LinkedHashMap.Entry&lt;K,V&gt;，而LinkedHashMap.Entry&lt;K,V&gt;又继承HashMap.Node： * 1.拥有了Node和链表Node的所有功能 * 2.具有额外6个属性Entry&lt;K,V&gt; before, after;final int hash;final K key;V value;Node&lt;K,V&gt; next; */static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // 父节点 TreeNode&lt;K,V&gt; left;//左子节点 TreeNode&lt;K,V&gt; right;//右子节点 TreeNode&lt;K,V&gt; prev; // 前一个元素的节点 boolean red;//是否是红节点 TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; ...&#125;/** * LinkedHashMap.Entry的实现 * HashMap.Node subclass for normal LinkedHashMap entries. * 可以发现，最终TreeNode还是继承了HashMap.Node的所有功能，底层实现还是Node */static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 重要方法确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap定位数组索引位置，直接决定了hash方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011//方法一：称为扰动函数static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算，高16位和低16位进行异或 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;//方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的Hash算法本质上就是三步：取key的hashCode值、高位运算、取模运算。 对于任意给定的对象，只要它的hashCode()返回值相同，那么程序调用方法一所计算得到的Hash码值总是相同的。我们首先想到的就是把hash值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在HashMap中是这样做的：调用方法二来计算该对象应该保存在table数组的哪个索引处。 这个方法非常巧妙，它通过h &amp; (table.length -1)来得到该对象的保存位，而HashMap底层数组的长度总是2的n次方，这是HashMap在速度上的优化。当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 Put方法(新增) ① 判断键值对数组table[i]是否为空或为null，否则执行resize()进行扩容； ② 根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③； ③ 判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals； ④ 判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤； ⑤ 遍历table[i]，判断链表长度是否大于8，大于8的话，进入到 treeifyBin()，会先去判断当前哈希桶大小是不是到了 MIN_TREEIFY_CAPACITY （默认是 64），如果没有就直接扩容，如果到了64（≥64），就把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可； ⑥ 插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;/** * Implements Map.put and related methods * 新增键值对 * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none *///注意 不可以被继承重载 如果使用hashMap的方式的话final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, i; //当数组为空时 if ((tab = table) == null || (n = tab.length) == 0) &#123; n = (tab = resize()).length;//当初始化或者当前数组长度为0时，需要重新resize并返回新的长度 &#125; //相当于通过 h &amp; (length-1) 计算下标并获取元素 if ((p = tab[i = (n - 1) &amp; hash]) == null)&#123; //若当前下标位置空置（即该key不存在），就新建一个普通(non-tree)节点 tab[i] = newNode(hash, key, value, null); &#125;else &#123; //当该key存在或者发生hash冲突时 Node&lt;K,V&gt; e; K k; //若在数组中通过hash和equals比较能够直接找到该值，就覆盖旧值 //即当前桶即非链表也非红黑树 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k))))&#123; e = p;//覆盖 &#125; //否则需要先判断节点是否是红黑树节点 else if (p instanceof TreeNode)&#123;//若是红黑树类型，执行树节点putTreeVal操作 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); &#125; else &#123; //此时发生了冲突 for (int binCount = 0; ; ++binCount) &#123; //如果此时的桶还不是链表，需要转变为链表 或者 如果在链表中没有，那就新增一个节点 if ((e = p.next) == null) &#123; //注意链表插入时1.7与1.8是不同的 //1.7:是头插入法，后来的留在数组上，先来的链在尾上（遍历时是先进后出） //1.8:是尾插入法，先来的留在数组上，后来的链在尾上（遍历时是先进先出） p.next = newNode(hash, key, value, null); //如果桶的链表长度&gt;=桶的树化阈值，需要将链表转变为红黑树 //这里需要注意：是先新增元素之后再判断树化条件，而不是先树化再新增 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); //当前桶树化 break; &#125; //如果在链表中已经存在该值，就覆盖旧值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //原则：用新值覆盖旧值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; //onlyIfAbsent 若是true，不允许覆盖 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e);//相当于1.7的afterNodeAccess，LinkedHashMap专用，用于有序控制 return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold)//超过阈值就扩容 resize(); afterNodeInsertion(evict);//LinkedHashMap专用，用于删除最旧元素 (remove eldest) return null;&#125;// Create a regular (non-tree) node 创建一个普通的非树节点Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next);&#125; resize方法(扩容)整体分为两部分：1. 寻找扩容后数组的大小以及新的扩容阈值，2. 将原有哈希表拷贝到新的哈希表中。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) &#123; //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; &#125; Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值&#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) &#123; src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 &#125; while (e != null); &#125; &#125;&#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107/** * Initializes or doubles table size. If null, allocates in accord with initial capacity * target held in field threshold. Otherwise, because we are using power-of-two expansion, * the elements from each bin must either stay at same index, or move with a power of two * offset in the new table. * 初始化Map或2倍扩容，且会均匀的把之前的冲突的节点分散到新的桶中 * 当Map为空时，将分配与阈值一样大小的容量 * 当Map不为空时，由于2次幂扩容，元素位置会产生两种情况 * 1.要么元素所在位置不变 * 2.要么元素所在位置变动：向右位移2次幂位置 * 注意：由于1.8中容量是根据阈值得来的，因此读者会在1.8中看到很多对阈值的判断和处理，这点一定要清楚 * @return the table */final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table;//由于新数组会覆盖旧数组，所以要临时先备份一份，用于对新数组重新赋值 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123;//当Map不为空时 //临界处理：最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE;//最大值其实是Integer的最大值 return oldTab; &#125; //若2倍容量 &lt; MAXIMUM_CAPACITY 同时 原容量&gt;=默认容量(即16)，那么就扩容2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold 阈值直接两倍（容量都是根据阈值来的） &#125; else if (oldThr &gt; 0)&#123;//当Map为空时，需要判断阈值是否&gt;0 newCap = oldThr;//阈值即新容量（注意：初始化时候就是执行该操作完成容量赋值） // initial capacity was placed in threshold（容量都是根据阈值来的） &#125; else &#123; //当Map为空，且阈值不是大于0（即无效阈值），那么就使用默认值 // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//1 &lt;&lt; 4 = 16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//0.75 * 16 = 12 &#125; //当新阈值没有被重置时，需要根据 新容量和负载因子 重新计算出新的阈值 //注意：初始化的时候，阈值会被重置，即此时 阈值！=容量 ，容量已经在(oldThr &gt; 0)时重置过了 if (newThr == 0) &#123; //等同于1.7版本：threshold = (int)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;//重置给真实阈值 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];//新建一个新容量的Node数组 table = newTab;//覆盖原数组（第一行已经备份了） //当原数组非空，需要对新数组重新填充 if (oldTab != null) &#123; //遍历 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e;//用于备份当前节点 //若该数组下标位置非空 if ((e = oldTab[j]) != null) &#123; oldTab[j] = null;//先把原数组的当前位置清空，因为已经备份了 help gc if (e.next == null)//当前桶既非链表也非红黑树 newTab[e.hash &amp; (newCap - 1)] = e;//位置可能不变或移动2次幂，跟newCap-1有关 else if (e instanceof TreeNode)//若当前桶是树节点，需要对树进行切分 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order 当前桶是链表，要保持顺序 1.7的会倒置 //扩容后，新数组中的链表顺序依然与旧数组中的链表顺序保持一致!!! Node&lt;K,V&gt; loHead = null, loTail = null;//lo=low，表示低位（即数组前半部分的链表） Node&lt;K,V&gt; hiHead = null, hiTail = null;//hi=high，表示高位（即数组后半部分的链表） Node&lt;K,V&gt; next; //遍历当前桶的链表 //1.8:是尾插入法，先来的留在数组上，后来的链在尾上（遍历时是先进先出） do &#123; next = e.next; //根据e.hash &amp; oldCap是否为零将原链表拆分成2个链表 //判断当前位置是否发生变动 0则没变 即保留在原链表中不需要移动 if ((e.hash &amp; oldCap) == 0) &#123; //原索引 在数组前半部分处理 //若队尾为空，当前元素即是队首元素（也就是第一个插入的元素），保证先进先出 if (loTail == null) loHead = e; else //若队尾不为空，当前元素链接到原队尾元素后面，保证先进先出 loTail.next = e; loTail = e;//为了保证插入顺序不变，当前元素都需先设置为队尾元素 &#125; //原索引+oldCap 否则移动到"原索引+oldCap"的新链表中 else &#123; //在数组后半部分处理 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e;//为了保证插入顺序不变，当前元素都需先设置为队尾元素 &#125; &#125; while ((e = next) != null); //原索引放到原桶中 if (loTail != null) &#123;//如果队尾元素非空 loTail.next = null;//loTail此时就是队尾元素 newTab[j] = loHead;//队首是放在数组里面的 &#125; //原索引+oldCap放到新桶中 if (hiTail != null) &#123;//如果队尾元素非空 hiTail.next = null;//hiTail此时就是队尾元素 newTab[j + oldCap] = hiHead;//队首是放在数组里面的 &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 所以说，JDK 1.8之后扩容的改进有： 链表上的节点是尾插法，先来的放到数组中，后来的放到链表中，先到先出，类似于栈。 JDK 1.8 不像 JDK1.7中会重新计算每个节点在新哈希表中的位置，而是通过 (e.hash &amp; oldCap) == 0是否等于0 就可以得出原来链表中的节点在新哈希表的位置。无需重新计算Hash，节省了时间，新索引=原索引+原容量 HashMap 在 1.7的时候扩容后，链表的节点顺序会倒置，1.8则不会出现这种情况。因为1.7是后进先出，1.8是先进先出。 treeifyBin方法(树化方法)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889 /** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. * 桶内链表树化：将桶内所有的链表节点替换成红黑树节点，当元素数量不够树化时会重新resize * 注意：不是整个Map转换，只是当前桶！ */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; //当数组为空 或者 数组长度 &lt; 树化阈值（64）时需要执行resize方法，重新决定内部的数据结构类型 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); //否则，需要树化 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null;//hd指的是head，tl指的是tail，分别指向红黑树的头、尾节点 //从链表头节点开始遍历链表，头节点是存放在数组中的 do &#123; //新建一个树形节点，内容和当前链表节点e保持一致 //此时next默认为null，会在后面按顺序重新对next赋值 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null)//当尾节点为空，即当前节点应为头节点（因为就这一个节点） hd = p; else &#123; p.prev = tl;//prev被赋值，主要是记录当前节点的上一个节点 tl.next = p;//p指向之前尾节点的next，保持插入顺序 &#125; tl = p;//当前节点设置为尾节点，保持插入顺序 &#125; while ((e = e.next) != null); //桶内第一个元素即链表头节点，并放在数组中 if ((tab[index] = hd) != null) hd.treeify(tab);//从头节点开始遍历，将整个桶树化 //注意头节点并不一定是树的根节点：树化后的根节点会重新设置为头节点，即tab[index]=root //具体参见moveRootToFront() &#125;&#125;// For treeifyBin 新建一个树形节点TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next);&#125;/** * Forms tree of the nodes linked from this node. * 塑造红黑树 * @return root of tree 这里比较有意思，明明时void但有注释@return，不知大神们何意 */final void treeify(Node&lt;K,V&gt;[] tab) &#123; TreeNode&lt;K,V&gt; root = null;//根节点需要排序后重新设置（之前链表的头节点不一定是树的根节点） //this指的是当前二叉树的头节点，从头节点开始遍历 for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; //当根节点为空时，先设置根节点为黑色，同时当前节点先当作根节点（即自上而下插入） if (root == null) &#123; x.parent = null; x.red = false;//红黑树的根节点为黑色 root = x; &#125; else &#123; //后面进入循环走的逻辑，x 指向树中的某个节点 K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; //重新循环，从根节点开始，遍历所有节点与当前节点x比较，重新调整位置，类似冒泡排序 for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h)//如果比较节点的hash比当前节点的hash大，查左子树 dir = -1; else if (ph &lt; h) dir = 1;//如果比较节点的hash比当前节点的hash小，查右子树 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0 ) //tieBreakOrder 用于hash相同时且key无法比较时，直接根据引用比较 //这里指的是如果当前比较节点的哈希值比x大，返回-1，否则返回1 dir = tieBreakOrder(k, pk); //经过前面的计算，得到了当前节点和要插入节点x的一个大小关系 //如果当前比较节点的哈希值比x大，x就是左子节点，否则x是右子节点 TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp;//把当前节点变成x的父节点 if (dir &lt;= 0) xp.left = x; else xp.right = x; root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; moveRootToFront(tab, root);//将根节点设置为头节点 get方法(查询) 根据键值对的 key 去获取对应的 Value 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; //通过 getNode寻找 key 对应的 Value 如果没找到，或者找到的结果为 null 就会返回null 否则会返回对应的 Value return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //现根据 key 的 hash 值去找到对应的链表或者红黑树 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 如果第一个节点就是那么直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //如果 对应的位置为红黑树调用红黑树的方法去寻找节点 if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //遍历单链表找到对应的 key 和 Value do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; JDK 1.8新增 get 方法，在寻找 key 对应 Value 的时候如果没找到则返回指定默认值。 12345@Overridepublic V getOrDefault(Object key, V defaultValue) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? defaultValue : e.value;&#125; delete方法(删除)HashMap 没有 set 方法，如果想要修改对应 key 映射的 Value ，只需要再次调用 put 方法就可以了。我们来看下如何移除 HashMap 中对应的节点的方法： 12345678910 public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; 这里有两个参数需要我们提起注意： matchValue 如果这个值为 true 则表示只有当 Value 与第三个参数 Value 相同的时候才删除对一个的节点 movable 这个参数在红黑树中先删除节点时候使用 true 表示删除并其他数中的节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //判断哈希表是否为空，长度是否大于0 对应的位置上是否有元素 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; // node 用来存放要移除的节点， e 表示下个节点 k ，v 每个节点的键值 Node&lt;K,V&gt; node = null, e; K k; V v; //如果第一个节点就是我们要找的直接赋值给 node if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; // 遍历红黑树找到对应的节点 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //遍历对应的链表找到对应的节点 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // 如果找到了节点 // !matchValue 是否不删除节点 // (v = node.value) == value || // (value != null &amp;&amp; value.equals(v))) 节点值是否相同， if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //删除节点 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; 1234567891011public void remove() &#123; if (current == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); Object k = current.key; current = null; HashMap.this.removeEntryForKey(k); //重要操作：迭代器中删除时同步了expectedModCount值与modCount相同 expectedModCount = modCount;&#125; 迭代Map 是没有迭代器的，需要转成 Set 操作 1234567Map map = new HashMap();Iterator iter = map.entrySet().iterator();while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue();&#125; 要强调的是，无论哪种迭代器都是通过遍历 table 表来获取下个节点，来遍历的，遍历过程可以理解为一种深度优先遍历，即优先遍历链表节点（或者红黑树），然后在遍历其他数组位置。 Fail-Fast当使用迭代器的过程中有其他线程修改了map，并且不是使用迭代器修改的，将引发ConcurrentModificationException 1234567891011121314//修改计数 put、remove或clear时mount++ clear时清空transient int modCount;HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125;&#125;final Entry&lt;K,V&gt; nextEntry() &#123; //期望变更数量不匹配 if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125; 与HashTable的区别 HashMap 是线程不安全的，HashTable是线程安全的。 HashMap 允许 key 和 Vale 是 null，但是只允许一个 key 为 null,且这个元素存放在哈希表 0 角标位置。 HashTable 不允许key、value 是 null。 HashMap 内部使用hash(Object key)扰动函数对 key 的 hashCode 进行扰动后作为 hash 值。HashTable 是直接使用 key 的 hashCode() 返回值作为 hash 值。 HashMap默认容量为 2^4 且容量一定是 2^n ; HashTable 默认容量是11,不一定是 2^n。 HashTable 取哈希桶下标是直接用模运算,扩容时新容量是原来的2倍+1。HashMap 在扩容的时候是原来的两倍，且哈希桶的下标使用 &amp;运算代替了取模。 Hashtable是Dictionary的子类同时也实现了Map接口，HashMap是Map接口的一个实现类。 面试题答案 HashMap的底层数据结构？ 数组 + 链表 + 红黑树 HashMap的主要方法？ put()、resize()、treeifyBin()、get()、delete()等等 HashMap 是如何确定元素存储位置的以及如何处理哈希冲突的？ 是通过hashcode和扰乱函数确定的，先计算好key的hashCode,然后将hashCode的低16位和高16位进行相异或，异或完成后再进行取模(其实也就是与容量-1进行相与[有0则全为0，全1才为1])。一句话总结就是： 取key的hashCode值、高位运算、取模运算 HashMap 扩容机制是怎样的？ 先判断是否超过了整数型的最大值，如果没有，再判断是否已经超过了扩容阈值，如果超过了，则直接扩容两倍，扩容的数量都是2的n次方，HashMap都是优先扩容的，然后其次才是树化等等。 总的来说分为两步：1. 寻找扩容后数组的大小以及新的扩容阈值，2. 将原有哈希表拷贝到新的哈希表中。 JDK 1.8 在扩容和解决哈希冲突上对 HashMap 源码做了哪些改动？有什么好处? 在扩容方面，由于是选用了2的n次方作为桶的数量，所以在取模的时候就非常方便了，直接看hashCode的前一位是1或者0就可以判断在扩容后的新位置，无需重新取模运算； 既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了，这一块就是JDK1.8新增的优化点； HashMap 在 1.7的时候扩容后，链表的节点顺序会倒置，1.8则不会出现这种情况。因为1.7是后进先出，1.8是先进先出。1.7是头插法，1.8是尾插法； 什么时候会使用HashMap？他有什么特点？ 基于Map接口实现的Key-Value容器，允许为空值，同时非有序，线程不安全，非同步。 你知道HashMap的工作原理吗？ 经过HashCode和扰乱函数，得到相应的索引位置，然后插入，如果非空直接插入即可，如果是有数据，则判断是链表还是红黑树，如果链表超过8则先判断是否需要扩容，如果不需要就将链表变成红黑树！ 你知道get和put的原理吗？equals()和hashCode()都有什么作用？ 通过对key的hashCode()进行哈希处理，并计算下标（n-1＆hash），从而获得存储桶的位置。如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的队列。 你知道hash的实现吗？为什么要这样实现？ 在Java 1.8的实现中，是通过hashCode（）的高16位异或低16位实现的：（h = k.hashCode（））^（h &gt;&gt;&gt; 16），主要是从速度，效益，质量来考虑的，这样做可以在bucket的n比较小的时候，也能保证考虑到高低位都参与到hash的计算中，同时不会有太大的开销。 使用hash还有一个好处就是可以确保每个链表中的长度一致 如果HashMap的大小超过了负载因子（load factor）定义的容量，怎么办？ 如果超过了负载因子（最小0.75），则重新重新调整一个原先长度的HashMap，并重新调用hash方法。 你了解重新调整HashMap大小存在什么问题吗？ 当数据过多时，很可能出现性能下降（包括rehash时间） 多线程情况下可能产生条件竞争竞争从而造成死循环（具体表现在CPU接近100％)，多线程环境下推荐使用ConcurrentHashMap 为什么使用String，Interger这样的wrapper类适合作为键？ 因为这种类具有final属性，可以保证线程的相对安全，也可以减少碰撞 [自动装箱，自动拆箱的知识点！！] 可以同时重建equals()和hashCode()方法。 我们可以使用自定义的对象作为键吗？ 当然你可能使用任何对象作为键，只要它遵守了equals（）和hashCode（）方法的定义规则，并且当对象插入到Map中之后将不会再改变了。自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 如何对HashMap进行排序？ 存入LinkedHashMap HashMap的删除陷阱？ 通过Iterator方式可正确遍历完成删除操作 直接调用list的remove方法就会抛异常 为什么只允许通过iterator进行删除操作？ HashMap和keySet的删除方法都可以通过传递key参数删除任意的元素 而iterator只能删除当前元素（当前），一旦删除的元素是iterator对象中下一个正在引用的，如果没有通过modCount，expectedModCount的比较实现快速失败异常，则下一循环该元素将成为当前指向，此时iterator就遍历了一个已移除的过期数据 之所以推荐迭代器remove的根本原因在于只有迭代器的remove方法中实现了变更时于modCount的同步工作expectedModCount = modCount; 如果是遍历过程中增加或修改数据呢？ 增加或修改数据只能通过Map的put方法实现，在遍历过程中修改数据可以，但如果增加新密钥就会在下次循环时抛出异常，因为在添加新密钥时modCount也会自增（迭代器只实现了删除方法也是原因之一） hashmap为什么初始长度为16？ 首先要为2的幂次。这是因为，hashmap计算key的hash值进行存储的时候采用的方法是 “用key的hash值和hashmap的长度减一（length-1）按位与（&amp;），2的幂次减一的二进制是111……，任何数和1与就是他本身，这样存储进来的hash位置就取决于key的二进制值了，这样会让hash分布相对分散。提升性能。 HashMap中的扰动函数是什么，有什么作用，TreeMap中有吗？ 就是返回 hashCode的高16位和低16位相异或然后与 容量-1 相与的结果，这样的话分散更均匀，TreeMap没有。 有什么方法可以减少碰撞？ 扰动函数可以减少碰撞 原理是如果两个不相等的对象返回不同的 hashcode 的话，那么碰撞的几率就会小些。这就意味着存链表结构减小，这样取值的话就不会频繁调用 equal 方法，从而提高 HashMap 的性能（扰动即 Hash 方法内部的算法实现，目的是让不同对象返回不同hashcode）。 使用不可变的、声明作 final 对象，并且采用合适的 equals() 和 hashCode() 方法，将会减少碰撞的发生 不可变性使得能够缓存不同键的 hashcode，这将提高整个获取对象的速度，使用 String、Integer 这样的 wrapper 类作为键是非常好的选择。 HashMap为何可以插入空值？为什么它是线程不安全的？ HashMap在put的时候会调用hash()方法来计算key的hashcode值，可以从hash算法中看出当key==null时返回的值为0。因此key为null时，hash算法返回值为0，不会调用key的hashcode方法。而Hashtable存入的value为null时，抛出NullPointerException异常。如果value不为null，而key为空，在执行到int hash = key.hashCode()时同样会抛出NullPointerException异常。 上面说到，HashMap会进行resize操作，在resize操作的时候会造成线程不安全。下面将举两个可能出现线程不安全的地方。 put的时候导致的多线程数据不一致。 另外一个比较明显的线程不安全的问题是HashMap的get操作可能因为resize而引起死循环（cpu100%)。 可以使用 CocurrentHashMap 来代替 Hashtable 吗？ 我们知道 Hashtable 是 synchronized 的，但是 ConcurrentHashMap 同步性能更好，因为它仅仅根据同步级别对 map 的一部分进行上锁。 ConcurrentHashMap 当然可以代替 HashTable，但是 HashTable 提供更强的线程安全性 它们都可以用于多线程的环境，但是当 Hashtable 的大小增加到一定的时候，性能会急剧下降，因为迭代时需要被锁定很长的时间。 由于 ConcurrentHashMap 引入了分割（segmentation），不论它变得多么大，仅仅需要锁定 Map 的某个部分，其它的线程不需要等到迭代完成才能访问 Map。 简而言之，在迭代的过程中，ConcurrentHashMap 仅仅锁定 Map 的某个部分，而 Hashtable 则会锁定整个 Map。 CocurrentHashMap（JDK 1.8） CocurrentHashMap 抛弃了原有的 Segment 分段锁，采用了 CAS + synchronized 来保证并发安全性。其中的 val next 都用了 volatile 修饰，保证了可见性。 最大特点是引入了 CAS 借助 Unsafe 来实现 native code。CAS有3个操作数，内存值 V、旧的预期值 A、要修改的新值 B。当且仅当预期值 A 和内存值 V 相同时，将内存值V修改为 B，否则什么都不做。Unsafe 借助 CPU 指令 cmpxchg 来实现。 CAS 使用实例 对 sizeCtl 的控制都是用 CAS 来实现的： -1 代表 table 正在初始化 N 表示有 -N-1 个线程正在进行扩容操作 如果 table 未初始化，表示table需要初始化的大小 如果 table 初始化完成，表示table的容量，默认是table大小的0.75倍，用这个公式算 0.75（n – (n &gt;&gt;&gt; 2)） CAS 会出现的问题：ABA 解决：对变量增加一个版本号，每次修改，版本号加 1，比较的时候比较版本号。 贴一个复习时可以看的问答 https://cloud.tencent.com/developer/article/1491634 TreeMap 接下来我们一起来分析一波 TreeMap ！！！ 参考TreeMap 源码分析 java集合（6）：TreeMap源码分析（jdk1.8） 如何决定使用HashMap Or TreeMap 问题引导 如何决定使用 HashMap 还是 TreeMap？ TreeMap的特点 概述 TreeMap 底层基于红黑树实现 该集合最重要的特点就是可排序，该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。 由上图可知，TreeMap 继承了 AbstractMap，同时实现了 NavigableMap (导航 map )，而 NavigableMap 则是继承了 SortedMap ,而 SortedMap 和 AbstractMap 则是实现了 Map 接口。 NavigableMap 接口，NavigableMap 接口声明了一些列具有导航功能的方法，通过这些导航方法，我们可以快速定位到目标的 key 或 Entry。比如： 12345678910111213141516/** * 返回红黑树中最小键所对应的 Entry */Map.Entry&lt;K,V&gt; firstEntry();/** * 返回最大的键 maxKey，且 maxKey 仅小于参数 key */K lowerKey(K key);/** * 返回最小的键 minKey，且 minKey 仅大于参数 key */K higherKey(K key);// 其他略 至于 SortedMap 接口，这个接口提供了一些基于有序键的操作，比如: 1234567891011/** * 返回包含键值在 [minKey, toKey) 范围内的 Map */SortedMap&lt;K,V&gt; headMap(K toKey);();/** * 返回包含键值在 [fromKey, toKey) 范围内的 Map */SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey);// 其他略 源码分析JDK 1.8中的TreeMap源码有两千多行，还是比较多的。TreeMap实现的核心部分是关于红黑树的实现，其绝大部分的方法基本都是对底层红黑树增、删、查操作的一个封装。如简介一节所说，只要弄懂了红黑树原理，TreeMap 就没什么秘密了。关于红黑树的原理，请参考本人的另一篇文章 红黑树详细分析。 类名及类成员变量1234567891011121314151617181920212223242526public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable, java.io.Serializable&#123; // 比较器对象 private final Comparator&lt;? super K&gt; comparator; // 根节点 private transient Entry&lt;K,V&gt; root; // 集合大小 private transient int size = 0; // 树结构被修改的次数 private transient int modCount = 0; // 静态内部类用来表示节点类型 static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; K key; // 键 V value; // 值 Entry&lt;K,V&gt; left; // 指向左子树的引用（指针） Entry&lt;K,V&gt; right; // 指向右子树的引用（指针） Entry&lt;K,V&gt; parent; // 指向父节点的引用（指针） boolean color = BLACK; // &#125;&#125; 构造方法123456789101112131415161718192021public TreeMap() &#123; // 1,无参构造方法 comparator = null; // 默认比较机制&#125;public TreeMap(Comparator&lt;? super K&gt; comparator) &#123; // 2，自定义比较器的构造方法 this.comparator = comparator;&#125;public TreeMap(Map&lt;? extends K, ? extends V&gt; m) &#123; // 3，构造已知Map对象为TreeMap comparator = null; // 默认比较机制 putAll(m);&#125;public TreeMap(SortedMap&lt;K, ? extends V&gt; m) &#123; // 4，构造已知的SortedMap对象为TreeMap comparator = m.comparator(); // 使用已知对象的构造器 try &#123; buildFromSorted(m.size(), m.entrySet().iterator(), null, null); &#125; catch (java.io.IOException cannotHappen) &#123; &#125; catch (ClassNotFoundException cannotHappen) &#123; &#125;&#125; 查找TreeMap基于红黑树实现，而红黑树是一种自平衡二叉查找树，所以 TreeMap 的查找操作流程和二叉查找树一致。二叉树的查找流程是这样的，先将目标值和根节点的值进行比较，如果目标值小于根节点的值，则再和根节点的左孩子进行比较。如果目标值大于根节点的值，则继续和根节点的右孩子比较。在查找过程中，如果目标值和二叉树中的某个节点值相等，则返回 true，否则返回 false。TreeMap 查找和此类似，只不过在 TreeMap 中，节点（Entry）存储的是键值对。在查找过程中，比较的是键的大小，返回的是值，如果没找到，则返回null。TreeMap 中的查找方法是get，具体实现在getEntry方法中，相关源码如下： 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings("unchecked") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; // 查找操作的核心逻辑就在这个 while 循环里 // key 与 root 作比较 while (p != null) &#123; int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; &#125; return null;&#125; 遍历 Iterator 顺序遍历 12345678TreeMap map = new TreeMap();Set set = map.entrySet();Iterator iter = set.iterator();while(iter.hasNext())&#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object value = entry.getValue();&#125; foreach 遍历 1234567891011//遍历Map 第二种方式Map&lt;String, Integer&gt; map1 = new TreeMap&lt;&gt;();map1.put("jack", 20);map1.put("rose", 18);map1.put("lucy", 17);map1.put("java", 25);//通过Map.Entry(String,Integer) 获取，然后使用entry.getKey()获取到键，通过entry.getValue()获取到值for(Map.Entry&lt;String, Integer&gt; entry : map1.entrySet())&#123; System.out.println("键 key ："+entry.getKey()+" 值value ："+entry.getValue());&#125; 从上面代码片段中可以看出，大家一般都是对 TreeMap 的 key 集合或 Entry 集合进行遍历。上面代码片段中用 foreach 遍历 entrySet 方法产生的集合，在编译时会转换成用迭代器遍历，等价于： 1234567Set entry = map.entrySet();Iterator ite = entry.iterator();while (ite.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); // do something&#125; 另一方面，TreeMap 有一个特性，即可以保证键的有序性，默认是正序。所以在遍历过程中，大家会发现 TreeMap 会从小到大输出键的值。那么，接下来就来分析一下keySet方法，以及在遍历 keySet 方法产生的集合时，TreeMap 是如何保证键的有序性的。相关代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public Set&lt;K&gt; keySet() &#123; return navigableKeySet();&#125;public NavigableSet&lt;K&gt; navigableKeySet() &#123; KeySet&lt;K&gt; nks = navigableKeySet; return (nks != null) ? nks : (navigableKeySet = new KeySet&lt;&gt;(this));&#125;static final class KeySet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt; &#123; private final NavigableMap&lt;E, ?&gt; m; KeySet(NavigableMap&lt;E,?&gt; map) &#123; m = map; &#125; public Iterator&lt;E&gt; iterator() &#123; if (m instanceof TreeMap) return ((TreeMap&lt;E,?&gt;)m).keyIterator(); else return ((TreeMap.NavigableSubMap&lt;E,?&gt;)m).keyIterator(); &#125; // 省略非关键代码&#125;Iterator&lt;K&gt; keyIterator() &#123; return new KeyIterator(getFirstEntry());&#125;final class KeyIterator extends PrivateEntryIterator&lt;K&gt; &#123; KeyIterator(Entry&lt;K,V&gt; first) &#123; super(first); &#125; public K next() &#123; return nextEntry().key; &#125;&#125;abstract class PrivateEntryIterator&lt;T&gt; implements Iterator&lt;T&gt; &#123; Entry&lt;K,V&gt; next; Entry&lt;K,V&gt; lastReturned; int expectedModCount; PrivateEntryIterator(Entry&lt;K,V&gt; first) &#123; expectedModCount = modCount; lastReturned = null; next = first; &#125; public final boolean hasNext() &#123; return next != null; &#125; final Entry&lt;K,V&gt; nextEntry() &#123; Entry&lt;K,V&gt; e = next; if (e == null) throw new NoSuchElementException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); // 寻找节点 e 的后继节点 next = successor(e); lastReturned = e; return e; &#125; /** * Returns the successor of the specified Entry, or null if no such. */ static &lt;K,V&gt; TreeMap.Entry&lt;K,V&gt; successor(Entry&lt;K,V&gt; t) &#123; if (t == null) return null; else if (t.right != null) &#123; Entry&lt;K,V&gt; p = t.right; while (p.left != null) p = p.left; return p; &#125; else &#123; Entry&lt;K,V&gt; p = t.parent; Entry&lt;K,V&gt; ch = t; while (p != null &amp;&amp; ch == p.right) &#123; ch = p; p = p.parent; &#125; return p; &#125; &#125; // 其他方法省略&#125; 上面的代码比较多，keySet 涉及的代码还是比较多的，大家可以从上往下看。从上面源码可以看出 keySet 方法返回的是KeySet类的对象。这个类实现了Iterable接口，可以返回一个迭代器。该迭代器的具体实现是KeyIterator，而 KeyIterator 类的核心逻辑是在PrivateEntryIterator中实现的。上面的代码虽多，但核心代码还是 KeySet 类和 PrivateEntryIterator 类的 nextEntry方法。KeySet 类就是一个集合，这里不分析了。而 nextEntry 方法比较重要，下面简单分析一下。 在初始化 KeyIterator 时，会将 TreeMap 中包含最小键的 Entry 传给 PrivateEntryIterator。当调用 nextEntry 方法时，通过调用 successor 方法找到当前 entry 的后继，并让 next 指向后继，最后返回当前的 entry。通过这种方式即可实现按正序返回键值的的逻辑。 插入与删除其实就是红黑树的插入和删除，具体红黑树的插入和删除见我另写的红黑树文章。 插入 1234567891011121314151617181920212223242526272829303132333435363738394041public V put(K key, V value) &#123; Entry&lt;K,V&gt; t = root; // 1.如果根节点为 null，将新节点设为根节点 if (t == null) &#123; compare(key, key); root = new Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; Entry&lt;K,V&gt; parent; // split comparator and comparable paths Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) &#123; // 2.为 key 在红黑树找到合适的位置 do &#123; parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; else &#123; // 与上面代码逻辑类似，省略 &#125; Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); // 3.将新节点链入红黑树中 if (cmp &lt; 0) parent.left = e; else parent.right = e; // 4.插入新节点可能会破坏红黑树性质，这里修正一下 fixAfterInsertion(e); size++; modCount++; return null;&#125; put 方法代码如上，逻辑和二叉查找树插入节点逻辑一致。重要的步骤我已经写了注释，并不难理解。插入逻辑的复杂之处在于插入后的修复操作，对应的方法fixAfterInsertion，该方法的源码和说明如下： 1234567891011121314151617181920212223242526272829303132333435363738394041/** From CLR */ private void fixAfterInsertion(Entry&lt;K,V&gt; x) &#123; x.color = RED; while (x != null &amp;&amp; x != root &amp;&amp; x.parent.color == RED) &#123; if (parentOf(x) == leftOf(parentOf(parentOf(x)))) &#123; Entry&lt;K,V&gt; y = rightOf(parentOf(parentOf(x))); if (colorOf(y) == RED) &#123; setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); &#125; else &#123; if (x == rightOf(parentOf(x))) &#123; x = parentOf(x); rotateLeft(x); &#125; setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateRight(parentOf(parentOf(x))); &#125; &#125; else &#123; Entry&lt;K,V&gt; y = leftOf(parentOf(parentOf(x))); if (colorOf(y) == RED) &#123; setColor(parentOf(x), BLACK); setColor(y, BLACK); setColor(parentOf(parentOf(x)), RED); x = parentOf(parentOf(x)); &#125; else &#123; if (x == leftOf(parentOf(x))) &#123; x = parentOf(x); rotateRight(x); &#125; setColor(parentOf(x), BLACK); setColor(parentOf(parentOf(x)), RED); rotateLeft(parentOf(parentOf(x))); &#125; &#125; &#125; root.color = BLACK; &#125; 删除 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public V remove(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); if (p == null) return null; V oldValue = p.value; deleteEntry(p); return oldValue;&#125;private void deleteEntry(Entry&lt;K,V&gt; p) &#123; modCount++; size--; /* * 1. 如果 p 有两个孩子节点，则找到后继节点， * 并把后继节点的值复制到节点 P 中，并让 p 指向其后继节点 */ if (p.left != null &amp;&amp; p.right != null) &#123; Entry&lt;K,V&gt; s = successor(p); p.key = s.key; p.value = s.value; p = s; &#125; // p has 2 children // Start fixup at replacement node, if it exists. Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); if (replacement != null) &#123; /* * 2. 将 replacement parent 引用指向新的父节点， * 同时让新的父节点指向 replacement。 */ replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; // Null out links so they are OK to use by fixAfterDeletion. p.left = p.right = p.parent = null; // 3. 如果删除的节点 p 是黑色节点，则需要进行调整 if (p.color == BLACK) fixAfterDeletion(replacement); &#125; else if (p.parent == null) &#123; // 删除的是根节点，且树中当前只有一个节点 root = null; &#125; else &#123; // 删除的节点没有孩子节点 // p 是黑色，则需要进行调整 if (p.color == BLACK) fixAfterDeletion(p); // 将 P 从树中移除 if (p.parent != null) &#123; if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; &#125; &#125;&#125; 从源码中可以看出，remove方法只是一个简单的保证，核心实现在deleteEntry方法中。deleteEntry 主要做了这么几件事： 如果待删除节点 P 有两个孩子，则先找到 P 的后继 S，然后将 S 中的值拷贝到 P 中，并让 P 指向 S 如果最终被删除节点 P（P 现在指向最终被删除节点）的孩子不为空，则用其孩子节点替换掉 如果最终被删除的节点是黑色的话，调用 fixAfterDeletion 方法进行修复 上面说了 replacement 不为空时，deleteEntry 的执行逻辑。上面说的略微啰嗦，如果简单说的话，7个字即可总结：找后继 -&gt; 替换 -&gt; 修复。这三步中，最复杂的是修复操作。修复操作要重新使红黑树恢复平衡，修复操作的源码分析如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** From CLR */ private void fixAfterDeletion(Entry&lt;K,V&gt; x) &#123; while (x != root &amp;&amp; colorOf(x) == BLACK) &#123; if (x == leftOf(parentOf(x))) &#123; Entry&lt;K,V&gt; sib = rightOf(parentOf(x)); if (colorOf(sib) == RED) &#123; setColor(sib, BLACK); setColor(parentOf(x), RED); rotateLeft(parentOf(x)); sib = rightOf(parentOf(x)); &#125; if (colorOf(leftOf(sib)) == BLACK &amp;&amp; colorOf(rightOf(sib)) == BLACK) &#123; setColor(sib, RED); x = parentOf(x); &#125; else &#123; if (colorOf(rightOf(sib)) == BLACK) &#123; setColor(leftOf(sib), BLACK); setColor(sib, RED); rotateRight(sib); sib = rightOf(parentOf(x)); &#125; setColor(sib, colorOf(parentOf(x))); setColor(parentOf(x), BLACK); setColor(rightOf(sib), BLACK); rotateLeft(parentOf(x)); x = root; &#125; &#125; else &#123; // symmetric Entry&lt;K,V&gt; sib = leftOf(parentOf(x)); if (colorOf(sib) == RED) &#123; setColor(sib, BLACK); setColor(parentOf(x), RED); rotateRight(parentOf(x)); sib = leftOf(parentOf(x)); &#125; if (colorOf(rightOf(sib)) == BLACK &amp;&amp; colorOf(leftOf(sib)) == BLACK) &#123; setColor(sib, RED); x = parentOf(x); &#125; else &#123; if (colorOf(leftOf(sib)) == BLACK) &#123; setColor(rightOf(sib), BLACK); setColor(sib, RED); rotateLeft(sib); sib = leftOf(parentOf(x)); &#125; setColor(sib, colorOf(parentOf(x))); setColor(parentOf(x), BLACK); setColor(leftOf(sib), BLACK); rotateRight(parentOf(x)); x = root; &#125; &#125; &#125; setColor(x, BLACK); &#125; 自定义比较器 在实体类对象中实现Comparable接口并实现了compareTo()方法【String，Integer对象就是此种方式】 12345678910111213141516171819202122232425262728293031323334353637class TreeMap2_Comparable &#123; public static void main(String[] args) &#123; Map&lt;User, String&gt; map = new TreeMap&lt;&gt;(); map.put(new User("jerome", 30), "hello"); map.put(new User("memory", 30), "hello"); map.put(new User("aa", 22), "hello"); map.put(new User("bb", 20), "hello"); for (Map.Entry&lt;User, String&gt; each : map.entrySet()) &#123; System.out.println(each.getKey()+"::"+each.getValue()); &#125; &#125;&#125;class User implements Comparable&lt;User&gt;&#123; private String username; private int age; public User(String username, int age) &#123; this.username = username; this.age = age; &#125; @Override public String toString() &#123; return "User [username=" + username + ", age=" + age + "]"; &#125; @Override public int compareTo(User user) &#123; // 先比较 age ，再比较 username int temp = this.age - user.age; return temp == 0 ? this.username.compareTo(user.username) : temp; &#125;&#125; 输出： 12345&gt; User [username=bb, age=20]::hello&gt; User [username=aa, age=22]::hello&gt; User [username=jerome, age=30]::hello&gt; User [username=memory, age=30]::hello&gt; 写一个类实现java.util.Comparator接口(注意这里是Comparator，上面是Comparable)，并将该类对象传递给TreeMap的构造方法。这种方式将实体类和比较机制解耦合，可以写很多个不同的比较器对象。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class TreeMap2_Comparator&#123; public static void main(String[] args) &#123; Map&lt;User2, String&gt; map = new TreeMap&lt;&gt;(new TreeMapComparator()); map.put(new User2("jerome", 30), "hello"); map.put(new User2("memory", 30), "hello"); map.put(new User2("aa", 22), "hello"); map.put(new User2("bb", 20), "hello"); for (Map.Entry&lt;User2, String&gt; each : map.entrySet()) &#123; System.out.println(each.getKey()+"::"+each.getValue()); &#125; &#125;&#125;class User2 &#123; // User对象不再实现任何接口 private String username; private int age; public User2(String username, int age) &#123; super(); this.username = username; this.age = age; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "User2 [username=" + username + ", age=" + age + "]"; &#125;&#125;class TreeMapComparator implements Comparator&lt;User2&gt; &#123; // 比较器类 @Override public int compare(User2 o1, User2 o2) &#123; int temp = o1.getAge() - o2.getAge(); return temp == 0 ? o1.getUsername().compareTo(o2.getUsername()) : temp; &#125;&#125; 输出同上 不写比较器类，而是使用匿名内部类的形式来写比较器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class TreeMap2_InnerComparator &#123; public static void main(String[] args) &#123; Map&lt;User3, String&gt; map = new TreeMap&lt;&gt;(new Comparator&lt;User3&gt;() &#123; @Override public int compare(User3 o1, User3 o2) &#123; int temp = o1.getAge() - o2.getAge(); return temp == 0 ? o1.getUsername().compareTo(o2.getUsername()) : temp; &#125; &#125;); map.put(new User3("jimmy1", 30), "hello"); map.put(new User3("jimmy2", 30), "hello"); map.put(new User3("jimmy", 22), "hello"); map.put(new User3("jimmy", 20), "hello"); for (Map.Entry&lt;User3, String&gt; each : map.entrySet()) &#123; System.out.println(each.getKey() + "::" + each.getValue()); &#125; &#125;&#125;class User3 &#123; // User对象不再实现任何接口 private String username; private int age; public User3(String username, int age) &#123; super(); this.username = username; this.age = age; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return "User3 [username=" + username + ", age=" + age + "]"; &#125;&#125; 面试相关题目 HashMap 和 TreeMap 有迭代器吗？ keySet()和entrySet()方法，在将HashMap的时候已经讲过了，Map没有迭代器，要将Map转化为Set，用Set的迭代器才能进行元素迭代。 如何决定使用 HashMap 还是 TreeMap？ 都是非线程安全的，如果你需要得到一个有序的结果时就应该使用TreeMap（因为HashMap中元素的排列顺序是不固定的）。除此之外，由于HashMap有更好的性能，故大多不需要排序的时候我们会使用HashMap。 TreeMap的特点？ 非线程安全，可排序，正常情况下不允许重复(默认没有重写比较器)，底层采用的是红黑树。因为其非线程安全，故可以允许null值，这个跟HashMap、ArrayList、LinkedList是一样的。 Map 的 key 值都是不可以重复的，故 Set 与 List 最大的区别就是一个数据不可重复，一个是可以重复的。 HashMap、TreeMap、ConcurrentHashMap、HashTable、ArrayDeque、ArrayList、LinkedList、Vector线程安全问题以及 key、value 是否能为空？ 集合 线程是否安全 key、value是否可以为空 原因 HashMap 非线程安全 Key、value均能为空 Key 可以为空，但必须唯一，value没有限制 TreeMap 非线程安 Key不能为null、value不能为null key不能为null，因为其需要排序，value可以为null ConcurrentHashMap 线程安全 Key、value均不能为空 key不能为空，因为采用了fail-safe机制，这种机制会使得读取的数据不一定是最新的，使用null值，就会使得其无法判断对应的key是不存在还是为空，因为你无法再调用一次contains(key）来对key是否存在进行判断，HashTable同理。故在入参时，若为 null 就报空指针异常，而且在取hashcode时，压根就没考虑空的情况。 HashTable 线程安全 Key、value均不能为空 value不能为空，当通过get(k)获取对应的value时，如果获取到的是null时，无法判断，它是put（k,v）的时候value为null，还是这个key从来没有做过映射。假如线程1调用m.contains（key）返回true，然后在调用m.get(key)，这时的m可能已经不同了。因为线程2可能在线程1调用m.contains（key）时，删除了key节点，这样就会导致线程1得到的结果不明确，产生多线程安全问题，因此，Hashmap和ConcurrentHashMap的key和value不能为null。 ArrayDeque 非线程安全 值不能为空 不能存入 null，因为在add时就有判断，如果是 null，就报空指针异常 ArrayList 非线程安全 值可以为空 可以为空，当成一个对象加入或删除，添加null也会增加size LinkedList 非线程安全 值可以为空 同ArrayList Vector 线程安全 值不可以为空 不能为空，因为是线程安全的，使用了fail-safe机制，同ConcurrentHashMap LinkedHashMap参考LinkedHashMap源码详细分析 LinkedHashMap 源码解读(JDK 1.8) LinkedHashMap详解 Java集合之LinkedHashMap Java集合框架之LinkedHashMap详解 Java LinkedHashMap类源码解析 Java8集合系列之LinkedHashMap LinkedHashMap、ConcurrentHashMap概括 概述 LinkedHashMap是HashMap的一个子类，它保留插入或者访问顺序，帮助我们实现了有序的HashMap。 其维护一个双向链表，并不是说其除了维护存入的数据，另外维护了一个双向链表对象，而是说其根据重写HashMap的实体类Entry，来实现能够将HashMap的数据组成一个双向列表，其存储的结构还是数组+链表+红黑树的形式，也就是链表上的节点有三个指针，分别是before、after、next。 与 HashMap 的区别这里用一个 demo 呈现出 LinkedHashMap 的有序的特点 123456789101112 public static void main(String[] args) &#123;// LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap(16, 0.75f,true); LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap(); map.put("apple",10000); map.put("huawei",5000); map.put("xiaomi",2000);// map.get("apple");// map.get("xiaomi"); for(Map.Entry a:map.entrySet())&#123; System.out.println(a.getValue()); &#125; &#125; 1234输出为：1000050002000 可以看到，在使用上，LinkedHashMap和HashMap的区别就是LinkedHashMap是有序的。 上面这个例子是根据插入顺序排序，此外，LinkedHashMap还有一个参数(accessOrder)决定是否在此基础上再根据访问顺序(get,put)排序,记住，是在插入顺序的基础上再排序，后面看了源码就知道为什么了。看下例子: 123456789101112 public static void main(String[] args) &#123; LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap(16, 0.75f,true);// LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap(); map.put("apple",10000); map.put("huawei",5000); map.put("xiaomi",2000); map.get("apple"); map.get("xiaomi"); for(Map.Entry a:map.entrySet())&#123; System.out.println(a.getValue()); &#125; &#125; 1234输出为：5000100002000 很明显能看出来，由于将 accessOrder == true，按照了读取顺序排列，最开始的顺序是 apple - huawei - xiaomi，由于读取了apple，apple则进入到尾(tail)，顺序变为 huawei - xiaomi - apple，又读取了 xiaomi，故顺序变为 huawei - apple - xiaomi ，故最后的答案是 5000 - 10000 - 2000。 LinkedHashMap的存储结构LinkedHashMap之所以能实现存取的顺序性，主要是他重新定义了 Entry ，这个新的 Entry 继承自HashMap.Node，并做了新的扩展，下面我们结合源码来分析一下。 123456789101112131415161718192021/*** 继承自 HashMap.Node，新增了 before、after 记录插入顺序*///链表结点static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125;private static final long serialVersionUID = 3801124242820219131L;//链表头部transient LinkedHashMap.Entry&lt;K,V&gt; head;//链表尾部transient LinkedHashMap.Entry&lt;K,V&gt; tail;/** * 访问顺序 * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. */final boolean accessOrder; 由上面的代码我们可以看出，这个自定义的 Entry比 HashMap.Node多了两个属性，before和after。正是使用这两个关键的属性，在LinkedHashMap内部实现了一个双向链表。双向链表就是每个节点除了存储数据本身之外，还保存着两个指针，在java里面就是指向对象的引用，一个是前驱节点，也就是他的前一个节点的引用，一个是后继节点，也就是他的后一个节点的引用。这样，就可以实现存储一个有序节点的数据结构了。（这里说明下，在jdk1.7中，使用的结构为环形双向链表）另外，继承自HashMap.Node的Entry自身还保留着用于维持单链表的next属性，因此LinkedHashMap的Entry节点具有三个指针域，next指针维护Hash桶中冲突key的链表，before和after维护双向链表。结构如下图所示： 重要方法构造函数两个构造方法，一个是继承 HashMap ，一个是可以选择 accessOrder 的值(默认 false，代表按照插入顺序排序)来确定是按插入顺序还是读取顺序排序。 123456789101112131415161718192021//调用父类HashMap的构造方法。public LinkedHashMap(int initialCapacity) &#123; super(initialCapacity); accessOrder = false;&#125;/** * //调用父类HashMap的构造方法。 * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the default initial capacity (16) and load factor (0.75). */public LinkedHashMap() &#123; super(); accessOrder = false;&#125;public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; put()LinkedHashMap 的 put 方法也是使用 HashMap 的方法，不同在于重写了 newNode(), afterNodeAccess() 和 afterNodeInsertion() 这几个方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// HashMap 中实现public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;// HashMap 中实现final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) &#123;...&#125; // 通过节点 hash 定位节点所在的桶位置，并检测桶中是否包含节点引用 if ((p = tab[i = (n - 1) &amp; hash]) == null) &#123;...&#125; else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) &#123;...&#125; else &#123; // 遍历链表，并统计链表长度 for (int binCount = 0; ; ++binCount) &#123; // 未在单链表中找到要插入的节点，将新节点接在单链表的后面 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) &#123;...&#125; break; &#125; // 插入的节点已经存在于单链表中 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) &#123;...&#125; afterNodeAccess(e); // 回调方法，后续说明！！！重点哈！！！ return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) &#123;...&#125; afterNodeInsertion(evict); // 回调方法，后续说明！！！重点哈！！！ return null;&#125;// HashMap 中实现Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next);&#125;// LinkedHashMap 中覆写！！！！！重点！！！Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); // 将 Entry 接在双向链表的尾部 linkNodeLast(p); return p;&#125;// LinkedHashMap 中实现！！！！！private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; // last 为 null，表明链表还未建立 if (last == null) head = p; else &#123; // 将新节点 p 接在链表尾部 p.before = last; last.after = p; &#125;&#125; 我把 newNode 方法红色背景标注了出来，这一步比较关键。LinkedHashMap 覆写了该方法。在这个方法中，LinkedHashMap 创建了 Entry，并通过 linkNodeLast 方法将 Entry 接在双向链表的尾部，实现了双向链表的建立。双向链表建立之后，我们就可以按照插入顺序去遍历 LinkedHashMap。 接下来就重点介绍 newNode() 、linkNodeLast()、afterNodeAccess() 、afterNodeInsertion()、afterNodeInsertion() newNode() 12345678910/** * 根据 key-value 创建双向链表节点 * e 表示下一个节点, 不过这里是空值, 不用理会 */Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125; linkNodeLast() 1234567891011121314/** * 把新节点插入到双向链表尾部 */private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; // 如果这是空链表, 新节点就是头结点 if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125;&#125; afterNodeAccess() &amp; afterNodeInsertion() &amp; afterNodeRemoval() 1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125; 根据这三个方法的注释可以看出，这些方法的用途是在增删查等操作后，通过回调的方式，让 LinkedHashMap 有机会做一些后置操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//标准的如何在双向链表中将指定元素放入队尾// LinkedHashMap 中覆写//访问元素之后的回调方法/** * 1. 使用 get 方法会访问到节点, 从而触发调用这个方法 * 2. 使用 put 方法插入节点, 如果 key 存在, 也算要访问节点, 从而触发该方法 * 3. 只有 accessOrder 是 true 才会调用该方法 * 4. 这个方法会把访问到的最后节点重新插入到双向链表结尾 */void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last // 用 last 表示插入 e 前的尾节点 // 插入 e 后 e 是尾节点, 所以也是表示 e 的前一个节点 LinkedHashMap.Entry&lt;K,V&gt; last; //如果是访问序，且当前节点并不是尾节点 //将该节点置为双向链表的尾部 if (accessOrder &amp;&amp; (last = tail) != e) &#123; // p: 当前节点 // b: 前一个节点 // a: 后一个节点 // 结构为: b &lt;=&gt; p &lt;=&gt; a LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 结构变成: b &lt;=&gt; p &lt;- a p.after = null; // 如果当前节点 p 本身是头节点, 那么头结点要改成 a if (b == null) head = a; // 如果 p 不是头尾节点, 把前后节点连接, 变成: b -&gt; a else b.after = a; // a 非空, 和 b 连接, 变成: b &lt;- a if (a != null) a.before = b; // 如果 a 为空, 说明 p 是尾节点, b 就是它的前一个节点, 符合 last 的定义 // 这个 else 没有意义，因为最开头if已经确保了p不是尾结点了，自然after不会是null else last = b; // 如果这是空链表, p 改成头结点 if (last == null) head = p; // 否则把 p 插入到链表尾部 else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 12345678910111213141516void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // 优美的一笔，学习一波如何在双向链表中删除节点 LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 将 p 节点的前驱后后继引用置空 p.before = p.after = null; // b 为 null，表明 p 是头节点 if (b == null) head = a; else b.after = a; // a 为 null，表明 p 是尾节点 if (a == null) tail = b; else a.before = b;&#125; 123456789101112131415161718192021222324252627// 在插入一个新元素之后，如果是按插入顺序排序，即调用newNode()中的linkNodeLast()完成// 如果是按照读取顺序排序，即调用afterNodeAccess()完成// 那么这个方法是干嘛的呢，这个就是著名的 LRU 算法啦// 在插入完成之后，需要回调函数判断是否需要移除某些元素！/** * 插入新节点才会触发该方法，因为只有插入新节点才需要内存 * 根据 HashMap 的 putVal 方法, evict 一直是 true * removeEldestEntry 方法表示移除规则, 在 LinkedHashMap 里一直返回 false * 所以在 LinkedHashMap 里这个方法相当于什么都不做 */void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 根据条件判断是否移除最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;// 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存// LinkedHashMap是默认返回false的，我们可以继承LinkedHashMap然后复写该方法即可// 例如 LeetCode 第 146 题就是采用该种方法，直接 return size() &gt; capacity;protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; get()前面说了插入顺序的实现，本节来讲讲访问顺序。默认情况下，LinkedHashMap 是按插入顺序维护链表。不过我们可以在初始化 LinkedHashMap，指定 accessOrder 参数为 true，即可让它按访问顺序维护链表。访问顺序的原理上并不复杂，当我们调用get、getOrDefault、replace等方法时，只需要将这些方法访问的节点移动到链表的尾部即可。相应的源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344// LinkedHashMap 中覆写public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; // 如果 accessOrder 为 true，则调用 afterNodeAccess 将被访问节点移动到链表最后 if (accessOrder) afterNodeAccess(e); return e.value;&#125;// LinkedHashMap 中覆写void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; // 如果 b 为 null，表明 p 为头节点 if (b == null) head = a; else b.after = a; if (a != null) a.before = b; /* * 这里存疑，父条件分支已经确保节点 e 不会是尾节点， * 那么 e.after 必然不会为 null，不知道 else 分支有什么作用 */ else last = b; if (last == null) head = p; else &#123; // 将 p 接在链表的最后 p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; delete()与插入操作一样，LinkedHashMap 删除操作相关的代码也是直接用父类的实现。在删除节点时，父类的删除逻辑并不会修复 LinkedHashMap 所维护的双向链表，这不是它的职责。那么删除及节点后，被删除的节点该如何从双链表中移除呢？当然，办法还算是有的。上一节最后提到 HashMap 中三个回调方法运行 LinkedHashMap 对一些操作做出响应。所以，在删除及节点后，回调方法 afterNodeRemoval 会被调用。LinkedHashMap 覆写该方法，并在该方法中完成了移除被删除节点的操作。相关源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110// HashMap 中实现public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;// HashMap 中实现final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) &#123;...&#125; else &#123; // 遍历单链表，寻找要删除的节点，并赋值给 node 变量 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) &#123;...&#125; // 将要删除的节点从单链表中移除 else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); // 调用删除回调方法进行后续操作 return node; &#125; &#125; return null;&#125;//具体分析 上面 put() 中讲的很清楚了！// LinkedHashMap 中覆写void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; // 将 p 节点的前驱后后继引用置空 p.before = p.after = null; // b 为 null，表明 p 是头节点 if (b == null) head = a; else b.after = a; // a 为 null，表明 p 是尾节点 if (a == null) tail = b; else a.before = b;&#125;// 我已经在 HashMap 部分讲过了final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; 删除的过程并不复杂，上面这么多代码其实就做了三件事： 根据 hash 定位到桶位置 遍历链表或调用红黑树相关的删除方法 从 LinkedHashMap 维护的双链表中移除要删除的节点 replacementTreeNode()在进行红黑树转换的时候一些方法也进行了重写，如下： 123456789101112131415161718192021TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; LinkedHashMap.Entry&lt;K,V&gt; q = (LinkedHashMap.Entry&lt;K,V&gt;)p; //TreeNode依旧是HashMap中的内部类 TreeNode&lt;K,V&gt; t = new TreeNode&lt;K,V&gt;(q.hash, q.key, q.value, next); transferLinks(q, t); return t;&#125;//就是对原来双向链表中的结点结点进行替换，替换成TreeNode结点（另外，TreeNode继承了LinkedHashMap.Entry）private void transferLinks(LinkedHashMap.Entry&lt;K,V&gt; src, LinkedHashMap.Entry&lt;K,V&gt; dst) &#123; LinkedHashMap.Entry&lt;K,V&gt; b = dst.before = src.before; LinkedHashMap.Entry&lt;K,V&gt; a = dst.after = src.after; if (b == null) head = dst; else b.after = dst; if (a == null) tail = dst; else a.before = dst;&#125; 迭代LinkedHashMap 的遍历方式和 HashMap 的一样，都是通过 entrySet 方法返回 Set 实例,，然后通过 iterator 方法返回迭代器进行遍历。 entrySet 12345678910111213141516171819/** * 返回 LinkedEntrySet 实例, 这是非静态内部类 */public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; return (es = entrySet) == null ? (entrySet = new LinkedEntrySet()) : es;&#125;/** * 和 HashMap 的 EntrySet 类一样继承 AbstractSet * iterator 方法返回 LinkedEntryIterator 实例 */final class LinkedEntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; ... public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return new LinkedEntryIterator(); &#125; ...&#125; next 和 hasNext 1234567891011121314151617181920212223242526272829303132333435363738394041/** * next 方法实际是调用父类 nextNode 方法返回节点 */final class LinkedEntryIterator extends LinkedHashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125;abstract class LinkedHashIterator &#123; LinkedHashMap.Entry&lt;K,V&gt; next; LinkedHashMap.Entry&lt;K,V&gt; current; int expectedModCount; /** * 构造函数, 从双向链表头节点开始遍历 */ LinkedHashIterator() &#123; next = head; expectedModCount = modCount; current = null; &#125; public final boolean hasNext() &#123; return next != null; &#125; /** * 遍历比较简单, 直接读取下一个节点就行 */ final LinkedHashMap.Entry&lt;K,V&gt; nextNode() &#123; LinkedHashMap.Entry&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); current = e; next = e.after; //直接遍历双向链表的下一个节点即可 return e; &#125; ...&#125; 重要补充点###为何 TreeNode 这个类继承的是 LinkedHashMap 的 Entry，而不是 HashMap 中的 Node ？ 先来看看继承体系结构图： 上面的继承体系乍一看还是有点复杂的，同时也有点让人迷惑。HashMap 的内部类 TreeNode 不继承它的内部类 Node，却继承自 Node 的子类 LinkedHashMap 内部类 Entry。这里这样做是有一定原因的，这里先不说。先来简单说明一下上面的继承体系。LinkedHashMap 内部类 Entry 继承自 HashMap 内部类 Node，并新增了两个引用，分别是 before 和 after。这两个引用的用途不难理解，也就是用于维护双向链表。同时，TreeNode 继承 LinkedHashMap 的内部类 Entry 后，就具备了和其他 Entry 一起组成链表的能力。但是这里需要大家考虑一个问题。当我们使用 HashMap 时，TreeNode 并不需要具备组成链表能力。如果继承 LinkedHashMap 内部类 Entry ，TreeNode 就多了两个用不到的引用，这样做不是会浪费空间吗？简单说明一下这个问题（水平有限，不保证完全正确），这里这么做确实会浪费空间，但与 TreeNode 通过继承获取的组成链表的能力相比，这点浪费是值得的。在 HashMap 的设计思路注释中，有这样一段话： Because TreeNodes are about twice the size of regular nodes, weuse them only when bins contain enough nodes to warrant use(see TREEIFY_THRESHOLD). And when they become too small (due toremoval or resizing) they are converted back to plain bins. Inusages with well-distributed user hashCodes, tree bins arerarely used. 大致的意思是 TreeNode 对象的大小约是普通 Node 对象的2倍，我们仅在桶（bin）中包含足够多的节点时再使用。当桶中的节点数量变少时（取决于删除和扩容），TreeNode 会被转成 Node。当用户实现的 hashCode 方法具有良好分布性时，树类型的桶将会很少被使用。 通过上面的注释，我们可以了解到。一般情况下，只要 hashCode 的实现不糟糕，Node 组成的链表很少会被转成由 TreeNode 组成的红黑树。也就是说 TreeNode 使用的并不多，浪费那点空间是可接受的。假如 TreeNode 机制继承自 Node 类，那么它要想具备组成链表的能力，就需要 Node 去继承 LinkedHashMap 的内部类 Entry。这个时候就得不偿失了，浪费很多空间去获取不一定用得到的能力。 利用LinkedHashMap实现LRU缓存( 或者 LeetCode 第 146 题)LRU即Least Recently Used，最近最少使用，也就是说，当缓存满了，会优先淘汰那些最近最不常访问的数据。我们的LinkedHashMap正好满足这个特性，为什么呢？当我们开启accessOrder为true时，最新访问(get或者put(更新操作))的数据会被丢到队列的尾巴处，那么双向队列的头就是最不经常使用的数据了。比如: 如果有1 2 3这3个Entry，那么访问了1，就把1移到尾部去，即2 3 1。每次访问都把访问的那个数据移到双向队列的尾部去，那么每次要淘汰数据的时候，双向队列最头的那个数据不就是最不常访问的那个数据了吗？换句话说，双向链表最头的那个数据就是要淘汰的数据。 此外，LinkedHashMap还提供了一个方法，这个方法就是为了我们实现LRU缓存而提供的，removeEldestEntry(Map.Entry eldest) 方法。该方法可以提供在每次添加新条目时移除最旧条目的实现程序，默认返回 false。 来，给大家一个简陋的LRU缓存: 1234567891011121314151617public class LRUCache extends LinkedHashMap&#123; public LRUCache(int maxSize) &#123; super(maxSize, 0.75F, true); maxElements = maxSize; &#125; protected boolean removeEldestEntry(java.util.Map.Entry eldest) &#123; //逻辑很简单，当大小超出了Map的容量，就移除掉双向队列头部的元素，给其他元素腾出点地来。 return size() &gt; maxElements; &#125; private static final long serialVersionUID = 1L; protected int maxElements;&#125; ConcurrentHashMap参考ConcurrentHashMap 源码分析 并发容器之ConcurrentHashMap(JDK 1.8版本) 写的最好！ 死磕 ConcurrentHashMap Java 8 ConcurrentHashMap源码分析 ConcurrentHashMap 详解一 java并发之ConcurrentHashMap 1.8原理详解 写的也非常有深度，可惜排版很差！！！ 写得好 主要就是参考上面三个自己觉得写得好的！ 概述在使用 HashMap 时在多线程情况下扩容会出现 CPU 接近 100%的情况，因为 hashmap 并不是线程安全的，通常我们可以使用在 java 体系中古老的 hashtable 类，该类基本上所有的方法都采用 synchronized 进行线程安全的控制，可想而知，在高并发的情况下，每次只有一个线程能够获取对象监视器锁，这样的并发性能的确不令人满意。另外一种方式通过 Collections 的Map synchronizedMap(Map m)将 hashmap 包装成一个线程安全的 map。比如 SynchronzedMap 的 put 方法源码为： 123public V put(K key, V value) &#123; synchronized (mutex) &#123;return m.put(key, value);&#125;&#125; 实际上 SynchronizedMap 实现依然是采用 synchronized 独占式锁进行线程安全的并发控制的。同样，这种方案的性能也是令人不太满意的。针对这种境况，Doug Lea 大师不遗余力的为我们创造了一些线程安全的并发容器，让每一个 java 开发人员倍感幸福。相对于 hashmap 来说，ConcurrentHashMap 就是线程安全的 map，其中利用了锁分段的思想提高了并发度。 ConcurrentHashMap 在 JDK1.7 的版本网上资料很多，有兴趣的可以去看看。 JDK 1.7 版本关键要素： segment 继承了 ReentrantLock 充当锁的角色，为每一个 segment 提供了线程安全的保障； segment 维护了哈希散列表的若干个桶，每个桶由 HashEntry 构成的链表。 而到了 JDK 1.8 的 ConcurrentHashMap 就有了很大的变化，光是代码量就足足增加了很多。1.8 版本舍弃了 segment，并且大量使用了 synchronized，以及 CAS 无锁操作以保证 ConcurrentHashMap 操作的线程安全性。至于为什么不用 ReentrantLock 而是 Synchronzied 呢？实际上，synchronzied 做了很多的优化，包括偏向锁，轻量级锁，重量级锁，可以依次向上升级锁状态，但不能降级，因此，使用 synchronized 相较于 ReentrantLock 的性能会持平甚至在某些情况更优，具体的性能测试可以去网上查阅一些资料。另外，底层数据结构改变为采用数组+链表+红黑树的数据形式。 作者: 你听___链接：https://juejin.im/post/5aeeaba8f265da0b9d781d16来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 与HashMap区别线程安全 与 非线程安全 的区别 关键属性和类table所有数据都存在table中，table的容量会根据实际情况进行扩容，table[i]存放的数据类型有以下3种： TreeBin 用于包装红黑树结构的结点类型 ForwardingNode 扩容时存放的结点类型，并发扩容的实现关键之一 Node 普通结点类型，表示链表头结点 作为 ConcurrentHashMap 的数据容器，采用 懒加载 的方式，直到第一次插入数据的时候才会进行初始化操作，数组的大小总是为 2 的幂次方。 nextTable12//扩容时使用，平时为 null，只有在扩容的时候才为非 nullvolatile Node&lt;K,V&gt;[] nextTable; sizeCtl1private transient volatile int sizeCtl; 控制标识符，用来控制table的初始化和扩容的操作，不同的值有不同的含义： 当为负数时：-1代表正在初始化（其实也就相当于第一次扩容），-N代表有N-1个线程正在帮忙进行扩容 当为0时：代表当时的table还没有被初始化 当为正数时：表示初始化或者下一次进行扩容的大小 sizeCtl 变量担当了 5 种角色， 设计非常精巧 ① 首次初始化时， 其变量含义为初始容量 ② 扩容以后， 其值为触发下一次扩容的元素数量阈值 ③ 其正负状态， 标识了当前数组是否处于扩容状态 ④ sizeCtl 为负值时，高16位bit反映了正在进行的扩容操作是针对哪个容量进行的 ⑤ sizeCtl 为负值时，低 16位bit 反映了参与此次扩容的线程有多少个 1234567891011if table未完成初始化: sizeCtl = 0 //未指定初始容量时的默认值 sizeCtl &gt; 0 //指定初始容量(非传入值，是2的幂次修正值)大小的两倍 sizeCtl =- 1 //表明table正在初始化else if nextTable为空: if 扩容时发生错误(如内存不足、table.length * 2 &gt; Integer.MAX_VALUE等): sizeCtl = Integer.MAX_VALUE //不必再扩容了！ else: sizeCtl = table.length * 0.75 //扩容阈值调为table容量大小的0.75倍else: sizeCtl = -(1+N) //N的低RESIZE_STAMP_SHIFT位表示参与扩容线程数，后面详细介绍 sun.misc.Unsafe U在 ConcurrentHashMap 的实现中可以看到大量的 U.compareAndSwapXXXX 的方法去修改 ConcurrentHashMap 的一些属性。这些方法实际上是利用了 CAS 算法 保证了线程安全性，这是一种乐观策略，假设每一次操作都不会产生冲突，当且仅当冲突发生的时候再去尝试。而 CAS 操作依赖于现代处理器指令集，通过底层CMPXCHG指令实现。CAS(V,O,N)核心思想为：若当前变量实际值 V 与期望的旧值 O 相同，则表明该变量没被其他线程进行修改，因此可以安全的将新值 N 赋值给变量；若当前变量实际值 V 与期望的旧值 O 不相同，则表明该变量已经被其他线程做了处理，此时将新值 N 赋给变量操作就是不安全的，在进行重试。而在大量的同步组件和并发容器的实现中使用 CAS 是通过sun.misc.Unsafe类实现的，该类提供了一些可以直接操控内存和线程的底层操作，可以理解为 java 中的“指针”。该成员变量的获取是在静态代码块中： 12345678static &#123; try &#123; U = sun.misc.Unsafe.getUnsafe(); ....... &#125; catch (Exception e) &#123; throw new Error(e); &#125;&#125; 在 ConcurrentHashMap 中，主要用到 CAS 算法的有三个方法： tabAt 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; 该方法用来获取 table 数组中索引为 i 的 Node 元素。 casTabAt 1234static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; 基于 CAS 尝试更新 table 上下标为 i 的结点的值为 v。 setTabAt 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 用于设置 table 上下标为 i 的结点为 v，相对于 casTabAt 方法的区别在于不关注历史值。 NodeNode 类实现了 Map.Entry 接口，主要存放 key-value 对，并且具有 next 域，另外可以看出很多属性都是用 volatile 进行修饰的，也就是为了保证内存可见性。 1234567static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; ......&#125; TreeNode树节点，继承于承载数据的 Node 类。而红黑树的操作是针对 TreeBin 类的，从该类的注释也可以看出，也就是 TreeBin 会将 TreeNode 进行再一次封装 1234567891011** * Nodes for use in TreeBins */static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; ......&#125; TreeBin这个类并不负责包装用户的 key、value 信息，而是包装的很多 TreeNode 节点。实际的 ConcurrentHashMap“数组”中，存放的是 TreeBin 对象，而不是 TreeNode 对象。 1234567891011static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock ......&#125; ForwardingNode在扩容时才会出现的特殊节点，在转移的时候放在头部的节点，是一个空节点，其 key,value,hash 全部为 null。并拥有 nextTable 指针引用新的 table 数组。 12345678static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; .....&#125; 部分 final 常量1234567891011121314151617181920212223242526272829// node数组最大容量：2^30=1073741824,因为第一位是符号位，所以是 30 次幂private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认初始值，必须是2的幂数private static final int DEFAULT_CAPACITY = 16;//数组可能最大值，需要与toArray（）相关方法关联static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;//并发级别，遗留下来的，为兼容以前的版本private static final int DEFAULT_CONCURRENCY_LEVEL = 16;// 负载因子private static final float LOAD_FACTOR = 0.75f;// 链表转红黑树阀值,&gt; 8 链表转换为红黑树static final int TREEIFY_THRESHOLD = 8;//树转链表阀值，小于等于6（tranfer时，lc、hc=0两个计数器分别++记录原bin、新binTreeNode数量，&lt;=UNTREEIFY_THRESHOLD 则untreeify(lo)）static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;private static final int MIN_TRANSFER_STRIDE = 16;private static int RESIZE_STAMP_BITS = 16;// 2^15-1，help resize的最大线程数private static final int MAX_RESIZERS = (1 &lt;&lt; (32 - RESIZE_STAMP_BITS)) - 1;// 32-16=16，sizeCtl中记录size大小的偏移量private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS;// forwarding nodes的hash值static final int MOVED = -1; // 树根节点的hash值static final int TREEBIN = -2; // ReservationNode的hash值static final int RESERVED = -3; // 可用处理器数量static final int NCPU = Runtime.getRuntime().availableProcessors(); 主要方法构造函数在使用 ConcurrentHashMap 第一件事自然而然就是 new 出来一个 ConcurrentHashMap 对象，一共提供了如下几个构造器方法： 12345678910// 1. 构造一个空的map，即table数组还未初始化，初始化放在第一次插入数据时，默认大小为16ConcurrentHashMap()// 2. 给定map的大小ConcurrentHashMap(int initialCapacity)// 3. 给定一个mapConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m)// 4. 给定map的大小以及加载因子ConcurrentHashMap(int initialCapacity, float loadFactor)// 5. 给定map大小，加载因子以及并发度（预计同时操作数据的线程，jdk1.8基本不用）ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) ConcurrentHashMap 一共给我们提供了 5 种构造器方法，具体使用请看注释，我们来看看第 2 种构造器，传入指定大小时的情况，该构造器源码为： 123456789101112public ConcurrentHashMap(int initialCapacity) &#123; //1. 小于0直接抛异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //2. 判断是否超过了允许的最大值，超过了话则取最大值，否则再对该值进一步处理 // 至于为什么这样取这个值...暂时我还没搞懂，后面也有tryPresize()也是这样 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); //3. 赋值给sizeCtl this.sizeCtl = cap;&#125; 这段代码的逻辑请看注释，很容易理解，如果小于 0 就直接抛出异常，如果指定值大于了所允许的最大值的话就取最大值，否则，在对指定值做进一步处理。最后将 cap 赋值给 sizeCtl,关于 sizeCtl 的说明请看上面的说明，当调用构造器方法之后，sizeCtl 的大小应该就代表了 ConcurrentHashMap 的大小，即 table 数组长度。tableSizeFor 做了哪些事情了？源码为： 12345678910111213/** * Returns a power of two table size for the given desired capacity. * See Hackers Delight, sec 3.2 */private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 通过注释就很清楚了，该方法会将调用构造器方法时指定的大小转换成一个 2 的幂次方数，也就是说 ConcurrentHashMap 的大小一定是 2 的幂次方，比如，当指定大小为 18 时，为了满足 2 的幂次方特性，实际上 concurrentHashMapd 的大小为 2 的 5 次方（32）。另外，需要注意的是，调用构造器方法的时候并未构造出 table 数组（可以理解为 ConcurrentHashMap 的数据容器），只是算出 table 数组的长度，当第一次向 ConcurrentHashMap 插入数据的时候才真正的完成初始化创建 table 数组的工作。 初始化table整体流程如下： 判断 sizeCtl 值是否小于 0，如果小于 0 则表示 ConcurrentHashMap 正在执行初始化操作，所以需要先等待一会，如果其它线程初始化失败还可以顶替上去 如果 sizeCtl 值大于等于 0，则基于 CAS 策略抢占标记 sizeCtl 为 -1，表示 ConcurrentHashMap 正在执行初始化，然后构造 table，并更新 sizeCtl 的值 1234567891011121314151617181920212223242526private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) // 1. 保证只有一个线程正在进行初始化操作，发现sizeCtl为负数时，得让出时间片等待 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // 2. 得出数组的大小 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings("unchecked") // 3. 这里才真正的初始化数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; // 4. 计算数组中可用的大小：实际大小n*0.75（加载因子） sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; put() 【重点，有很多引申，包括扩容】先放源码，然后我们慢慢来分析一波，这是整个 ConcurrentHashMap() 的核心，我们以点推面，将 put() 用到的比较有代表性的函数都挑出来讲一讲！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); //1.两次hash，减少hash冲突，可以均匀分布，后面会讲 int binCount = 0; //用于记录元素个数,也就是链表长度 for (Node&lt;K,V&gt;[] tab = table;;) &#123; //对这个table进行迭代 Node&lt;K,V&gt; f; int n, i, fh; //2.这里就是上面构造方法没有进行初始化，在这里进行判断，为null就调用initTable进行初始化，属于懒汉模式初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); //上面已经讲过了 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123;//3.如果i位置没有数据，就直接无锁CAS插入,上文已经讲过了 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED)//4.如果在进行扩容(当前节点是forwardingNode)，则先进行扩容操作 tab = helpTransfer(tab, f); //后面会详解 else &#123; V oldVal = null; //5.如果以上条件都不满足，那就要进行加锁操作，即存在hash冲突，锁住链表或者红黑树的头结点 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; //f改变再次循环 if (fh &gt;= 0) &#123; //表示该节点是链表结构(红黑树 -2，或者正在转移都为负数) binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //这里涉及到相同的key进行put就会覆盖原先的value if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) //只有不存在的时候才插入 e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; //插入链表尾部 pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123;//红黑树结构 Node&lt;K,V&gt; p; binCount = 2; //红黑树结构旋转插入 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //6.如果链表的长度大于8时就会进行红黑树的转换 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount);//7.统计size，并且检查是否需要扩容 return null; 故整个 put() 的流程分为 7 步： 1.调用 spread()，进行两次哈希 2.如果没有初始化就先调用 initTable() 方法来进行初始化过程 3.如果没有hash冲突就直接 CAS 插入 4.如果还在进行扩容操作就先进行扩容 5.如果存在hash冲突，就加锁来保证线程安全，这里有两种情况，一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入 6.最后一个如果该链表的数量大于阈值8，就要先转换成黑红树的结构，break再一次进入循环 7.如果添加成功就调用 addCount() 方法统计 size，并且检查是否需要扩容 接下来针对每一步进行细致的分析，包括内部函数的具体实现！ spread()首先是 spread() 函数！先上源码！ 1234static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashstatic final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 就是我们在 HashMap 章节讲过的，将 hashcode 值的低16位和高16位 相异或，也称之为 扰动函数！ initTable() &amp; tabAt() &amp; casTabAt()上文已经讲过这三个函数了，让我们 go on！！ helpTransfer()如果在进行扩容，则先进行扩容操作，这个函数就是帮助扩容的！这样可以提高扩容的速度！ 该方法的主要作用就是基于 CAS 尝试添加一个线程去协助扩容操作，如果能够成功加入则将 sizeCtl 值加 1。方法 transfer 是真正执行扩容操作的地方，并在多个步骤中被触发。这里先给出该方法的定义（如下），具体的实现后面会专门进行分析，该方法接收 2 个参数，其中 tab 是当前需要被扩容的 table，而 nextTab 是扩容之后的 table，容量上是之前的两倍，helpTransfer 传递的 nextTab 是一个非 null 值，因为触发 helpTransfer 的前提就是当前已经处于扩容阶段。 1234567891011121314151617181920212223242526272829303132 /** * Helps transfer if a resize is in progress. * 帮助从旧的table的元素复制到新的table中 */final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; // 新的table nextTab已经存在前提下才能帮助扩容 // 当前结点是 ForwardingNode 类型 if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); // 下面会讲，扩容戳 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; // 条件1判断是否为当前扩容戳 // 条件2判断是否扩容结束 // 条件3判断扩容线程是否已经超过最大并发扩容线程数 (这其实是一个bug,见下面详解) // 条件4判断当前是否有可以分配的任务 // sc=((rs=resizeStamp())&lt;&lt;&lt;RESIZE_STAMP_SHIFT ) // sc 高16位用于记录本次扩容戳,低16位用于记录扩容线程数量,rs = sc - 1 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; // 基于 CAS 将 扩容线程数 + 1，说明又有一个线程加入扩容了 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab);// 调用扩容方法，扩容最核心的函数 break; &#125; &#125; return nextTab; &#125; return table;&#125; 在 helptransfer() 中，调用了 resizeStamp() 函数(当然还有 transfer()，这个是扩容核心代码，下面单独会拎出来讲)，我们现在分析一下这两个函数！ 123456789 /* ---------------- Table Initialization and Resizing -------------- */// 生成表的扩容戳，每个n都有不同的扩容戳 /** * Returns the stamp bits for resizing a table of size n. * Must be negative when shifted left by RESIZE_STAMP_SHIFT. */ static final int resizeStamp(int n) &#123; return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1)); &#125; Integer.numberOfLeadingZeros(n) 在指定 int 值的二进制补码表示形式中最高位（最左边）的 1 位之前，返回零位的数量。具体我也不懂啥意思，反正就是会生成一个特殊的扩容戳！ treeifyBin()来到这里，就是 put() 的第六步了，上源码！ 12345678910111213141516171819202122232425262728293031private final void treeifyBin(Node&lt;K, V&gt;[] tab, int index) &#123; Node&lt;K, V&gt; b; int n, sc; if (tab != null) &#123; // 1. 如果 table 长度小于 64，执行扩容操作 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) &#123; this.tryPresize(n &lt;&lt; 1); &#125; // 2. 否则，将链表转换成红黑树 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; // 头结点 hash 大于 0，说明是链表 synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K, V&gt; hd = null, tl = null; // 将链表转换成一棵红黑树 for (Node&lt;K, V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K, V&gt; p = new TreeNode&lt;&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) &#123; hd = p; &#125; else &#123; tl.next = p; &#125; tl = p; &#125; // 将红黑树设置到 table 对应的位置 // 注意哦，数组中存储的可是 TreeBin setTabAt(tab, index, new TreeBin&lt;&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 细心的小伙伴发现了，这里又有一个扩容操作的函数 tryPresize() ，来，咱不能放过它，进去看看！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private final void tryPresize(int size) &#123; // 如果当前期望的大小（size）小于最大允许容量的一半，则扩容大小为 size 的 1.5 倍加 1，在向上取最小的 2 次幂,注意哦，这里的 size 传进来的时候就已经是原数组的两倍大小了！！！ // 这里我没搞清他要干嘛... 因为 c 可是一次性扩了好多倍 // 真正去执行扩容的还是 transfer 这个函数 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; // 检查当前未处于扩容阶段 Node&lt;K, V&gt;[] tab = table; int n; // 初始化 nextTable if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K, V&gt;[] nt = (Node&lt;K, V&gt;[]) new Node&lt;?, ?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; &#125; &#125; else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) &#123; break; &#125; else if (tab == table) &#123; int rs = resizeStamp(n); // 2. 基于 CAS 将 sc 的值加 1，然后执行 transfer 方法 if (sc &lt; 0) &#123; Node&lt;K, V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) &#123; break; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; // 执行 transfer 方法 this.transfer(tab, nt); &#125; &#125; // 1. 基于 CAS 将 sizeCtl 的值设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) &#123; // 执行 transfer 方法，此时 nextTable 是 null this.transfer(tab, null); &#125; &#125; &#125;&#125; 该方法的核心操作在于最后一个添加 transfer 任务，并设置 sizeCtl 值，该方法第一次调用 transfer 方法时 sizeCtl 一定是大于等于 0 的，所以方法会尝试将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，这是大负数，并执行 transfer(tab, null) 操作，后面的循环 sizeCtl 均小于 0，所以会执行 transfer(tab, nt)，并将 sizeCtl 加 1。注意整个过程中 sizeCtl 值的变化，在一次扩容操作中第一次调用 transfer 方法时将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，并在扩容过程再次调用 transfer 方法时将 sizeCtl 加 1。这对于下一节理解扩容操作什么时候结束至关重要。 transfer()来了，它终于来了…在之前我们已经铺垫了很久了，本来是打算再放到后面一点的，因为在 put() 的第七步 addCount() 方法中也调用了这个方法，但是… 我忍不住了！！！ 在讲 transfer() 之前，我们来回顾一下总共有哪些函数调用了它，也就是我们需要在什么时候扩容呢？ helpTransfer()。当一个线程要对table中元素进行操作的时候，如果检测到节点的HASH值为MOVED的时候，就会调用helpTransfer方法，在helpTransfer中再调用transfer方法来帮助完成数组的扩容。 tryPresize()。treeIfybin 和 putAll 方法中调用，treeIfybin 主要是在put添加元素完之后，判断该数组节点相关元素是不是已经超过8个的时候，如果超过则会调用这个 tryPresize 方法来扩容数组或者把链表转为树。 addCount()。当对数组进行操作，使得数组中存储的元素个数发生了变化的时候会调用的方法。 总结一下，也就是两种情况： 只有在往map中添加元素的时候，在某一个节点的数目已经超过了8个，同时数组的长度又小于64的时候，才会触发数组的扩容。 当数组中元素达到了sizeCtl的数量的时候，则会调用transfer方法来进行扩容。 扩容操作简单地说就是新建一个长度翻倍的 nextTable，然后将之前 table 上的结点重新哈希迁移到新的 nextTable 上，并在迁移完成之后使用 nextTable 替换原先的 table。对于一个 table 而言，上面分布着 n 个 bin 结点，而结点迁移的过程可以是并发的，这样可以提升迁移的效率。ConcurrentHashMap 使用了一个 stride 变量用于指定将 stride 个 bin 结点组成一个任务单元由一个线程负责处理，在单核 CPU 下 stride 的值为 table 的长度 n，在多核 CPU 下为 (n &gt;&gt;&gt; 3) / NCPU，最小值为 16。 ConcurrentHashMap 定义了一个类实例变量 transferIndex，用于指定任务的边界。任务划分的过程在 table 上是从后往前进行的，例如现在有 n 个结点，则编号 （n-1-stride, ..., n-1） 的任务交给第 1 个线程进行处理，编号 （n-1-2*stride, ..., n-1-stride） 的任务交给第 2 个线程进行处理，以此类推。当有新的线程加入时可以依据 transferIndex 值知道接下去应该分配哪一块的 bin 结点给当前线程。 具体代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203private final void transfer(Node&lt;K, V&gt;[] tab, Node&lt;K, V&gt;[] nextTab) &#123; int n = tab.length, stride; /* * stride 即步进， * 在单核下为 table 的长度 n，在多核模式下为 (n &gt;&gt;&gt; 3) / NCPU，最小值为 16 */ // 每核处理的量小于16，则强制赋值16 // NCPU为CPU核心数，每个核心均分复制任务，如果均分小于16个，那么以16为步长分给处理器 // 例如0-15号给处理器1，16-32号分给处理器2。处理器3就不用接任务了。 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) &#123; stride = MIN_TRANSFER_STRIDE; // subdivide range &#125; // 1. 如果 nextTable 未初始化，则先进行初始化，容量是之前的两倍 // 如果nextTab为空则初始化为原tab的两倍，这里只会有单线程进得来，因为初始化nextTab只需要一个， // addcount里面判断了nextTab为空则不执行扩容任务 if (nextTab == null) &#123; try &#123; @SuppressWarnings("unchecked") Node&lt;K, V&gt;[] nt = (Node&lt;K, V&gt;[]) new Node&lt;?, ?&gt;[n &lt;&lt; 1]; // 容量翻倍 nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; //扩容总进度，transferIndex之后的桶都已分配出去,因为是倒序分配 &#125; // 2. 执行迁移工作 int nextn = nextTab.length; // ForwardingNode 表示一个正在被迁移的结点，对应的 hash 值是 MOVED ForwardingNode&lt;K, V&gt; fwd = new ForwardingNode&lt;&gt;(nextTab); boolean advance = true; // 标记一个结点是否迁移完成 boolean finishing = false; // 标记扩容任务是否完成 // i 是索引，bound 是边界值（左边界值），从后往前迁移 for (int i = 0, bound = 0; ; ) &#123; Node&lt;K, V&gt; f; int fh; /* * 2.1 基于 CAS 计算本次任务的边界值，即 i 和 bound 值， * 将 i 指向 transferIndex，将 bound 指向 transferIndex - stride */ while (advance) &#123; int nextIndex, nextBound; // 标记当前结点迁移完成 if (--i &gt;= bound || finishing) &#123; advance = false; &#125; // 一旦 transferIndex &lt;= 0，表示所有任务已经分配给相应的线程进行处理 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; // 分配完成，下面直接执行迁移任务 advance = false; &#125; // 基于 CAS 计算 transferIndex 值（即 transferIndex - stride） // nextBound 是本次任务的边界 //确定当前线程每次分配的待迁移桶的范围[bound, nextIndex) //每个线程执行完之前的任务以后会走这个分支继续获取新的任务,直到所有任务都分配完毕 else if (U.compareAndSwapInt( this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; // ~end while /* * 2.2 执行迁移任务 */ if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 完成了所有结点的迁移 if (finishing) &#123; nextTable = null; table = nextTab; // 更新 table 为 nextTable sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); // sizeCtl 值更新为 table 长度的 1.5 倍 return; &#125; // 任务继续 /* * 基于 CAS 将 sizeCtl 减 1 * 在迁移操作开始前会将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，每一个线程加入迁移任务就会将 sizeCtl 加 1， * 所以这里执行 sizeCtl 减 1，代表当前任务完成 */ if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) &#123; // 当前任务结束，但是整体任务还未完成 return; &#125; // 此时 sizeCtl == (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，说明所有的任务都执行完了 finishing = advance = true; i = n; // recheck before commit,再次检查一下整张表 &#125; &#125; // 否则，获取 table 中位置为 i 的头结点，且为 null else if ((f = tabAt(tab, i)) == null) &#123; // 在当前位置设置一个空的 ForwardingNode 节点 advance = casTabAt(tab, i, null, fwd); &#125; // 否则，当前位置已经是一个 ForwardingNode，代表正在执行迁移工作 else if ((fh = f.hash) == MOVED) &#123; advance = true; // already processed &#125; // 否则，开始对当前结点执行迁移工作 else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; // 再次校验结点 Node&lt;K, V&gt; ln, hn; // 当前 bin 是一个链表 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; // n 为老 table 的长度 Node&lt;K, V&gt; lastRun = f; // 遍历当前链表，找到最后 p.hash &amp; n 值相同的第一个结点 for (Node&lt;K, V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // runBit == 0 表示还在老 table 原先的位置 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; // 此处 runBit 等于老 table 的长度，即 n else &#123; hn = lastRun; ln = null; &#125; // 以 lastRun 为界 for (Node&lt;K, V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) &#123; ln = new Node&lt;&gt;(ph, pk, pv, ln); &#125; else &#123; hn = new Node&lt;&gt;(ph, pk, pv, hn); &#125; &#125; // 将其中一个链表放置在 nextTable 的 i 位置 setTabAt(nextTab, i, ln); // 将另外一个链表放置在 nextTable 的 i+n 位置 setTabAt(nextTab, i + n, hn); // 设置当前 table 的 i 位置为 ForwardingNode 空结点，代表已经处理完 setTabAt(tab, i, fwd); advance = true; &#125; // 当前 bin 是一颗红黑树 else if (f instanceof TreeBin) &#123; TreeBin&lt;K, V&gt; t = (TreeBin&lt;K, V&gt;) f; TreeNode&lt;K, V&gt; lo = null, loTail = null; TreeNode&lt;K, V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K, V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K, V&gt; p = new TreeNode&lt;&gt;(h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) &#123; lo = p; &#125; else &#123; loTail.next = p; &#125; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) &#123; hi = p; &#125; else &#123; hiTail.next = p; &#125; hiTail = p; ++hc; &#125; &#125; /* 如果将红黑树一分为二后，结点数目小于 6，则将红黑树转换成链表 */ ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;&gt;(hi) : t; // 将其中一个红黑树（或链表）放置在 nextTable 的 i 位置 setTabAt(nextTab, i, ln); // 将另外一个红黑树（或链表）放置在 nextTable 的 i+n 位置 setTabAt(nextTab, i + n, hn); // 设置当前 table 的 i 位置为 ForwardingNode 空结点，代表已经处理完 setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 方法的实现很复杂，不过整体流程可以概括为 2 大部分： 如果 nextTable 未初始化，则先执行初始化操作，新的 table 容量翻倍 执行迁移任务 其中过程 1 比较简单，不过需要注意的是并不是所有触发 transfer 方法都需要执行初始化 table 的操作，只有主动触发扩容的线程需要执行该操作，对于后来加入“帮忙”的线程会跳过过程 1，直接进入过程 2。 过程 2 通过 transferIndex 实例变量协调任务的分配，并为每个线程分配 stride 个结点进行迁移，任务分配的过程实际上就是确定当前线程迁移结点的上下界的过程，该过程位于 while 循环中（即代码注释 2.1），该循环整体上就是一个 CAS 操作，如果迁移任务已经完成，或者没有剩余的结点可以迁移（实例变量 transferIndex 小于等于 0），则退出 CAS，否则尝试为本次任务分配新的上下界，同时更新 transferIndex 值。 接下来正式开始迁移工作，整体流程可以概括为： 检查整体迁移任务是否完成，如果完成则更新 table 和 sizeCtl 值 否则，检查当前任务是否已经完成，如果完成则退出本次任务 对于仍在进行中的任务会继续执行迁移操作，如果当前结点是一个空结点，则在该位置设置一个空的 ForwardingNode 结点，用于标记当前结点正在迁移中 否则，如果当前结点是一个 ForwardingNode 结点，即当前结点正在迁移中，进入下一轮任务分配 否则，对当前结点执行迁移操作 下面针对流程中的一些关键点进行说明，首先来看一下过程 2 相关的代码： 123456789101112131415/* * 基于 CAS 将 sizeCtl 减 1 * 在迁移操作开始前会将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，每一个线程加入迁移任务就会将 sizeCtl 加 1， * 前面两个调用 transfer函数的，在调用之前都会先加 sizeCtl + 1 * 所以这里执行 sizeCtl 减 1，代表当前任务完成 */if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) &#123; // 当前任务结束，但是整体任务还未完成 return; &#125; // 此时 sizeCtl == (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，说明所有的任务都执行完了 finishing = advance = true; i = n; // recheck before commit&#125; 前面我们曾提到当新增一个线程支持迁移任务时会执行 U.compareAndSwapInt(this, SIZECTL, sc, sc + 1) 操作，并且在扩容操作开始前会设置 sizeCtl 的值为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，而这里在完成一个任务的时候会执行 U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1) 操作将 sizeCtl 的值减 1。上面的代码会判定当前 sizeCtl 值是否等于 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，如果相等则说明整体扩容任务完成，否则仅说明当前任务完成，将线程任务数减 1。 接下来我们继续来看一个结点迁移的过程，迁移区分链表和红黑树，不过基本思想是想通的，这里以链表进行说明，相关实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041// 当前 bin 是一个链表if (fh &gt;= 0) &#123; int runBit = fh &amp; n; // n 为老 table 的长度 Node&lt;K, V&gt; lastRun = f; // 遍历当前链表，找到最后 p.hash &amp; n 值相同的第一个结点 for (Node&lt;K, V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // runBit == 0 表示还在老 table 原先的位置 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; // 此处 runBit 等于老 table 的长度，即 n else &#123; hn = lastRun; ln = null; &#125; // 以 lastRun 为界 for (Node&lt;K, V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) &#123; ln = new Node&lt;&gt;(ph, pk, pv, ln); &#125; else &#123; hn = new Node&lt;&gt;(ph, pk, pv, hn); &#125; &#125; // 将其中一个链表放置在 nextTable 的 i 位置 setTabAt(nextTab, i, ln); // 将另外一个链表放置在 nextTable 的 i+n 位置 setTabAt(nextTab, i + n, hn); // 设置当前 table 的 i 位置为 ForwardingNode 空结点，代表已经处理完 setTabAt(tab, i, fwd); advance = true;&#125; 这一段代码如果希望更好的理解，建议自己模拟一个 table，并 debug 一下执行流程。其实也不难，这段代码的工作就是将一个链表的拆分成两个链表，并将它们插入到新 table 适当的位置。假设老的 table 长度为 16，那么上面的实现有一个巧妙的地方在于对链表中所有结点的哈希值执行 p.hash &amp; n 操作，其结果不是 0 就是 16（老 table 的长度），所以我们可以依据 p.hash &amp; n 的值将一个链表拆分成两个链表，其中值均为 0 的结点构成的链表仍然放置在新 table 的当前位置 i，而值均为 16 的结点构成的链表则放置在新的位置，即 i + 16。变量 lastRun 所表示的结点实际上是最后几个具备相同 p.hash &amp; n 值的连续结点的最左边结点，因为这样可以减少该结点右边几个结点的迁移工作，因为它们具备相同的 p.hash &amp; n 值，自然也就位于同一个链表上。 addCount()在 put 方法结尾处调用了 addCount 方法，把当前 ConcurrentHashMap 的元素个数+1， 这个方法一共做了两件事,更新 baseCount 的值，检测是否进行扩容。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 从 putVal 传入的参数是 1//binCount，binCount 默认是0，只有 hash 冲突了才会大于 1.且他的大小是链表的长度（如果不是红黑数结构的话）private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; //利用CAS方法更新baseCount的值 // 如果计数盒子不是空 或者 // 如果修改 baseCount 失败 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; // 如果计数盒子是空（尚未出现并发） // 如果随机取余一个数组位置为空 或者 // 修改这个槽位的变量失败（出现并发了） // 执行 fullAddCount 方法。并结束 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; // 如果需要检查,检查是否需要扩容，在 putVal 方法调用时，默认就是要检查的。 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // 如果map.size() 大于 sizeCtl（达到扩容阈值需要扩容） 且 // table 不是空；且 table 的长度小于 1 &lt;&lt; 30。（可以扩容） while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; // 根据 length 得到一个标识 int rs = resizeStamp(n); // 如果正在扩容 if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 如果已经有其他线程在执行扩容操作 // 可以帮助扩容，那么将 sc 加 1. 表示多了一个线程在帮助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) // 扩容 transfer(tab, nt); &#125; // 如果不在扩容，将 sc 更新：标识符左移 16 位 然后 + 2. 也就是变成一个负数。高 16 位是标识符，低 16 位初始是 2. else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) // 更新 sizeCtl 为负数后，开始扩容。 transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 总结一下该方法的逻辑： x 参数表示的此次需要对表中元素的个数加几。check 参数表示是否需要进行扩容检查，大于等于0 需要进行检查，而我们的 putVal 方法的 binCount 参数最小也是 0 ，因此，每次添加元素都会进行检查。（除非是覆盖操作） 判断计数盒子属性是否是空，如果是空，就尝试修改 baseCount 变量，对该变量进行加 X。 如果计数盒子不是空，或者修改 baseCount 变量失败了，则放弃对 baseCount 进行操作。 如果计数盒子是 null 或者计数盒子的 length 是 0，或者随机取一个位置取于数组长度是 null，那么就对刚刚的元素进行 CAS 赋值。 如果赋值失败，或者满足上面的条件，则调用 fullAddCount 方法重新死循环插入。 这里如果操作 baseCount 失败了（或者计数盒子不是 Null），且对计数盒子赋值成功，那么就检查 check 变量，如果该变量小于等于 1. 直接结束。否则，计算一下 count 变量。 如果 check 大于等于 0 ，说明需要对是否扩容进行检查。 如果 map 的 size 大于 sizeCtl（扩容阈值），且 table 的长度小于 1 &lt;&lt; 30，那么就进行扩容。 根据 length 得到一个标识符，然后，判断 sizeCtl 状态，如果小于 0 ，说明要么在初始化，要么在扩容。 如果正在扩容，那么就校验一下数据是否变化了（具体可以看上面代码的注释）。如果检验数据不通过，break。 如果校验数据通过了，那么将 sizeCtl 加一，表示多了一个线程帮助扩容。然后进行扩容。 如果没有在扩容，但是需要扩容。那么就将 sizeCtl 更新，赋值为标识符左移 16 位 —— 一个负数。然后加 2。 表示，已经有一个线程开始扩容了。然后进行扩容。然后再次更新 count，看看是否还需要扩容。 从上面的分析中我们可以看，addCount()是扩容是老老实实按容量x2来扩容的，tryPresize()会传入一个size参数，可能一次性扩容很多倍。后面采用一样的方式调用transfer()来进行真正的扩容处理。 作者：exposure链接：https://juejin.im/post/5c36bcc0e51d45513236ee78来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 get()方法 put 的执行流程可以加深我们对于 ConcurrentHashMap 存储结构的理解，而理解了 ConcurrentHashMap 的存储结构，那么分析 get 方法的运行机制也是水到渠成的事情，实现如下： 12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; e, p; int n, eh; K ek; // 计算 key 的 hash 值 int h = spread(key.hashCode()); // table 表不为空，且 key 对应的 table 头结点存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) &#123; // 找到对应的 key，返回 value return e.val; &#125; &#125; // 当前 bin 为树，执行 find 方法检索 else if (eh &lt; 0) &#123; return (p = e.find(h, key)) != null ? p.val : null; &#125; // 当前 bin 是链表，直接遍历检索 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; return e.val; &#125; &#125; &#125; return null;&#125; 方法首先依据相同的实现计算 key 的哈希值，然后定位 key 在 table 中的 bin 位置。如果 bin 结点存在，则依据当前 bin 类型（链表或红黑树）检索目标值。 补充Q：弱一致性的问题？ A：之前经常有看到说 ConcurrentHashMap 在jdk1.6 版本中是弱一致性的，原因是其跟 CopyOnWriteArrayList 一样，只有在写数据时加锁，在读数据时不需要加锁，且数组引用是 volatile 关键字修饰的，这个弱一致性很好理解，就是读到的数据可能不是最新的，因为在写完后数组引用并没有发生改变，只是元素的值发生了变化，这个时候 volatile 是不会去刷新数据的，只有等写完后的数据从工作内存写回到主内存中，读到的数据才是写后的数据，所以是弱一致性。至于 jdk1.8而言，此时的锁已经是行粒度了，并且读数据时使用了 CAS，强行去读到 volatile的值，所以此时应该是强一致性的。 HashTable由于这个基本已经被淘汰了…所以就不再进行系统的源码分析了。 其实 HashTable 源代码只有1000多行点，源代码很少，也基本上是 HashMap 的原来的版本加上 synchronized 关键字，每次都是锁定整个哈希表，所以效率非常低下，注意哈~ 跟 HashMap 的区别主要其基本元素还是 Entry，然后默认初始值是 11，不是 2 的幂次方，同时扩容的时候也不是按 2 的幂次方进行扩容的，而是 2 的幂次方 + 1来扩容的，并且，其扰动函数依旧是原来的 jdk7的扰动函数方式，即扰动4次，效率低下，取余也是直接取余，因为其总数不是 2 的幂次方，故不能直接相与得到哈希桶的下标值。 与HashMap区别 实现方式不一样 Hashtable继承自 Dictionary 类，而 HashMap 继承自 AbstractMap 类。但二者都实现了 Map 接口。 12public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 12public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable 线程安全性不同 Hashtable 中的方法是Synchronize的，而HashMap中的方法在缺省情况下是非Synchronize的。 是否提供contains方法HashMap把Hashtable的contains方法去掉了，改成containsValue和containsKey，因为contains方法容易让人引起误解。Hashtable则保留了contains，containsValue和containsKey三个方法，其中contains和containsValue功能相同。 key和value是否允许null值 【重点！！！可以引申到 fail-fast 和 fail-safe 机制，进而引发到多线程上】 迭代器不同 HashMap 中的 Iterator 迭代器是 fail-fast 的，而 Hashtable 的 Enumerator 不是 fail-fast 的。 所以，当其他线程改变了HashMap 的结构，如：增删，将会抛出ConcurrentModificationException 异常，而 Hashtable 则不会。[fail-fast 和 fail-safe] 扩容机制不同 当现有容量大于总容量 * 负载因子时，HashMap 扩容规则为当前容量翻倍，Hashtable 扩容规则为当前容量翻倍 + 1。 初始化容量不同 HashMap 的初始容量为：16，Hashtable 初始容量为：11，两者的负载因子默认都是：0.75。 Hash值获取不一样 HashMap 内部使用hash(Object key)扰动函数对 key 的 hashCode 进行扰动后作为 hash 值。HashTable 是直接使用 key 的 hashCode() 返回值作为 hash 值。 HashSet &amp; TreeSet &amp; LinkedHashSetSet 的底层实现就是 Map，唯一的区别就是 Set 存的值就是 Map 的 key，value 值是一个固定值，所以…就不用分析啦！！！ 有个注意点哈~~ TreeMap 由于 key 不可以为 null，value 可以为null。所以在 TreeSet 中，就不能插入 null 元素。 相关知识点fail-fast 和 fail-safe 机制一：快速失败（fail—fast） 在用迭代器遍历一个集合对象时，如果遍历过程中对集合对象的内容进行了修改（增加、删除、修改），则会抛出Concurrent Modification Exception。 原理：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。 注意：这里异常的抛出条件是检测到 modCount！=expectedmodCount 这个条件。如果集合发生变化时修改modCount值刚好又设置为了expectedmodCount值，则异常不会抛出。因此，不能依赖于这个异常是否抛出而进行并发操作的编程，这个异常只建议用于检测并发修改的bug。 场景：java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改。 二：安全失败（fail—safe） 采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。 原理：由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发Concurrent Modification Exception。 缺点：基于拷贝内容的优点是避免了Concurrent Modification Exception，但同样地，迭代器并不能访问到修改后的内容，即：迭代器遍历的是开始遍历那一刻拿到的集合拷贝，在遍历期间原集合发生的修改迭代器是不知道的。 场景：java.util.concurrent包下的容器都是安全失败，可以在多线程下并发使用，并发修改。 快速失败和安全失败是对迭代器而言的。 快速失败：当在遍历一个集合的时候，如果有另外一个线程在修改这个集合，就会抛出ConcurrentModification异常，java.util下都是快速失败。 安全失败：在遍历时会在集合二层做一个拷贝，所以在修改集合上层元素不会影响下层。在java.util.concurrent下都是安全失败。 线程安全详解参考线程安全问题]]></content>
      <tags>
        <tag>源码分析</tag>
        <tag>ConcurrentHashMap</tag>
        <tag>集合类</tag>
        <tag>Map</tag>
        <tag>HashMap</tag>
        <tag>TreeMap</tag>
        <tag>LinkedHashMap</tag>
        <tag>HashTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树专题总结]]></title>
    <url>%2F2020%2F01%2F10%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%93%E9%A2%98%E9%83%A8%E5%88%86.html</url>
    <content type="text"><![CDATA[四种遍历先序、中序、后序遍历，非常简单！接下来我会用一种通用的方式完成这三种遍历哟！ 保证你看完能迅速手写三种遍历的非递归！ 先序遍历递归1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; result.add(root.val); // 访问根节点 preorderHelper(root.left, result); // 递归遍历左子树 preorderHelper(root.right, result); //递归遍历右子树 &#125;&#125; 非递归12345678910111213141516171819202122232425/** * 再写一个非递归的 * 统一先序中序后序的写法 * 从此压栈时再无左子树压栈，全部都是根节点和其右子树！ * @param root */private List&lt;Integer&gt; preorderTraversalByIteration(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); //得用一个栈来进行处理 Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while(cur != null || !stack.isEmpty())&#123; while (cur != null)&#123; //先把根节点加入到list，然后把所有的根节点加入 //左节点其实也就是子树的根节点 res.add(cur.val); stack.push(cur); cur = cur.left; &#125; //然后拿到根节点的右子树，进行遍历 cur = stack.pop(); cur = cur.right; &#125; return res;&#125; Tip: 我这里再拓展一种非递归的想法吧…但是我觉得这种想法不是很好，没有通用的非递归思想简单！强烈建议使用上面的非递归思想！！！ 至于为何非要提这种思想捏，原因就是 栈 如果换成 队列，左右节点顺序调一下，就是层序遍历了！ 思想： 上面的非递归的做法是把一棵树看做只有根节点和右节点，先全部把根节点读完了，再去读其右节点，这样也就做到了 根 - 左 - 右 下面要讲的非递归的做法是 先读根节点，然后将其加入栈，然后只要栈非空，就弹栈，将其加入需要返回的列表中，然后先将其右子树节点加入到栈中，然后将左子树节点加入到栈中，注意，这里是先右子树，后左子树，因为栈是后进先出！下一步就是左子树出栈，将其加入列表中。 12345678910111213141516class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); if (root == null) return result; Stack&lt;TreeNode&gt; toVisit = new Stack&lt;&gt;(); toVisit.push(root); TreeNode cur; while (!toVisit.isEmpty()) &#123; cur = toVisit.pop(); result.add(cur.val); // 访问根节点 if (cur.right != null) toVisit.push(cur.right); // 右节点入栈 if (cur.left != null) toVisit.push(cur.left); // 左节点入栈 &#125; return result; &#125;&#125; 中序遍历递归1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; preorderHelper(root.left, result); // 递归遍历左子树 result.add(root.val); // 访问根节点 preorderHelper(root.right, result); //递归遍历右子树 &#125;&#125; 非递归123456789101112131415161718private List&lt;Integer&gt; inorderTraversalByIteration(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; while(cur != null || !stack.isEmpty())&#123; while (cur != null)&#123; //左节点其实也就是子树的根节点 stack.push(cur); cur = cur.left; &#125; //从最底下的根节点开始，放入列表 cur = stack.pop(); res.add(cur.val); //然后拿到根节点的右子树，进行遍历 cur = cur.right; &#125; return res;&#125; 后序遍历递归1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; preorderHelper(root.left, result); // 递归遍历左子树 preorderHelper(root.right, result); //递归遍历右子树 result.add(root.val); // 访问根节点 &#125;&#125; 非递归后序非递归相对前两个遍历的非递归需要一个小技巧！ 后序是 左 - 右 - 根，反过来就是根 - 右 - 左,那其实跟先序基本就一模一样了，先序是根 - 左 - 右，所以只需要在先序的非递归中 改两行代码 + 加一行代码 就完成了！ 12345678910111213141516private List&lt;Integer&gt; postorderTraversalByIteration(TreeNode root)&#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); TreeNode cur = root; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); while(cur != null || !stack.isEmpty())&#123; while(cur != null)&#123; list.add(cur.val); stack.push(cur); cur = cur.right; &#125; cur = stack.pop(); cur = cur.left; &#125; Collections.reverse(list); return list;&#125; 层序遍历层序是不是还有一个名字 叫 BFS 递归这个竟然时间和空间击败了100%的人… 莫名有点开心啊 123456789101112131415161718192021public List&lt;List&lt;Integer&gt;&gt; levelOrder(Node root) &#123; if (root == null) return res; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); helper(root, 0, res); return res;&#125;private void helper(Node root, int depth, List&lt;List&lt;Integer&gt;&gt; res) &#123; if (root == null) return; //判断是否是新的一层 if (depth + 1 &gt; res.size()) &#123; res.add(new ArrayList&lt;&gt;()); &#125; res.get(depth).add(root.val); //处理子节点 for (Node node : root.children) &#123; if (node != null) &#123; helper(node, depth + 1, res); &#125; &#125;&#125; 非递归上面讲了一种用队列实现的非递归思想 1234567891011121314public static List&lt;Integer&gt; levelOrder(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); if (root == null) return result; Queue&lt;TreeNode&gt; toVisit = new ArrayDeque&lt;&gt;(); toVisit.add(root); TreeNode cur; while (!toVisit.isEmpty()) &#123; cur = toVisit.poll(); result.add(cur.val); // 访问根节点 if (cur.left != null) toVisit.add(cur.left); // 左节点入队列 if (cur.right != null) toVisit.add(cur.right); // 右节点入队列 &#125; return result;&#125; 1234567891011121314151617181920212223242526272829/** * 非递归方法二，返回值不一样 * @param root * @return */public List&lt;List&lt;Integer&gt;&gt; levelOrderBy(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if (root == null) return res; Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); queue.add(root); TreeNode cur; while (!queue.isEmpty()) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int pre_number = queue.size(); while(pre_number &gt; 0)&#123; cur = queue.poll(); pre_number--; list.add(cur.val); if (cur.left != null)&#123; queue.add(cur.left); // 左节点入队列 &#125; if (cur.right != null) &#123; queue.add(cur.right); // 右节点入队列 &#125; &#125; res.add(list); &#125; return res;&#125; 相关题型二叉树的右视图题目给定一棵二叉树，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 示例: 123456789输入: [1,2,3,null,5,null,4]输出: [1, 3, 4]解释: 1 &lt;--- / \2 3 &lt;--- \ \ 5 4 &lt;--- 思路 层序遍历 BFS。先写出双重 list 的层序遍历，然后在每一个内层list中拿到最后一个值即可； 深度遍历 DFS。递归 DFS，从右向左，拿到第一个数即可。 实现 BFS 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; lists = levelOrderBy(root); List&lt;Integer&gt; res = new ArrayList(); for(List&lt;Integer&gt; list : lists)&#123; res.add(list.get(list.size()-1)); &#125; return res; &#125;public List&lt;List&lt;Integer&gt;&gt; levelOrderBy(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if (root == null) return res; Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); queue.add(root); TreeNode cur; while (!queue.isEmpty()) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int pre_number = queue.size(); while(pre_number &gt; 0)&#123; cur = queue.poll(); pre_number--; list.add(cur.val); if (cur.left != null)&#123; queue.add(cur.left); // 左节点入队列 &#125; if (cur.right != null) &#123; queue.add(cur.right); // 右节点入队列 &#125; &#125; res.add(list); &#125; return res;&#125;&#125; DFS 1234567891011121314151617class Solution &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); solve(root, 0, res); return res; &#125; public void solve(TreeNode root, int level, List&lt;Integer&gt; res) &#123; if (root != null) &#123; if (res.size() == level) &#123; res.add(root.val); &#125; solve(root.right, level + 1, res); solve(root.left, level + 1, res); &#125; &#125;&#125; 对称二叉树题目给定一个二叉树，检查它是否是镜像对称的。 例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 12345 1 / \ 2 2 / \ / \3 4 4 3 但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的: 12345 1 / \2 2 \ \ 3 3 思路 先讲一下自己的思路: 利用层序非递归，用双层List存储树的节点，每一层存入一个单层List中，然后单层List可以采用二分判断是否对称！ 答案思路：树的复制，这个想法很nice！复制原树，判断其左子树节点是否和右子树节点相同且判断其左子树的左是否等于右子树的右，左子树的右是否等于右子树的左。在非递归的写法中，树的复制这种思想体现的更加明显，这个就是典型的用空间换时间！ 实现 树复制思想之递归 12345678910111213141516171819202122232425262728293031323334/** * 对称二叉树 递归做法 * 我的想法是使用层序遍历，找到对称点，每一层判断完即可！ * 结果是没写出来 * 看了答案是自己跟自己镜像比较，太妙了！！ * @param root * @return */ private static boolean isSymmetricByRecursion(TreeNode root)&#123; if (root == null) return true; return helper(root.left,root.right); &#125; /** * 递归三部曲 * 1、确定递归出口，当节点为空时，即无需再递归 * 2、确定返回值，返回值为boolean，判断当前 * 3、镜像嘿嘿，这个想法很nice * @param tree1 * @param tree2 * @return */ private static boolean helper(TreeNode tree1, TreeNode tree2) &#123; //判断是否是新的一层 if(tree1 == null &amp;&amp; tree2 == null)&#123; return true; &#125; else if(tree1 == null || tree2 == null)&#123; return false; &#125; else &#123; return tree1.val == tree2.val &amp;&amp; helper(tree1.left,tree2.right) &amp;&amp; helper(tree1.right,tree2.left); &#125; &#125; 树复制之非递归 Tip: 在复现非递归时，发现一个问题，就是Queue的实现类ArrayDeque是非线程安全的，但是其不允许存储null元素，这个跟ArrayList、LinkedList、HashMap、TreeMap等非线程安全的集合不同！！！！因为系统根据某个位置是否为null来判断元素的存在，null插入直接会报空指针异常。[ArrayDeque当作为栈使用时，性能比Stack好；当作为队列使用时，性能比LinkedList好。] 所以这里使用了Queue的另外一个实现类LinkedList，它也是非线程安全的哟，但是可以存储null！！！ 123456789101112131415161718private static boolean isSymmetricByIteration(TreeNode root)&#123; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); if(root == null) return true; queue.add(root.left); queue.add(root.right); while(!queue.isEmpty())&#123; TreeNode tree1 = queue.poll(); TreeNode tree2 = queue.poll(); if(tree1 == null &amp;&amp; tree2 == null ) continue; if(tree1 == null || tree2 == null) return false; if(tree1.val != tree2.val) return false; queue.add(tree1.left); queue.add(tree2.right); queue.add(tree1.right); queue.add(tree2.left); &#125; return true;&#125; 层序非递归 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 迭代 * @param root * @return */ private static boolean isSymmetricByLevel(TreeNode root)&#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); if (root == null) return true; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); TreeNode cur; //得到双层list while (!queue.isEmpty()) &#123; List&lt;Integer&gt; list = new LinkedList&lt;&gt;(); int pre_number = queue.size(); System.out.println(pre_number); while(pre_number &gt; 0 )&#123; cur = queue.poll(); pre_number--; if(cur == null) &#123; list.add(null); continue; &#125; list.add(cur.val); if (cur.left != null)&#123; queue.add(cur.left); // 左节点入队列 &#125; else&#123; queue.add(null); &#125; if (cur.right != null) &#123; queue.add(cur.right); // 右节点入队列 &#125; else&#123; queue.add(null); &#125; &#125; res.add(list); &#125; //对每一层list进行判断，看是否是对称的，可以使用dp for(List list:res)&#123; int size = list.size(); for(int i = 0;i &lt; size/2;i++)&#123; if(list.get(i) != list.get(size-1-i))&#123; return false; &#125; &#125; &#125; return true; &#125; 二叉树的最大深度吖！这个是二叉树最简单的题目了吧…但是绝对是一道经典好题！下面将用最常用的三种方法解决它！ 题目给定一个二叉树，找出其最大深度。 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 说明: 叶子节点是指没有子节点的节点。 示例： 给定二叉树 [3,9,20,null,null,15,7]， 3 / \ 9 20 / \ 15 7 返回它的最大深度 3 。 思路 方法一：递归。这个我在另外一篇递归中的文章中有提到，可以一行代码解决哟！ 方法二：层序遍历。即用BFS解决，每遍历完一层就记录深度。 方法三：DFS。用栈实现即可。(还学到Pair这玩意…为何我以前都不知道) 实现 递归 1、找出口，节点为空即可退出递归。 2、找返回值，返回的是该节点所在层的深度。 3、确定一次递归需要做什么。当前节点深度 = 子节点最大深度 + 1。 12345class Solution &#123; public int maxDepth(TreeNode root) &#123; return root == null ? 0 : Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; &#125;&#125; BFS 层序非递归遍历，稍微改写一下就好了，这里不需要存储节点，所以只需要用一个值来记录深度即可。 第一种非递归方式改写 12345678910111213141516171819import javafx.util.Pair;class Solution &#123; public int maxDepth(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); if (root == null) return 0; Queue&lt;Pair&lt;TreeNode,Integer&gt;&gt; toVisit = new ArrayDeque&lt;&gt;(); toVisit.add(new Pair&lt;&gt;(root,1)); Pair&lt;TreeNode,Integer&gt; cur; int max_depth = 0; while (!toVisit.isEmpty()) &#123; cur = toVisit.poll(); int depth = cur.getValue(); max_depth = Math.max(max_depth,depth); if (cur.getKey().left != null) toVisit.add(new Pair&lt;&gt;(cur.getKey().left,depth + 1)); // 左节点入队列 if (cur.getKey().right != null) toVisit.add(new Pair&lt;&gt;(cur.getKey().right,depth + 1)); // 右节点入队列 &#125; return max_depth; &#125;&#125; 第二种非递归方式改写 1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public int maxDepth(TreeNode root) &#123; // List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); //这里无需存储节点，只需记录深度即可 int depth = 0; if (root == null) return depth; Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); queue.add(root); TreeNode cur; while (!queue.isEmpty()) &#123;// List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int pre_number = queue.size(); while(pre_number &gt; 0)&#123; cur = queue.poll(); pre_number--;// list.add(cur.val); //这里可以不用加入列表 if (cur.left != null)&#123; queue.add(cur.left); // 左节点入队列 &#125; if (cur.right != null) &#123; queue.add(cur.right); // 右节点入队列 &#125; &#125;// res.add(list); depth++; &#125;// return res.size(); return depth; &#125;&#125; DFS 其实就是在先序非递归的基础上稍加改动，压栈的不再只是节点，还包括了节点的深度，用Pair类型存储！ 尝试了用Map类型存储，发现行不通…因为拿不到栈顶的节点！ 123456789101112131415161718192021222324import javafx.util.Pair;class Solution &#123; public int maxDepth(TreeNode root) &#123; //得用一个栈来进行处理 Stack&lt;Pair&lt;TreeNode,Integer&gt;&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; int depth= 0; int MaxDepth = 0; while(cur != null || !stack.isEmpty())&#123; while (cur != null)&#123; //左节点其实也就是子树的根节点 depth++; stack.push(new Pair&lt;&gt;(cur,depth)); cur = cur.left; &#125; //然后拿到根节点的右子树，进行遍历 Pair&lt;TreeNode,Integer&gt; top = stack.pop(); depth = top.getValue(); MaxDepth = Math.max(depth,MaxDepth); cur = top.getKey().right; &#125; return MaxDepth; &#125;&#125; 二叉树的最小深度题目给定一个二叉树，找出其最小深度。 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 说明: 叶子节点是指没有子节点的节点。 示例: 给定二叉树 [3,9,20,null,null,15,7], 12345 3 / \9 20 / \ 15 7 返回它的最小深度 2。 思路 BFS，套用 BFS 模板。 代码1234567891011121314151617181920212223242526class Solution &#123; public int minDepth(TreeNode root) &#123; if(root == null) return 0; LinkedList&lt;TreeNode&gt; queue = new LinkedList(); queue.add(root); int depth = 1; while(!queue.isEmpty())&#123; int sz = queue.size(); for(int i = 0;i &lt; sz;i++)&#123; TreeNode cur = queue.poll(); if(cur.left == null &amp;&amp; cur.right == null)&#123; return depth; &#125; if(cur.left != null)&#123; queue.add(cur.left); &#125; if(cur.right != null)&#123; queue.add(cur.right); &#125; &#125; depth++; &#125; return depth; &#125;&#125; 路径总和题目给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。 说明: 叶子节点是指没有子节点的节点。 示例:给定如下二叉树，以及目标和 sum = 22， 5 / \ 4 8 / / \ 11 13 4 / \ \ 7 2 1 返回 true, 因为存在目标和为 22 的根节点到叶子节点的路径 5-&gt;4-&gt;11-&gt;2。 思路 递归。我都递归的要吐了，二叉树日常递归！ DFS，自己建栈模拟递归。 实现 递归 1 （三元表达式纯粹装逼哈哈哈） 12345678910class Solution &#123; public boolean hasPathSum(TreeNode root, int sum) &#123; return root == null ? false : hasPathSumByRecursion(root,0,sum); &#125; private boolean hasPathSumByRecursion(TreeNode root, int total, int sum) &#123; if(root == null) return false; total = total + root.val; return (root.left == null &amp;&amp; root.right == null) ? total == sum : hasPathSumByRecursion(root.left,total,sum) || hasPathSumByRecursion(root.right,total,sum); &#125;&#125; 递归 2 12345678public boolean hasPathSum(TreeNode root,int sum)&#123; if(root == null) return false; sum-=root.val; if(root.left == null &amp;&amp; root.right == null)&#123; return sum == 0; &#125; return hasPathSum(root.left,sum) || hasPathSum(root.right,sum);&#125; DFS 我觉得思想都很简单啊，其实二叉树的基本上所有的题目都是建立在前序和层序遍历的基础上的，熟练掌握了前序和层序的递归非递归就基本能解所有题目了… (在前序非递归的基础上稍加改动就行了…) 123456789101112131415import javafx.util.Pair;class Solution &#123; public boolean hasPathSum(TreeNode root, int sum) &#123; if (root == null) return false; Stack&lt;Pair&lt;TreeNode,Integer&gt;&gt; toVisit = new Stack&lt;&gt;(); toVisit.push(new Pair&lt;&gt;(root,root.val)); while (!toVisit.isEmpty())&#123; Pair&lt;TreeNode,Integer&gt; cur = toVisit.pop(); if (cur.getKey().right != null) toVisit.push(new Pair&lt;&gt;(cur.getKey().right,cur.getValue()+cur.getKey().right.val)); // 右节点入栈 if (cur.getKey().left != null) toVisit.push(new Pair&lt;&gt;(cur.getKey().left,cur.getValue()+cur.getKey().left.val)); // 左节点入栈 if(cur.getKey().left == null &amp;&amp; cur.getKey().right == null &amp;&amp; cur.getValue() == sum) return true; &#125; return false; &#125;&#125; 从中序与后序遍历序列构造二叉树题目根据一棵树的中序遍历与后序遍历构造二叉树。 注意:你可以假设树中没有重复的元素。 例如，给出 12中序遍历 inorder = [9,3,15,20,7]后序遍历 postorder = [9,15,7,20,3] 返回如下的二叉树： 12345 3 / \9 20 / \ 15 7 思路我写的应该比较简洁，参数少的原因是抓住了后序是左--右--根的特点，所以后序遍历从后向前是根--右--左，故在递归时根本没有必要将后序遍历的数组界限传入，每次有节点加入就自减一就行，只要满足了先拿到根节点然后遍历右子树，再遍历左子树这个原则就Okay。 至于递归如何写，其实把握三个原则就行： 递归出口，这里就不解释了 每次递归的返回值，这里很明显就是题目要求的root节点 单次递归需要做的事，我们需要构建一个树，那么单次递归就是拿到根节点，并且指向正确的左右子树即可。 代码123456789101112131415161718192021222324class Solution &#123; int index; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); public TreeNode buildTree(int[] inorder,int[] postorder)&#123; index = inorder.length - 1; for(int i = 0;i &lt;= index; i++)&#123; map.put(inorder[i],i); &#125; return buildTreeByRecursion(inorder,postorder,0,index); &#125; private TreeNode buildTreeByRecursion(int[] inorder,int[] postorder,int inorder_start,int inorder_end)&#123; if(inorder_start &gt; inorder_end) return null; //先拿到根节点的值,确定其在中序遍历的位置，并且将其后序遍历的索引值的末尾往前一位 int inorder_root = map.get(postorder[index]); TreeNode root = new TreeNode(postorder[index--]); //然后确定左右子树，并且递归即可 // 注意哦！是必须先递归右子树，再递归左子树，因为后序是左右根的顺序，后序末尾自减一，此时应该是右子树的根节点！ // 所以必须全部右子树构建完成再去构建左子树 root.right = buildTreeByRecursion(inorder,postorder,inorder_root + 1,inorder_end); root.left = buildTreeByRecursion(inorder,postorder,inorder_start,inorder_root - 1); return root; &#125;&#125; 从前序与中序遍历序列构造二叉树题目根据一棵树的前序遍历与中序遍历构造二叉树。 注意:你可以假设树中没有重复的元素。 例如，给出 12前序遍历 preorder = [3,9,20,15,7]中序遍历 inorder = [9,3,15,20,7] 返回如下的二叉树： 12345 3 / \9 20 / \ 15 7 思路跟上面的思路一致，就是刚好相反而已嘛… 代码1234567891011121314151617181920212223class Solution &#123; int pre = 0; Map&lt;Integer,Integer&gt; pre_in_map = new HashMap&lt;&gt;(); public TreeNode buildTree(int[] preorder,int[] inorder)&#123; int inorder_end = inorder.length - 1; for(int i = 0;i &lt;= inorder_end; i++)&#123; pre_in_map.put(inorder[i],i); &#125; return buildTreeByRecursion(preorder,inorder,0,inorder_end); &#125; private TreeNode buildTreeByRecursion(int[] preorder,int[] inorder,int inorder_start,int inorder_end)&#123; if(inorder_start &gt; inorder_end) return null; //先拿到根节点的值,确定其在中序遍历的位置，并且将先序遍历的索引值的开始值向前一位 int inorder_root = pre_in_map.get(preorder[pre]); TreeNode root = new TreeNode(preorder[pre++]); //然后确定左右子树，并且递归即可 // 注意哦！是必须先递归左子树，再递归右子树，因为先序是根左右的顺序 // 所以必须全部左子树构建完成再去构建右子树 root.left = buildTreeByRecursion(preorder,inorder,inorder_start,inorder_root - 1); root.right = buildTreeByRecursion(preorder,inorder,inorder_root + 1,inorder_end); return root; &#125;&#125; 填充每个节点的下一个右侧节点指针题目给定一个完美二叉树，其所有叶子节点都在同一层，每个父节点都有两个子节点。二叉树定义如下： 123456struct Node &#123; int val; Node *left; Node *right; Node *next;&#125; 填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 初始状态下，所有 next 指针都被设置为 NULL。 示例： 思路 最简单的思路：层序遍历，分别获得每层的节点，然后生成链表，所以这个不是完美二叉树也可以做，也就是说没有用到 完全二叉树 的性质！所以必定不是最优解！而且这个使用的不是常量级的空间，其实是不太符合要求的。 (看了答案的)高赞的 拉拉链解法，将一棵大的二叉树一分为二，处理两棵小二叉树的连接，再递归解决小二叉树内部连接。 还有一种非递归的方法，个人觉得也是非常的精妙，之前我们是用列表保存每一层的节点，其实这是一种浪费。只需要解决三个问题就够了。参考网址 每一层怎么遍历？ 之前是用队列将下一层的节点保存了起来。 这里的话，其实只需要提前把下一层的next构造完成，到了下一层的时候就可以遍历了。 什么时候进入下一层？ 之前是得到当前队列的元素个数，然后遍历那么多次。 ​ 这里的话，注意到最右边的节点的next为null，所以可以判断当前遍历的节点是不是null。 怎么得到每层开头节点？ 之前队列把当前层的所以节点存了起来，得到开头节点当然很容易。 这里的话，我们额外需要一个变量把它存起来。 三个问题都解决了，就可以写代码了。利用三个指针，start指的是当前层的最左节点，也就是本层的起点，cur指的是当前正在遍历的节点，cur.next指的是正在遍历的节点的下一个节点！我觉得这个方法最难想的就是 在遍历本层的时候，将下层的next构造完成！！！ 代码 方法一：层序遍历 12345678910111213141516171819202122232425262728293031/** * 层序遍历 * @param root * @return */public Node connect1(Node root) &#123; if (root == null) &#123; return root; &#125; Queue&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; int size = queue.size(); Node pre = null; for (int i = 0; i &lt; size; i++) &#123; Node cur = queue.poll(); //从第二个节点开始，将前一个节点的 pre 指向当前节点 if (i &gt; 0) &#123; pre.next = cur; &#125; pre = cur; if (cur.left != null) &#123; queue.offer(cur.left); &#125; if (cur.right != null) &#123; queue.offer(cur.right); &#125; &#125; &#125; return root;&#125; 方法二：拉拉链法 123456789101112131415161718/** * 方法二 拉拉链法 * @param root * @return */public Node connect2(Node root)&#123; if(root == null) return null; Node left = root.left; Node right = root.right; while(left != null)&#123; left.next = right; left = left.right; right = right.left; &#125; connect1(root.left); connect1(root.right); return root;&#125; 方法三：非递归解法（个人最推荐的做法） 12345678910111213141516171819/** * 非递归解法，这个应该是最符合题意的解法 * @param root * @return */public Node connect3(Node root)&#123; if(root == null) return null; Node start = root; while(start.left != null)&#123; Node cur = start; while(cur != null)&#123; cur.left.next = cur.right; if(cur.next != null) cur.right.next = cur.next.left; cur = cur.next; &#125; start = start.left; &#125; return root;&#125; 填充每个节点的下一个右侧节点指针 II题目给定一个二叉树 123456struct Node &#123; int val; Node *left; Node *right; Node *next;&#125; 填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 初始状态下，所有 next 指针都被设置为 NULL。 进阶： 你只能使用常量级额外空间。 使用递归解题也符合要求，本题中递归程序占用的栈空间不算做额外的空间复杂度。 示例： 123输入：root = [1,2,3,4,5,null,7]输出：[1,#,2,3,#,4,5,7,#]解释：给定二叉树如图 A 所示，你的函数应该填充它的每个 next 指针，以指向其下一个右侧节点，如图 B 所示。 提示： 树中的节点数小于 6000 -100 &lt;= node.val &lt;= 100 思路 方法一：上一题已经讲过，依旧可以用上面的代码，BFS。 方法二：题目要求只能使用常量级额外空间，意味着其实严格上不能使用队列，那怎么做呢？ 可以使用一个尾指针，将所有的节点连接起来，刚好就是题目的next指针，可以做到这一点，至于每一层如何开始，只需要再引入一个指针进行标记即可，不同于BFS的是，BFS在遍历当前层时，是处理当前层的横向连接，并且将子节点加入队列中以便下一次的遍历，而方法二则是在每次循环时，直接处理下一层节点的横向连接问题，因为我并不知道每一层的节点数量和具体位置(无法保存)，只能是如果该层下的子节点存在，就将他们连接！ 1234567891011cur 指针利用 next 不停的遍历当前层。如果 cur 的孩子不为 null 就将它接到 tail 后边，然后更新tail。当 cur 为 null 的时候，再利用 dummy 指针得到新的一层的开始节点。dummy 指针在链表中经常用到，他只是为了处理头结点的情况，它并不属于当前链表。作者：windliang链接：https://leetcode-cn.com/problems/populating-next-right-pointers-in-each-node-ii/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-28/来源：力扣（LeetCode） 代码 方法一：同上(略) 方法二： 12345678910111213141516171819202122232425public Node connectII(Node root)&#123; if(root == null) return null; Node cur = root; //遍历的当前节点 Node tail; //中间节点，起承接的作用 while(cur != null)&#123; //遍历整棵树 //注意这里的start应该是一个链表头结点，其指向的next为当前层的下一层的第一个节点 Node start = new Node(); tail = start; //遍历当前层 while(cur != null)&#123; if(cur.left != null)&#123; tail.next = cur.left; tail = tail.next; &#125; if(cur.right != null)&#123; tail.next = cur.right; tail = tail.next; &#125; cur = cur.next; &#125; cur = start.next; &#125; return root;&#125; 二叉树的最近公共祖先题目给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” 例如，给定如下二叉树: root = [3,5,1,6,2,0,8,null,null,7,4] 示例 1: 123输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1输出: 3解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 示例 2: 123输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4输出: 5解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。 说明: 所有节点的值都是唯一的。 p、q 为不同节点且均存在于给定的二叉树中。 思路注意p,q必然存在树内, 且所有节点的值唯一！！！递归思想，对以root为根的(子)树进行查找p和q，如果root == null || p || q 直接返回root表示对于当前树的查找已经完毕，否则对左右子树进行查找，根据左右子树的返回值判断: 左右子树的返回值都不为null, 由于值唯一左右子树的返回值就是p和q, 此时root为LCA 如果左右子树返回值只有一个不为null, 说明只有p和q存在于左或右子树中, 最先找到的那个节点为LCA 左右子树返回值均为null, p和q均不在树中, 返回null 代码123456789class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null || p == root || q == root)return root; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null &amp;&amp; right != null)return root; return left == null ? right : left; &#125;&#125; 二叉树的序列化与反序列化题目序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。 请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。 示例: 123456789你可以将以下二叉树： 1 / \ 2 3 / \ 4 5序列化为 "[1,2,3,null,null,4,5]" 提示: 这与 LeetCode 目前使用的方式一致，详情请参阅 LeetCode 序列化二叉树的格式。你并非必须采取这种方式，你也可以采用其他的方法解决这个问题。 说明: 不要使用类的成员 / 全局 / 静态变量来存储状态，你的序列化和反序列化算法应该是无状态的。 思路这题做了两个小时…还没整出来…思路其实特别的简单，就是用层序遍历，将所有结点存入列表中（包括了空节点），然后转成字符串，字符串再转成列表然后建立树 卡在建立树这块…心态有点炸…冷静会 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class Codec &#123; // Encodes a tree to a single string. public String serialize(TreeNode root) &#123; if (root == null) &#123; return "[]"; &#125; Queue&lt;String&gt; res = new LinkedList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.offer(root); res.offer(String.valueOf(root.val)); while(!queue.isEmpty()) &#123; TreeNode node = queue.poll(); // 有左节点就插入左节点，没有就插入null if (node.left != null) &#123; queue.offer(node.left); res.offer(String.valueOf(node.left.val)); &#125; else &#123; res.offer("null"); &#125; // 有右节点就插入右节点，没有就插入null if (node.right != null) &#123; queue.offer(node.right); res.offer(String.valueOf(node.right.val)); &#125; else &#123; res.offer("null"); &#125; &#125; StringBuilder sb = new StringBuilder(); sb.append("["); while(!res.isEmpty()) &#123; sb.append(res.poll()); if (!res.isEmpty()) &#123; sb.append(","); &#125; &#125; sb.append("]"); return sb.toString(); &#125; // Decodes your encoded data to tree. public TreeNode deserialize(String data) &#123; data = data.substring(1, data.length()-1); if (data.length() == 0) &#123; return null; &#125; Queue&lt;String&gt; ls = new LinkedList&lt;&gt;(Arrays.asList(data.split(","))); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); TreeNode root = null; while(!ls.isEmpty()) &#123; String res = ls.poll(); // 创建根节点 if (root == null) &#123; root = new TreeNode(Integer.valueOf(res)); queue.offer(root); continue; &#125; // 注意：ls的长度总是奇数的，所以除了根节点，其余节点创建时可以一次取两个ls中的元素 TreeNode father = queue.poll(); // 创建左节点 if (!res.equals("null")) &#123; TreeNode curr = new TreeNode(Integer.valueOf(res)); father.left = curr; queue.offer(curr); &#125; // 创建右节点 res = ls.poll(); if (!res.equals("null")) &#123; TreeNode curr = new TreeNode(Integer.valueOf(res)); father.right = curr; queue.offer(curr); &#125; &#125; return root; &#125;&#125; 翻转二叉树题目翻转一棵二叉树。 示例： 输入： 4 / \ 2 7 / \ / \ 1 3 6 9 输出： 4 / \ 7 2 / \ / \ 9 6 3 1 思路递归…注释有说 代码1234567891011121314151617/** * 递归三部曲 * 1、找到出口，节点为空即为出口 * 2、找到返回值，返回值，返回值应该是子树的根节点 * 3、一次递归需要做啥，需要将左右子树翻转，现在我们手上也就是三个节点 * 分别是根节点，左子树根节点，右子树根节点，要做的就只是左右子树根节点翻转即可 * @param root * @return */private static TreeNode invertTree(TreeNode root)&#123; if(root == null) return null; TreeNode left = invertTree(root.left); TreeNode right = invertTree(root.right); root.left = right; root.right = left; return root;&#125; 合并二叉树题目给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。 你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。 示例 1: 1234567891011121314输入: Tree 1 Tree 2 1 2 / \ / \ 3 2 1 3 / \ \ 5 4 7 输出: 合并后的树: 3 / \ 4 5 / \ \ 5 4 7 注意: 合并必须从两个树的根节点开始。 思路使用递归。我们可以对这两棵树同时进行前序遍历，并将对应的节点进行合并。在遍历时，如果两棵树的当前节点均不为空，我们就将它们的值进行相加，并对它们的左孩子和右孩子进行递归合并；如果其中有一棵树为空，那么我们返回另一颗树作为结果；如果两棵树均为空，此时返回任意一棵树均可（因为都是空）。 代码123456789101112131415161718192021/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */public class Solution &#123; public TreeNode mergeTrees(TreeNode t1, TreeNode t2) &#123; if (t1 == null) return t2; if (t2 == null) return t1; t1.val += t2.val; t1.left = mergeTrees(t1.left, t2.left); t1.right = mergeTrees(t1.right, t2.right); return t1; &#125;&#125; 验证二叉搜索树题目给定一个二叉树，判断其是否是一个有效的二叉搜索树。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 示例 1: 12345输入: 2 / \ 1 3输出: true 示例 2: 123456789输入: 5 / \ 1 4 / \ 3 6输出: false解释: 输入为: [5,1,4,null,null,3,6]。根节点的值为 5 ，但是其右子节点值为 4 。 思路二叉搜索树的特点，就是根节点要大于左子树全部节点的数，小于右子树全部节点的数，故如果我们是中序遍历，则二叉搜索树遍历的节点的值一定是递增的！当然我们没有必要用一个数组将所有节点的值存起来，只需要存储当前遍历节点的之前一个节点的值即可！ 代码123456789101112131415class Solution &#123; public boolean isValidBST(TreeNode root) &#123; // 不用int[]是为了防止初始化值等于TreeNode中的val return validByInorderTraversal(root, new long[] &#123;Long.MIN_VALUE&#125;); &#125; boolean validByInorderTraversal(TreeNode root, long[] prev) &#123; if (root == null) return true; if (!validByInorderTraversal(root.left, prev)) return false; // 每次跟只跟前一个元素做判断，符合条件则替换，否则返回false if (prev[0] &gt;= root.val) return false; prev[0] = root.val; if (!validByInorderTraversal(root.right, prev)) return false; return true; &#125;&#125; 二叉树的直径题目给定一棵二叉树，你需要计算它的直径长度。一棵二叉树的直径长度是任意两个结点路径长度中的最大值。这条路径可能穿过根结点。 示例 :给定二叉树 12345 1 / \ 2 3 / \ 4 5 返回 3, 它的长度是路径 [4,2,1,3] 或者 [5,2,1,3]。 注意：两结点之间的路径长度是以它们之间边的数目表示。 思路按照常用方法计算一个节点的深度：max(depth of node.left, depth of node.right) + 1。在计算的同时，经过这个节点的最大直径为 (depth of node.left) + (depth of node.right) 。搜索每个节点并记录这些路径经过的点数最大值，期望长度是就是结果。 代码12345678910111213141516class Solution &#123; int ans; public int diameterOfBinaryTree(TreeNode root) &#123; ans = 0; //拿到每个节点的左右子树的深度的和，ans代表了最长直径，其实也就是最大的左右子树深度之和 depth(root); return ans; &#125; public int depth(TreeNode node) &#123; if (node == null) return 0; int L = depth(node.left); int R = depth(node.right); ans = Math.max(ans, L+R); return Math.max(L, R) + 1; &#125;&#125; 把二叉搜索树转换为累加树题目给定一个二叉搜索树（Binary Search Tree），把它转换成为累加树（Greater Tree)，使得每个节点的值是原来的节点值加上所有大于它的节点值之和。 例如： 123456789101112输入: 二叉搜索树: 5 / \ 2 13输出: 转换为累加树: 18 / \ 20 13来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/convert-bst-to-greater-tree 思路二叉搜索树，第一想法就是用中序递归去做，累加树的意思是 根节点的值 = 根节点的值 + 右子树的值 ，然后再更新左子树的值 = 根节点的值 + 左子树的值，所以就符合右中左的遍历顺序，只要写出来右中左的递归，然后用一个全局变量记录上一个遍历的节点的值再加上本身的值，就okay了！ 代码1234567891011class Solution &#123; int add = 0; public TreeNode convertBST(TreeNode root) &#123; if (root == null) return root; convertBST(root.right); root.val += add; add = root.val; convertBST(root.left); return root; &#125;&#125; Tip: 其实个人觉得这个题目有问题，既然是累加树，为何是按照右中左的遍历顺序来累加呢…我的根节点还得加到我左子树的最右叶子节点上！真的荒唐啊… 二叉树展开为链表题目给定一个二叉树，原地将它展开为链表。 例如，给定二叉树 12345 1 / \ 2 5 / \ \3 4 6 将其展开为： 12345678910111 \ 2 \ 3 \ 4 \ 5 \ 6 思路注意题目的要求是 原地 展开！ 这里强推一手题解里一位大佬写的！总共3个大方法，非常好！！！ 二叉树转换成链表 解法一[自顶向下非递归]可以发现展开的顺序其实就是二叉树的先序遍历。算法和 94 题中序遍历的 Morris 算法有些神似。 将左子树插入到右子树的地方 将原来的右子树接到左子树的最右边节点 代码的话也很好写，首先我们需要找出左子树最右边的节点以便把右子树接过来。 1234567891011121314151617181920public void flatten(TreeNode root) &#123; while (root != null) &#123; //左子树为 null，直接考虑下一个节点 if (root.left == null) &#123; root = root.right; &#125; else &#123; // 找左子树最右边的节点 TreeNode pre = root.left; while (pre.right != null) &#123; pre = pre.right; &#125; //将原来的右子树接到左子树的最右边节点 pre.right = root.right; // 将左子树插入到右子树的地方 root.right = root.left; root.left = null; // 考虑下一个节点 root = root.right; &#125; &#125; 解法二[自顶向上，推荐]题目中，要求说是 in-place，之前一直以为这个意思就是要求空间复杂度是 O(1)。偶然看见评论区大神的解释， in-place 的意思可能更多说的是直接在原来的节点上改变指向，空间复杂度并没有要求。所以这道题也可以用递归解一下。利用变形的后序遍历 右左根即可。 1234567891011private TreeNode pre = null;public void flatten(TreeNode root) &#123; if (root == null) return; flatten(root.right); flatten(root.left); root.right = pre; root.left = null; pre = root;&#125; 相应的左孩子也要置为 null，同样的也不用担心左孩子丢失，因为是后序遍历，左孩子已经遍历过了。和 112 题一样，都巧妙的利用了后序遍历。 既然后序遍历这么有用，利用 112 题介绍的后序遍历的迭代方法，把这道题也改一下吧。 12345678910111213141516171819202122232425public void flatten(TreeNode root) &#123; Stack&lt;TreeNode&gt; toVisit = new Stack&lt;&gt;(); TreeNode cur = root; TreeNode pre = null; while (cur != null || !toVisit.isEmpty()) &#123; while (cur != null) &#123; toVisit.push(cur); // 添加根节点 cur = cur.right; // 递归添加右节点 &#125; cur = toVisit.peek(); // 已经访问到最右的节点了 // 在不存在左节点或者右节点已经访问过的情况下，访问根节点 if (cur.left == null || cur.left == pre) &#123; toVisit.pop(); /**************修改的地方***************/ cur.right = pre; cur.left = null; /*************************************/ pre = cur; cur = null; &#125; else &#123; cur = cur.left; // 左节点还没有访问过就先访问左节点 &#125; &#125; &#125; 解法三[自顶向上]解法二中提到如果用先序遍历的话，会丢失掉右孩子，除了用后序遍历，还有没有其他的方法避免这个问题。在 Discuss 又发现了一种解法。 为了更好的控制算法，所以我们用先序遍历迭代的形式，正常的先序遍历代码如下， 123456789101112131415public static void preOrderStack(TreeNode root) &#123; if (root == null) &#123; return; &#125; Stack&lt;TreeNode&gt; s = new Stack&lt;TreeNode&gt;(); while (root != null || !s.isEmpty()) &#123; while (root != null) &#123; System.out.println(root.val); s.push(root); root = root.left; &#125; root = s.pop(); root = root.right; &#125;&#125; 还有一种特殊的先序遍历，提前将右孩子保存到栈中，我们利用这种遍历方式就可以防止右孩子的丢失了。由于栈是先进后出，所以我们先将右节点入栈。 1234567891011121314151617public static void preOrderStack(TreeNode root) &#123; if (root == null)&#123; return; &#125; Stack&lt;TreeNode&gt; s = new Stack&lt;TreeNode&gt;(); s.push(root); while (!s.isEmpty()) &#123; TreeNode temp = s.pop(); System.out.println(temp.val); if (temp.right != null)&#123; s.push(temp.right); &#125; if (temp.left != null)&#123; s.push(temp.left); &#125; &#125;&#125; 之前我们的思路如下： 题目其实就是将二叉树通过右指针，组成一个链表。 11 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 6 我们知道题目给定的遍历顺序其实就是先序遍历的顺序，所以我们可以利用先序遍历的代码，每遍历一个节点，就将上一个节点的右指针更新为当前节点。12345先序遍历的顺序是 1 2 3 4 5 6。遍历到 2，把 1 的右指针指向 2。1 -&gt; 2 3 4 5 6。遍历到 3，把 2 的右指针指向 3。1 -&gt; 2 -&gt; 3 4 5 6。 因为我们用栈保存了右孩子，所以不需要担心右孩子丢失了。用一个 pre 变量保存上次遍历的节点。修改的代码如下：1234567891011121314151617181920212223242526public void flatten(TreeNode root) &#123; if (root == null)&#123; return; &#125; Stack&lt;TreeNode&gt; s = new Stack&lt;TreeNode&gt;(); s.push(root); TreeNode pre = null; while (!s.isEmpty()) &#123; TreeNode temp = s.pop(); /***********修改的地方*************/ if(pre!=null)&#123; pre.right = temp; pre.left = null; &#125; /********************************/ if (temp.right != null)&#123; s.push(temp.right); &#125; if (temp.left != null)&#123; s.push(temp.left); &#125; /***********修改的地方*************/ pre = temp; /********************************/ &#125;&#125; 总结解法一和解法三可以看作自顶向下的解决问题，解法二可以看作自底向上。以前觉得后序遍历比较麻烦，没想到竟然连续遇到了后序遍历的应用。先序遍历的两种方式自己也是第一次意识到，之前都是用的第一种正常的方式。 作者：windliang链接：https://leetcode-cn.com/problems/flatten-binary-tree-to-linked-list/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by--26/ Tips: 这个大佬写的是真好！ 深度和广度优先搜索BFS框架12345678910111213141516171819202122232425262728// 计算从起点 start 到终点 target 的最近距离int BFS(Node start, Node target) &#123; Queue&lt;Node&gt; q; // 核心数据结构 Set&lt;Node&gt; visited; // 避免走回头路 q.offer(start); // 将起点加入队列 visited.add(start); int step = 0; // 记录扩散的步数 while (q not empty) &#123; int sz = q.size(); /* 将当前队列中的所有节点向四周扩散 */ for (int i = 0; i &lt; sz; i++) &#123; Node cur = q.poll(); /* 划重点：这里判断是否到达终点 */ if (cur is target) return step; /* 将 cur 的相邻节点加入队列 */ for (Node x : cur.adj()) if (x not in visited) &#123; q.offer(x); visited.add(x); &#125; &#125; /* 划重点：更新步数在这里 */ step++; &#125;&#125; 队列 q 存储当前需要遍历的节点，BFS 的核心数据结构；cur.adj() 泛指 cur 相邻的节点，比如说二维数组中，cur 上下左右四面的位置就是相邻节点；visited 的主要作用是防止走回头路，大部分时候都是必须的，但是像一般的二叉树结构，没有子节点到父节点的指针，不会走回头路就不需要 visited。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>二叉树遍历</tag>
        <tag>二叉树相关题型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复试准备]]></title>
    <url>%2F2020%2F01%2F05%2F%E5%A4%8D%E8%AF%95%E5%87%86%E5%A4%87.html</url>
    <content type="text"><![CDATA[复试准备大概说一下准备策略吧…最近有考试，也没啥时间，就初略的写一下叭…凑合看！ 概述今年的复试跟去年只有一个不同，就是将往年的30分的计算机网络更换为算法设计与分析，其他全部保持一致。 复试分为机试(200) + 面试(140) + 英语笔试(10)，总分共计350分，最后分数是直接与初试分数相加，按照总成绩 分数进行排名，其中机试尤为关键，按照往年经验来说，机试分差在50分左右都是非常常见的！至于面试，大家 差的都不多，我下面会主要讲一下机试准备，顺带讲一下面试！ 复试流程一般会在3月初进行复试，我们去年是2月14出的成绩，3月7号就得到达哈尔滨进行考前的一些手续办理，中间大概只有15天的时间准备，所以务必要提前备考(我就很后悔没有提前备考…流下悔恨的泪水啊…) 考前一两天会先让你交各种材料，然后心理测评，然后就开始复试。复试是一整天的，上午机试，下午面试，机试时间是两小时，时间是比较紧张的，因为题量比较大，下面我会详细说的！在机试开始之前，会让你先做英语笔试，也就是坐在电脑面前会先给你发一张英语试卷(其实就是一篇英文论文)，然后你需要看这篇论文，回答相关的5个问题，一共10分，每小问2分(这个大家都差不多的，不要影响自己的机试就行！) 然后是开始机试！两个小时，应该是9点半到11点半？(我也不记得了呀) ，机试完之后基本没有休息，大概中午一点就需要进入面试准备了，所有人会放到一个大房间里，去年是有8条线，每条线都是不一样的老师，也就是流水线作业，一共有四轮面试，每轮35分，共140分，每个人会分到一个数字，代表你是8条线的哪一条，然后进行面试，然后只有等到所有人全部面试结束，方可离开。 复试完当天就会有深圳校区的宣讲会，所有老师都会到场！！！报考深圳的！！！！千万千万不要错过，脸皮一定要厚！！！！！当天就需要确定导师的，如果你不去找，到时候上线了只能挑别人剩下的，所以不管你复试到底成绩咋样(复试成绩会在之后两天才出)，先找老师，上线了就可以直接跟他走，没上线的话就再联系本部老师就行了！ Tip: 当然了，报考本部的，按理说在出分后第一时间就应该去联系老师，因为一般都是先到先得的，到后头可能就只能是挑剩下的了… 机试部分首先是200分的机试，按照往年推算，今年应该题型大概保持一致，也就是120分选择题 + 80分主观题，其中80分主观题去年是分为20分编程题 + 60分改错题。这里强烈强烈强烈建议一定要先全部看一下题型，因为机试可能每年的题型都不一样，我考试的时候就没看后面的题目，就做着做着，还剩50来分钟做大题，突然发现改错由一题变为了三题，瞬间就慌了… 选择题：120分选择题，包含了数据库系统、离散数学、算法设计与分析、逻辑思维能力。也就是各占30分，其中要说明的是数据库系统、离散数学、算法设计与分析都是2分一题，也就是共有45题，而逻辑思维题是6分一题(忘了是5分还是6分了…)，所以加起来选择题有50来道，这个题量是很大的，务必要考前计划好时间！！！！因为去年坐我旁边的两位大哥…我选择题都做完了他们还在做20题，最后他们我看分数都是80上下… 选择题的话，我再多嘴一句，最好是先去做逻辑题，因为这个虽然简单但是还是需要充裕的时间去推理的，去年坐我旁边的两位大哥就很可惜…逻辑题好几题没写… 主观题：80分，我只讲去年的题型。60分改错，20分编程题。 先讲20分编程题，题目要求用C语言作答！题目的难度基本与C语言那本书(苏小红的)的课后题持平，或者说 略微难一些些，去年是考的字符串排序之类的(我也有点忘记啦，你们可以上王道论坛看看最新的复试试卷，我是没有收集的哈~我给的资料都是我去年的，没有集合19年的)。 再讲60分改错。改错也是给你一道题，给你代码，然后你修改，全部改对才能ac，否则就是0分，一共3题，每题20分！难度略高，放最后做吧。 数据库系统考试题目是在中国大学MOOC中战德臣讲的《数据库系统》中的测验里的题目出，不会超出这个范围哟！！！！！如果后期时间不太够，可以直接看题目背答案，当然你需要背的很熟哦(我当时是刷了很多遍，直接背的题目和答案，看到题目就知道答案！)，因为我当时时间很紧迫，就没有全部看完视频，因为视频太多了…上中下…感觉得看几星期。 预期如果一边看视频，一边背题的话，一星期大概就能拿下！你可以自己去感受下题量还有视频的多少，然后自己估计需要多久时间，能把里面的题目全部做会，并且要反应特别快，这样会给机试争取大量时间…这可能就是我机试比别人做得快的原因吧… 离散数学这是所有复试科目里面最难的一门！！因为包括了集合论与图论以及近世代数！建议看Mooc，看咱工大老师的离散数学！具体名字我给忘了！一定要趁早复习，不然基本上后期就是完全放弃的状态，这个一定提早复习，预期复习时间至少要给到 15-20 天… 习题的话建议把我资料里那些离散数学部分的习题做完就差不多了，难度基本与之持平。 算法设计与分析这个我没考过..预期是算时间复杂度之类的题型吧…毕竟是选择题… 逻辑思维能力这个考前无需准备，稍微看一下我给的资料里的题目就行，题目比较简单，建议考试先做这个，分值很重，很多同学在考试的时候最后都没时间做这个！！！！贼可惜的吖！！送分题变成了送命题… C语言这个80分是大头哟，基本上决定你能不能上你的一志愿！基本上做出两道改错就很稳了，毕竟20分一题吖！我建议这个在出分前就基本要准备完成，出分后可以进行数据库系统的复习和离散数学近世代数部分复习。准备方式就是刷苏小红老师那本C语言的书，把重要章节(字符串那几章)课后题全部刷一遍，找到 ac 题目的感觉，同时温馨提醒可以上 LeetCode 找一些相关的专题进行练习，这样在考场会有一种熟悉的感觉！ 这个准备起来时间应该差不多15天左右，一定要多写代码，有感觉是最重要的。 面试部分上面有讲过，面试是在下午1点左右就差不多准备开始的。共 4 轮，每轮 35 分。面试的整体流程是，会给你排号，然后到你了会叫你出去，然后到指定教室的门外，拿到一张纸，上面有该轮面试要问的题目，你在门外准备 5分钟左右，然后进去直接作答就好，一轮面试大概是 5 分钟左右，时间很短。 一面 35 分，第一面自我介绍，题目大概是，给你一张图，让你看看能看到啥(鬼知道这是在整啥玩意…)，全程可以选择英文 or 中文，可能用英文全程作答会有点加分吧… 二面 35 分，第二面是讲竞赛经历和项目经历，主要是看证书(奖学金证书啥的没用哈)，要的是CCF之类的获奖证书，其他的一律不看！这一面如果没有竞赛经历和获奖经历扣分会比较严重，项目的话建议自己包装一下，最好是能商用的项目讲出来，然后涉及的技术栈最好也是企业正在用的，举个小例子，就比如 web 开发，正常现在都是前后端分离，前端 react/vue，后端 springboot，然后外带微服务 springCloud/Dubbo，并且使用容器技术 Docker/K8s，使用数据库 Redis，穿插使用消息中间件 RabbitMQ/Kafka，大概这样包装一下我觉得几分钟下来老师是问不出什么破绽的，最好是全程都是你在讲这样子，效果会好一些。 三面是专业面，35 分，就是给你一道题，你在外面思考5分钟，一般就是普通的智力题，不用太担心，挺简单的，我当时的题目是爬山问题，一个人从山下爬上去，一个从山下下来，什么时候相遇…这类的问题。个人感觉这一面想考察的还是你对具体生活场景的抽象化，能够转换成计算机思维的方式，并且尽可能的考虑问题全面一些，尤其是一些条件的设定，可以跟老师多探讨一下，主要还是不要冷场，全程都有在交流我觉得就比较 okay 的。 四面！！！！35 分，这个我最想讲，因为我面的不好，没有准备，也没经验！！我的建议是：一定要看看人工智能，自然语言，机器翻译，模式识别等等之类的东西，这一面主要就问这个，我当时是面的人工智能，妈呀啥也不会啊，讲不出话…你准备的话就大概看看人工智能是个啥…用于哪些领域，有大概哪些技术，初略的大概看看就行。对了，好像也有一两条线会问编译原理和操作系统的问题，我这条线倒是没问过，如果有时间的话可以回顾一下本科的 ppt，应该就差不多了。]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>工大复试相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归总结(转)]]></title>
    <url>%2F2020%2F01%2F04%2F%E9%80%92%E5%BD%92%E8%AF%A6%E8%A7%A3.html</url>
    <content type="text"><![CDATA[3道题彻底搞定：套路解决递归问题(转)前言转的哈~我自己学习看的！！ 原文链接 ——-&gt; 解决递归 相信不少同学和我一样，在刚学完数据结构后开始刷算法题时，遇到递归的问题总是很头疼，而一看解答，却发现大佬们几行递归代码就优雅的解决了问题。从我自己的学习经历来看，刚开始理解递归思路都很困难，更别说自己写了。 我一直觉得刷算法题和应试一样，既然是应试就一定有套路存在。在刷题中，我总结出了一套解决递归问题的模版思路与解法，用这个思路可以秒解很多递归问题。 递归解题三部曲何为递归？程序反复调用自身即是递归。 我自己在刚开始解决递归问题的时候，总是会去纠结这一层函数做了什么，它调用自身后的下一层函数又做了什么…然后就会觉得实现一个递归解法十分复杂，根本就无从下手。 相信很多初学者和我一样，这是一个思维误区，一定要走出来。既然递归是一个反复调用自身的过程，这就说明它每一级的功能都是一样的，因此我们只需要关注一级递归的解决过程即可。 我们需要关心的主要是以下三点： 整个递归的终止条件。 一级递归需要做什么？ 应该返回给上一级的返回值是什么？ 因此，也就有了我们解递归题的三部曲： 找整个递归的终止条件：递归应该在什么时候结束？ 找返回值：应该给上一级返回什么信息？ 本级递归应该做什么：在这一级递归中，应该完成什么任务？ 一定要理解这3步，这就是以后递归秒杀算法题的依据和思路。 但这么说好像很空，我们来以题目作为例子，看看怎么套这个模版，相信3道题下来，你就能慢慢理解这个模版。之后再解这种套路递归题都能直接秒了。 例1：求二叉树的最大深度先看一道简单的Leetcode题目： Leetcode 104. 二叉树的最大深度 题目很简单，求二叉树的最大深度，那么直接套递归解题三部曲模版： 找终止条件。 什么情况下递归结束？当然是树为空的时候，此时树的深度为0，递归就结束了。 找返回值。 应该返回什么？题目求的是树的最大深度，我们需要从每一级得到的信息自然是当前这一级对应的树的最大深度，因此我们的返回值应该是当前树的最大深度，这一步可以结合第三步来看。 本级递归应该做什么。 首先，还是强调要走出之前的思维误区，递归后我们眼里的树一定是这个样子的，看下图。此时就三个节点：root、root.left、root.right，其中根据第二步，root.left和root.right分别记录的是root的左右子树的最大深度。那么本级递归应该做什么就很明确了，自然就是在root的左右子树中选择较大的一个，再加上1就是以root为根的子树的最大深度了，然后再返回这个深度即可。 具体Java代码如下： 12345678910111213class Solution &#123; public int maxDepth(TreeNode root) &#123; //终止条件：当树为空时结束递归，并返回当前深度0 if(root == null)&#123; return 0; &#125; //root的左、右子树的最大深度 int leftDepth = maxDepth(root.left); int rightDepth = maxDepth(root.right); //返回的是左右子树的最大深度+1 return Math.max(leftDepth, rightDepth) + 1; &#125;&#125; 当足够熟练后，也可以和Leetcode评论区一样，很骚的几行代码搞定问题，让之后的新手看的一脸懵逼(这道题也是我第一次一行代码搞定一道Leetcode题)： 12345class Solution &#123; public int maxDepth(TreeNode root) &#123; return root == null ? 0 : Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; &#125;&#125; 例2：两两交换链表中的节点看了一道递归套路解决二叉树的问题后，有点套路搞定递归的感觉了吗？我们再来看一道Leetcode中等难度的链表的问题，掌握套路后这种中等难度的问题真的就是秒：Leetcode 24. 两两交换链表中的节点 直接上三部曲模版： 找终止条件。 什么情况下递归终止？没得交换的时候，递归就终止了呗。因此当链表只剩一个节点或者没有节点的时候，自然递归就终止了。 找返回值。 我们希望向上一级递归返回什么信息？由于我们的目的是两两交换链表中相邻的节点，因此自然希望交换给上一级递归的是已经完成交换处理，即已经处理好的链表。 本级递归应该做什么。 结合第二步，看下图！由于只考虑本级递归，所以这个链表在我们眼里其实也就三个节点：head、head.next、已处理完的链表部分。而本级递归的任务也就是交换这3个节点中的前两个节点，就很easy了。 附上Java代码： 123456789101112131415class Solution &#123; public ListNode swapPairs(ListNode head) &#123; //终止条件：链表只剩一个节点或者没节点了，没得交换了。返回的是已经处理好的链表 if(head == null || head.next == null)&#123; return head; &#125; //一共三个节点:head, next, swapPairs(next.next) //下面的任务便是交换这3个节点中的前两个节点 ListNode next = head.next; head.next = swapPairs(next.next); next.next = head; //根据第二步：返回给上一级的是当前已经完成交换后，即处理好了的链表部分 return next; &#125;&#125; 例3：平衡二叉树相信经过以上2道题，你已经大概理解了这个模版的解题流程了。 那么请你先不看以下部分，尝试解决一下这道easy难度的Leetcode题（个人觉得此题比上面的medium难度要难）：Leetcode 110. 平衡二叉树 我觉得这个题真的是集合了模版的精髓所在，下面套三部曲模版： 找终止条件。 什么情况下递归应该终止？自然是子树为空的时候，空树自然是平衡二叉树了。 应该返回什么信息： 为什么我说这个题是集合了模版精髓？正是因为此题的返回值。要知道我们搞这么多花里胡哨的，都是为了能写出正确的递归函数，因此在解这个题的时候，我们就需要思考，我们到底希望返回什么值？ 何为平衡二叉树？平衡二叉树即左右两棵子树高度差不大于1的二叉树。而对于一颗树，它是一个平衡二叉树需要满足三个条件：它的左子树是平衡二叉树，它的右子树是平衡二叉树，它的左右子树的高度差不大于1。换句话说：如果它的左子树或右子树不是平衡二叉树，或者它的左右子树高度差大于1，那么它就不是平衡二叉树。 而在我们眼里，这颗二叉树就3个节点：root、left、right。那么我们应该返回什么呢？如果返回一个当前树是否是平衡二叉树的boolean类型的值，那么我只知道left和right这两棵树是否是平衡二叉树，无法得出left和right的高度差是否不大于1，自然也就无法得出root这棵树是否是平衡二叉树了。而如果我返回的是一个平衡二叉树的高度的int类型的值，那么我就只知道两棵树的高度，但无法知道这两棵树是不是平衡二叉树，自然也就没法判断root这棵树是不是平衡二叉树了。 因此，这里我们返回的信息应该是既包含子树的深度的int类型的值，又包含子树是否是平衡二叉树的boolean类型的值。可以单独定义一个ReturnNode类，如下： 123456789class ReturnNode&#123; boolean isB; int depth; //构造方法 public ReturnNode(boolean isB, int depth)&#123; this.isB = isB; this.depth = depth; &#125;&#125; 本级递归应该做什么。 知道了第二步的返回值后，这一步就很简单了。目前树有三个节点：root，left，right。我们首先判断left子树和right子树是否是平衡二叉树，如果不是则直接返回false。再判断两树高度差是否不大于1，如果大于1也直接返回false。否则说明以root为节点的子树是平衡二叉树，那么就返回true和它的高度。 具体的Java代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; //这个ReturnNode是参考我描述的递归套路的第二步：思考返回值是什么 //一棵树是BST等价于它的左、右俩子树都是BST且俩子树高度差不超过1 //因此我认为返回值应该包含当前树是否是BST和当前树的高度这两个信息 private class ReturnNode&#123; boolean isB; int depth; public ReturnNode(int depth, boolean isB)&#123; this.isB = isB; this.depth = depth; &#125; &#125; //主函数 public boolean isBalanced(TreeNode root) &#123; return isBST(root).isB; &#125; //参考递归套路的第三部：描述单次执行过程是什么样的 //这里的单次执行过程具体如下： //是否终止?-&gt;没终止的话，判断是否满足不平衡的三个条件-&gt;返回值 public ReturnNode isBST(TreeNode root)&#123; if(root == null)&#123; return new ReturnNode(0, true); &#125; //不平衡的情况有3种：左树不平衡、右树不平衡、左树和右树差的绝对值大于1 ReturnNode left = isBST(root.left); ReturnNode right = isBST(root.right); if(left.isB == false || right.isB == false)&#123; return new ReturnNode(0, false); &#125; if(Math.abs(left.depth - right.depth) &gt; 1)&#123; return new ReturnNode(0, false); &#125; //不满足上面3种情况，说明平衡了，树的深度为左右俩子树最大深度+1 return new ReturnNode(Math.max(left.depth, right.depth) + 1, true); &#125;&#125; 一些可以用这个套路解决的题暂时就写这么多啦，作为一个高考语文及格分，大学又学了工科的人，表述能力实在差因此啰啰嗦嗦写了一大堆，希望大家能理解这个很好用的套路。 下面我再列举几道我在刷题过程中遇到的也是用这个套路秒的题，真的太多了，大部分链表和树的递归题都能这么秒，因为树和链表天生就是适合递归的结构。 我会随时补充，正好大家可以看了上面三个题后可以拿这些题来练练手，看看自己是否能独立快速准确的写出递归解法了。 Leetcode 101. 对称二叉树 Leetcode 111. 二叉树的最小深度 Leetcode 226. 翻转二叉树：这个题的备注是最骚的。Mac OS下载神器homebrew的大佬作者去面试谷歌，没做出来这道算法题，然后被谷歌面试官怼了：”我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。” Leetcode 617. 合并二叉树 Leetcode 654. 最大二叉树 Leetcode 83. 删除排序链表中的重复元素 Leetcode 206. 翻转链表]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>模板</tag>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[历年面经总结]]></title>
    <url>%2F2019%2F12%2F27%2F%E5%8E%86%E5%B9%B4%E9%9D%A2%E7%BB%8F%E6%80%BB%E7%BB%93.html</url>
    <content type="text"><![CDATA[快手篇数据结构 说说B+树和B树的区别，优缺点等？ B 树每个节点都有存储 Date域，并且 B 树的叶子节点并没有用链表连接，但是 B+ 树每个非叶子节点只有索引值和指针域，并不存储 Date域，并且其叶子节点存储 Date 用链表连接，B树的话比平衡二叉树「是一棵空树或它的左右两个子树的高度差的绝对值不超过1』而言，增加和删除节点改动很少，效率高，B+是为了数据库而生的，其链表可以更好的支持范围查询，并且由于他的非叶子节点是不存储 Date域的，我们知道，每一个节点都是采用的磁盘预读，也就是说一个节点占用一页的数据，这样的话B+ 在一页中可以存储更多的指针域，这样就大大降低了树的高度，这样磁盘读写的次数就下降了，非常好。 经有一个查询好友的接口，设计一个微信朋友圈，可以实现发表朋友圈，添加评论，查看评论等功能。主要是设计数据结构 操作系统计算机网络 TCP三次握手四次挥手，四次挥手过程中服务端的哪几种状态，哪几种包 三次握手：客户端发一个 syn 服务器端发一个 syn ack 然后客户端发一个ack 四次挥手：客户端发一个 fin 服务器端发送一个 ack 然后发送一个 fin 然后客户端再发送一个ack 算法 算法题：无序数列中求第k大的数(维护最小堆，然后依次遍历，与堆顶比较 写一个选择排序或者插入排序 写一个生产者消费者 「Get!!!」 三种方法。 synchronized（Object）的 wait/notify； ReentrantLock 中的 Condition 的 await/signal； 阻塞队列直接实现：LinkedBlockingQueue 一个二维矩阵进行逆置操作，也就是行变列列变行。扩展一下，二维数组存在500g的文件中，怎么做才能完成上面算法的操作，我就说先按行拆分，最后再拼接。 java基础 HashMap的底层数据结构 「Get!!!」 红黑树+链表+数组 HashMap哈希函数的认识，JDK1.8采用的hash函数 「Get!!!」 扰动函数，高16位和低16位相异或，32位都参与运算，这样hash更均匀 至于如何计算所在的数组位置，直接 hash ^ tab.length 相与 多线程 Q：多线程JUC包下的一些常见的类，比如CountDownLatch、Semaphore等 「Get!!!」 CountDownLatch 倒计时器 Semphore 信号量 Q：锁的概念，锁相关的关键字，volatile，synchronized。还比较了ReentrantLock与synchronized。 「Get!!!」 ReentrantLock &amp; syncronized 区别 ReentrantLock 有三个 syncronized 没有的特性，第一个是 条件队列可以有多个，Condition 可以分组，但是 syncronized 不可以；第二个是 ReentrantLock 提供 公平锁 和 非公平锁的选择，但是 syncronized 不提供；第三个是 ReentrantLock有 lockInterruptibly() 方法，提供优先响应中断，就是当线程正在等待时，可以调用这个等待线程的 Thread.interrupt()，停止等待，直接返回。 ReentrantLock 是一个类，jdk层面实现，而 syncronized 则是 jvm层面实现，是一个关键字； 当然了，都是可重入锁，而且现在 syncronized 的性能已经基本持平 ReentrantLock 了。 Q：线程池有哪些参数？分别有什么用？如果任务数超过的核心线程数，会发生什么？阻塞队列大小是多少？ 「Get!!!」 线程池的参数有 corePoolSize（线程池核心数）、线程最大数（maxmiumPoolSize）、阻塞队列(worQueue)、keepAliveTime(线程池维护线程所允许的空闲时间)、线程工厂（ThreadFactory）、拒绝策略（handler）。如果任务数超过核心线程数，会先将任务放到阻塞队列中，然后如果阻塞队列满，则扩大线程。阻塞队列大小分为无界阻塞队列和有界阻塞队列。 Q：在高并发的情况下，我们怎么选择最优的线程数量呢？选择原则又是什么呢？线程数是不是越多越好？ 「Get!!!」 选择线程池的数量需要从 CPU 的角度考虑，因为考虑一个系统的吞吐量上限就是CPU，充分利用 cpu 是关键。CPU密集型应用，则线程池大小设置为N+1（由于cpu很忙，所以额外的切换上下文非常的消耗资源，所以一个线程干活对应一核cpu即可，多的那一个是为了防止某个线程失败或者挂起导致浪费cpu时钟），IO密集型，线程池大小是 2N+1,准确的说是 最佳线程数目 = （（线程等待时间+线程CPU时间）/线程CPU时间 ）* CPU数目，混合型的话就将二者结合就好。当然，要想提高系统吞吐量，真正的是要去解决短板效应，也就是 I/O，可以将其变为 NIO 之类的。 线程数肯定不是越多越好，首先 cpu 就那几个，并行的也就那几个，加上切换线程上下文也是需要消耗的。 https://www.cnblogs.com/jpfss/p/10967703.html Q：有哪些比较好的线程池，各自的特点是什么？ 「Get!!!」 四大线程池 下面有写 Q：怎么实现一个线程安全的计数器？ 「Get!!!」 使用加锁的方式，synchronized or ReentrantLock 都可以，或者使用 juc 包下的 Atomic 原子类也是可以的。 123456789101112131415161718&gt; public class Count1 &#123;&gt; // 1. 使用 synchronized，用不用 volatile 都是可以的&gt; private static int a = 0;&gt; &gt; public static void main(String[] args) &#123;&gt; final ExecutorService executorService = Executors.newCachedThreadPool();&gt; // 启动1000个线程&gt; for (int i = 0; i &lt; 1000; i++) &#123;&gt; executorService.execute(() -&gt; count());&gt; &#125;&gt; executorService.shutdown();&gt; System.out.println(a);&gt; &#125;&gt; private static synchronized void count() &#123;&gt; a++;&gt; &#125;&gt; &#125;&gt; 123456789101112131415161718192021222324252627&gt; class Count2 &#123;&gt; // 1. 使用 volatile + ReentrantLock&gt; // 不用 volatile 也是可以的&gt; private static int a = 0;&gt; private static final ReentrantLock lock = new ReentrantLock();&gt; &gt; public static void main(String[] args) &#123;&gt; final ExecutorService executorService = Executors.newCachedThreadPool();&gt; // 启动1000个线程&gt; for (int i = 0; i &lt; 1000; i++) &#123;&gt; executorService.execute(() -&gt; count());&gt; &#125;&gt; executorService.shutdown();&gt; System.out.println(a);&gt; &#125;&gt; &gt; private static void count() &#123;&gt; try &#123;&gt; lock.lock();&gt; a++;&gt; &#125; finally &#123;&gt; lock.unlock();&gt; &#125;&gt; &gt; &#125;&gt; &#125;&gt; 123456789101112131415&gt; class Count3 &#123;&gt; // 1. 直接使用 Atomic就可以了&gt; private static AtomicInteger a = new AtomicInteger(0);&gt; &gt; public static void main(String[] args) &#123;&gt; final ExecutorService executorService = Executors.newCachedThreadPool();&gt; // 启动1000个线程&gt; for (int i = 0; i &lt; 10000; i++) &#123;&gt; executorService.execute(() -&gt; a.getAndIncrement());&gt; &#125;&gt; executorService.shutdown();&gt; System.out.println(a.get());&gt; &#125;&gt; &#125;&gt; 接着聊ConcurrentHashMap，底层实现 「Get!!!」 底层实现？ 红黑树+数组+链表 Java多线程了解么，什么时候一个int，类型的操作是不安全的，自加呢，赋值呢。如果使用volatile修饰的话有什么作用。 「Get!!!」 自加，不安全；赋值，安全。因为前者有读和写两个操作，后者只有写操作 用volatile 也一样，volatile不能保证原子性，可以使用加锁或者使用 Atomic原子类。 有一个场景，多线程并发，为每个线程安排一个随机的睡眠时间，设想一种数据结构去唤醒睡眠时间结束的线程，应该用哪种结构 「Get!!!」 应该用优先级队列，也就是小顶堆，顶部是剩余睡眠时间最短的那个线程。PriorityQueue。 JVM 你了解哪些收集器？CMS和G1。详细谈谈G1的优点？什么时候进行Full GC呢？ Serial Serial Old ParNew Parllelscatw。。。 parallelOld CMS G1 G1优点： 不区分新生代和老年代，是连续的空间，然后是一个一个rezone进行管理，有一个优先级的排序，就是会把这个rezone垃圾最多的先回收，然后是采用的标记整理解决内存的碎片问题，兼顾了 CMS 低延迟的一个特性和 Parallel Scavenge 高吞吐率的特性。 老年代区域满了不就 Full GC 了吗… 消息中间件spring全家桶 Spring中涉及的一些设计模式 聊聊Spring，主要IOC等等 数据库 MySQL创建索引的原则，好处 数据库索引，索引底层的实现，B+树的结构以及与普通查找树的优点 MySQL和redis的区别是什么 为什么MySQL和Redis都要使用独立进程来部署，开放端口来提供服务，为什么不写在内核中。 其他 设计模式：讲了单例，工厂方法，抽象工厂，策略模式，观察者模式，代理模式，还顺便讲了下spring动态代理的实现原理 红黑树的具体结构及实现，红黑树与查找树的区别体现 这个我会…复习一波 项目中用到dubbo？那你说说什么是rpc框架？和http调用的区别是什么？ Redis有哪些数据结构？持久化方案和区别？ Redis哨兵、集群的设计原理和区别？ Redis缓存和数据库会存在一致性问题吗？怎么解决 Kafka怎么保证数据可靠性？讲了生产者端发送消息到broker持久化，分区和副本机制，消费者消费消息的at-least-once和at-most-once？怎么实现Exactly-Once？ 字节篇数据结构操作系统 用户态如何切换到内核态 进程间通信的方式，哪种最快 进程间通信（IPC，InterProcess Communication）是指在不同进程之间传播或交换信息。 IPC的方式通常有管道（包括无名管道和命名管道）、消息队列、信号量、共享存储、Socket、Streams等。其中 Socket和Streams支持不同主机上的两个进程IPC。 1.进程与线程 进程：具有独立功能的程序关于某个数据集合上的一次运行活动。线程：进程的一个实体。比喻：一列火车是一个进程，火车的每一节车厢是线程。 2.进程与线程的联系 ①一个线程只能属于一个进程，一个进程可以有多个线程；②系统资源分配给进程，同一进程的所有线程共享该进程的所有资源；③真正在处理机上运行的是线程；④不同进程的线程间利用消息通信的方式实现同步。 3.进程与线程的区别 ①调度：线程是系统调度和分配的基本单位，进程是作为拥有系统资源的基本单位；②并发性：进程之间可以并发执行，同一进程的多个线程时间亦可以并发执行；③拥有资源：进程是拥有资源的独立单位，线程不拥有资源，但可以访问隶属于进程的资源；④系统开销：创建和撤销进程的开销更大；进程拥有独立的地址空间，一个进程的崩溃不会影响其他进程；线程拥有自己的堆栈和局部变量，没有独立的地址空间，因此进程里的一个线程崩溃会导致其他线程均崩溃。 4.进程间通信方式 进程间通信方式有很多，网上一说有十几种。面试的时候说上以下几种差不多：①管道：半双工的通信方式，数据只能单向流动，且只能在有亲缘关系（父子进程或兄弟进程）的进程间使用；②命名管道：FIFO，半双工的通信方式，但允许在无亲缘关系的进程间通信；③消息队列：消息的链表，存放在内核中，并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点；④信号量：是一个计数器，用于控制多个进程间对共享资源的访问；⑤共享内存：映射一段能被其他进程访问的内存，这段内存由一个进程创建，但多个进程都可以访问；「最快，可以联系到零拷贝把 mmap」⑥套接字 5.线程间通信方式 多个线程在处理同一个资源，并且任务不同时，需要线程通信来帮助解决线程之间对同一个变量的使用或操作。就是多个线程在操作同一份数据时， 避免对同一共享变量的争夺。①锁机制：包括互斥锁、条件变量、读写锁 互斥锁提供了以排他方式防止数据结构被并发修改的方法。读写锁允许多个线程同时读共享数据，而对写操作是互斥的。条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。②信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量③信号机制(Signal)：类似进程间的信号处理线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。————————————————版权声明：本文为CSDN博主「冯Jungle」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/sinat_21107433/article/details/82809946 信号量怎么实现对共享资源的访问 「Get!!!」 semphore，共享锁加一个访问数量的限制，主体还是 AQS 的共享锁实现，超过了信号量的值就加入等待队列等待，这里的等待队列就是一个普通的 Queue，不像AQS中的 CLH 队列。 select和epoll 在Linux Socket服务器短编程时，为了处理大量客户的连接请求，需要使用非阻塞I/O和复用，select、poll和epoll是Linux API提供的I/O复用方式，自从Linux 2.6中加入了epoll之后，在高性能服务器领域得到广泛的应用，现在比较出名的nginx就是使用epoll来实现I/O复用支持高并发，目前在高并发的场景下，nginx越来越收到欢迎。 (1)select==&gt;时间复杂度O(n) 它仅仅知道了，有I/O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。 (2)poll==&gt;时间复杂度O(n) poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态， 但是它没有最大连接数的限制，原因是它是基于链表来存储的. (3)epoll==&gt;时间复杂度O(1) epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I/O事件通知我们。所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 计算机网络 TCP的流量控制和拥塞控制 一、流量控制 1.什么是流量控制 Sender won’t overflow receiver’s buffer by transmitting too much, too fast. （防止发送方发的太快，耗尽接收方的资源，从而使接收方来不及处理）2.流量控制的一些知识点 （1）接收端抑制发送端的依据：接收端缓冲区的大小（2）流量控制的目标是接收端，是怕接收端来不及处理（3）流量控制的机制是丢包3.怎么样实现流量控制？ 使用滑动窗口滑动窗口1.滑动窗口是什么？滑动窗口是类似于一个窗口一样的东西，是用来告诉发送端可以发送数据的大小或者说是窗口标记了接收端缓冲区的大小，这样就可以实现ps：窗口指的是一次批量的发送多少数据2.为什么会出现滑动窗口？ 在确认应答策略中，对每一个发送的数据段，都要给一个ACK确认应答，收到ACK后再发送下一个数据段，这样做有一个比较大的缺点，就是性能比较差，尤其是数据往返的时间长的时候使用滑动窗口，就可以一次发送多条数据，从而就提高了性能3.滑动窗口的一些知识点 （1）接收端将自己可以接收的缓冲区大小放入TCP首部中的“窗口大小”字段，通过ACK来通知发送端（2）窗口大小字段越大，说明网络的吞吐率越高（3）窗口大小指的是无需等待确认应答而可以继续发送数据的最大值，即就是说不需要接收端的应答，可以一次连续的发送数据（4）操作系统内核为了维护滑动窗口，需要开辟发送缓冲区，来记录当前还有那些数据没有应答，只有确认应答过的数据，才能从缓冲区删掉ps：发送缓冲区如果太大，就会有空间开销 二、拥塞控制 慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。 「乘法减少，加法增加」 三、流量控制和拥塞控制的区别1.相同点 （1）现象都是丢包；（2）实现机制都是让发送方发的慢一点，发的少一点2.不同点 （1）丢包位置不同流量控制丢包位置是在接收端上拥塞控制丢包位置是在路由器上（2）作用的对象不同流量控制的对象是接收方，怕发送方发的太快，使得接收方来不及处理拥塞控制的对象是网络，怕发送发发的太快，造成网络拥塞，使得网络来不及处理3.联系拥塞控制 拥塞控制通常表示的是一个全局性的过程，它会涉及到网络中所有的主机、 所有的路由器和降低网络传输性能的所有因素流量控制 流量控制发生在发送端和接收端之间，只是点到点之间的控制 ————————————————版权声明：本文为CSDN博主「dangzhangjing97」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/dangzhangjing97/article/details/81008836 不同子网可以通过ARP获取mac地址吗 浏览器中输入网址到获得页面的全过程 输入 www.baidu.com，怎么变成 https://www.baidu.com 的，怎么确定用HTTP还是HTTPS 三次握手 握手时产生的序列号干什么用的 讲一下接收窗口 TCP和UDP的本质区别 什么时候选择TCP/UDP HTTPS的连接过程 7层模型和4层模型，每一层有哪些常见协议？ 路由器/交换机是哪一层 网络层用来干嘛？传输层用来干嘛？ HTTP可以使用UDP吗 HTTPS怎么确认收到的包就是服务器发来的 确定发送窗口的大小，如何最大利用带宽，假设延迟100ms，发送端10Mb/s，接收端100Mb/s 网络五层模型，每一层分别有哪些协议 ICMP，TCP，IP，HTTP分别是哪一层，为什么网络模型要分层，每一层之间怎么传递 http头里的gzip 反向***说一下 三次握手，为什么要三次，每一次分别确认了什么 TCP协议的三次握手和四次挥手过程？ Tcp怎么保证可靠的 确认重传机制； 流量控制，滑动窗口协议 拥塞控制：慢启动、拥塞避免、快重传、快恢复 算法 求x的y次方，想出比直接for循环更好的方案 求绝对众数 算法题：红蓝两种球，总共N个， N&gt;2, 排列组合，连续3个颜色一样是非法的，求合法的排列数量 描述一下堆排的过程？建堆的时间复杂度？最大堆中求前k个最大值的时间复杂度？ 算法题：海岛面积计算题，给一个矩阵，0表示海水，相连的1表示海岛，上下左右表示相连。000111101110001（1）求最大海岛面积。（2）求最大海岛面积和对应海岛的所有坐标。（3）求所有海岛的所有坐标，按海岛分。每小题讲思路，最后写第二题。 java基础 Bean的默认作用范围是什么？其他的作用范围？ 平常用线程主要是怎么写的，会用一些线程框架吗？（没有用框架）Java线程池的概念？线程池有哪些？线程池工厂有哪些线程池类型，及其线程池参数是什么？ 用线程都可以啊，线程池 or 直接 new Thread 线程框架 Executor中的 ThreadPoolExecutor、FutureTask Fork-join Exchanger Semphore CycliBarrier CountDownLatch 线程池的概念：就是会复用一堆线程 四个主要使用的线程池， newFixedThreadPool是一个典型且优秀的线程池，它具有线程池的高效率和节省创建线程时所耗的开销的优点。但是在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。但是注意其只有核心线程池，在线程池中没有额外的线程。使用的是 LinkedBlockingQueue。我不知道为什么还要指定核心线程数和最大线程数一致，这里的 LinkedBlockingQueue 明明就是无界的，也就是说不可能会去使用到核心线程以外的线程。 newCachedThreadPool的特点就是在线程池空闲时，即线程池中没有可运行任务时，它会释放工作线程，从而释放工作线程所占用的资源。但是当出现新任务时，又要创建新的工作线程，这会带来一定的系统开销。并且在使用CachedThreadPool时，一定要注意控制任务的数量，否则大量线程同时运行，可能会造成系统瘫痪。使用的是 SynchronousQueue。因为他是无界的，其实也是空队列，线程数没有限制，任务来了，如果没有空闲线程，直接new 一个线程。corePoolSize设置为0，将maximumPoolSize设置为Integer.MAX_VALUE。 newScheduledThreadPool是周期线程池。支持定时及周期性任务执行。使用的是 DelayBlockingQueue。 newSingleThreadPool是一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。当然如果当前线程死了，会新建一个重新执行任务。将corePoolSize和maximumPoolSize都设置为1，也使用的LinkedBlockingQueue； ConcurrentHashMap讲一讲？ 讲什么…简单的要死的东西哈哈哈哈哈哈哈…好叭这种问题好没意思 Java bin包下面的工具用过哪些？ 同步IO和异步IO的区别？ 线程如何实现同步？ synchronized lock ConcurrentHashMap 在Java7和Java8中的区别？为什么Java8并发效率更好？什么情况下用HashMap，什么情况用ConcurrentHashMap？ 因为锁粒度更加细粒度了，并且使用了轻量级锁、偏向锁。 线程安全使用 ConcurrentHashMap，线程不安全使用 HashMap 加锁有什么机制？ Synchronized lock ThreadLocal？应用场景？ 线程局部变量，应用场景为变量是线程间隔离，线程内全局变量使用，而且还可以做到局部变量做不到的，也就是可以跨线程通信。 Spring 单例bean、hibernate中的session等等都使用了 ThreadLocal。 多线程JVM Java GC机制？GC Roots有哪些？ 消息中间件spring全家桶 微服务注册发现，为什么用微服务，grpc，protobuf，dubbo讲一下 IoC是什么概念？ Web容器用的什么？你项目里涉及了SSM框架，一个请求到Tomcat容器了，怎么到SSM代码中去？ 数据库 事务的四大特性 原子性、隔离性、 四种隔离级别 什么是幻读 InnoDB 怎么防止幻读 B+树原理，为什么使用B+而不是二叉平衡树 1000万个object的文件，有ABC三个date member，怎么根据ABC的条件查询比较快 联合索引 索引是什么概念有什么作用？MySQL里主要有哪些索引结构？哈希索引和B+树索引比较？ MySQL了解到什么程度？binLog知道吗？ 其他 redis分布式锁两种实现 怎么理解微服务 什么是Python生成器，迭代器； 什么是装饰器； 浏览器缓存、CPU的缓存和Redis的缓存的关系，为什么要有缓存 元组和list的区别； Python中的dict底层怎么实现的 list的底层怎么实现的； 双等于和is有什么区别 JSON和protobuf区别 BloomFilter干嘛的，解决了什么问题 REST api和RPC 为什么protobuf效率更高，JSON缺点 1000万条相同class格式object组成的json，怎么压缩 123456789设计题：一个数据库存了以下数据：用户id，登录时间，登出时间；如何找到一天当中的用户峰值（用一个hash map存所有秒数的在线人数）为什么要分用户态和内核态Git 切换分支，提交，具体如何合并分支Python多线程用了几个CPU算法：二叉树，输出所有和为n的路径（可以从中间结点到中间结点） 阿里数据结构操作系统计算机网络算法 红黑树、二叉树的算法 java基础 Servlet规范了解吗？Servlet的整个业务流程？session和cookie的区别？session怎么变成cookie，怎么变回session？谁来实现整个流程？ 刚才提到的分布式实现多个应用的Session共享问题？ 多线程JVM消息中间件spring全家桶 项目提到的SSM框架里面了解哪些？Spring IoC中Bean的生命周期？谁来管理Bean的生命周期？BeanFactory和ApplicationContext的关系？@Autowired和@Resource的区别？ Spring的自动扫描怎么实现的？谁实现的？ MyBatis介绍一下？你说到这是一个持久层框架，那你了解JPA规范吗？ 数据库其他 UML了解吗？（不会）设计模式？ Zookeeper了解吗？ 单例模式、工厂模式、***模式……单例模式有什么需要注意的吗？（多线程并发问题：synchronized+volatile）你写单例的时候如果这个单例对象有字段值，你会去改变这些字段值吗？为什么呢？ 鹅厂数据结构操作系统计算机网络 http三次握手四次挥手 HTTP常见错误码？TCP三次握手？ 算法 海量数据寻找TopK 海量数据排序 16G的文件储存的是一个数组，要求只用1G内存把他们排好序。 排序算法了解哪些？Java里内置的是用什么排序方法？快排是稳定的吗？快排排对象的时候有什么问题？（提示我，三个字段，第一个字段和第二个字段做hash，第三个不做，还是不太懂） 十亿个IP，获得访问次数最多的十个。 五个单词，在一个长文本中查找是否存在。 快速排序算法 java基础 Java中各种锁聊一下，CAS机制。 stl说一下，map原理，时间复杂度 HashMap有了解吗？HashMap的时间复杂度？HashMap中Hash冲突是怎么解决的？链表的上一级结构是什么？Java8中的HashMap有什么变化？红黑树需要比较大小才能进行插入，是依据什么进行比较的？其他Hash冲突解决方式？ 实现线程安全的方式？ThreadLocal原理？线程池了解吗说说看？自己用线程池怎么定参数？ 多线程 volatile有用过么，底层实现，这个问题是讲到了CPU填充缓存行指令。 Java线程池 JVM GC回收器JVM调优参数配置 JJava堆中怎么分区？怎么判断对象是否需要回收？Java内存占用过大怎么查看？Java内存溢出有碰到过吗？Java内存溢出怎么定位？ 消息中间件spring全家桶数据库 数据库的事务隔离级别，mvcc机制聊一下。 数据库索引，B+树结构和特点。怎么进行优化 mysql间隙锁为了解决什么问题，什么条件下会触发。 MySQL数据库引擎？应用场景？查询优化？NoSQL有用或了解吗？ 其他 红黑树，AVL树，查询时间复杂度，什么情况下用红黑树而不用map 进程和线程的区别？平常的开发环境是Windows还是Linux？Linux命令知道哪些？要在多个文本文件中找一个关键词用什么命令？（grep）网络相关的命令是什么？ 度娘数据结构操作系统计算机网络算法java基础多线程JVM消息中间件spring全家桶 springboot和spring比有什么优点 springboot自动配置原理 数据库 高并发数据库大表优化，分库分表，水平怎么拆，垂直怎么拆 知道哪些索引，索引数据结构 联合索引规则 数据库怎么优化，查询性能瓶颈怎么查（explain） 其他美团数据结构操作系统计算机网络算法java基础多线程JVM消息中间件spring全家桶数据库其他拼夕夕数据结构 二叉树的后序遍历，非递归算法。 操作系统计算机网络 TCP三次握手的过程，如果没有第三次握手有什么问题。 算法java基础 如何访问一个文件的字节流呢 多线程 做的主要是Java对吧，讲一下多线程把，用到哪些写一下，写了thread和runnable，然后写了线程池，她问我线程池由哪些组件组成，有哪些线程池，分别怎么使用，以及拒绝策略有哪些。 什么时候多线程会发生死锁，写一个例子吧，然后我写了一个两个线程，两个锁，分别持有一个，请求另一个的死锁实例 集合类熟悉吧，写一个题目，一个字符串集合，找出pdd并且删除，我直接写了一个list然后for循环判断相等时删除，她说明显问题，我才发现list直接删位置会出错，于是我说改用数组，她说不太符合要求，所以应该使用iterator删除会好一点，修改会反映到集合类，并且不会出错。 然后说一下Redis吧，是单线程还是多线程，Redis的分布式怎么做，说了集群。 RPC了解么，我说了主要是协议栈+数据格式+序列化方式，然后需要有服务注册中心管理生产者和消费者，他问我注册中心宕机怎么办，我说可以做高可用，他说要问的不是这个，是想问我注册中心宕机时消费者是否能访问生产者。 线程数很多会怎么样，我说会占内存，还有就是切换线程比较频繁，他问切换线程会发生什么，应该就是CPU切换上下文，具体就是寄存器和内存地址的刷新。 JVM消息中间件spring全家桶数据库 MySQL的主从复制怎么做的，答日志，具体原理是什么，有什么优缺点。 其他 Redis了解哪些内容，是单线程么，为什么是单线程呢，数据一定是存在物理内存中么，我不懂这话啥意思，就问了一下是不是指可能也在虚拟内存中。他说那讲一下虚拟内存的机制把，我就讲了分页，页表，换页这些东西。 分布式了解哪些东西，消息队列了解么，用在什么场景，说了削峰，限流和异步。说了kafka，问我怎么保证数据不丢失，以及确保消息不会被重复消费。还问了消息送达确认是怎么做的。 cap了解么，分别指什么，base呢，强一致性和弱一致性有什么方法来做，2pc了解么，说一下大概过程。 这样一个题目，一个节点要和客户连接建立心跳检测，大概有百万数量的连接，并且会定期发送心跳包，要写一个update方法和check方法，update方法更新心跳状态，check删除超时节点，怎么做，刚开始做了个hash发现check要轮询太慢了，然后用计时器和开线程检测也不行，最后说了个LRU，他说OK的。 华为数据结构操作系统计算机网络算法java基础多线程JVM消息中间件spring全家桶 请求到SSH框架的流程图画一下？远程调用Shell脚本用到哪些命令？ 数据库 数据量多大？项目PG（PostgreSQL）版本？非结构化数据指的是什么？有没有考虑过nosql？分库分表怎么分？查询的表会合并在一起吗？ 项目性能有没有考虑过？（我是做服务端的，主要考虑请求并发量）数据库性能呢？（了解一点，MySQL主从结构） 其他 Nginx原理了解吗？（只用到负载均衡，介绍了一下负载均衡策略 网易数据结构操作系统计算机网络算法 判断树对称 数组顺时针旋转90度 用wait和notify模拟生产者消费者模式 java基础 public protected private区别？final和finally区别？final可以用在方法参数上吗？RuntimeException和非RuntimeException？各举几个例子？比方说文件读写的时候会有什么异常？怎么实现序列化？除了Java原生序列化方法，序列化还有什么格式？ ArrayList和LinkedList区别？ ArrayList和LinkedList是线程安全的吗？为什么说他们不是线程安全的，举实际场景？ 有什么线程安全的List？（CopyOnWriteArrayList）讲一下怎么实现线程安全的？（读时共享，写时复制，加锁机制） 线程A和线程B同时针对一个共享变量进行操作，如何实现线程安全？ 平常怎么创建线程？线程池里闲置线程怎么保活？（不清楚，说了自己的想法）那怎么唤醒？如果用wait()和notify()，对谁加锁？FF synchronized 和 ReentrantLock的区别？平常有用哪些集合类？Concurrent包有用吗？ 多线程JVM JVM内存模型？如果给一个类，里面只有一个main方法，方法里面只有一句System.out.println(“helloworld”)，问运行这个类会在Java内存模型里发生什么？ “helloworld”存储在哪里？ JVM内存模型？每个区是做什么的？垃圾回收机制？ 消息中间件spring全家桶 Spring中Bean生命周期？提到的Aware相关接口指的是什么？平常会自己会用Aware相关接口吗？Bean生命周期这么长是为什么？ 数据库 给一个数据库表，ID、score两个字段分别代表学生ID和成绩，写SQL语句求ID=？的学生排第几名？ 假设是InnoDB，给上述SQL语句加索引怎么加？为什么这样建立？聚集索引和非聚集索引有什么区别？ MySQL数据库，给一个用户表格，ID、用户名、性别、用户信息…，假设经常对性别字段进行查询，问怎么建立索引？为什么？假设用户名需要是唯一的，问怎么建索引？ 添加索引的SQL语句？给一个abc三个字段的索引，where a=0 order by c能用到索引吗？where a=0 and b大于0 order by c能用到索引吗？Hash索引和B树索引的区别？Hash索引有区间查询吗？有没有用nosql？ 其他 介绍一下觉得做得最好的项目？画一下项目的框架结构图？Nginx用来做什么？采用了什么负载均衡策略？万一某一个服务器挂掉怎么办？（一致性hash）如果添加一个节点呢？ 平常用到哪些设计模式？介绍一下模式？模式和装饰器模式区别？ NIO和BIO区别？NIO怎么写？阻塞和非阻塞，同步和异步区别？ CVTE数据结构操作系统计算机网络算法java基础 Java为什么说是面向对象的？ ava的三大特性？如果说有两个方法，同名同参数但不同返回值，问是重载吗？ 平常用到哪些集合类？ArrayList和LinkedList区别？HashMap内部数据结构？ConcurrentHashMap分段锁？ 多线程 Volatile关键词？是线程安全的吗？ 如何实现synchronized一样的效果？ JVM消息中间件spring全家桶 Spring Cloud用到什么东西？如何实现负载均衡？服务挂了注册中心怎么判断 Spring的优点？Spring AOP实现原理？AOP应用场景？拦截器用来做什么业务？ 数据库 MySQL数据库引擎和应用场景 MySQL行锁是否会有死锁的情况？ MySQL事务隔离级别 MySQL平常有索引优化吗？怎么去知道一个SQL语句需不需要优化？一个表，建立了索引（B，A），问where A=1 and B=2索引是否能够生效？ 乐观锁和悲观锁了解吗？JDK中涉及到乐观锁和悲观锁的内容 其他 Nginx负载均衡策略？ ip_hash的优缺点？ Nginx和其他负载均衡框架对比过吗？ Tomcat集群Session共享问题？ MySQL采用了什么存储引擎，为什么？ 分布式锁？]]></content>
      <tags>
        <tag>往年面经汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[滑动窗口技巧汇总]]></title>
    <url>%2F2019%2F12%2F24%2F%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB.html</url>
    <content type="text"><![CDATA[本文详解「滑动窗口」这种高级双指针技巧的算法框架，带你秒杀几道高难度的子字符串匹配问题。 LeetCode 上至少有 9 道题目可以用此方法高效解决。但是有几道是 VIP 题目，有几道题目虽不难但太复杂，所以本文只选择点赞最高，较为经典的，最能够讲明白的三道题来讲解。第一题为了让读者掌握算法模板，篇幅相对长，后两题就基本秒杀了。 本文代码为 C++ 实现，不会用到什么编程方面的奇技淫巧，但是还是简单介绍一下一些用到的数据结构，以免有的读者因为语言的细节问题阻碍对算法思想的理解： unordered_map 就是哈希表（字典），它的一个方法 count(key) 相当于 containsKey(key) 可以判断键 key 是否存在。 可以使用方括号访问键对应的值 map[key]。需要注意的是，如果该 key 不存在，C++ 会自动创建这个 key，并把 map[key] 赋值为 0。 所以代码中多次出现的 map[key]++ 相当于 Java 的 map.put(key, map.getOrDefault(key, 0) + 1)。 本文大部分代码都是图片形式，可以点开放大，更重要的是可以左右滑动方便对比代码。下面进入正题。 一、最小覆盖子串 题目链接 题目不难理解，就是说要在 S(source) 中找到包含 T(target) 中全部字母的一个子串，顺序无所谓，但这个子串一定是所有可能子串中最短的。 如果我们使用暴力解法，代码大概是这样的： 1234for (int i = 0; i &lt; s.size(); i++) for (int j = i + 1; j &lt; s.size(); j++) if s[i:j] 包含 t 的所有字母: 更新答案 思路很直接吧，但是显然，这个算法的复杂度肯定大于 O(N^2) 了，不好。 滑动窗口算法的思路是这样： 1、我们在字符串 S 中使用双指针中的左右指针技巧，初始化 left = right = 0，把索引闭区间 [left, right] 称为一个「窗口」。 2、我们先不断地增加 right 指针扩大窗口 [left, right]，直到窗口中的字符串符合要求（包含了 T 中的所有字符）。 3、此时，我们停止增加 right，转而不断增加 left 指针缩小窗口 [left, right]，直到窗口中的字符串不再符合要求（不包含 T 中的所有字符了）。同时，每次增加 left，我们都要更新一轮结果。 4、重复第 2 和第 3 步，直到 right 到达字符串 S 的尽头。 这个思路其实也不难，第 2 步相当于在寻找一个「可行解」，然后第 3 步在优化这个「可行解」，最终找到最优解。左右指针轮流前进，窗口大小增增减减，窗口不断向右滑动。 下面画图理解一下，needs 和 window 相当于计数器，分别记录 T 中字符出现次数和窗口中的相应字符的出现次数。 初始状态： 0 增加 right，直到窗口 [left, right] 包含了 T 中所有字符： 0 现在开始增加 left，缩小窗口 [left, right]。 0 直到窗口中的字符串不再符合要求，left 不再继续移动。 0 之后重复上述过程，先移动 right，再移动 left…… 直到 right 指针到达字符串 S 的末端，算法结束。 如果你能够理解上述过程，恭喜，你已经完全掌握了滑动窗口算法思想。至于如何具体到问题，如何得出此题的答案，都是编程问题，等会提供一套模板，理解一下就会了。 上述过程可以简单地写出如下伪码框架： 123456789101112131415161718string s, t;// 在 s 中寻找 t 的「最小覆盖子串」int left = 0, right = 0;string res = s;while(right &lt; s.size()) &#123; window.add(s[right]); right++; // 如果符合要求，移动 left 缩小窗口 while (window 符合要求) &#123; // 如果这个窗口的子串更短，则更新 res res = minLen(res, window); window.remove(s[left]); left++; &#125;&#125;return res; 如果上述代码你也能够理解，那么你离解题更近了一步。现在就剩下一个比较棘手的问题：如何判断 window 即子串 s[left…right] 是否符合要求，是否包含 t 的所有字符呢？ 可以用两个哈希表当作计数器解决。用一个哈希表 needs 记录字符串 t 中包含的字符及出现次数，用另一个哈希表 window 记录当前「窗口」中包含的字符及出现的次数，如果 window 包含所有 needs 中的键，且这些键对应的值都大于等于 needs 中的值，那么就可以知道当前「窗口」符合要求了，可以开始移动 left 指针了。 现在将上面的框架继续细化： 123456789101112131415161718192021222324252627282930313233343536373839404142string s, t;// 在 s 中寻找 t 的「最小覆盖子串」int left = 0, right = 0;string res = s;// 相当于两个计数器unordered_map&lt;char, int&gt; window;unordered_map&lt;char, int&gt; needs;for (char c : t) needs[c]++;// 记录 window 中已经有多少字符符合要求了int match = 0; while (right &lt; s.size()) &#123; char c1 = s[right]; if (needs.count(c1)) &#123; window[c1]++; // 加入 window if (window[c1] == needs[c1]) // 字符 c1 的出现次数符合要求了 match++; &#125; right++; // window 中的字符串已符合 needs 的要求了 while (match == needs.size()) &#123; // 更新结果 res res = minLen(res, window); char c2 = s[left]; if (needs.count(c2)) &#123; window[c2]--; // 移出 window if (window[c2] &lt; needs[c2]) // 字符 c2 出现次数不再符合要求 match--; &#125; left++; &#125;&#125;return res; 上述代码已经具备完整的逻辑了，只有一处伪码，即更新 res 的地方，不过这个问题太好解决了，直接看解法吧！ 123456789101112131415161718192021222324252627282930313233343536373839404142string minWindow(string s, string t) &#123; // 记录最短子串的开始位置和长度 int start = 0, minLen = INT_MAX; int left = 0, right = 0; unordered_map&lt;char, int&gt; window; unordered_map&lt;char, int&gt; needs; for (char c : t) needs[c]++; int match = 0; while (right &lt; s.size()) &#123; char c1 = s[right]; if (needs.count(c1)) &#123; window[c1]++; if (window[c1] == needs[c1]) match++; &#125; right++; while (match == needs.size()) &#123; if (right - left &lt; minLen) &#123; // 更新最小子串的位置和长度 start = left; minLen = right - left; &#125; char c2 = s[left]; if (needs.count(c2)) &#123; window[c2]--; if (window[c2] &lt; needs[c2]) match--; &#125; left++; &#125; &#125; return minLen == INT_MAX ? "" : s.substr(start, minLen);&#125; 如果直接甩给你这么一大段代码，我想你的心态是爆炸的，但是通过之前的步步跟进，你是否能够理解这个算法的内在逻辑呢？你是否能清晰看出该算法的结构呢？ 这个算法的时间复杂度是 O(M + N)，M 和 N 分别是字符串 S 和 T 的长度。因为我们先用 for 循环遍历了字符串 T 来初始化 needs，时间 O(N)，之后的两个 while 循环最多执行 2M 次，时间 O(M)。 读者也许认为嵌套的 while 循环复杂度应该是平方级，但是你这样想，while 执行的次数就是双指针 left 和 right 走的总路程，最多是 2M 嘛。 二、找到字符串中所有字母异位词 题目链接 这道题的难度是 Easy，但是评论区点赞最多的一条是这样： 1How can this problem be marked as easy??? 实际上，这个 Easy 是属于了解双指针技巧的人的，只要把上一道题的代码改中更新 res 部分的代码稍加修改就成了这道题的解： 12345678910111213141516171819202122232425262728293031323334353637vector&lt;int&gt; findAnagrams(string s, string t) &#123; // 用数组记录答案 vector&lt;int&gt; res; int left = 0, right = 0; unordered_map&lt;char, int&gt; needs; unordered_map&lt;char, int&gt; window; for (char c : t) needs[c]++; int match = 0; while (right &lt; s.size()) &#123; char c1 = s[right]; if (needs.count(c1)) &#123; window[c1]++; if (window[c1] == needs[c1]) match++; &#125; right++; while (match == needs.size()) &#123; // 如果 window 的大小合适 // 就把起始索引 left 加入结果 if (right - left == t.size()) &#123; res.push_back(left); &#125; char c2 = s[left]; if (needs.count(c2)) &#123; window[c2]--; if (window[c2] &lt; needs[c2]) match--; &#125; left++; &#125; &#125; return res;&#125; 因为这道题和上一道的场景类似，也需要 window 中包含串 t 的所有字符，但上一道题要找长度最短的子串，这道题要找长度相同的子串，也就是「字母异位词」嘛。 三、无重复字符的最长子串 题目链接 遇到子串问题，首先想到的就是滑动窗口技巧。 类似之前的思路，使用 window 作为计数器记录窗口中的字符出现次数，然后先向右移动 right，当 window 中出现重复字符时，开始移动 left 缩小窗口，如此往复： 123456789101112131415161718192021int lengthOfLongestSubstring(string s) &#123; int left = 0, right = 0; unordered_map&lt;char, int&gt; window; int res = 0; // 记录最长长度 while (right &lt; s.size()) &#123; char c1 = s[right]; window[c1]++; right++; // 如果 window 中出现重复字符 // 开始移动 left 缩小窗口 while (window[c1] &gt; 1) &#123; char c2 = s[left]; window[c2]--; left++; &#125; res = max(res, right - left); &#125; return res;&#125; 需要注意的是，因为我们要求的是最长子串，所以需要在每次移动 right 增大窗口时更新 res，而不是像之前的题目在移动 left 缩小窗口时更新 res。 最后总结通过上面三道题，我们可以总结出滑动窗口算法的抽象思想： 12345678910111213int left = 0, right = 0;while (right &lt; s.size()) &#123; window.add(s[right]); right++; while (valid) &#123; window.remove(s[left]); left++; &#125;&#125; 其中 window 的数据类型可以视具体情况而定，比如上述题目都使用哈希表充当计数器，当然你也可以用一个数组实现同样效果，因为我们只处理英文字母。 稍微麻烦的地方就是这个 valid 条件，为了实现这个条件的实时更新，我们可能会写很多代码。比如前两道题，看起来解法篇幅那么长，实际上思想还是很简单，只是大多数代码都在处理这个问题而已。]]></content>
      <tags>
        <tag>双指针</tag>
        <tag>滑动窗口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双指针技巧汇总]]></title>
    <url>%2F2019%2F12%2F24%2F%E5%8F%8C%E6%8C%87%E9%92%88%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB.html</url>
    <content type="text"><![CDATA[我把双指针技巧再分为两类，一类是「快慢指针」，一类是「左右指针」。前者解决主要解决链表中的问题，比如典型的判定链表中是否包含环；后者主要解决数组（或者字符串）中的问题，比如二分查找。 一、快慢指针的常见算法快慢指针一般都初始化指向链表的头结点 head，前进时快指针 fast 在前，慢指针 slow 在后，巧妙解决一些链表中的问题。 1、判定链表中是否含有环 这应该属于链表最基本的操作了，如果读者已经知道这个技巧，可以跳过。 单链表的特点是每个节点只知道下一个节点，所以一个指针的话无法判断链表中是否含有环的。 如果链表中不含环，那么这个指针最终会遇到空指针 null 表示链表到头了，这还好说，可以判断该链表不含环。 12345boolean hasCycle(ListNode head) &#123; while (head != null) head = head.next; return false;&#125; 但是如果链表中含有环，那么这个指针就会陷入死循环，因为环形数组中没有 null 指针作为尾部节点。 经典解法就是用两个指针，一个跑得快，一个跑得慢。如果不含有环，跑得快的那个指针最终会遇到 null，说明链表不含环；如果含有环，快指针最终会超慢指针一圈，和慢指针相遇，说明链表含有环。 123456789101112boolean hasCycle(ListNode head) &#123; ListNode fast, slow; fast = slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; fast = fast.next.next; slow = slow.next; if (fast == slow) return true; &#125; return false;&#125; 2、已知链表中含有环，返回这个环的起始位置 1 这个问题一点都不困难，有点类似脑筋急转弯，先直接看代码： 12345678910111213141516ListNode detectCycle(ListNode head) &#123; ListNode fast, slow; fast = slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; fast = fast.next.next; slow = slow.next; if (fast == slow) break; &#125; // 上面的代码类似 hasCycle 函数 slow = head; while (slow != fast) &#123; fast = fast.next; slow = slow.next; &#125; return slow;&#125; 可以看到，当快慢指针相遇时，让其中任一个指针指向头节点，然后让它俩以相同速度前进，再次相遇时所在的节点位置就是环开始的位置。这是为什么呢？ 第一次相遇时，假设慢指针 slow 走了 k 步，那么快指针 fast 一定走了 2k 步，也就是说比 slow 多走了 k 步（也就是环的长度）。 2 设相遇点距环的起点的距离为 m，那么环的起点距头结点 head 的距离为 k - m，也就是说如果从 head 前进 k - m 步就能到达环起点。 巧的是，如果从相遇点继续前进 k - m 步，也恰好到达环起点。 3 所以，只要我们把快慢指针中的任一个重新指向 head，然后两个指针同速前进，k - m 步后就会相遇，相遇之处就是环的起点了。 3、寻找链表的中点 类似上面的思路，我们还可以让快指针一次前进两步，慢指针一次前进一步，当快指针到达链表尽头时，慢指针就处于链表的中间位置。 123456while (fast != null &amp;&amp; fast.next != null) &#123; fast = fast.next.next; slow = slow.next;&#125;// slow 就在中间位置return slow; 当链表的长度是奇数时，slow 恰巧停在中点位置；如果长度是偶数，slow 最终的位置是中间偏右： center 寻找链表中点的一个重要作用是对链表进行归并排序。 回想数组的归并排序：求中点索引递归地把数组二分，最后合并两个有序数组。对于链表，合并两个有序链表是很简单的，难点就在于二分。 但是现在你学会了找到链表的中点，就能实现链表的二分了。关于归并排序的具体内容本文就不具体展开了。 4、寻找链表的倒数第 k 个元素 我们的思路还是使用快慢指针，让快指针先走 k 步，然后快慢指针开始同速前进。这样当快指针走到链表末尾 null 时，慢指针所在的位置就是倒数第 k 个链表节点（为了简化，假设 k 不会超过链表长度）： 1234567891011ListNode slow, fast;slow = fast = head;while (k-- &gt; 0) fast = fast.next;while (fast != null) &#123; slow = slow.next; fast = fast.next;&#125;return slow; 二、左右指针的常用算法左右指针在数组中实际是指两个索引值，一般初始化为 left = 0, right = nums.length - 1 。 1、二分查找 前文「二分查找」有详细讲解，这里只写最简单的二分算法，旨在突出它的双指针特性： 1234567891011121314int binarySearch(int[] nums, int target) &#123; int left = 0; int right = nums.length - 1; while(left &lt;= right) &#123; int mid = (right + left) / 2; if(nums[mid] == target) return mid; else if (nums[mid] &lt; target) left = mid + 1; else if (nums[mid] &gt; target) right = mid - 1; &#125; return -1;&#125; 2、两数之和 直接看一道 LeetCode 题目吧： title 只要数组有序，就应该想到双指针技巧。这道题的解法有点类似二分查找，通过调节 left 和 right 可以调整 sum 的大小： 123456789101112131415int[] twoSum(int[] nums, int target) &#123; int left = 0, right = nums.length - 1; while (left &lt; right) &#123; int sum = nums[left] + nums[right]; if (sum == target) &#123; // 题目要求的索引是从 1 开始的 return new int[]&#123;left + 1, right + 1&#125;; &#125; else if (sum &lt; target) &#123; left++; // 让 sum 大一点 &#125; else if (sum &gt; target) &#123; right--; // 让 sum 小一点 &#125; &#125; return new int[]&#123;-1, -1&#125;;&#125; 3、反转数组 1234567891011void reverse(int[] nums) &#123; int left = 0; int right = nums.length - 1; while (left &lt; right) &#123; // swap(nums[left], nums[right]) int temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; left++; right--; &#125;&#125; 4、滑动窗口算法 这也许是双指针技巧的最高境界了，如果掌握了此算法，可以解决一大类子字符串匹配的问题，不过「滑动窗口」稍微比上述的这些算法复杂些。 幸运的是，这类算法是有框架模板的，下篇文章就准备讲解「滑动窗口」算法模板，帮大家秒杀几道 LeetCode 子串匹配的问题。]]></content>
      <tags>
        <tag>双指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[labuladong公众号笔记]]></title>
    <url>%2F2019%2F12%2F22%2Flabuladong%E5%85%AC%E4%BC%97%E5%8F%B7%E7%AC%94%E8%AE%B0(dp%26%26%E4%BA%8C%E5%88%86).html</url>
    <content type="text"><![CDATA[动态规划：不同的定义产生不同的解法文章链接键盘组合问题 收获对状态机的定义不同，得到的状态转移方程就完全不同 经典面试题：最长回文子串（LeetCode第5题）文章链接经典面试题：最长回文子串 收获回文子串有两个做法，一个是从中间展开的算法，一个是两侧向中间靠齐的dp 单调栈文章链接单调栈 Monotonic Stack 的使用 收获 单调栈算法过程 处理Next Greater Number，从后向前遍历 先判断 栈是否为空 和 栈顶的值是不是比即将入栈的值小 如果栈空，则说明找不到比该数大的数，将其Next Greater Number置为-1 栈不为空，直到找到比该数大的数，将其置为栈顶，比其小的也就没有存在的意义了 然后将该数的Next Greater Number定为栈顶元素，并将该数加入栈中 处理循环数组 将该数组的双倍长度构造出来，这样的话就就不仅能比较右边的数，还能比较左边的数了 二分查找算法详解文章链接二分查找算法详解 二分查找的框架 最基本的框架1（形式是[a,b)） 123456789101112131415int binarySearch(int nums[],int target)&#123; int left = 0; int right = nums.length; while(left &lt; right)&#123; //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) return mid; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid; &#125; return -1;&#125; 最基本的框架2 (形式是[a,b]) 123456789101112131415int binarySearch(int nums[],int target)&#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) return mid; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid - 1; &#125; return -1;&#125; 寻找左侧边界的二分搜索 12345678910111213141516171819202122232425/** * 返回的是比target小的数量，同时也是最左target的下标 * @param nums * @param target * @return */int binarySearchLeft(int nums[],int target)&#123; int left = 0; int right = nums.length; while(left &lt; right)&#123; //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) right = mid; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid; &#125; //要时刻注意处理边界，如果整个数组都没有这个target，则left会到nums.length // target 比所有数都大 if (left == nums.length) return -1; // 类似之前算法的处理方式 return nums[left] == target ? left : -1;&#125; 寻找右侧边界的二分搜索 1234567891011121314151617181920212223242526/** * 返回的是最右target的下标 * @param nums * @param target * @return */int binarySearchRight(int nums[],int target)&#123; int left = 0; int right = nums.length; while(left &lt; right)&#123; //防止计算mid时溢出 int mid = left + (right - left) / 2; if(nums[mid] == target) left = mid + 1; else if(nums[mid] &lt; target) left = mid + 1; else if(nums[mid] &gt; target) right = mid; &#125; //要时刻注意处理边界，如果整个数组都没有这个target，则left会到nums.length // target 比所有数都大 // return left - 1; if (left == 0) return -1; // 类似之前算法的处理方式 return nums[left-1] == target ? (left-1) : -1;&#125; 收获 左侧边界的二分搜索，left即代表比target小的数的数量 右侧边界的二分搜索，left即代表比target大的第一个数的下标，比如是5，则代表索引为4是target的最右侧数 最长递归子序列(LeetCode第300题)文章链接动态规划之最长递归子序列 两个方法： dp 扑克牌算法，运用二分查找 dp注意事项最重要的是先确定状态转移方程，并且明确dp数组的含义，这里要求的是最长递归子序列，所建立的dp数组是一个一维数组(为何选取的是一维数组呢？)，并且dp[i]的含义是指以索引i结尾的最长递归子序列。 采用归纳法可知，若dp[0…i-1]可知，则 1234for(int j = 0; j &lt; i;j++)&#123; if(nums[j] &lt; nums[i]) dp[i] = Math.max(dp[i],dp[j]+1);&#125; 即dp[i]是之前的dp[0…i-1]，判断其是否小于nums[i]，若小于则nums[i]可以接到其后面，最后选取一个最长的递归子序列即可。 还有要考虑的就是base case，由于是最短递归子序列至少为1，所以base case为1，所以dp数组全部初始化为1即可。 代码123456789101112131415public int lengthOfLtsByDp(int nums[])&#123; int[] dp = new int[nums.length]; Arrays.fill(dp,1); for(int i = 0;i &lt; nums.length;i++)&#123; for(int j = 0; j &lt; i;j++)&#123; if(nums[j] &lt; nums[i]) dp[i] = Math.max(dp[i],dp[j]+1); &#125; &#125; int res = 0; for(int i = 0; i &lt; dp.length; i++)&#123; res = Math.max(res,dp[i]); &#125; return res;&#125; 扑克牌算法最长递归子序列其实类似于我们平时玩的蜘蛛纸牌，eg：2，3，1，9，7，4，6，3 ，同一堆的牌只能是小的放在大的上面，如上，就分成2，1 3 9，7，6，3 4 一共4堆，则最长递归子序列数为 4 。 至于如何处理扑克牌放置问题，则可以采用二分查找，因为堆顶都是有序的。 12345678910111213141516171819202122232425public int lengthOfLtsByBinarySearch(int[] nums)&#123; //堆的个数 int piles = 0; //记录每个堆的最上面的牌，保证有序，所以必须利用最左侧边界的二分法 int[] top = new int[nums.length]; for(int i = 0;i &lt; nums.length;i++)&#123; int poker = nums[i]; int left = 0; int right = piles; while(left &lt; right)&#123; int mid = left + (right - left) / 2; if(poker &lt; top[mid]) right = mid; else if(poker &gt; top[mid]) left = mid + 1; else right = mid; &#125; //left的取值有[0,nums.length],当left为nums.length时，这里是piles，说明牌堆上的第一张牌全部比它小，则说明此时需要新建一个牌堆 if (left == piles) piles++; top[left] = poker; &#125; return piles;&#125; ​ 子序列解题模板文章链接子序列解题模板 收获两种思路 dp数组为一维数组。常见题型有 最长递增子序列。在子数组array[0..i]中，以array[i]结尾的目标子序列（最长递增子序列）的长度是dp[i]。 模板： 12345678int n = array.length;int[] dp = new int[n];for (int i = 1; i &lt; n; i++) &#123; for (int j = 0; j &lt; i; j++) &#123; dp[i] = 最值(dp[i], dp[j] + ...) &#125;&#125; dp数组为二维数组。二维数组又分为是对两字符串而言和一个字符串而言。 模板： 1234567891011int n = arr.length;int[][] dp = new dp[n][n];for (int i = 0; i &lt; n; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; if (arr[i] == arr[j]) dp[i][j] = dp[i][j] + ... else dp[i][j] = 最值(...) &#125;&#125; 对一个字符串使用的dp为二维数组。常见题型有之前做过的 最长回文子串，使用双指针，从两边向中间靠拢，二维分别指的是左侧指针和右侧指针的索引值。可以推广到最长回文子序列。 对两个字符串使用的dp为二维数组。例如 编辑距离 和 最长公共子序列 回文子序列的求解(LeetCode第516题)回文子序列的求解，是在回文子串的基础上进行的，dp[i][j] 的长度取决于s[i]和s[j]以及dp[i+1][j-1] 当s[i] == s[j]时，dp[i][j] = dp[i+1][j-1] + 2 当s[i] != s[j]时， dp[i][j] = Math.max(dp[i][j-1]，dp[i+1][j]) 当转移方程写好后，最大的难点就是如何遍历dp，建议采用表的形式，从方程中可以看到，我们必须得到左边、下边、左下，三个dp表达式才能得到下一个dp表达式，故可以采用斜着遍历的方式和逆循环从下往上遍历即可。 123456789101112131415161718192021222324/** * 斜着遍历的dp * @param nums * @return */ private static int dp(int[] nums)&#123; int[][] dp = new int[nums.length][nums.length];// Arrays.fill(dp, 0); int j = 0; for(int i = 0;i &lt; nums.length;i++)&#123; dp[i][i] = 1; &#125; //斜着循环遍历 for(int n = 1;n &lt; nums.length;n++)&#123; for(int i = 0; i &lt; nums.length - n;i++)&#123; j = i + n; if(nums[i] == nums[j]) dp[i][j] = dp[i+1][j-1] + 2; else dp[i][j] = Math.max(dp[i][j-1],dp[i+1][j]); &#125; &#125; return dp[0][nums.length-1]; &#125; 12345678910111213141516171819202122232425 /** * 逆循环遍历的dp * @param nums * @return */ private static int dpByInverse(int[] nums)&#123; int[][] dp = new int[nums.length][nums.length];// Arrays.fill(dp, 0);// int j = 0; int n = nums.length; for(int i = 0;i &lt; nums.length;i++)&#123; dp[i][i] = 1; &#125; // 反着遍历保证正确的状态转移 for (int i = n - 1; i &gt;= 0; i--) &#123; for (int j = i + 1; j &lt; n; j++) &#123; // 状态转移方程 if (nums[i] == nums[j]) dp[i][j] = dp[i + 1][j - 1] + 2; else dp[i][j] = Math.max(dp[i + 1][j], dp[i][j - 1]); &#125; &#125; return dp[0][nums.length-1]; &#125; 编辑距离（LeetCode第72题）先总结一哈：从后往前，然后写递归函数，然后画DP Table，写非递归，注意Base case！ 文章链接编辑距离 题目给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符删除一个字符替换一个字符示例 1: 123456输入: word1 = "horse", word2 = "ros"输出: 3解释: horse -&gt; rorse (将 'h' 替换为 'r')rorse -&gt; rose (删除 'r')rose -&gt; ros (删除 'e') 示例 2: 12345678输入: word1 = "intention", word2 = "execution"输出: 5解释: intention -&gt; inention (删除 't')inention -&gt; enention (将 'i' 替换为 'e')enention -&gt; exention (将 'n' 替换为 'x')exention -&gt; exection (将 'n' 替换为 'c')exection -&gt; execution (插入 'u') 思路个人觉得公众号说的有的啰嗦哈~ 我就稍微总结一下我的想法： 拿到题目，又是两个字符串之间的问题，毫无疑问第一想法想到使用双指针的dp，因为一般遇到两个字符串的问题都是采用dp二维数组解决(双指针)； dp解决问题，首先就要搞清楚dp数组代表什么，其次就是状态转移有哪几种情况！ 一般dp二维数组 dp[i][j] ,代表以s1[i-1]、s2[j-1]结尾的转换的最少操作数，为啥是i-1,j-1！因为dp是从dp[1][1]开始的，代表字符串的第一个字符！！ 搞明白了dp数组是啥含义，现在就要搞清楚有哪几种状态转移，从公众号中可以看到，dp[i][j]想要往前移动，则可以进行删除、修改、插入操作，如果双指针指向的字符相同，则无需进行任何操作，直接向前移动就行，也就是此时 dp[i][j] = dp[i-1][j-1] ; 剩下的插入、删除、修改如何做呢…取三者最小值就好了嘛！！！ 其实这个画个DP表就非常的清晰了，画DP表也能提醒我们注意Base Case,因为没有Base Case ，我们是肯定写不出来DP表中的值的； 有个注意的点，就是备忘录做法是从0开始的，所以当i或者j = -1时，此时的i、j代表的是数组的下标，是从0开始的，比如说此时j = 5的，说明还有6个字符需要删除，所以要记得+1 ，而dp是从dp[1][1]开始的(比如两个字符串’’abc””acd”，dp[1][1]指的是都指向a)！！！！这个很容易错！ 三种做法 常规暴力递归(超时) 123456789101112131415161718192021class Solution &#123; public int minDistance(String word1, String word2) &#123; int len = dp_violence(word1,word2,word1.length()-1,word2.length()-1); return len; &#125; private int dp_violence(String s1,String s2,int i, int j) &#123; if(i == -1) return j + 1; if(j == -1) return i + 1; if(s1.charAt(i) == s2.charAt(j))&#123; return dp_violence(s1,s2,i-1,j-1); &#125; else&#123; int min = Math.min(dp_violence(s1,s2,i-1,j-1)+1, dp_violence(s1,s2,i-1,j)+1); min = Math.min( dp_violence(s1,s2,i,j-1)+1,min); return min; &#125; &#125;&#125; 带备忘录的递归(这个貌似比dp更优秀) 1234567891011121314151617181920212223242526272829class Solution &#123; public int minDistance(String word1, String word2) &#123; int[][] beiwanglu = new int[word1.length()][word2.length()]; int len = dp_beiwanglu(beiwanglu,word1,word2,word1.length()-1,word2.length()-1); return len; &#125; private int dp_beiwanglu(int[][] beiwanglu,String s1, String s2, int i, int j) &#123; if(i == -1) return j + 1; if(j == -1) return i + 1; if(beiwanglu[i][j] != 0)&#123; return beiwanglu[i][j]; &#125; if(s1.charAt(i) == s2.charAt(j))&#123; beiwanglu[i][j] = dp_beiwanglu(beiwanglu,s1,s2,i-1,j-1); return dp_beiwanglu(beiwanglu,s1,s2,i-1,j-1); &#125; else&#123; int min = Math.min(dp_beiwanglu(beiwanglu,s1,s2,i-1,j-1)+1, dp_beiwanglu(beiwanglu,s1,s2,i-1,j)+1); min = Math.min( dp_beiwanglu(beiwanglu,s1,s2,i,j-1)+1,min); beiwanglu[i][j] = min; return min; &#125; &#125;&#125; dp 1234567891011121314151617181920212223class Solution &#123; public int minDistance(String s1, String s2) &#123; int[][] dp = new int[s1.length()+1][s2.length()+1]; for(int i = 0;i&lt;= s1.length();i++)&#123; dp[i][0] = i; &#125; for(int j = 0;j&lt;=s2.length();j++)&#123; dp[0][j] = j; &#125; for(int i = 1;i&lt;= s1.length();i++)&#123; for(int j = 1;j &lt;= s2.length();j++)&#123; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1]; &#125; else&#123; dp[i][j] = Math.min(dp[i-1][j-1] + 1,dp[i-1][j] + 1); dp[i][j] = Math.min(dp[i][j],dp[i][j-1] + 1); &#125; &#125; &#125; return dp[s1.length()][s2.length()]; &#125;&#125; 最长公共子序列问题（LeetCode第1143题）典型的dp二维数组解决，太简单了… 第一个要注意的就是该dp可以优化，当双指针指向的字符不等时，dp[i-1][j-1]可以不用考虑！！！！因为max(dp[i][j-1],dp[i-1][j],dp[i-1][j-1])是一定轮不到最后这个的； 第二个要注意的点就是记得Base Case,记得要画DP表！ 同样要注意这里的第一个字符串对应的就是dp[1][1]！！！！！ 代码1234567891011121314151617class Solution &#123; public int longestCommonSubsequence(String text1, String text2) &#123; int[][] dp = new int[text1.length() + 1][text2.length() + 1]; for(int i = 1;i &lt;= text1.length();i++)&#123; for(int j = 1;j &lt;=text2.length();j++)&#123; if(text1.charAt(i-1) == text2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1] + 1; &#125; else&#123; dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]); &#125; &#125; &#125; return dp[text1.length()][text2.length()]; &#125;&#125; 滑动窗口算法(示例:LeetCode第1143：最小覆盖子串问题) Tip：最重要的是过最后一个用例的艰辛，因为需要考虑到Integer是一个对象，超过-127~128之后会new一个新的对象，导致就算是值相等，其也会显示不相等！！！！！！！ 文章链接滑动窗口算法 代码文章中使用C++实现的，我是在LeetCode用的Java实现的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Solution &#123; public String minWindow(String s, String t) &#123; // 利用双指针的思想，先记录下t会出现的单词以及次数 // 然后右指针开始遍历，每当一个单词及其次数到位后，当前单词就匹配完成，直到所有单词都匹配完成 // 然后记录下当前的子串的开始和结尾，即左右指针的位置 // 然后开始试图移动左指针，当然前提是t中所有字符然后全部匹配完成，否则就跳出循环，继续走右边指针 // 直到右边指针走到底，如果走到底长度依旧没有，则说明找不到最小子串，返回"" 否则返回适当的子串 int left = 0,right = 0; //计算匹配的单词数量 int match = 0; //统计目标字符串的不同字符的个数 int nums = 0; int min_length = Integer.MAX_VALUE; int start = 0; char[] t_char = t.toCharArray(); char[] s_char = s.toCharArray(); HashMap&lt;Character, Integer&gt; windows = new HashMap&lt;Character, Integer&gt;(); HashMap&lt;Character,Integer&gt; tt = new HashMap&lt;Character, Integer&gt;(); // 用两个map来存储两个字符串，存储方式为key为单词，value为单词出现的数量 // for (char t1: t_char)&#123; if(tt.containsKey(t1)) &#123; int value = tt.get(t1).intValue(); tt.put(t1, ++value); &#125; else&#123; nums++; tt.put(t1,1); &#125; &#125; while(right &lt; s.length())&#123; if(windows.containsKey(s_char[right]))&#123; int windows_value = windows.get(s_char[right]).intValue(); windows.put(s_char[right],++windows_value); &#125; else&#123; windows.put(s_char[right],1); &#125; //如果全部符合要求 if(tt.containsKey(s_char[right]) &amp;&amp; windows.get(s_char[right]).intValue() == tt.get(s_char[right]).intValue())&#123; match++; &#125; right++; while(match == nums)&#123; //说明已经找到一个符合条件的了，记录下该字符串的起始和结束位置 int Len = right - left; if(Len &lt; min_length) &#123; min_length = Math.min(min_length,Len); start = left; &#125; int left_value = windows.get(s_char[left]).intValue(); left_value--; windows.put(s_char[left],left_value); if(tt.containsKey(s_char[left]) &amp;&amp; windows.get(s_char[left]).intValue() &lt; tt.get(s_char[left]).intValue())&#123; match--; &#125; left++; &#125; &#125; return min_length == Integer.MAX_VALUE ? "" : s.substring(start,min_length +start); &#125;&#125; 最大子序列和(LeetCode第53题)先总结一下，可以运用 dp 和 滑动窗口 两种方法！！！ 题目给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 示例: 123输入: [-2,1,-3,4,-1,2,1,-5,4],输出: 6解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 思路 dp 要的是子串，必然可以用dp，而且是单子串且非回文，则必然使用一维dp数组 只要定义好了dp数组，这道题就没有任何难度，dp[i]定义为以i为索引的字符结尾的最大和 则dp[i] = Math.max(dp[i-1],nums[i])，然后取dp[i]中最大的即可！ 这题只有一个小难点，那就是找Base Case,好像并不存在Base Case,但是我们可以创造Base Case,可以令dp[0] = nums[0]，这样就有Base Case了，同时注意哈，这里不需要将dp数组的大小变为nums.length+1,因为这个题目不存在Base Case,所以dp只会产生n个数，不需要n+1个存储空间，像上文的dp二维数组，就存在BaseCase，当有一个为0时，有相应的取值！总而言之，一定要特别注意dp的数组大小以及BaseCase。 滑动窗口 比较简单的思路。就是sum如果大于0了，就移动右指针就行，如果是小于0了，就比较sum和num[i]，取最大值(相当于移动左指针)，直到循环结束。 代码 dp 12345678910111213class Solution &#123; public int maxSubArray(int[] nums) &#123; //res[i] = Math.max(res[i-1] + nums[i],nums[i]) int[] res = new int[nums.length]; res[0] = nums[0]; int max = res[0]; for(int i = 1;i &lt; nums.length;i++)&#123; res[i] = Math.max(res[i-1] + nums[i],nums[i]); max = Math.max(res[i],max); &#125; return max; &#125;&#125; 滑动窗口 1234567891011121314class Solution &#123; public int maxSubArray(int[] nums) &#123; int res = nums[0]; int sum = 0; for(int num:nums)&#123; if(sum &gt; 0) sum += num; else sum = num; res = Math.max(sum,res); &#125; return res; &#125;&#125; 打家劫舍 I、II、III (LeetCode第198、213、337)文章链接打家劫舍系列 I 的代码 自己写的(感觉写的有点啰嗦~) 123456789101112131415161718192021222324252627class Solution &#123; public int rob(int[] nums) &#123; int[] dp = new int[nums.length]; int res = 0; if(nums.length == 0 )&#123; return 0; &#125; if(nums.length == 1)&#123; dp[0] = nums[0]; res = dp[0]; return res; &#125; if(nums.length == 2)&#123; dp[1] = Math.max(nums[1],nums[0]); res = dp[1]; return res; &#125; dp[0] = nums[0]; dp[1] = Math.max(nums[1],nums[0]); for(int i = 2;i &lt; nums.length;i++)&#123; dp[i] = Math.max(dp[i-1],dp[i-2]+nums[i]); &#125; res = Math.max(dp[nums.length-1],res) return res; &#125;&#125; 高分题解 123456789101112class Solution &#123; public int rob(int[] nums) &#123; int n = nums.length; if (n &lt;= 1) return n == 0 ? 0 : nums[0]; int[] memo = new int[n]; memo[0] = nums[0]; memo[1] = Math.max(nums[0], nums[1]); for (int i = 2; i &lt; n; i++) memo[i] = Math.max(memo[i - 1], nums[i] + memo[i - 2]); return memo[n - 1]; &#125;&#125; II 的代码分两次dp就好了，一次[0,nums.length-2]，一次[1,nums.length-1]，取最大值就好~ 12345678910111213141516171819202122232425262728293031class Solution &#123; public int rob(int[] nums) &#123; int[] dp = new int[nums.length]; int res = 0; if(nums.length == 0 )&#123; return 0; &#125; if(nums.length == 1)&#123; dp[0] = nums[0]; res = dp[0]; return res; &#125; if(nums.length == 2)&#123; dp[1] = Math.max(nums[1],nums[0]); res = dp[1]; return res; &#125; dp[0] = nums[0]; dp[1] = Math.max(nums[1],nums[0]); for(int i = 2;i &lt; nums.length-1;i++)&#123; dp[i] = Math.max(dp[i-1],dp[i-2]+nums[i]); &#125; dp[1] = nums[1]; dp[2] = Math.max(nums[1],nums[2]); for(int i = 3;i &lt; nums.length;i++)&#123; dp[i] = Math.max(dp[i-1],dp[i-2]+nums[i]); &#125; res = Math.max(dp[nums.length-1],dp[nums.length-2]); return res; &#125;&#125; III 的代码这是一个树形的dp问题，解决方法是采用一个一维数组，之前的I II只用到了dp[nums.length-1]，这里用到了两个,而且这里的dp竟然用了递归哟… Tip:特殊做法记忆记忆记忆！！！ 12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public int rob(TreeNode root) &#123; int[] res = robIn(root); return Math.max(res[0],res[1]); &#125; private static int[] robIn(TreeNode root)&#123; //用一个数组记录偷根节点和不偷根节点两种情况，这里很特殊用了dp同时还搭配了递归 //这就是自底向上的递归，详情可以见LeetCode递归专题 int[] res = new int[2]; if(root == null)&#123; return res; &#125; int[] left = robIn(root.left); int[] right = robIn(root.right); //偷根节点 res[0] = root.val + left[1] + right[1]; //不偷根节点 res[1] = Math.max(left[0],left[1]) + Math.max(right[0],right[1]); return res; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>单调栈</tag>
        <tag>子序列问题</tag>
        <tag>LeetCode</tag>
        <tag>dp</tag>
        <tag>二分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringCloud 微服务笔记]]></title>
    <url>%2F2019%2F12%2F13%2FSpringCloud%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[微服务架构概述单体架构一个归档包(war格式)包含所有功能的应用程序，通常称为单体应用。 存在的问题： 复杂性高 技术债务(不坏不修) 部署频率低 可靠性差 扩展能力受限，不同模块可能需要的配置不一样，有的需要CPU好一点的，有的需要存储大一点的，不能根据业务模块的需要进行伸缩 阻碍技术创新，单体应用往往使用统一的技术平台或者方案解决所有的问题 微服务微服务架构风格：将一个单一应用程序开发为一组小型服务的方法，每个服务运行在自己的进程中，服务间通信采用轻量级通信机制，可通过全自动部署机制独立部署。这些服务共用一个最小型的集中式的管理，不同服务可以采用不同的语言开发，可以使用不同的数据存储技术。 特性 每个微服务可以独立运行在自己的进程中 每个服务为独立的业务开发 微服务通过类似于RESTful API等一些轻量级的通信机制进行通信 全自动的部署机制 优点 易于开发和维护 启动速度快 局部修改易部署 技术栈并不受限 按需伸缩 缺点 运维要求高，因为会有几十甚至上百个服务协作运行 分布式固有的复杂性 接口调整成本高 重复劳动 微服务设计原则 单一职责原则(SOLID原则之一) 服务自治原则。即每个微服务应具备独立的业务能力、依赖与运行环境。 轻量级通信机制。常用的有REST、AMQP、STOMP、MQTT等。 微服务粒度。这个比较难以控制，一般通过领域驱动设计(Domain Driven Design，DDD)中的限定上下文可作为划分微服务边界、确定微服务粒度。 微服务架构技术选型主流的解决方案是 SpringCloud、Dubbo 具体的框架对比：Dubbo VS SpringCloud 微服务开发框架——Spring CloudSpring Cloud 简介 并不是云计算解决方案，是在Spring Boot基础上用于快速构建分布式系统的通用模式的工具集 使用Spring Cloud开发的程序非常适合在Docker或者PaaS上部署,所以又叫云原生应用，云原生可理解为面向云环境的软件架构 云原生架构的方法论和最佳实践：12-factor Apps Tips： 云服务分为三类，分别是 IaaS(基础设施服务)、PaaS(平台服务)、SaaS(软件服务) 传送门：IaaS，PaaS，SaaS 的区别 Spring Cloud 和 Spring Boot 的联系首先，知乎上对这个有比较好的回答：Spring Cloud &amp; Spring Boot 总结一下说的比较好的： spring Cloud是一个基于Spring Boot实现的云应用开发工具，它为基于JVM的云应用开发中的配置管理、服务发现、断路器、智能路由、微代理、控制总线、全局锁、决策竞选、分布式会话和集群状态管理等操作提供了一种简单的开发方式。 spring boot 的优点是可以快速启动，快速构建应用程序，而不需要太多的配置文件。 spring cloud 是分布式开发的解决方案，基于spring boot,在spring boot做较少的配置，便可成为 spring cloud 中的一个微服务。 最基础的都是spring，然后在这个基础上spring boot做了一些自动配置和实现，然后又在spring boot的基础上加入了分布式负载均衡等功能，这才有了spring cloud。 Spring cloud是微服务开发套件。springboot=spring+springmvc，springcloud=springboot+ribbon+注册中心+熔断器+…等一系列组件。 Spring Cloud 特点 约定优于配置 隐藏了组件复杂性，并且提供声明式的配置方式 轻量级的组件，如Eureka、Zuul等 解耦，灵活 Spring Cloud 版本以SRX形式命名版本号，版本号前面会有一个 release train。 从以往的版本有Dalston、Edgware、Finchley、Greenwich，正在孵化的有SpringCloud Alibaba。 Spring Cloud 实战微服务(Finchley版本)概述Spring Cloud支持的插件众多，先来看一下Spring Cloud中文网提供的比较优秀的插件： Spring Cloud集成的相关优质项目推荐 Eureka：注册中心，云端服务发现，实现云端中间层服务发现和故障转移 Ribbon：负载均衡，可以有多种负载均衡的策略供选择，可配合Eureka和Hystrix使用 Hystrix：熔断器，容错管理工具，旨在通过熔断机制控制服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力 Turbine：Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下hystrix的metrics情况（可视化） Feign：声明式、模块化的HTTP客户端，集成了负载均衡 Zuul：云平台上提供动态路由，监控，弹性，安全等边缘服务的框架。Zuul 相当于是设备和 Netflix 流应用的 Web 网站后端所有请求的前门 Config：配置管理工具包，让你可以把配置放到远程服务器，集中化管理集群配置，目前支持本地存储、Git以及Subversion Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与Spring Cloud Config联合实现热部署 Sleuth：日志收集工具包，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，为SpringCloud应用实现了一种分布式追踪解决方案 Stream：数据流操作开发包，封装了与Redis,Rabbit、Kafka等发送接收消息 Security：基于spring security的安全工具包，为你的应用程序添加安全控制 实例使用服务提供者与服务消费者来描述微服务之间的调用关系。下表解释了服务提供者与服务消费者。 ​ 表-服务提供者与服务消费者 名词 定义 服务提供者 服务的被调用方（即：为其他服务提供服务的服务） 服务消费者 服务的调用方（即：依赖其他服务的服务） 以电影售票系统为例。如图，用户向电影微服务发起了一个购票的请求。在进行购票的业务操作前，电影微服务需要调用用户微服务的接口，查询当前用户的余额是多少、是不是符合购票标准等。在这种场景下，用户微服务就是一个服务提供者，电影微服务则是一个服务消费者。 围绕该场景，先来编写一个用户微服务，然后编写一个电影微服务。 Eureka注册中心，云端服务发现，实现云端中间层服务发现和故障转移 不妨先思考一下，怎样才能让服务消费者总能找到服务提供者呢？或者说，怎样才能让服务消费者感知到服务提供者地址的变化呢？ TIPS 目前市面上把服务消费者找到服务提供者的这种机制称为服务发现，又或者服务注册。下面来探索服务发现究竟是怎么回事。 服务发现原理初探其实，服务发现机制非常简单，不妨用大家熟悉的MySQL来类比——只需一张表（图中的registry表）即可实现服务发现！ 如图，如果我们能在： 应用启动时，自动往registry表中插入一条数据，数据包括服务名称、IP、端口等信息。 应用停止时，自动把自己在registry表中的数据的status设为DOWN 。 这样，服务消费者不就永远都能找到服务提供者了嘛！当服务消费者想调用服务提供者接口时，只需向数据库发送SQL语句 SELECT * FROM registry where service_name = &#39;user&#39; and status = &#39;UP&#39; 即可找到服务提供者的所有实例！IP、端口啥的都有了，自己拼接一下，再去调用就行了！ TIPS 看，服务发现机制是不是很简单？程序猿给图中的”MySQL“的组件起了一个牛叉的名字叫：”注册中心“，也有的书将其称为”服务发现组件“。 但，这毕竟只是一个最简陋的服务发现原理。完整的服务发现要考虑的问题有很多，例如： 当服务抑或所在主机突然崩溃或者进入某种不正常的情况无法提供服务（例如应用的数据库挂了）时，对应的数据理应标记DOWN，或者索性删除； 如果每次调用之前，都得向服务发现组件发送类似SELECT * FROM registry where service_name = &#39;user&#39; and status = &#39;UP&#39; 的语句，那么服务发现组件的压力得有多大？更重要的，这与当下流行的去中心化设计的思想相悖； 服务发现组件即使挂掉，也不应该影响微服务之间的调用。 那么，一个完善的服务发现组件应该具备哪些能力呢？ 服务发现原理深入不妨来看一下使用服务发现组件后的架构图，如图所示。 服务提供者、服务消费者、服务发现组件这三者之间的关系大致如下： 各个微服务在启动时，将自己的网络地址等信息注册到服务发现组件中，服务发现组件会存储这些信息； 服务消费者可从服务发现组件查询服务提供者的网络地址，并使用该地址调用服务提供者的接口； 各个微服务与服务发现组件使用一定机制（例如心跳）通信。服务发现组件如长时间无法与某微服务实例通信，就会自动注销（即：删除）该实例； 当微服务网络地址发生变更（例如实例增减或者IP端口发生变化等）时，会重新注册到服务发现组件； 客户端缓存：各个微服务将需要调用服务的地址缓存在本地，并使用一定机制更新（例如定时任务更新、事件推送更新等）。这样既能降低服务发现组件的压力，同时，即使服务发现组件出问题，也不会影响到服务之间的调用。 综上，服务发现组件应具备以下功能。 服务注册表：服务注册表是服务发现组件的核心（其实就是类似于上面的registry表），它用来记录各个微服务的信息，例如微服务的名称、IP、端口等。服务注册表提供查询API和管理API，查询API用于查询可用的微服务实例，管理API用于服务的注册和注销； 服务注册与服务发现：服务注册是指微服务在启动时，将自己的信息注册到服务发现组件上的过程。服务发现是指查询可用微服务列表及其网络地址的机制； 服务检查：服务发现组件使用一定机制定时检测已注册的服务，如发现某实例长时间无法访问，就会从服务注册表中移除该实例。 用途将微服务的提供者和消费者都注册到Eureka服务器上，由Eureka服务器进行管理 源码分析Eureka原理分析 集成方式编写Eureka Server 加依赖 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 加注解 1234567@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaApplication.class, args); &#125;&#125; 写配置 12345678910server: port: 8761eureka: client: # 是否要注册到其他Eureka Server实例 register-with-eureka: false # 是否要从其他Eureka Server实例获取数据 fetch-registry: false service-url: defaultZone: http://localhost:8761/eureka/ TIPS 这里，大家可先不去探究registerWithEureka 以及fetchRegistry 究竟是什么鬼，笔者将在下一节为大家揭晓。 将应用注册到Eureka Server上 加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 加注解 123456@SpringBootApplicationpublic class ProviderUserApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProviderUserApplication.class, args); &#125;&#125; 注意：早期的版本（Dalston及更早版本）还需在启动类上添加注解@EnableDiscoveryClient 或@EnableEurekaClient ，从Edgware开始，该注解可省略。 添加配置： 123456789101112spring: application: # 指定注册到eureka server上的服务名称，对于电影微服务，本系列将名称设为microservice-consumer-movie name: microservice-provider-usereureka: client: service-url: # 指定eureka server通信地址，注意/eureka/小尾巴不能少 defaultZone: http://localhost:8761/eureka/ instance: # 是否注册IP到eureka server，如不指定或设为false，那就会注册主机名到eureka server prefer-ip-address: true 测试 依次启动Eureka Server以及用户微服务、电影微服务； 访问http://localhost:8761 可观察到类似如下界面： 将用户微服务停止，可看到Eureka Server首页变成类似如下界面： Ribbon用途一般来说，提到负载均衡，大家一般很容易想到浏览器 -&gt; NGINX -&gt; 反向代理多个Tomcat这样的架构图——业界管这种负载均衡模式叫“服务器端负载均衡”，因为此种模式下，负载均衡算法是NGINX提供的，而NGINX部署在服务器端。 Nginx扫盲 本节所讲的Ribbon则是一个客户端侧负载均衡组件——通俗地说，就是集成在客户端（服务消费者一侧），并提供负载均衡算法的一个组件。 Ribbon默认为我们提供了很多的负载均衡算法，例如轮询、随机、响应时间加权等——当然，为Ribbon自定义负载均衡算法也非常容易，只需实现IRule 接口即可。 内置负载均衡的规则负载均衡规则是Ribbon的核心，下面来看一下Ribbon内置的负载均衡规则。 AvailabilityFilteringRule：过滤掉一直连接失败的被标记为circuit tripped的后端Server，并过滤掉那些高并发的后端Server或者使用一个AvailabilityPredicate来包含过滤server的逻辑，其实就就是检查status里记录的各个Server的运行状态； BestAvailableRule：选择一个最小的并发请求的Server，逐个考察Server，如果Server被tripped了，则跳过。 RandomRule：随机选择一个Server； ResponseTimeWeightedRule：作用同WeightedResponseTimeRule，二者作用一样； RetryRule：对选定的负载均衡策略机上重试机制，在一个配置时间段内当选择Server不成功，则一直尝试使用subRule的方式选择一个可用的server； RoundRobinRule：轮询选择， 轮询index，选择index对应位置的Server； WeightedResponseTimeRule：根据响应时间加权，响应时间越长，权重越小，被选中的可能性越低； ZoneAvoidanceRule：复合判断Server所在区域的性能和Server的可用性选择Server； 如需自定义负载均衡规则，只需实现IRule 接口或继承AbstractLoadBalancerRule 、PredicateBasedRule即可 ，可参考RandomRule 、RoundRobinRule 、ZoneAvoidanceRule 等内置Rule编写自己的负载均衡规则。 引入Ribbon在Spring Cloud中，当Ribbon与Eureka配合使用时，Ribbon可自动从Eureka Server获取服务提供者地址列表，并基于负载均衡算法，选择其中一个服务提供者实例。下图展示了Ribbon与Eureka配合使用时的大致架构。 集成方式代码示例 复制项目microservice-consumer-movie ，将ArtifactId修改为microservice-consumer-movie-ribbon 。 加依赖：由于spring-cloud-starter-netflix-eureka-client 已经包含spring-cloud-starter-netfilx-ribbon ，故而无需额外添加依赖。 写代码： 12345@Bean@LoadBalancedpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; 如代码所示，只需在RestTemplate 上添加LoadBalanced 注解，即可让RestTemplate整合Ribbon！ 调用： 1234567891011@GetMapping("/users/&#123;id&#125;")public User findById(@PathVariable Long id) &#123; // 这里用到了RestTemplate的占位符能力 User user = this.restTemplate.getForObject( "http://microservice-provider-user/users/&#123;id&#125;", User.class, id ); // ...电影微服务的业务... return user;&#125; 由代码可知，我们将请求的目标服务改成了http://microservice-provider-user/users/{id} ，也就是http://{目标服务名称}/{目标服务端点} 的形式，Ribbon会自动在实际调用时，将目标服务名替换为该服务的IP和端口。 测试 依次启动microservice-discovery-eureka 、microservice-provider-user 两个实例、microservice-consumer-movie-ribbon 访问http://localhost:8010/movies/users/1 多次，会发现两个user服务实例都会打印日志。 Feign简介Feign是Netflix开发的声明式、模板化的HTTP客户端，集成了 RestTemplate + Eureka + Ribbon + Hystrix 集成方式下面来将前面的例子用Feign改写，让其达到与Ribbon + RestTemplate相同的效果。 复制项目microservice-consumer-movie，将ArtifactId修改为microservice-consumer-movie-feign ； 加依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 加注解：启动类上添加@EnableFeignClients ； 编写Feign Client： 12345@FeignClient(name = "microservice-provider-user")public interface UserFeignClient &#123; @GetMapping("/users/&#123;id&#125;") User findById(@PathVariable("id") Long id);&#125; 这样一个Feign Client就写完啦！其中，@FeignClient 注解中的microservice-provider-user是想要请求服务的名称，这是用来创建Ribbon Client的（Feign整合了Ribbon）。在本例中，由于使用了Eureka，所以Ribbon会把microservice-provider-user 解析成Eureka Server中的服务。 除此之外，还可使用url属性指定请求的URL（URL可以是完整的URL或主机名），例如@FeignClient(name = &quot;abcde&quot;, url = &quot;http://localhost:8000/&quot;) 。此时，name可以是任意值，但不可省略，否则应用将无法启动！ Controller： 1234567891011@RequestMapping("/movies")@RestControllerpublic class MovieController &#123; @Autowired private UserFeignClient userFeignClient; @GetMapping("/users/&#123;id&#125;") public User findById(@PathVariable Long id) &#123; return this.userFeignClient.findById(id); &#125;&#125; 只需使用@Autowire注解，即可注入上面编写的Feign Client。 RestTemplate与Feign对比相信通过本文的例子，聪明的你对如何使用Feign已经了然于心了。文章的最后，对比一下RestTemplate + Ribbon与Feign。 角度 RestTemplate + Ribbon Feign（自带Ribbon） 可读性、可维护性 欠佳（无法从URL直观了解这个远程调用是干什么的） 极佳（能在接口上写注释，方法名称也是可读的，能一眼看出这个远程调用是干什么的） 开发体验 欠佳（需要拼凑URL） 极佳（无需拼凑url，自动集合Eureka和Ribbon） 风格一致性 欠佳（本地API调用和RestTemplate调用的代码风格截然不同） 极佳（完全一致，不点开Feign的接口，根本不会察觉这是一个远程调用而非本地API调用） 性能 较好 中等（性能是RestTemplate的50%左右；如果为Feign配置连接池，性能可提升15%左右） 灵活性 极佳 中等（内置功能能满足大多数项目的需求） Hystrix容错技术的引入至此，我们已实现服务发现、负载均衡，同时，使用Feign也实现了良好的远程调用——我们的代码是可读、可维护的。理论上，我们现在已经能构建一个不错的分布式应用了，但微服务之间是通过网络通信的，网络可能出问题；微服务本身也不可能100%可用。 如何提升应用的可用性呢？这是我们必须考虑的问题—— 举个例子：某大型系统中，服务A调用服务B，某个时刻，微服务B突然崩溃了。微服务A中，依然有大量请求在请求B，如果没有任何措施，微服务A很可能很快就会被拖死——因为在Java中，一次请求往往对应着一个线程，如果不做任何措施，那意味着微服务A请求B的线程要等Feign Client/RestTemplate超时才会释放（这个时间一般非常长，长达几十秒），于是就会有大量的线程被阻塞，而线程又对应着计算资源（CPU/内存），于是乎，大量的资源被浪费，并且越积越多，最终服务器终于没有资源给微服务A浪费了，微服务A也挂了。 因此，在大型应用中，微服务之间的容错必不可少，下面来讨论如何实现微服务的容错。 容错三板斧 超时机制：一旦超时，就释放资源。由于释放资源速度较快，应用就不会那么容易被拖死。 舱壁模式：不把鸡蛋放在一个篮子里。你有你的线程池，我有我的线程池，你的线程池满了和我没关系，你挂了也和我没关系。 断路器：实时监测应用，如果发现在一定时间内失败次数/失败率达到一定阈值，就“跳闸”，断路器打开——此时，请求直接返回，而不去调用原本调用的逻辑。 跳闸一段时间后（例如15秒），断路器会进入半开状态，这是一个瞬间态，此时允许一次请求调用该调的逻辑，如果成功，则断路器关闭，应用正常调用；如果调用依然不成功，断路器继续回到打开状态，过段时间再进入半开状态尝试——通过”跳闸“，应用可以保护自己，而且避免浪费资源；而通过半开的设计，可实现应用的“自我修复“。 断路器状态转换 引入HystrixHystrix是由Netflix开源的一个延迟和容错库，用于隔离访问远程系统、服务或者第三方库，防止级联失败，从而提升系统的可用性与容错性。Hystrix主要通过以下几点实现延迟和容错。 包裹请求 使用HystrixCommand（或HystrixObservableCommand）包裹对依赖的调用逻辑，每个命令在独立线程中执行。这使用到了设计模式中的“ 命令模式 ”。 跳闸机制（断路器的应用） 当某服务的错误率超过一定阈值时，Hystrix可以自动或者手动跳闸，停止请求该服务一段时间。 资源隔离（舱壁模式的应用） Hystrix为每个依赖都维护了一个小型的线程池（或者信号量）。如果该线程池已满，发往该依赖的请求就被立即拒绝，而不是排队等候，从而加速失败判定。 监控（超时模式的应用） Hystrix可以近乎实时地监控运行指标和配置的变化，例如成功、失败、超时、以及被拒绝的请求等。 回退机制 当请求失败、超时、被拒绝，或当断路器打开时，执行回退逻辑。回退逻辑可由开发人员自行提供，例如返回一个缺省值。 自我修复 断路器打开一段时间后，会自动进入“半开”状态。断路器打开、关闭、半开的逻辑转换，前面我们已经详细探讨过了，不再赘述。 通用方式使用Hystrix服务降级 加依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 加注解：在启动类上添加@EnableCircuitBreaker 注解。 使用： 12345678910111213141516@HystrixCommand(fallbackMethod = "findByIdFallback")@GetMapping("/users/&#123;id&#125;")public User findById(@PathVariable Long id) &#123; // 这里用到了RestTemplate的占位符能力 User user = this.restTemplate.getForObject( "http://microservice-provider-user/users/&#123;id&#125;", User.class, id ); // ...电影微服务的业务... return user;&#125;public User findByIdFallback(Long id) &#123; return new User(id, "默认用户", "默认用户", 0, new BigDecimal(1));&#125; 由代码可知，只需使用@HystrixCommand 注解，就可保护该API。这里的”保护“，其实带有三层含义——”超时机制“、”仓壁模式“、”断路器“！ TIPS 本例使用了fallbackMethod 属性，指定了一个降级方法，如不指定，Hystrix会有一个默认的降级方案，那就是抛异常，哈哈哈。 如何知道断路器打开还是关闭呢？只需访问应用的/actuator/health 端点，即可查看！断路器的状态——当然，你必须添加如下配置： 1234management: endpoint: health: show-details: always 测试 启动microservice-discovery-eureka 启动microservice-provider-user 启动microservice-consumer-movie-ribbon-hystrix-common 访问http://localhost:8010/movies/users/1 ，能正常返回结果 关闭microservice-provider-user ，再次访问http://localhost:8010/movies/users/1 ，可返回类似如下结果，说明当服务提供者时，服务消费者进入了回退方法。 1&#123;"id":1,"username":"默认用户","name":"默认用户","age":0,"balance":1&#125; 访问http://localhost:8010/actuator/health ，可获得类似如下结果： 1234567891011&#123; "status": "UP", "details": &#123; "diskSpace": ..., "refreshScope": ..., "discoveryComposite": ..., "hystrix": &#123; "status": "UP" &#125; &#125;&#125; 由结果不难发现，此时断路器并未打开！这是为什么呢？ 原因是：此时只请求了一次，没有达到Hystrix的阈值——Hystrix设计来保护高并发应用的，它要求10秒（可用hystrix.command.default.metrics.rollingStats.timeInMilliseconds 自定义）以内API错误次数超过20次（用circuitBreaker.requestVolumeThreshold 自定义），此时才可能触发断路器。 持续不断地访问http://localhost:8010/movies/users/1 多次（至少20次） 再次访问http://localhost:8010/actuator/health ，可获得类似如下结果： 1234567891011121314&#123; "status": "UP", "details": &#123; "diskSpace": ..., "refreshScope": ..., "discoveryComposite": ..., "hystrix": &#123; "status": "CIRCUIT_OPEN", "details": &#123; "openCircuitBreakers": ["MovieController::findById"] &#125; &#125; &#125;&#125; 由结果可知，此时断路器已经打开，并且列出了是哪个API的断路器被打开了。 获得造成fallback的原因在实际项目中，很可能需要获得造成fallback的原因，此时可将代码修改为如下： 1234567891011121314151617@HystrixCommand(fallbackMethod = "findByIdFallback")@GetMapping("/users/&#123;id&#125;")public User findById(@PathVariable Long id) &#123; // 这里用到了RestTemplate的占位符能力 User user = this.restTemplate.getForObject( "http://microservice-provider-user/users/&#123;id&#125;", User.class, id ); // ...电影微服务的业务... return user;&#125;public User findByIdFallback(Long id, Throwable throwable) &#123; log.error("进入回退方法", throwable); return new User(id, "默认用户", "默认用户", 0, new BigDecimal(1));&#125; Feign 整合 Hystrix默认Feign是不启用Hystrix的，如果需要启用，只需要在配置文件中配置即可： 123feign: hystrix: enabled: true Hystrix Command 详解监控端点与数据应用整合Hystrix，同时应用包含spring-boot-starter-actuator 依赖，就会存在一个/actuator/hystrix.stream 端点，用来监控Hystrix Command。当被@HystrixCommand 注解了的方法被调用时，就会产生监控信息，并暴露到该端点中。当然，该端点默认是不会暴露的，需使用如下配置将其暴露。 12345management: endpoints: web: exposure: include: 'hystrix.stream' 此时，访问/actuator/hystrix.stream 可返回如下结果： 1&#123;"type":"HystrixCommand","name":"findById","group":"MovieController","currentTime":1547905939151,"isCircuitBreakerOpen":false,"errorPercentage":0,"errorCount":0,"requestCount":0,"rollingCountBadRequests":0,"rollingCountCollapsedRequests":0,"rollingCountEmit":0,"rollingCountExceptionsThrown":0,"rollingCountFailure":0,"rollingCountFallbackEmit":0,"rollingCountFallbackFailure":0,"rollingCountFallbackMissing":0,"rollingCountFallbackRejection":0,"rollingCountFallbackSuccess":0,"rollingCountResponsesFromCache":0,"rollingCountSemaphoreRejected":0,"rollingCountShortCircuited":0,"rollingCountSuccess":0,"rollingCountThreadPoolRejected":0,"rollingCountTimeout":0,"currentConcurrentExecutionCount":0,"rollingMaxConcurrentExecutionCount":0,"latencyExecute_mean":0,"latencyExecute":&#123;"0":0,"25":0,"50":0,"75":0,"90":0,"95":0,"99":0,"99.5":0,"100":0&#125;,"latencyTotal_mean":0,"latencyTotal":&#123;"0":0,"25":0,"50":0,"75":0,"90":0,"95":0,"99":0,"99.5":0,"100":0&#125;,"propertyValue_circuitBreakerRequestVolumeThreshold":20,"propertyValue_circuitBreakerSleepWindowInMilliseconds":5000,"propertyValue_circuitBreakerErrorThresholdPercentage":50,"propertyValue_circuitBreakerForceOpen":false,"propertyValue_circuitBreakerForceClosed":false,"propertyValue_circuitBreakerEnabled":true,"propertyValue_executionIsolationStrategy":"THREAD","propertyValue_executionIsolationThreadTimeoutInMilliseconds":1000,"propertyValue_executionTimeoutInMilliseconds":1000,"propertyValue_executionIsolationThreadInterruptOnTimeout":true,"propertyValue_executionIsolationThreadPoolKeyOverride":null,"propertyValue_executionIsolationSemaphoreMaxConcurrentRequests":10,"propertyValue_fallbackIsolationSemaphoreMaxConcurrentRequests":10,"propertyValue_metricsRollingStatisticalWindowInMilliseconds":10000,"propertyValue_requestCacheEnabled":true,"propertyValue_requestLogEnabled":true,"reportingHosts":1,"threadPool":"MovieController"&#125; 对于Feign前面讲过Feign默认已经整合了Hystrix，但这个整合其实是“不完整”，因为它默认不带有监控端点，如果你在使用Feign的同时，也想使用监控端点，需按照如下步骤操作： 加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 在启动类上添加注解@EnableCircuitBreaker 在application.yml 中添加如下配置： 12345management: endpoints: web: exposure: include: 'hystrix.stream' 可视化监控数据至此，我们已可通过/actuator/hystrix.strem 端点观察Hystrix运行情况，但文字形式的监控数据很不直观。现实项目中一般都需要一个可视化的界面，这样才能迅速了解系统的运行情况。Hystrix提供了一个轮子——Hystrix Dashboard，它的作用只有一个，那就是将文字形式的监控数据转换成图表展示。 编写Hystrix Dashboard 加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; 加注解：@EnableHystrixDashboard 写配置： 123# 端口随便写，这里只是表明下自己的端口规划而已server: port: 8030 启动后，访问http://localhost:8030/hystrix 即可看到类似如下的界面： 将上文的/actuator/hystrix.stream 端点的地址贴到图中，并指定Title，然后点击Monitor Stream 按钮，即可看到类似如下的图表： 图表解读Turbine Turbine上文的Hystrix Dashboard只支持一次监控一个微服务实例，显然是不能满足需求的，所以为了能监控多个微服务，Netflix官方再次发挥造轮子的精神——它们又编写了一个组件，Turbine。 不过这个项目刚出生就死了，14年发布后就再也没有进行更新维护。 简介Turbine是一个聚合Hystrix监控数据的工具，它可将所有相关/hystrix.stream端点的数据聚合到一个组合的/turbine.stream中，从而让集群的监控更加方便。 引入Turbine后，架构图如下： 引入Turbine编写Turbine Server 加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-turbine&lt;/artifactId&gt;&lt;/dependency&gt; 加注解：@EnableTurbine 写配置： 123456789101112131415server: port: 8031spring: application: name: microservice-hystrix-turbineeureka: client: service-url: defaultZone: http://localhost:8761/eureka/ instance: prefer-ip-address: trueturbine: # 要监控的微服务列表，多个用,分隔 appConfig: microservice-consumer-movie,microservice-consumer-movie-feign clusterNameExpression: "'default'" 测试这样，Tubine即可聚合microservice-consumer-movie,microservice-consumer-movie-feign两个服务的/actuator/hystrix.stream 信息，并暴露在http://localhost:8031/turbine.stream ，将该地址贴到Hystrix Dashboard上，即可看到类似如下的图表： Zuul]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划笔记]]></title>
    <url>%2F2019%2F12%2F09%2F%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[递归、递推 递归 自顶向下 递推 自底向上 动态规划题的一般思路 先写出递归思路 然后剪枝(前面二者都是自上而下) 然后写出dp(不同于剪枝的是 dp是自底向上) dp问题核心是穷举，dp特点是存在 重叠子问题 最优子结构 然后只要在此基础上列出 状态转移方程 即可 动态规划的三要素： 重叠子问题 最优子结构 状态转移方程 思维框架： 明确「状态」 -&gt; 定义 dp 数组/函数的含义 -&gt; 明确「选择」-&gt; 明确 base case。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>dp</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm笔记]]></title>
    <url>%2F2019%2F12%2F04%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3java%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[第二章 Java内存区域与内存溢出异常1. 运行时数据区域包括 方法区、堆（前两者都是共享的）、虚拟机栈、本地方法栈、程序计数器（三者是线程私有）。 方法区：与Java堆一样，是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量，别名称为Non-Heap。 堆：Java Heap是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，此内存区域的唯一目的是存放对象实例，同时Java堆是垃圾收集管理的主要区域，很多时候也被称为 GC堆。 虚拟机栈：描述的是Java方法执行的内存模型，即每个方法会在执行的同时创建一个栈帧用于存储局部变量表、操作数栈、动态链接、方法出口等信息。经常有人把Java内存分为堆内存和栈内存，这里的栈内存就是指的虚拟机栈。 本地方法栈：与虚拟机栈发挥的功能基本一致，区别不过是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则为虚拟机使用到的Native方法服务。 程序计数器：一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。 运行时常量池：方法区的一部分。常量池（用于存放编译期生成的各种字面量和符号引用）。 直接内存：并不是虚拟机运行时数据区的一部分，也不是Java虚拟机规范中定义的内存区域。「NIO 中会使用」 对它的使用还是不是特别了解… 堆外内存：https://blog.csdn.net/y3over/article/details/88791958 2. OutOfMemoryError异常包括 Java堆溢出、虚拟机栈和本地方法栈溢出、方法区和运行时常量池溢出、本机直接内存溢出。 堆溢出，主要就是不停的 new 对象即可；「-Xms」 栈溢出，主要是定义大量的本地变量，然后递归，使得爆栈，这样会有 StackOverflowError，如果想要有 OOM，最好的方法就是不停的创建线程，因为系统分配给栈的内存是有限的，比如一般总的是 2G，然后减去堆和方法区的容量，就是栈的容量了，如果不断地 new 线程的话，那么总有一个线程会内存不够，此时就能够弹出 OOM 了；「」 方法区中的运行时常量池的溢出，一般也就是使用 String.inter() 方法，这个方法是一旦池中没有需要的常量，就定义一个放到池中去，否则就拿出这个常量的地址，我不断的定义常量放入常量池，那么就很容易 OOM；「PermGen Space」 方法区溢出，最基本的思路就是运行时产生大量的类去填满方法区直到溢出，这里借助的是 GGLib 直接操作字节码生成大量的动态类。「这个地方我不太会…」「-XX:PermSize」 本机直接内存溢出「-XX:MaxDirectMemorySize」 3. 补充：虚拟机部分参数-vmargs -Xms128M -Xmx512M -XX:PermSize=64M -XX:MaxPermSize=128M-vmargs 说明后面是VM的参数，所以后面的其实都是JVM的参数了-Xms128m JVM初始分配的堆内存-Xmx512m JVM最大允许分配的堆内存，按需分配-XX:PermSize=64M JVM初始分配的非堆内存-XX:MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配 第三章 垃圾收集器与内存分配策略问题： 哪些内存需要回收 什么时候回收 如何回收 Q：哪些内存需要回收？ 其中，程序计数器、虚拟机栈、本地方法栈三个区域随线程而生、随线程而灭，每一个栈帧分配多少内存基本上是在类结构确定下来时就已知的（尽管 JIT 编译期会在运行期间进行一些优化，但大体可以理解为编译期间就可知），所以这几个区域的分配和回收都具备确定性，不需要过多考虑回收的问题，但是 Java 堆和方法区 就不一样了，这里是公共区域，不是线程私有的，所以只有在运行期间才能确定内存的占用分配情况，这部分的内存的分配和回收都是动态的，所以 GC 就是指的这一块的内存。 很多人认为方法区（或者说 HotSpot 虚拟机中的永久代）是没有垃圾收集机制的，的确在 java 虚拟机规划中确实没有要求要在方法区实现垃圾收集，因为性价比很低，在堆中，尤其是新生代中，常规应用一次垃圾收集可以回收 70% - 95% 的空间，而方法区的垃圾收集效率远低于此。 但是，其实还是有必要进行垃圾收集的，尤其是在大量使用反射、动态代理、GGLib等 bytecode 框架的场景是需要具备类卸载的功能防止方法区 OOM 的。永久代的垃圾收集主要回收两部分：废弃常量和无用的类，废弃常量的收集比较简单，就是没人用了，就会被请出常量池，但是无用的类这个条件就很严苛了，只有同时，注意使用时满足下面 3 个条件才能算是“无用的类”： 该类所有实例被回收； 加载该类的 ClassLoader 已经被回收； 该类对应的 java.lang.Class 对象没有被使用，且无法在任何地方通过反射访问该类的方法。「这个好像非常的严格…」 Q：什么时候回收？ 第一步肯定是确定这个对象是否还“活着”。那么如何确定呢？有哪些方法呢？ 第一种方法虽然不是 Java 中使用的，但是也有一些著名案例使用过。那就是计数算法：给对象添加一个计数器，引用了一次就加一，引用失效就减一，在任何时刻计数器都为0的对象就是不可能再被使用的，就可以回收。但是这里漏掉了一种情况，就是类似于死锁的情况，一个引用互相持有另外一个引用，这样虽然计数器的值都为 1 ，但是二者其实已经不可能再被访问了，理应被回收，但这个算法做不到； 第二种方法就是 Java 和 C# 用的方法了，为根搜索算法，算法的基本思路就是：如果 GC Roots 对象到这个对象不可达，则这个对象不可用。 大家可能会问了，什么是 GC Roots 对象，又如何定义不可达？ 可以做 GC Roots 的，其实就是部分对象： 栈中本地变量表 or native 方法栈中引用的对象； 方法区(non-heap)中的类静态属性引用的变量； 方法区(non-heap)中的常量引用的对象； 那么又如何定义不可达呢？即以 GC Roots 的对象为起始点，从这些节点向下搜索，所走过的路径称为引用链，当GC Roots 和这个对象不存在引用链，那就是不可达。 现在我们已经清楚了如何判断一个对象是 “活着” 还是 “死亡” 了，那是不是对象 “活着” 它就不会被回收呢，对象 “死亡” 我们就要将其回收呢，答案是否定的（如果是肯定的我还说个毛啊哈哈哈哈）。注意，这里的两句话都是不对的。 首先是 “活着” 的对象不是一定不被回收的，要“活着”，也要看它怎么活，怎么个活法，就是对应着这个链路的强度，在 jdk1.2 以后，引用分为了四类，这个我在 多线程（四）— ThreadLocal 一文中的内存泄漏也有提及。以下的顺序，就是引用从强到弱的顺序。 强引用。听这个名字就知道这是上等人，这就是我们在程序代码中普遍存在的，类似于 “Object obj = new Object()” ，只要对象的强引用还在，他就一定能好好的活着，不会被 GC 回收；「Reference类」 软引用。就是一些有用但是又不是必要的，类似于工具人，被软引用关联的对象，在系统即将要发生OOM时，会将这些对象进行回收，回收之后发现还是内存不够，才会抛出 OOM 异常。「softReference类」 弱引用。这个更没用，他的生存周期很短，只能活在下一次 GC 工作之前，当 GC 开始干活时，被弱引用关联的对象就得滚蛋了。「WeakReference」这里再唠叨一下，ThreadLocalMap 中的 key 对 ThreadLocal 的引用就是弱引用，目的就是为防止内存泄漏添加一个保障，当 ThreadLocal 失去了强引用后，会被 GC 处理，而此时 key 为 null，可以触发ThreadLocalMap中清除脏 Entry 的方法防止内存泄漏。 虚引用。这个又被称为幽灵引用或者幻影引用(妈的我有点怕鬼..)，一个对象被这个虚引用关联，等于没关联，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例，唯一的作用就是可以在这个对象被 GC 回收时收到一个系统通知，就是阎王爷来收尸前派来告诉你要死的那几个小兵的作用(把我自己说怕了…)。「PhantomReference」 再来解决第二个问题，那就是“死亡” 的对象，也不一定就要被回收，他只是处于缓刑阶段，还可以通过上诉来活着哈哈。(这一章是真的有意思啊) 要真正宣告一个对象死亡，至少要经历两次标记过程：第一次标记就是标记它不可达，即“死亡”的状态，同时他有一次“上诉”的机会，很明显上诉成功就不会死，上诉失败那肯定死定了。那，如何才能拥有这次难得的机会呢？那就是这个被标记为“死亡”的对象有覆盖 finalize() 方法，并且第一次调用，注意，他只有一次“上诉”的机会。当满足上诉机会的时候，即复写 finalize() 并且是第一次调用，它会被放置到 F-Queue中，并在稍后由一条由虚拟机自动创建的、低优先级的 Finalizer 去触发这个 他复写的 finalize()「注意，这里的 Finalizer由于优先级比较低，所以我们需要在线程中等待他一段时间，以确保它已经执行了。同时，注意，这个 Finalizer 很严苛，它不会允许 finalize() 一直执行，是有时间限制的，就跟上诉一样，是有期限的，过了这个期限就没用了。」，那转机肯定就在这个 finalize() 中了，只要“死亡”的对象在这个时候将自己与引用链上的任何一个对象建立关联，比如自己赋值给某个类成员变量，这就意味着上诉成功并且得到解放啦！当然如果没抓住这次机会，那肯定就死定了。 注意这里 finalize() 只有一次机会哦。在周老师的书中，也建议我们避免使用它，虽然挺好玩的…但是 finalize() 没啥用，而且运行代价高昂，不确定性大，它能做的工作 try-finally 完全可以做的更好。 Q：如何回收？ 这个问题比较宽泛，我们做一件事，首先需要理论的指导，然后再去实践。在如何回收这件问题上，我们先要提出一些垃圾收集（回收）的算法，然后再利用这个方法论去具体实现内存的回收——-垃圾收集器。 垃圾收集算法。其实就是以一种方法为基础，然后两种方法在某个领域（新生代 or 老年代）更专业，最后一种方法集大成。 最基础的方法，标记-清除算法。分为两步，即“标记” 和 “清除”：首先标记出要回收的对象，然后再标记完成后统一回收掉。缺点：一个是效率问题，标记和清除都是需要较多时间，效率低，一个是空间问题，标记清除之后容易产生大量不连续的内存碎片，非常容易造成下一次想分配对象时找不到连续内存而不得触发再一次的 GC ； 复制算法。主要是用在新生代中，因为新生代 98% 的对象都是死得快的…所以我们可以将内存分为一块较大的 Eden 空间和两块较小的 Survivor 空间「为什么要设立 Survivor 区，为何是两块，为何不是一块、三块、四块？设立 Survivor 区主要是考虑到防止频繁 full gc，而设立两块，则是为了解决minor gc内存碎片的问题，设立太多反而会造成 Survivor空间太小，导致full gc 频繁，所以两块 Survivor 是最好的。」，然后每次使用 Eden 和 一块 Survivor，另外一块 Survivor 就可以用来存垃圾收集完之后还存活着的对象，这样清理也方便，效率得到解决，同时也不会有空间上的问题，因为可以在复制的时候直接移动堆顶指针按顺序分配就行了。但是吧，这也有个很明显的缺点，就是 Survivor 这块内存被浪费了，并且我们并不能保证每次回收之后的对象大小不超过 Survivor，当 Survivor 空间不够时，我们还需要用到一个策略：分配担保。即当 Survivor 空间不够时，这些对象会直接通过分配担保策略进入到老年代； 标记-整理算法。因为老年代存活率很高，肯定就不能用复制算法了，这里的标记-整理算法，就是对标记-清除的升级，在清理之前，由于不能像复制算法一样直接清理，只能是让所有存活的对象都向一端移动，然后再清理掉边界以外的内存； 「G1 就用了这种算法。」 分代收集算法。只是根据对象的存活周期的不同将内存划分为新生代和老年代，然后新生代使用复制算法，老年代使用标记-清理 or 标记-整理。 垃圾收集算法的实现——垃圾收集器 既然垃圾收集算法，是按照新生代和老年代分别去设计的，那自然垃圾收集器肯定也是按照新生代和老年代去实现。 新生代的收集器包括 Serial ParNew Parallel Scavenge 2.老年代的收集器包括 Serial Old Parallel Old CMS 3.回收整个Java堆(新生代和老年代) G1收集器 具体的搭配如上图所示。下面具体讲一下用的场景和优缺点。 新生代收集器： Serial串行收集器-复制算法 Serial收集器是新生代单线程收集器，优点是简单高效，是最基本、发展历史最悠久的收集器。它在进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集完成。但依然是虚拟机运行在Client模式下默认新生代收集器，对于运行在Client模式下的虚拟机来说是一个很好的选择。 ParNew收集器-复制算法 也是新生代并行收集器，其实就是Serial收集器的多线程版本。主义只能做到并行，不能并发。除了使用多线程进行垃圾收集之外，其余行为包括Serial收集器可用的所有控制参数、收集算法、Stop The World、对象分配规则、回收策略等都与Serial 收集器完全一样。 并行：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态； 并发：用户线程和多垃圾收集线程同时执行（但不一定是并行的，可能是交替执行） Parallel Scavenge（并行回收）收集器-复制算法新生代并行收集器，追求高吞吐量，高效利用 CPU。该收集器的目标是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即 吞吐量=运行用户代码时间/（运行用户代码时间+垃圾收集时间），所以该收集器也被常称为 “吞吐量优先”收集器。 它和 ParNew 的主要区别有两个： 关注点不一样，一个是为了停顿时间更短，这适合用户交互，一个则是为了最高的利用 CPU 时间，适合后台运算； Parallel Scanvenge 有 GC 自适应的调节策略，无需自己设置新生代大小、Eden 和 Survivor的比例等等，只需要把基本的内存数据（如堆大小、吞吐量）设置好即可。 好像都是优点，但是其有一个致命的缺陷，就是它不能跟 CMS 搭配，所以CMS 还是一般和 ParNew 联合使用。 老年代收集器： Serial Old 收集器-标记 - 整理算法 Serial Old是Serial收集器的老年代版本，它同样是一个单线程(串行)收集器，使用标记整理算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。 如果在Server模式下，主要两大用途： （1）在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用； （2）作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用。 Parallel Old 收集器-标记 - 整理算法 Parallel Old 是Parallel Scavenge收集器的老年代版本，使用多线程和“标记-整理”算法。这个收集器在1.6中才开始提供。 CMS 收集器-标记 - 清除算法 CMS(Concurrent Mark Sweep)收集器是一种以获取最短回收停顿时间为目标的收集器。适用于互联网站的服务器端。提高响应速度。 它的运作过程相对前面几种收集器来说更复杂一些，整个过程分为4个步骤： 初始标记。标记 GC Roots 能直接关联到的对象，速度非常快； 并发标记，收集器线程和用户线程同步进行； 重新标记，修正并发标记期间的变化； 并发清除。 所以优点很明显：1. 并发收集；2. 低停顿，所以又称为 “并发低停顿收集器”； 缺点也有： 对 CPU 资源敏感，默认启用 （CPU 数量 + 3）/4，cpu少的话那影响很大； 无法处理浮动垃圾「清理阶段可能会有新的垃圾产生」，可能出现“Concurrent Mode Failure”失败而导致另一次Full GC的产生。在JDK1.5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活。 CMS是基于“标记-清除”算法实现的收集器，手机结束时会有大量空间碎片产生。空间碎片过多，可能会出现老年代还有很大空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前触发FullGC。 新生代和老年代垃圾收集器： G1 收集器-分代收集算法 — 兼顾 吞吐量 &amp; 低停顿 独特的分代垃圾回收器,分代GC: 分代收集器, 同时兼顾年轻代和老年代； 使用分区算法, 不要求eden, 年轻代或老年代的空间都连续； 并行性: 回收期间, 可由多个线程同时工作, 有效利用多核cpu资源； 空间整理: 回收过程中, 会进行适当对象移动, 减少空间碎片； 可预见性: G1可选取部分区域进行回收, 可以缩小回收范围, 减少全局停顿。 G1收集器的阶段分以下几个步骤： 1、初始标记（它标记了从GC Root开始直接可达的对象）； 2、并发标记（从GC Roots开始对堆中对象进行可达性分析，找出存活对象）； 3、最终标记（标记那些在并发标记阶段发生变化的对象，将被回收）； 4、筛选回收（首先对各个Region的回收价值和成本进行排序，根据用户所期待的GC停顿时间指定回收计划，优先回收垃圾最多的Region，因此称为 Garbage First）。 经常使用的参数： -XX:+UseSerialGC：在新生代和老年代使用串行收集器-XX:+UseParNewGC：在新生代使用并行收集器-XX:+UseParallelGC ：新生代使用并行回收收集器，更加关注吞吐量-XX:+UseParallelOldGC：老年代使用并行回收收集器-XX:ParallelGCThreads：设置用于垃圾回收的线程数-XX:+UseConcMarkSweepGC：新生代使用并行收集器，老年代使用CMS+串行收集器-XX:ParallelCMSThreads：设定CMS的线程数量-XX:+UseG1GC：启用G1垃圾回收器 上面三个问题，基本解决了垃圾收集方面的问题了，那回过头来，对象又是如何分配内存的呢？ 对象优先在新生代 Eden 区中分配，当 Eden 区没有足够的空间进行分配时，虚拟机会发起一次 Minor GC，将 GC 后还存活的对象转移Survivor区，如果Survivor区溢出了会转移到老年代中。 新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为 Java 对象大都具有死得快的特性，所以 Minor GC 会非常的频繁，速度也很快； 老年代GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC，经常会伴随至少一次 Minor GC，当然也并非绝对，在 ParallelScaVenge 就有直接进行 Major GC 的策略选择过程。Major GC 的速度比 Minor GC 慢 10倍以上。 大对象会直接进入老年代，因为就算安排进新生代的 Eden 区，Eden 区及两个 Survivor区之间会进行大量的内存拷贝，而且极有可能 Survivor 装不下进而触发分配担保机制转移到老年代，与其这样，不如直接将大对象直接进入老年代。 上文一直在讲新生代和老年代，那二者到底是如何界定呢？ 虚拟机给每个对象都定义了一个对象年龄计数器，如果对象在新生代的 Eden 区域出生并且经过第一次 Minor GC 后仍然留在 Survivor 中，那么他的对象年龄变为 1 岁，从此之后，该对象在 Survivor 每熬过一次 Minor GC ，年龄就会增加一岁，默认会到 15 岁才晋升到老年代中，阈值可以通过参数 -XX:MaxTenuringThreshold设置。 当然，为了适应不同程序的内存情况，虚拟机并不会总是按照对象年龄大于MaxTenuringThreshold才晋升到老年代，如果 Survivor 空间中相同年龄的对象的总大小已经超过了 Survivor 的空间的一半，则年龄大于或等于该年龄的对象直接就可以晋升到老年代。 那为何要有这种动态年龄计算呢？ 如果固定按照MaxTenuringThreshold设定的阈值作为晋升条件： a）MaxTenuringThreshold设置的过大，原本应该晋升的对象一直停留在Survivor区，直到Survivor区溢出，一旦溢出发生，Eden+Svuvivor中对象将不再依据年龄全部提升到老年代，这样对象老化的机制就失效了。 b）MaxTenuringThreshold设置的过小，“过早晋升”即对象不能在新生代充分被回收，大量短期对象被晋升到老年代，老年代空间迅速增长，引起频繁的Major GC。分代回收失去了意义，严重影响GC性能。 在本章的最后，再来详细叙述一下空间分配担保。 在发生 Minor GC时，虚拟机 会检测一下之前每次晋升到老年代的平均大小是否大于老年代的剩余空间大小（因为在实际回收之前是无法确定本轮的对象大小是多少，只能按以往的经验值估计） 如果大于，说明，崩了，只能进行 Full GC了; 如果小于，则再查看 HandlePromotionFailure（处理晋升失败）的设置（默认打开，不然Full GC 太频繁了，慢的一批）， 如果允许，则只会进行 Minor GC，如果Minor GC后担保失败，那就没办法了只能选择Full GC，否则如果不允许失败，那就只能进行 Full GC。 谈到这么多次 Full GC，Minor GC那我想问一个问题，如果 Full GC or Minor GC 频繁，有哪些原因导致？ Full GC 频繁： 先来分析一下，什么时候会触发 Full GC: 程序执行了System.gc() //建议jvm执行fullgc，并不一定会执行 执行了jmap -histo:live pid命令 //这个会立即触发fullgc…这个可以不讲 在执行minor gc的时候进行的一系列检查 // 三次 fullgc 的可能 使用了很多短命的大对象 // 大对象直接进入到老年代 在程序中长期持有了对象的引用 //对象年龄达到指定阈值也会进入老年代 首先 1 2 这两情况我们不考虑，第 4 5 种情况也不用分析了，重点是第 3 种情况，minor gc 后导致 full gc，我觉得原因可能有： Eden 设置过小 or Survivor 设置过小，导致频繁触发 minor gc or 担保策略触发，这种情况下一次 full gc后，剩余的对象应该很少。 可能是老年代内存设置过小，导致很容易 full gc。这种情况下，一次 full gc 后，在老年代中内存应该不会减少很多，回收率很低。 所以总的来说，先看一次回收 full gc 后的结果，然后再进行判断。 Minor GC 频繁： 那就只能是 Eden 设置的太小了。 JVM 调优的基本思路： 如果是 CMS 通过看JDK自带的图形化工具，检查线程的状态。了解到一个Java进程有多少线程，每个线程什么状态，是不是在等着锁：进程的CPU和内存占用了多少； 如果发现程序跑得很慢，就用图形化界面去看 GC 日志； 总的来说就是要去合理设置年轻代和老年代的大小，这是一个迭代的过程，可以采用 JVM 默认的配置通过压力测试去分析 GC 日志； 如果发现经常 Minor GC，并且回收率并不高，则说明 Eden 区设置的过小； 如果发现经常 Full GC，先考虑是不带自己写的代码有问题，比如执行了 System.gc()、使用了过多全局变量、使用了很多短命的大对象，然后如果排查不出来； 再考虑参数调优，如果发现 Full GC 后内存占有率下来了，说明清除了大量垃圾，可以考虑调大年轻代 Eden or Survivor，如果没下来，可能是 内存泄漏 or 老年代太小。 如果是 G1，则直接调大堆的内存应该就行了。因为G1收集器采用了局部区域收集策略，单次垃圾收集的时间可控，可以管理较大的Java堆。 第六章 类文件结构编译过程这一章其实还是比较简单但是枯燥的，主要是讲了 Class 文件字节码结构。 将 .java 文件变成 .class 文件，就是我们常说的编译过程，这一过程是靠 javac 这个编译器完成的。Java 和 C/C++ 不一样，C/C++ 是通过直接将源代码编译成目标机器码（CPU直接执行的指令集合），而 Java 则是在中间加了一层字节码，也就是把 java文件转换成 class 字节码，然后 JVM 能够识别字节码，也就是能够加载 .class 文件（进而再通过 JVM 将字节码转换为机器码，这一步也可没有）。 ————————————————版权声明：本文为CSDN博主「麦田」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/itmyhome1990/article/details/78847266 Class 文件组成任何一个Class文件都对应着唯一一个类或接口的定义信息，但反过来说，类或接口并不一定都得定义在文件里（譬如类或接口也可以通过类加载器直接生成）。Class 文件是一组以 8 位字节为基础单位的二进制流。 根据 Java 虚拟机规范的规定，Class 文件格式采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只有两种数据类型：无符号数和表 无符号数属于基本的数据类型，以u1、 u2、 u4、 u8来分别代表1个字节、 2个字节、 4个字节和8个字节的无符号数，无符号数可以用来描述数字、 索引引用、 数量值或者按照UTF-8编码构成字符串值。 表是由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性地以“_info”结尾。 表用于描述有层次关系的复合结构的数据，整个 Class 文件本质上就是一张表，它由如下表所示的数据项构成。 如上图所示，整个 Class 文件 按如下排列：魔数(4) – 次版本号(2) – 主版本号(2) – 常量池常量数(2) – 常量池表(不确定长度) – 访问标志(2) – 类索引(2) – 父类索引(2) – 实现的接口数(2) – 实现的接口集合(2) – 实现的字段数(2) – 字段集合表(不确定长度) – 方法数(2) – 方法集合表(不确定长度) – 属性数(2) – 属性集合表(不确定长度)。 具体组成部分1. 魔数每个Class文件的头4个字节称为魔数（Magic Number），它的唯一作用是确定这个文件是否为一个能被虚拟机接受的Class文件。 2. Class文件的版本紧接着魔数的4个字节存储的是Class文件的版本号：第5和第6个字节是次版本号（MinorVersion），第7和第8个字节是主版本号（Major Version）。 Java的版本号是从45开始的，JDK 1.1之后的每个JDK大版本发布主版本号向上加1（JDK 1.0～1.1使用了45.0～45.3的版本号），高版本的JDK能向下兼容以前版本的Class文件，但不能运行以后版本的Class文件，即使文件格式并未发生任何变化，虚拟机也必须拒绝执行超过其版本号的Class文件。 3. 常量池常量池可以理解为Class文件之中的资源仓库，它是Class文件结构中与其他项目关联最多的数据类型，也是占用Class文件空间最大的数据项目之一，同时它还是在Class文件中第一个出现的表类型数据项目。 常量池中主要存放两大类常量：字面量（Literal）和符号引用（Symbolic References）。字面量比较接近于Java语言层面的常量概念，如文本字符串、 声明为final的常量值等。 而符号引用则属于编译原理方面的概念，包括了下面三类常量： 类和接口的全限定名（Fully Qualified Name） 字段的名称和描述符（Descriptor） 方法的名称和描述符 常量池中的每一项又对应着一个表，总共11种数据类型的结构，注意 tag = 2 这个值没有： 4. 访问标志在常量池结束之后，紧接着的两个字节代表访问标志(access_flags),这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等。 5. 类索引、 父类索引与接口索引集合类索引（this_class）和父类索引（super_class）都是一个u2类型的数据，而接口索引集合（interfaces）是一组u2类型的数据的集合，Class文件中由这三项数据来确定这个类的继承关系。 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。 由于Java语言不允许多重继承，所以父类索引只有一个，除了java.lang.Object之外，所有的Java类都有父类，因此除了java.lang.Object外，所有Java类的父类索引都不为0。 接口索引集合就用来描述这个类实现了哪些接口，这些被实现的接口将按implements语句（如果这个类本身是一个接口，则应当是extends语句）后的接口顺序从左到右排列在接口索引集合中。 类索引、 父类索引和接口索引集合都按顺序排列在访问标志之后，类索引和父类索引用两个u2类型的索引值表示，它们各自指向一个类型为CONSTANT_Class_info的类描述符常量，通过CONSTANT_Class_info类型的常量中的索引值可以找到定义在CONSTANT_Utf8_info类型的常量中的全限定名字符串。 对于接口索引集合，入口的第一项——u2类型的数据为接口计数器（interfaces_count），表示索引表的容量。 如果该类没有实现任何接口，则该计数器值为0，后面接口的索引表不再占用任何字节。 6. 字段表集合字段表（field_info）用于描述接口或者类中声明的变量。其实也就是类的静态变量或者是实例成员变量。 字段（field）包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。 我们可以想一想在Java中描述一个字段可以包含什么信息？可以包括的信息有：字段的作用域（public、 private、 protected修饰符）、 是实例变量还是类变量（static修饰符）、 可变性（final）、 并发可见性（volatile修饰符，是否强制从主内存读写）、 可否被序列化（transient修饰符）、 字段数据类型（基本类型、 对象、 数组）、 字段名称。 上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。 而字段叫什么名字、 字段被定义为什么数据类型，这些都是无法固定的，只能引用常量池中的常量来描述。 7. 方法表集合和字段表基本一样，仅在访问标志和属性表集合的可选项有些区别。就是看方法有没有被 synchronized、abstract、final等修饰。 8. 属性表集合属性表集合中包含了大量的数据信息，上面的所有类型都有十分严格顺序，长度，大小。而属性表中就没有那么严格了，我们编写的最多的Code就存放在属性表集合中的CODE表中，一共有21项比如还包含：Exception表等等。具体的每一个项都是有意义的，有点多简单的介绍一下主要的： 1、Code 属性 Java方法体里面的代码经过Javac编译之后，最终变为字节码指令存储在Code属性内，Code属性出现在方法表的属性集合中，但在接口或抽象类中就不存在Code属性。 2、Exception属性 列举出方法中可能抛出的受查异常。 3、LineNumberTable属性 描述Java源码行号与字节码行号（字节码的偏移量）之间的对应关系。主要是如果抛出异常时，编译器会显示行号，就是这个属性的作用。 4、LocalVariableTable属性 描述栈帧中局部变量表中的变量与Java源码中定义的变量之间的关系。用处在于当别人使用这个方法是能够显示出方法定义的参数名。 5、SourseFile属性 记录生成这个Class文件的源码文件名称，抛出异常时能够显示错误代码所属的文件名。 6、ConstantValue属性 通知虚拟机自动为静态变量赋值，只有被static字符修饰的变量（类变量）才可以有这项属性。 7、InnerClass属性 用于记录内部类与宿主类之间的关联。 8、Deprecated和Synthetic属性 这两个都是标志类型的布尔属性，Deprecated表示不再推荐使用，注解@deprecatedSynthetic表示此字段或方法是由编译器自行添加的。 Demo为了更好的理解 Class 的组成部分，我决定写一个 demo，让我们一步步来分析一下。 源代码12345678910111213141516package JVM;public class SubClass extends SuperClass &#123; @Override public void A()&#123; super.A(); System.out.println(2222); &#125; public static void main(String[] args) &#123; test_heap_outOfMemory t = new test_heap_outOfMemory(); t.C(); SubClass subClass = new SubClass(); subClass.A(); &#125;&#125; Byte Code viewer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// class version 52.0 (52)// access flags 0x21public class JVM/SubClass extends JVM/SuperClass &#123; // compiled from: SubClass.java // access flags 0x1 public &lt;init&gt;()V L0 LINENUMBER 3 L0 ALOAD 0 INVOKESPECIAL JVM/SuperClass.&lt;init&gt; ()V RETURN L1 LOCALVARIABLE this LJVM/SubClass; L0 L1 0 MAXSTACK = 1 MAXLOCALS = 1 // access flags 0x1 public A()V L0 LINENUMBER 6 L0 ALOAD 0 INVOKESPECIAL JVM/SuperClass.A ()V L1 LINENUMBER 7 L1 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; SIPUSH 2222 INVOKEVIRTUAL java/io/PrintStream.println (I)V L2 LINENUMBER 8 L2 RETURN L3 LOCALVARIABLE this LJVM/SubClass; L0 L3 0 MAXSTACK = 2 MAXLOCALS = 1 // access flags 0x9 public static main([Ljava/lang/String;)V L0 LINENUMBER 11 L0 NEW JVM/test_heap_outOfMemory DUP INVOKESPECIAL JVM/test_heap_outOfMemory.&lt;init&gt; ()V ASTORE 1 L1 LINENUMBER 12 L1 ALOAD 1 INVOKEVIRTUAL JVM/test_heap_outOfMemory.C ()V L2 LINENUMBER 13 L2 NEW JVM/SubClass DUP INVOKESPECIAL JVM/SubClass.&lt;init&gt; ()V ASTORE 2 L3 LINENUMBER 14 L3 ALOAD 2 INVOKEVIRTUAL JVM/SubClass.A ()V L4 LINENUMBER 17 L4 RETURN L5 LOCALVARIABLE args [Ljava/lang/String; L0 L5 0 LOCALVARIABLE t LJVM/test_heap_outOfMemory; L1 L5 1 LOCALVARIABLE subClass LJVM/SubClass; L3 L5 2 MAXSTACK = 2 MAXLOCALS = 3&#125; jsclasslib我们使用 jsclasslib 这个可视化界面来看一下具体的 class 信息。 首先可以看到， jdk的版本的主版本号是 1.8，次版本号是 0，常量池的总数是34，访问标志，即该类是 public or default，然后左边对应的有常量池、接口表、字段表、属性表、方法表。 详细分析可以看这个：https://juejin.im/post/5b5ac6d76fb9a04f8d6bc7d6#heading-29 第七章 虚拟机类加载机制 参考：https://juejin.im/post/5ae2d580f265da0b851c95ea#heading-17 1. 概述在第六章，讲到了 Class 文件，那我们是如何将 Class 文件加载到虚拟机，然后运行和使用呢？ 1.1 虚拟机类加载机制的概念虚拟机把描述类的数据从class文件加载到内存，并对数据进行校验、转换解析和初始化。最终形成可以被虚拟机最直接使用的java类型的过程就是虚拟机的类加载机制。 1.2 Java语言的动态加载和动态连接另外需要注意的很重要的一点是：java语言中类型的加载连接以及初始化过程都是在程序运行期间完成的，这种策略虽然会使类加载时稍微增加一些性能开销，但是会为java应用程序提供高度的灵活性。java里天生就可以动态扩展语言特性就是依赖运行期间动态加载和动态连接这个特点实现的。比如，如果编写一个面向接口的程序，可以等到运行时再指定其具体实现类。 注意，下文的 Class 文件其实是指一串二进制的字节流，可以没有必要是文件形式，同时，每个 Class 文件都可能代表一个类或接口。 2. 类加载时机类的整个生命周期包括： 加载(Loading) —— 验证(Verification) —— 准备(Preparation) —— 初始化(Initialization) ——- 使用(Using) —— 卸载(Unloading) ，其中 验证、准备、解析 统称为连接(Linking) 需要注意的是，只有加载、验证、准备、初始化、卸载这 5 个阶段的顺序是确定的，当然确定的也只是开始的时间，因为这些阶段都是互相交叉地混合进行，通常会在一个阶段执行的过程中去对调用和激活另外一个阶段。而解析阶段，可以在初始化后再开始，这就是为了支持 Java 的运行时绑定。 虚拟机 jvm 并没有规范什么时候开始加载，但是规定了初始化的时间，有且只有4种情况才会立即对类进行“初始化”(自然加载、验证、准备都要在之前开始)： 使用 new 关键字实例化对象的时候、读取或设置一个类的静态字段的时候（注意：final修饰并且在编译前间就已经将结果放置在了常量池中的静态字段除外），以及调用一个类的静态方法的时候； 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有初始化，则需要先触发其初始化。「反射的过程我在另外一篇文章已经讲过了。」 当初始化一个类的时候，如果发现其父类没有被初始化就会先初始化它的父类。 当虚拟机启动的时候，用户需要指定一个要执行的主类（就是包含main()方法的那个类），虚拟机会先初始化这个类； 以上 4 种称之为对类的主动使用，会触发相应的初始化，而比较典型几种被动使用，不会触发初始化的情况有： 通过子类访问父类的 static 变量，不会引起子类的初始化； 定义引用数组，不会初始化类； 12Obj[] arrays = new Obj[10]; 上面说的，用 final 修饰并且在编译期间(生成.class文件期间)就已经将结果写入了常量池中。 当然，对于接口来说，与类相比在初始化阶段也略有不同（主动使用只有3种情况），就是在初始化一个接口时，如果发现父亲接口没有初始化，并不会去初始化父亲接口，而是等用到了才会去初始化。 3. 类加载过程接下来详细讲一下类加载的全过程，也就是加载、验证、准备、解析、初始化这 5 个阶段的过程。 3.1 加载“加载” 是 “类加载” 过程的一个阶段，切不可将二者混淆。 加载阶段由三个基本动作组成： 通过类型的完全限定名，产生一个代表该类型的二进制数据流（根本没有指明从哪里获取、怎样获取，可以说一个非常开放的平台了）； 解析这个二进制数据流为方法区内的运行时数据结构； 在 Java 堆中创建一个表示该类型的java.lang.Class类的实例，作为方法区这个类的各种数据的访问入口。 通过类型的完全限定名，产生一个代表该类型的二进制数据流的几种常见形式： 从zip包中读取，成为日后JAR、EAR、WAR格式的基础； 从网络中获取，这种场景最典型的应用就是Applet； 运行时计算生成，这种场景最常用的就是动态代理技术了； 由其他文件生成，比如我们的JSP。 3.2 验证验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 虚拟机如果不检查输入的字节流，并对其完全信任的话，很可能会因为载入了有害的字节流而导致系统崩溃，所以验证是虚拟机对自身保护的一项重要工作。这个阶段是否严谨，直接决定了java虚拟机是否能承受恶意代码的攻击。 从整体上看，验证阶段大致上会完成4个阶段的校验工作：文件格式验证、元数据验证、字节码验证、符号引用验证。 文件格式验证。主要是确保输入的字节流能够正确的解析并存储于方法区之中，这阶段的验证基于字节流进行，该验证结束后，字节流就进入到了内存中的方法区，进行存储，所以后续三个验证阶段全是基于方法区的存储结构； 元数据验证。现在其实 jdk1.8 之后没有方法区了，改名叫做元数据区。这个阶段是对字节码描述的信息进行语义分析，保证是 java规范的信息； 字节码验证。验证中最复杂的，主要工作是进行数据流和控制流分析，保证类的方法在运行时不会做出危害虚拟机安全的行为； 符号引用验证。这个验证会发生在虚拟机将符号引用转化为直接引用的时候，也就是在解析过程中的发生，可以看作是对类自身以外的信息进行匹配性的校验，确保解析动作能够正常执行。 验证的内容主要有： 符号引用中通过字符串描述的全限定名是否能找到对应的类； 在指定类中是否存在符号方法的字段描述及简单名称所描述的方法和字段； 符号引用中的类、字段和方法的访问性（private、protected、public、default）是否可被当前类访问。 3.3 准备 这一块我有一些不太理解，为什么在初始化阶段，字节码就被编译成指令存放在构造器\() 方法中了？？？？啥时候编译成指令的啊，这个构造方法又是什么时候产生的。。。。。 回答：编译成指令，是在源代码通过 javac 编译成 class 文件时生成的，在第六章有讲，在方法表中，有存放属性表，而属性表里有一个 Code 属性，里面就是存储的 Java 代码编译后的字节码指令。 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。（备注：这时候进行内存分配的仅包括类变量（被static修饰的变量），而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在Java堆中）。 初始值通常是数据类型的零值： 对于： 1public static int value = 123; 那么变量value在准备阶段过后的初始值为0而不是123，这时候尚未开始执行任何java方法，把value赋值为123的动作将在初始化阶段才会被执行。 一些特殊情况： 对于： 1public static final int value = 123; 编译时 Javac 将会为 value 生成 ConstantValue属性，在准备阶段 虚拟机 就会根据 ConstantValue 的设置将 value 赋值为123。 基本数据类型的零值： 3.4 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 那么符号引用与直接引用有什么关联呢？ 3.4.1 看两者的概念符号引用(Symbolic References)： 符号引用以一组符号来描述所引用的目标，符号可以是符合约定的任何形式的字面量，符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到内存中。 直接引用（Direct References）: 直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用与虚拟机实现的内存布局相关，引用的目标必定已经在内存中存在。 虚拟机规范没有规定解析阶段发生的具体时间，虚拟机实现可以根据需要来判断到底是在类被加载时解析「一般是静态方法和私有方法两大类，符合“编译期可知，运行期不可变”，因为这两类方法不可能通过继承或者其他方式进行重写。第八章有详细讲。」还是等到一个符号引用将要被使用前才去解析。 3.4.2 对解析结果进行缓存同一符号引用进行多次解析请求是很常见的，除 invokedynamic 指令以外，虚拟机实现可以对第一次解析结果进行缓存，来避免解析动作重复进行。无论是否真正执行了多次解析动作，虚拟机需要保证的是在同一个实体中，如果一个引用符号之前已经被成功解析过，那么后续的引用解析请求就应当一直成功；同样的，如果 第一次解析失败，那么其他指令对这个符号的解析请求也应该收到相同的异常。 3.4.3 解析动作的目标解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。前面四种引用的解析过程，对于后面三种，与JDK1.7新增的动态语言支持息息相关，由于java语言是一门静态类型语言，因此没有介绍 invokedynamic 指令的语义之前，没有办法将他们和现在的java语言对应上。 3.5 初始化类初始化阶段是类加载的最后一步，前面的类加载过程中，除了在加载阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的java程序代码（或者说是字节码）。 初始化阶段，主要还是去执行类构造器 \() 方法的过程。「啥时候有这个构造方法的书中说后面会说。。。。好的吧。。。。继续往下看」也就是进行第二次的赋值，其中 \() ： \() 是由编译器(javac???)自动收集类中的所有类变量的赋值动作和静态语句块(也就是所有 static 变量 和 静态语句块) 合并产生的，顺序按照先后顺序。同时注意哦，static 静态代码块，可以赋值后面定义的静态变量，不过貌似没毛用。。。。 12345678public class SuperClass &#123; static &#123; System.out.println("SubClass init!"); A = 2; // 反正它也不会被 used，因为会被覆盖掉 &#125; public static int A = 1;&#125; \() 这个方法很牛叉，为啥呢，因为它会默认先调用父类的 \() ，压根就不需要去显示的调用，所以第一个被执行的 \() 方法的类肯定是java.lang.Object； 既然父类要先执行，那肯定静态语句块肯定也是优先执行的，这个让我想到了之前的被动调用类的例子，如果子类调用父类的 static 变量，那么子类是不会被初始化的； \() 对于类或者接口不是必须的，如果没有 static 变量 和 静态语句块，那么就不会有这个方法，当然接口本身就不可能有 静态语句块，同时要注意的是，执行接口的 \() 方法不需要去先执行 父接口的 \() ，父接口只有被调用的时候才去执行，这点跟接口主动调用原则保持一致。 \() 线性安全，是通过加锁保持同步的。 4. 类加载器回顾一下，类加载的第一步 “加载” 总共进行了三个基本动作： 通过类型的完全限定名，产生一个代表该类型的二进制数据流（根本没有指明从哪里获取、怎样获取，可以说一个非常开放的平台了）； 解析这个二进制数据流为方法区内的运行时数据结构； 在 Java 堆中创建一个表示该类型的java.lang.Class类的实例，作为方法区这个类的各种数据的访问入口。 我们知道，第一个动作，很牛逼，只要得到类型的完全限定名，就能产生一个代表该类型的二进制数据流，那这个是如何做到的呢？就是通过类加载器，注意哦，这个是放到了 jvm 外部去实现的，也就是我们可以自己决定如何去获取需要的类，只要你把二进制数据流没毛病的转化到方法区的运行时数据结构即可。 4.1 类与类加载器对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性。如果两个类来源于同一个Class文件，只要加载它们的类加载器不同，那么这两个类就必定不相等。其实很容易理解，类加载器不同，那产生的二进制流必然不同。 4.2 类加载器介绍从 Java 虚拟机的角度分为两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），使用C++语言实现，是虚拟机自身的一部分； 其他类加载器。由Java语言实现，独立于虚拟机之外，并且全都继承自java.lang.ClassLoader类。（这里只限于HotSpot虚拟机）。 从 Java 开发人员的角度看，大部分Java程序都会使用到以下3种系统提供的类加载器： 启动类加载器（Bootstrap ClassLoader）： 这个类加载器负责将存放在&lt;JAVA_HOME&gt;\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如rt.jar，名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机内存中。 扩展类加载器（Extension ClassLoader）： 这个加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载&lt;JAVA_HOME&gt;\lib\ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）： 这个类加载器由sun.misc.Launcher$AppClassLoader实现。由于这个类加载器是ClassLoader中的getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 我们的应用程序都是由这3种类加载器互相配合进行加载的，如果有必要，还可以加入自己定义的类加载器。 4.3 双亲委派模型要求除了顶层的启动类加载器以外，所有的类加载器都应该有父类加载器，这里的父子关系不是通过继承来的，而是通过组合关系。 Tip: 组合和继承的区别与联系 继承与组合都是面向对象中代码复用的方式。 继承在编码过程中就要指定具体的父类，其关系在编译期就确定，而组合的关系一般在运行时确定。 继承强调的是 is-a 的关系，而组合强调的是has-a 的关系。 其实很好理解啦，继承就是要在类声明时显式 extends，而组合不用，组合可以简单的理解为在一个对象（类）中用到了另外一个对象（类）。 一般优先考虑组合，因为java里不允许多重继承。 哈哈哈，我又有问题了，为什么 java 不支持多重继承呢？ 我觉得主要是有一种情况很容易造成二义性，就是 A 是最顶层的，然后 B 和 C 继承了 A ，同时复写了 A 的 一个方法，如果 D 此时多重继承 B 和 C，那调用同样的方法的时候就会产生二义性了… 双亲委派模型的工作过程： 如果一个类加载器收到了类加载的请求，先把这个请求委派给父类加载器去完成（所以所有的加载请求最终都应该传送到顶层的启动类加载器中），只有当父加载器反馈自己无法完成加载请求(它的搜索范围没有找到所需要的类)时，子加载器才会尝试自己去加载。 使用双亲委派模型来组织类加载器之间的关系，有一个显而易见的好处就是java类随着它的类加载器一起具备了一种带有优先级的层次关系。 注意：双亲委派模型是 Java 设计者们推荐给开发者们的一种类加载器实现方式，并不是一个强制性 的约束模型。在 Java 的世界中大部分的类加载器都遵循这个模型，但也有例外。 4.4 破坏双亲委派模型三次大破坏。 第一次破坏是因为类加载器和抽象类java.lang.ClassLoader在JDK1.0就存在的，而双亲委派模型在JDK1.2之后才被引入，为了兼容已经存在的用户自定义类加载器，引入双亲委派模型时做了一定的妥协：在java.lang.ClassLoader中引入了一个findClass()方法，在此之前，用户去继承java.lang.Classloader的唯一目的就是重写loadClass()方法。JDK1.2之后不提倡用户去覆盖loadClass()方法，而是把自己的类加载逻辑写到findClass()方法中，如果loadClass()方法中如果父类加载失败，则会调用自己的findClass()方法来完成加载，这样就可以保证新写出来的类加载器是符合双亲委派模型规则的。 第二次破坏是因为模型自身的缺陷，现实中存在这样的场景：基础的类加载器需要求调用用户的代码，而基础的类加载器可能不认识用户的代码。为此，Java设计团队引入的设计是 “线程上下文类加载器（Thread Context ClassLoader）”。这样可以通过父类加载器请求子类加载器去完成类加载动作。已经违背了双亲委派模型的一般性原则。 第三次破坏 是由于用户对程序动态性的追求导致的。这里所说的动态性是指：“代码热替换”、“模块热部署”等等比较热门的词。说白了就是希望应用程序能够像我们的计算机外设一样，接上鼠标、U盘不用重启机器就能立即使用。OSGi是当前业界“事实上”的Java模块化标准，OSGi实现模块化热部署的关键是它自定义的类加载器机制的实现。每一个程序模块（OSGi中称为Bundle）都有一个自己的类加载器，当需要更换一个Bundle时，就把Bundle连同类加载器一起换掉以实现代码的热替换。在OSGi环境下，类加载器不再是双亲委派模型中的树状结构，而是进一步发展为更加复杂的网状结构。 「好叭，这一块我不太懂…」 第八章 虚拟机字节码执行引擎 1. 概述执行引擎是虚拟机中最核心的组成部分之一，“虚拟机”可以自行制定指令集与执行引擎的结构体系，并且能够执行那些不被硬件直接支持的指令集格式。 在 Java 虚拟机规范中，置顶了虚拟机字节码执行引擎的概念模型，这个概念模型成为了各个虚拟机执行引擎的统一外观，但是内部实现各不相同，有的是通过解释器解释执行，有的则是通过即时编译器(JIT)执行，但从外观上看，都是输入字节码文件，输出执行结果。 2. 运行时栈帧栈帧（Stack Frame）是用于支持虚拟机进行 方法调用和方法执行 的数据结构，它是虚拟机运行时数据区中的虚拟机栈的栈元素，每一个栈帧都包含了 局部变量表、操作数栈、动态连接、方法返回地址和一些额外的附加信息。栈帧的大小早就写入到了 class 文件中的 code 属性中了，并且对于执行引擎来说，只有栈顶的栈帧才是有效的，称为当前栈帧，这个栈帧所关联的方法称为当前方法。 2.1 局部变量表局部变量表是一组变量值存储空间，用于存放方法参数和方法内定义的局部变量。 当然，局部变量表的大小早在 Code 属性的 max_locals 数据项中就定义了。 局部变量表以 变量槽（Slot） 为最小单位，默认一个 Slot 为32位，如果是64位的数据结构会为其分配连续两个 Slot 空间。 对于局部变量表，有三点需要提醒： 方法执行时，虚拟机使用局部变量表完成从操作栈栈顶的参数值到参数变量列表的传递过程，如果是实例方法则第一个参数默认是对象实例的引用，也就是this，剩下的参数从 1 开始； Slot 是可以重用的； 局部变量并没有像类变量一样存在“准备阶段”(类加载的连接过程的第二步)，所以如果要使用局部变量肯定要先赋初值。 2.2 操作数栈操作数栈，也被称为操作栈，同局部变量表一样，容量也是在 Code 属性的 max_stacks 数据项之中早就写入了，同样的，栈中的基本单位也是 Slot(32位)，32位数据类型占栈容量为 1，64 位的占 2 ，方法的执行过程，就是入栈出栈的过程，所以，Java 虚拟机的解释执行引擎称为 “基于栈的执行引擎”。 另外，在概念模型中，两个栈帧作为虚拟机栈的元素，相互之间是完全独立的，但是在大多数虚拟机中会做出优化—— 令两个栈帧出现一部分重叠，这样在进行方法调用的时候就可以共用一部分数据，无需进行额外的参数复制传递了。 2.3 动态连接动态连接就是用来存储部分未转化为直接引用的符号引用，将其转化为直接引用。在之前的类加载过程中的解析部分我们谈到过，Class 文件的常量池存储了大量的符号引用，因为 java 不像 C++ 一样在编译之前有连接的过程，所以 java 在中间穿插了连接过程，第一次将符号引用转化为直接引用是在类加载中的解析，这种转化称为静态解析，第二次则是在每一次的运行期间转化为直接引用，称为 动态连接。 至于哪些符号引用是静态解析完成，哪些是动态连接，会在接下来的方法调用一节中谈到。 2.4 方法返回地址存储的就是 用来帮助恢复上层方法的执行状态 的一些信息，如果方法是正常退出的，那么一般会存储调用者（上层方法）的 PC 计数值，来作为返回地址。而方法异常退出的话，返回地址一般就是通过异常处理器表来确定了，这个时候，方法返回地址这一块一般不会保存任何信息。 这里提到了两种退出方式： 正常完成出口（Normal Method Invocation Completion，话说翻译不应该是正常的完成方法调用？？？）。此时退出方法的操作应该是：恢复上层方法的局部变量表和操作数栈，如果有返回值就压入调用者的栈帧中的操作数栈中，然后调整PC计数值。 异常完成出口（Abrupt Method Invocation Completion，我觉得翻译成异常的方法调用会更好…）。此时退出方法的操作应该和上者一样，当然了，这里肯定是不会有返回值的。 2.5 附加信息可以增加例如调试相关的信息，一般在实际开发中，会把 动态连接、方法返回地址和附加信息归为一类，称之为栈帧信息。 3. 方法调用方法调用的唯一任务就是把该方法调用的方法确定，暂时不涉及到方法内部的具体运行过程，为什么这个要单独拎出来讲呢？因为在程序运行过程中，进行方法调用时最普遍、最频繁的操作，并且由于动态连接的原因，所以方法调用还是挺复杂的… 3.1 解析之前在类加载过程中已经见过一次 解析 了，没错，这两个解析是一个意思。之前没具体讲的原因是，这里又要讲一遍… 在类加载的解析阶段，会将其中的一部分符号引用转换为直接引用，这部分要具备下面这个条件：方法在程序运行之前就有一个可确定的调用模板，并且这个方法的调用版本在运行期间是不可以改变的，也就是说必须在javac编译时就需要确定下来，符合“编译期可知，运行期不可变的要求”。这类方法调用就称之为解析。 符合这个要求的有： 静态方法，与类型直接相关，不会被重写； private修饰的方法，不会被外部访问而导致重写； 实例化构造器\方法，也同样符合“编译期可知，运行期不可变”，不会被重写； 父类方法。这个地方是难点，可能理解上会有点歧义，这里讲的是指在重写的方法中调用 super.方法名()，在这里调用父类的这个方法，可以确保不会被重写，是固定的调用了这个方法。如下例所示： 12345678910package JVM;public class SuperClass &#123; protected void A() &#123; System.out.println("aaaaaa"); &#125; protected void B() &#123; System.out.println("bbbbbb"); &#125;&#125; 1234567891011121314151617package JVM;public class SubClass extends SuperClass &#123; @Override public void A()&#123; super.A(); // invokespecial System.out.println(2222); &#125; public static void main(String[] args) &#123; test_heap_outOfMemory t = new test_heap_outOfMemory(); t.C(); SubClass subClass = new SubClass(); subClass.A(); // invokevirtual,因为可能被重写 // subClass.B() 也是 invokevirtual，因为也可能被重写 &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// class version 52.0 (52)// access flags 0x21public class JVM/SubClass extends JVM/SuperClass &#123; // compiled from: SubClass.java // access flags 0x1 public &lt;init&gt;()V L0 LINENUMBER 3 L0 ALOAD 0 INVOKESPECIAL JVM/SuperClass.&lt;init&gt; ()V RETURN L1 LOCALVARIABLE this LJVM/SubClass; L0 L1 0 MAXSTACK = 1 MAXLOCALS = 1 // access flags 0x1 public A()V L0 LINENUMBER 6 L0 ALOAD 0 INVOKESPECIAL JVM/SuperClass.A ()V L1 LINENUMBER 7 L1 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; SIPUSH 2222 INVOKEVIRTUAL java/io/PrintStream.println (I)V L2 LINENUMBER 8 L2 RETURN L3 LOCALVARIABLE this LJVM/SubClass; L0 L3 0 MAXSTACK = 2 MAXLOCALS = 1 // access flags 0x9 public static main([Ljava/lang/String;)V L0 LINENUMBER 11 L0 NEW JVM/test_heap_outOfMemory DUP INVOKESPECIAL JVM/test_heap_outOfMemory.&lt;init&gt; ()V ASTORE 1 L1 LINENUMBER 12 L1 ALOAD 1 INVOKEVIRTUAL JVM/test_heap_outOfMemory.C ()V L2 LINENUMBER 13 L2 NEW JVM/SubClass DUP INVOKESPECIAL JVM/SubClass.&lt;init&gt; ()V ASTORE 2 L3 LINENUMBER 14 L3 ALOAD 2 INVOKEVIRTUAL JVM/SubClass.A ()V L4 LINENUMBER 17 L4 RETURN L5 LOCALVARIABLE args [Ljava/lang/String; L0 L5 0 LOCALVARIABLE t LJVM/test_heap_outOfMemory; L1 L5 1 LOCALVARIABLE subClass LJVM/SubClass; L3 L5 2 MAXSTACK = 2 MAXLOCALS = 3&#125; 如上例所示，谈到的 invokespecial、invokestatic 就是解析阶段调用的指令，具体的调用字节码指令如下： invokestatic：调用静态方法； invokespecial：调用实例构造器方法 \、私有方法、父类方法； invokevirtual：调用所有的虚方法； invokeinterface：调用接口方法，会在运行时确认一个实现此接口的对象； invokedynamic：JDK1.7新加入的一个虚拟机指令，相比于之前的四条指令，他们的分派逻辑都是固化在JVM内部，而 invokedynamic 则用于处理新的方法分派：它允许应用级别的代码来确定执行哪一个方法调用，只有在调用要执行的时候，才会进行这种判断,从而达到动态语言的支持。(Invoke dynamic method) 只要被 invokestatic、invokespecial调用的方法都是可以在解析阶段确定唯一的调用版本的，所以他们在类加载的时候就会把符号引用转换为对该方法的直接引用，这四类方法（静态方法、实例构造器方法、私有方法、父类方法）统称为非虚方法「当然其实非虚方法，还有一类，那就是用 final 修饰的方法」，其他方法统称为虚方法（final 修饰的方法除外）。其实 final 修饰的方法，也是在编译期间就可以确定的，但是它比较特殊，并不是用的上述两个指令完成调用，而是使用了 invokevirtual… 至于为什么，我觉得这位朋友说的挺好的的： 作者：RednaxelaFX链接：https://www.zhihu.com/question/45131640/answer/98820081来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Q：为什么java中调用final方法是用invokevirtual指令而不是invokespecial? A： 不知道当时高司令和团队成员实际是出于什么原因这样设计的。这里我只能做些推断。 被final修饰的non-private方法，确实跟private方法一样可以静态判定调用目标。我们说这两种方法都可以被statically resolved。 它们最大的一个差别是： 一个private方法是只可能在一个类里声明并定义的，它不可能覆写（override）任何基类的方法。 而一个final non-private方法则可以覆写基类的虚方法，并且可以被基类引用通过invokevirtual调用到。 前者很明显，举例说明一下后者。 1234567891011121314151617181920&gt; &gt; class Base &#123;&gt; void foo() &#123; System.out.println("Base"); &#125;&gt; &#125;&gt; &gt; class Derived extends Base &#123;&gt; @Override&gt; final void foo() &#123; System.out.println("Derived"); &#125;&gt; &#125;&gt; &gt; public class Test &#123;&gt; public static void main(String[] args) &#123;&gt; Derived d = new Derived();&gt; d.foo(); // (1) 题主的问题是为什么这个情况下仍然用invokevirtual&gt; Base b = d; // (2) Base类型引用指向Derived类型实例&gt; b.foo(); // 通过invokevirtual调用到final Derived.foo()&gt; &#125;&gt; &#125;&gt; &gt; 所以，原始设计者或许是出于一致性的考虑，选择让这些final方法也用invokevirtual来调用，而不是用invokespecial。另外正如评论区所说，这一致性也带来了更好的二进制兼容性——如果上例中Test与Derived分离编译，在Test编译时Derived.foo()是final，而后来Derived去掉了foo()的final并且单独重新编译了的话，已编译的Test代码仍然可以正确执行——甚至如果Derived进一步有子类覆写了foo()也没问题。 二进制兼容性（binary compatibility）和分离编译（separate compilation）可是Java的卖点，然而同时也是毒瘤…（叹气 关于性能 事实上此处在Class文件里使用invokevirtual来实现对final方法的调用并不会影响一个实现得好的JVM的性能。HotSpot VM会对上述(1)的情况在解释器里可以把invokevirtual改写为一个行为跟invokespecial相似的内部字节码指令，叫做fast_invokevfinal，不需要查vtable就可以调用到目标方法（不过目前在x86上HotSpot并没有使用这个优化）。SPARC上的例子看这里：jdk8u/jdk8u/hotspot: d109bda16490 src/cpu/sparc/vm/templateTable_sparc.cpp而在JIT编译器里这种invokevirtual也会被看作跟invokespecial一样来处理。一个简单的例子可以看C1这里：jdk8u/jdk8u/hotspot: d109bda16490 src/share/vm/c1/c1_GraphBuilder.cpp 静态解析的方法调用称为解析调用，还有一种调用方式，称为 分派调用（Dispatch） 3.2 分派调用即将要讲的分派调用，其实囊括了解析调用的步骤，解析与分派并不是排他互斥的关系，而是不同层次上筛选和确定目标方法的过程，二者是联合起来一起使用的，这里的分派更多的是揭示多态性的特征。「好叭，这一块其实我是有点不赞同的，我觉得就是有点互斥的感觉啊，一个是用在非虚函数，一个用在了虚函数。」 3.2.1 静态分派 — 重载也就是我们常说的多态中的重载过程。虚拟机（准确的说是编译期）在重载时是通过参数的静态类型而不是实际类型作为判定依据的。并且由于静态类型在编译期间就可知，所以在编译期间，javac 编译期就根据参数的静态类型决定了使用哪个重载版本。 所有依赖静态类型来定位方法执行版本的分派动作，就称为静态分派，最典型的就是方法重载，因此确定静态分派的动作实际上并不是由jvm来完成的。另外，编译期虽然能够确定出方法的重载版本，但是在很多情况下这个重载版本并不是“唯一的”，往往只能去确定一个“更加合适的”版本，甚至有的时候会因为类型不明确，选择过多（实现的接口很多），而导致报错。 当然，这里得明确一下静态类型和实际类型的概念。 Human human = new Man() 固定对象.方法(human) 静态类型：变量 human 的静态类型为 Human 动态类型：变量 human 的动态类型为 Man 简单记忆，重载就是同一个类下同名方法参数不同，所以决定权在参数，如果参数个数相同，那决定权就是参数的类型，而参数的类型在编译期间就已经确定了，是由其静态类型决定的。 3.2.2 动态分派 — 重写重写跟重载很不一样，重写是类对接口或父类方法的重写，这个时候对象都不一样了，所以会用到实际类型。 People human = new Human(); human.方法() 简单记忆，重写就是一个类覆盖了接口或者父类的方法，此时的变量是一个对象，不是上文的参数，所以此时肯定就是由动态类型决定了。 3.2.3 单分派和多分派宗量：方法的接收者与方法的参数统称为方法的宗量。 静态分派是多分派类型，动态分派是单分派类型…「好绕啊…不太懂…」 总而言之，静态多分派、动态单分派。。。。 3.2.4 动态单分派的实现优化由于动态单分派是非常频繁的动作，所以基于性能的考虑，以空间换时间，最常用的“稳定优化”手段就是在方法区中建立一个 虚方法表。如果子类重写了父类方法，则两者对象的方法的入口地址不一样，如果子类没有重写，则都指向父类的方法的入口地址。 除了建立虚方法表，还会使用 “内联缓存” 和 基于 “类型继承关系分析” 技术的守护内联两种非稳定的优化方式。 4. 基于栈的字节码解释执行引擎上一节讲完了如何调用方法，这一节讲如何执行方法内的字节码指令。执行方法内的字节码指令有两种方法：通过解释器解释执行 和 通过 JIT 等即时编译器编译执行，下面讲一下第一种：基于栈的字节码解释执行。 Java 编译器输出的指令流，基本上就是一种基于栈的指令集架构，指令流里面的指令大部分都是零地址指令，因为都依赖操作数栈进行工作。而主流的基于寄存器的指令集一般都是二地址指令集。 基于栈的指令集最大优点就是可移植性，编译器实现更简单、代码更紧凑…但是缺点就是慢,而且要频繁的入栈出栈，意味着频繁访问内存，速度就下来了，虽然可以采用栈顶缓存，但是毕竟治根不治本… 第九章 类加载及执行子系统的案例与实战在 Class 文件格式和执行引擎操作这部分里，基本都是有虚拟机直接控制，用户程序无法对其进行改变。能通过程序进行操作的，主要是字节码生成与类加载器这两部分的功能。 类加载器的增强 主要是 Tomcat服务器的类加载器 和 OSGi 这个灵活的类加载器架构 字节码生成技术与动态代理的实现 包括 javac、AOP、动态代理、反射等都用到了字节码生成技术。 第十章 早期（编译期）优化1. 概述编译期，在 java 里分为三类， 一类是从 .java 文件 转变为 .class 文件的过程，这是前端编译的过程，我们之前提到的编译器都是指这个。使用到的前端编译器一般就是 javac。 还有指虚拟机的后端运行期编译，把字节码转变成机器码的过程。使用到的后端运行期编译器有 JIT编译器（Just In Time Compiler）。 还有一类很少见，称为静态提前编译器（AOT编译器，Ahead Of Time Compiler）直接把 *.java 文件编译成本地机器代码的过程。 对于 Javac 编译器来说，对代码的运行效率没有优化措施，但是对程序员的编码风格和编码效率做了很多优化，对于 JIT 来说，对程序优化做了很多优化。 2. Javac 编译器2.1 编译过程编译过程分为三个过程： 解析与填充符号表的过程； 插入式注解处理器的注解处理过程； 语义分析与字节码生成过程。 2.2 解析与填充符号表解析步骤 由 parseFiles() 方法完成，包括了词法分析、语法分析； 词法分析就是将源代码的字符流转变为标记（Token）集合，词法分析过程由 com.sun.tools.javac.parser.Scanner 类实现； 语法分析就是根据 Token 序列来构造抽象语法树的过程，抽象语法树（AST，Abstract Syntax Tree）是一种用来描述程序代码语法结构的树形表示方法。 填充符号表符号表（Symbol Table）是一组由符号地址和符号信息构成的表格，类似于键值对形式。 符号表中所登记的信息在编译的不同阶段都要用到。在语义分析中，符号表所登记的内容将用于语义检查和产生中间代码。在目标代码生成阶段，当对符号表进行地址分配时，符号表是地址分配的依据。由 com.sun.tools.javac.comp.Enter 类实现。 2.3 注解处理器类似于编译器的插件。插入式注解处理器的初始过程是在 initPorcessAnnotations() 方法中完成的，而执行过程是在 processAnnotations() 方法中完成的。 2.4 语义分析与字节码生成语法分析之后，编译器获得了程序代码的抽象语法树，语法树能表示一个结构正确的源程序的抽象，但是却无法保证源程序是符合逻辑的。而语义分析的主要任务就是对结构上正确的源程序进行上下文有关性质的审查。 语义分析的具体步骤如下： 标记检查：检查诸如变量使用前是否已经被声明、变量与赋值之间的数据类型能够匹配等等； 数据及控制流分析：四队上下文逻辑更进一步的验证，可以监察处诸如程序局部变量在使用前是否有赋值、方法的每条路径是否都有返回值、是否所有的受查异常都被正确处理了等问题。 解语法糖：语法糖，也称糖衣语法，指的是在计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是会更方便程序员使用。通常来说语法糖能够增加程序的可读性，减少程序出错的机会。由 desugar() 方法触发。 字节码生成，这是编译过程的最后一个阶段，把前面各个步骤生成的信息（语法树、符号表）转换成字节码写到磁盘去，还进行了少量的代码添加和转换工作，例如之前一直没有解决的 类构造器 \ 还要 实例构造器 \方法「这里的实例构造器不是默认的构造器」就是在这个阶段被添加到语法树之中去的。 3. Java 语法糖的味道3.1 泛型本质是参数化类型，也就是说所操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口、方法的创建中，分为泛型类、泛型接口、泛型方法。 java 里面的泛型是伪泛型，并不是像 C++ 一样真正的泛型，Java 里面的泛型实现方式称为 类型擦除。 12345678910import java.util.List;public class Test &#123; public static void method(List&lt;Integer&gt; list)&#123; System.out.println("Integer 的泛型"); &#125; public static void method(List&lt;String&gt; list)&#123; System.out.println("String 的泛型"); &#125;&#125; 这样的写法在 idea 中并不能通过，会提示两个方法一样，无法重载，原因就是在编译时就会将两者的泛型擦除。 3.2 自动装箱、拆箱、遍历循环123456789101112131415161718192021222324252627282930313233import java.util.Arrays;import java.util.Iterator;import java.util.List;/** * 自动装箱、拆箱、遍历循环 */public class Test &#123; public static void main(String[] args) &#123; // 自动装箱、拆箱、遍历循环的语法糖 List&lt;Integer&gt; list1 = Arrays.asList(1,2,3,4); // 或者写成 List&lt;Integer&gt; list = [1,2,3,4]; int sum1 = 0; for(int i : list1)&#123; sum1 = sum1 + i; &#125; System.out.println(sum1); // 编译之后的原样 List list2 = Arrays.asList(new Integer[]&#123; Integer.valueOf(1), Integer.valueOf(2), Integer.valueOf(3), Integer.valueOf(4), &#125;); int sum2 = 0; Iterator localIterator = list2.iterator(); while(localIterator.hasNext())&#123; sum2 = sum2 + ((Integer) localIterator.next()).intValue(); &#125; System.out.println(sum2); &#125;&#125; 123456789101112131415161718192021/** * 自动装箱的陷阱 */class Test2&#123; public static void main(String[] args) &#123; Integer a = 1; Integer b = 2; Integer c = 3; Integer d = 3; Integer e = 321; Integer f = 321; Long g = 3L; System.out.println(c == d); // true System.out.println(e == f); // false System.out.println(c == (a + b)); // true System.out.println(c.equals(a + b)); // true// System.out.println(g == d); // 报错，因为 == 不会自动拆箱，只有遇到了算术运算才会拆箱 System.out.println(g == (a + b)); // true System.out.println(g.equals(a + b)); // false，虽然 equals()会自动拆箱，但是不会处理数据转型的问题 &#125;&#125; Tip: Integer 的缓存问题，之前刷 LeetCode 就已经遇到过了。这里再提及一次！ 具体的 Integer 和 int 的区别见 “零碎知识点” 这篇文章！ 第十一章 晚期（运行期）优化1. 概述即时编译器（Just In Time Compiler），也称为 JIT 编译器，它的主要工作是把热点代码编译成与本地平台相关的机器码，并进行各种层次的优化，从而提高代码执行的效率。它并不是必需的。 那么什么是热点代码呢？我们知道虚拟机通过解释器（Interpreter）来执行字节码文件，当虚拟机发现某个方法或代码块的运行特别频繁时，就会把这些代码认定为“热点代码”（Hot Spot Code）。 即时编译器编译性能的好坏、代码优化程度的高低是衡量一款商用虚拟机优秀与否的关键指标之一，它也是虚拟机最核心且最能体现技术水平的部分。 2. HotSpot虚拟机内的即时编译器2.1 解释器与编译器HotSpot 虚拟机包含解释器和编译器。它们是怎么搭配工作的呢？当程序启动的时候，解释器首先发挥作用，它能直接运行字节码文件；随着时间的推移，越来越多的热点代码被编译器编译成机器码，从而获取更高的执行效率。同时，解释器还可以作为编译器激进优化时的一个“逃生门”，当编译器的激进优化手段不成立时，如加载了新类后类型继承结构出现变化等，可以通过逆优化（Deoptimization）退回到解释状态继续由解释器执行。解释器与编译器的交互如图所示： HotSpot中的编译器又分为两种，C1 编译器（Client Compiler）和 C2 编译器（Server Compiler），HotSpot 虚拟机会选择哪个编译器是由虚拟机运行于 Client 模式还是 Server 模式决定的。 默认情况下，虚拟机采用解释器和一种编译器搭配的方式工作，但是在分层编译策略下，C1 编译器和 C2 编译器将会同时工作，分层编译根据编译器编译、优化的规模和耗时，划分出不同的编译层次： 第0层：程序解释执行，解释器不开启性能监控功能，触发 C1 编译。 第1层：C1 编译，将字节码编译成本地代码，进行简单、可靠的优化，如有必要解释器将开始性能监控。 第2层：C2 编译，将字节码编译成本地代码，启用一些编译耗时较长的优化，甚至会根据性能监控信息进行一些不可靠的激进优化。 tips： 使用 “-client” 强制虚拟机运行于 Client 模式。 使用 “-server” 强制虚拟机运行于 Server 模式。 使用 “-Xint” 强制虚拟机只使用解释器执行程序，编译器不工作。 使用 “-Xcomp” 强制虚拟机只使用编译器执行程序，解释器作为编译器的“逃生门”。 使用 “-XX:+TieredCompilation” 开启分层编译。虚拟机 Server 模式下默认开启。 2.2 编译对象与触发条件“热点代码” 分为两类： 被多次调用的方法；「普通的编译请求」「方法调用计数器」 被多次执行的循环体。「OSR编译请求」「回边计数器」 要测试其成为“热点代码”，有两种比较主流的热点探测方式： 基于采样的热点探测，周期性探测各个线程栈顶，虽然简单高效，但是容易受到线程阻塞的影响而扰乱探测； 基于计数器的热点探测，为每个方法（甚至是代码块）建立计数器，统计执行次数，如果执行次数达到一定的阈值，就把这部分代码编译成机器码。 HotSpot 采用的是基于计数器的热点探测，为每个方法准备了两个计数器：方法调用计数器（Invocation Counter）和 回边计数器（Back Edge Counter）。 在默认设置下，方法计数器统计的并不是方法被调用的绝对次数，而是一定时间内的执行次数，超过了时间如果还没有达到阈值，就会将方法计数器的值减去一半，这个过程称为 方法计数器的 热度衰减（Counter Decay），而这段时间就称为此方法统计的半衰周期（Counter Half Life Time），进行热度衰减的动作实在虚拟机进行垃圾收集顺带进行的。而回边计数器适用于统计方法中循环体代码的执行次数，之所以叫“回边”，可以理解为一个循环体结束一次又回到开头（死空循环不算回边），显然建立回边计数器统计的目的是为了触发OSR（On Stack Replacement，栈上替换）编译。回边计数器没有计数热度衰减的过程，统计的就是方法体内循环体执行的绝对次数。 3. 优化技术3.1 方法内联方法内联的重要性要优于其他优化措施，它的主要目的有两个，一是去除方法调用的成本，二是为其他优化建立良好的基础。 方法内联的行为很简单，就是把目标方法的代码“复制”到发起调用的方法之中，避免发生真实的方法调用而已。在分派调用中讲过。 3.2 公共子表达式消除如果一个表达式 E 已经计算过了，并且从先前的计算到现在 E 中所有变量的值都没有发生变化，那么 E 的这次出现就成为了公共子表达式。对于这种表达式，没有必要花时间再对它进行计算，只需要直接用前面计算过的表达式结果代替 E 就可以了。我们来举个例子来模拟下它的优化过程： 1234567891011public static void main(String[] args) &#123; int a = 1; int b = 1; int c = 1; int d = (c * b) * 12 + a + (a + b * c); // 1. 提取公共子表达式 int E = c * b; d = E * 12 + a + (a + E); // 2. 代数化简 d = E * 13 + a * 2;&#125; 3.3 数组边界检查消除当我们尝试对数组越界访问的时候，Java 会向我们抛一个 java.lang.ArrayIndexOutOfBoundsException，这对软件开发者来说是一件很好的事情，即使没有专门编写防御代码，也可以避免大部分的溢出攻击，但是对虚拟机来说，意味着每一次的数组访问都带有一次隐含的条件判定操作，即数组边界检查，那么有没有办法消除这种检查呢？ 虚拟机一般是在即时编译期间通过数据流分析来确定是否可以消除这种检查，比如 foo[3] 的访问，只有在编译的时候确定 3 不会超过 foo.length - 1 的值，就可以判断该次数组访问没有越界，就可以把数组边界检查消除。 3.4 逃逸分析逃逸分析的基本行为就是分析对象动态作用域：当一个对象在方法被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他方法中，称为方法逃逸；甚至还有可能被外部线程访问到，譬如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸。 如果能证明一个对象不会逃逸到方法或者线程之外，则可以为这个变量进行一些高效的优化： 1) 栈上分配如果确定一个对象不会逃逸出方法之外，假如能使用栈上分配这个对象，那大量的对象就会随着方法的结束而自动销毁了，垃圾收集系统的压力将会小很多。然而遗憾的是，目前的 HotSpot 虚拟机还没有实现这项优化。 2）同步消除如果确定一个对象不会被其他线程访问到，那么这个变量就不存在线程间的争抢，对这个变量实施的同步措施也可以消除掉。 3）标量替换标量：无法被进一步分解的数据，比如原始数据类型（int、long以及 reference 类型）聚合量：可以被持续分解的数据，典型的就是 Java 中对象，它们还可以被分解成成员变量等。 标量替换指的是如果把一个 Java 对象拆散分解，根据程序访问的情况，将其使用到的成员变量恢复到原始类型来访问。 如果能确定一个对象不会被外部访问，并且这个对象可以被拆散的话，那程序真正执行的时候就可能不创建这个对象，而改为直接创建它的若干个被这个方法使用到的成员变量来代替。 tips： -XX:+DoEscapeAnalysis 手动开启/关闭逃逸分析，默认开启，C2 编译器有效 -XX:+PrintEscapeAnalysis 查看逃逸分析的结果（debug 虚拟机支持） -XX:+EliminateAllocations 手动开启/关闭标量替换，默认开启 -XX:+PrintEliminateAllocations 查看标量替换情况（debug 虚拟机支持） -XX:+EliminateLocks 手动开启/关闭同步消除，默认开启 第十二章 Java内存模型与线程1. 硬件的效率与一致性虚拟机与物理计算机的并发有一些相似之处。 1.1 缓存一致性协议为了解决多个处理器的运算任务可能涉及到同一块主内存，而他们的缓存数据又不一致的问题，引入了缓存一致性协议，volatile 就是用到了 MESI 这个缓存一致性协议。 1.2 指令重排序优化为了保证处理器利用率最大化，处理器会对指令重排序，而 jvm 中的 JIT 编译器 也同样如此，有类似的指令重排序优化，在 volatile 中为了保证 happens-before 是插入 内存屏障 防止 指令重排序的。 2. Java 内存模型java 虚拟机规范中试图定义一种 Java 内存模型（Java Memory Model，JMM）来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的并发效果。 2.1 主内存和工作内存Java 内存模型规定所有的共享变量都应该存储在主内存中，每条线程有自己的工作内存，工作内存中保存了线程独有的变量，线程之间是隔离的，值传递必须通过主内存操作。 2.2 内存间的交互操作关于主内存和工作内存如何交互，JMM 定义了8种原子操作： lock：作用于主内存的变量，将变量标识为线程独占的状态。类似独占锁。 unlock：作用于主内存的变量，将变量的锁释放。 read：作用于主内存变量，将变量从主内存读到工作内存中。 load：作用于工作内存变量，read操作完后需要load来复制一份变量副本放到工作内存中。 use：作用于工作内存变量，把工作内存中的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量值的字节码指令时就会执行这个操作。 assign：作用于工作内存变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令就执行这个操作。 store：作用于工作内存的变量，把一个工作内存中的变量传送到主内存中，以便随后的 write 使用。 write：作用于主内存的变量，把 store 操作得到的变量写入到主内存中的变量。 2.3 对于 volatile 型变量的特殊规则Volatile 只具有可见性和有序性，并不能保证原子性。 在以下两条规则的运算场景下，volatile 才具有原子性： 运算结果不依赖于变量的当前值，或者能够确保只有单一的线程修改变量的值； 变量不需要与其他的状态变量共同参与不变约束。 可见性，是指一条线程修改了当前变量的值，其他线程能立即感知到这个新值。而普通变量必须通过主内存变量传递来完成。 有序性，是指禁止了指令重排序优化，普通变量仅仅保证在该方法的执行过程中所有依赖赋值的结果都能够得到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执行顺序一致。volatile通过插入内存屏障，保证了有序性，但是这就带来了volatile的写操作会比较慢一些。 有个典型的例子就是单例模式下的双重检验锁： 1234567891011121314public class Singleton &#123; private volatile static Singleton instance; private Singleton () &#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 称其为双重检查锁，是因为会有两次检查 instance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了 而 instance = new Singleton() 这句，并非是一个原子操作，事实上在 JVM 中这句话做了下面 3 件事： 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将 instance 对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错 所以需要将 instance 变量声明成 volatile。 2.4 对于 long 和 double 型变量的特殊规则JMM 要求lock、unlock、read、load、use、assign、store、write 这 8 个操作具有原子性，但是对于 64 位的数据类型（long 和 double），有一条宽松的规定：允许虚拟机将没有被 volatile 修饰的 64 位数据的读写操作划分为两次 32 位的操作来进行，即允许虚拟机实现选择可以不保证 64 位数据类型的 load、store、read、write 这四个操作的原子性，这点就是所谓的 long 和 double 的非原子性协定，所以可能多个线程共享一个未声明为 volatile 的 long 或者 double 变量时，读取到的值会很奇怪，不过现在基本所有商用的虚拟机都实现了 64 位数据的读写操作作为原子操作来对待。 2.5 原子性、可见性、有序性JMM 的三个特征：原子性、可见性、有序性。 原子性：有 JMM 直接保证的原子性变量操作包括 read、load、use、assign、store、write，long 和 double 的非原子性协定例外…如果想要更大范围的原子性保证，有 lock 和 lock 保证，底层提供了 monitorenter、monitorexit 两个字节码指令保证具有原子性。 可见性：是指一条线程修改了当前变量的值，其他线程能立即感知到这个新值。而普通变量必须通过主内存变量传递来完成。除了 volatile，还可以实现可见性的关键字有 synchronized 和 final。 volatile 实现可见性的原因就是遵循了 MESI 缓存一致性协议，可以保证变量修改完后立即同步到主内存，而其他的线程中的工作内存的变量值作废，需要从主内存再重新读取。 Synchronized 实现可见性的原因是 “对一个变量执行 unlock 操作之前，会确保先把该变量同步回主内存中（执行 store 和 write 指令）”。 final 实现可见性的原因是被 final 修饰好的字段在构造器中一旦被初始化完成，其他线程就能看见 final 字段的值，并且无法修改，所以就能保证可见性了。 有序性：如果在本线程观察，所有的操作都是有序的；如果在一个线程中观察另外一个线程，所有的操作都是无序的。前半句是指“线程内表现为串行的语义”，后半句是指“指令重排序”和“工作内存与主内存同步延迟”现象，Java 提供了 volatile 和 synchronized 两个关键字保证线程的有序性。volatile 关键字本身就包含了禁止指令重排序的语义，而 synchronized 则是由“一个变量在同一时刻只允许一条线程对其进行加锁操作”这条规则获得的。 2.6 先行发生（happens-before）原则happens-before 原则非常重要，它是判断是否存在竞争，线程是否安全的主要依据。指令重排序的前提是要遵守 happens-before 原则。 先行发生是 JMM 中定义的两项操作之间的偏序关系，如果说 A 先行发生于 B，其实就是说在操作 B 之前，操作 A 产生的影响能够被操作 B 观察到。「Tip：时间上的先后顺序和先行发生原则没有太大的关系，所以在衡量安全问题的时候需要以 happens-before 原则为准」 天然的 happens-before原则有： 程序次序规则，一个线程内的代码自上而下运行； 管程锁定规则，unlock必须在lock之后，当然肯定操作的是同一个同步对象； volatile变量规则，对一个 volatile 变量的写操作总是先行发生于后面对这个变量的读操作； 线程启动规则，任何线程的运作都必须在 start 之后； 线程终止规则，任何线程的结束都必须在该线程任何操作之后； 线程中断规则，interrupt() 调用之后才会有中断时间的发生； 对象终结规则，构造函数先于 finalize() 发生； 传递性， A 先于 B，B 先于 C，则 A 先于 C。 3. Java 与线程3.1 线程的实现实现线程主要有三种方式： 使用内核线程实现； 使用用户线程实现； 使用用户线程加上轻量级进程混合实现。 3.1.1 使用内核线程实现内核线程（Kernel Thread，KLT）就是直接由操作系统内核支持的线程，这种线程由内核来完成线程切换，内核通过操纵调度器来对线程进行调度。而程序一般不会使用内核线程，会通过一个中间件—- 轻量级进程，来和内核线程1：1映射。 3.1.2 使用用户线程实现用户线程就是系统内核感受不到的线程，所以用户的建立、同步、销毁和调度完全在用户态中完成，不需要内核的帮助。但是这个实现起来很困难，基本都放弃使用了。进程和内核线程 1：N。 3.1.3 混合使用多对多的线程模型，轻量级进程和用户线程 M ：N。 3.1.4 Java 线程的实现我们使用的 jdk 是采用的内核线程实现的，轻量级进程和内核线程1：1。 3.2 Java 线程调度线程调度是指系统为线程分配处理器使用权的过程，主要的调度方式有两种，分别是 协同式（Cooperative Threads-Scheduling）线程调度 和 抢占式（Preemptive Threads-Scheduling）线程调度。 Java 使用的当然是抢占式线程调度，因为协同式线程调度虽然实现简单但是线程执行时间不可控，如果线程编写有问题不通知系统进行系统切换那么程序就会一直堵塞。虽然说 Java 线程调度是系统自动完成的，但是我们可以设置线程优先级，优先级越高越容易被系统选择，不过其实线程优先级也不靠谱，因为 java 的线程是被映射到系统的原生线程上实现，所以最终的调度还是 os 说了算。 3.3 状态转换Java 的线程有 5 种状态： 新建（New）:创建后尚未启动的线程处于这种状态 ； 运行（Runnable）：包括了操作系统线程状态中的 Running 和 Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待 cpu 分配时间片； 等待（Waiting）：处于这种状态的进程不会被分配 cpu 执行时间，它们要让线程显示的唤醒。以下方法会让线程进入到这个状态： Object.wait() 「可设置时间」 Thread.join() 「可设置时间」 LockSupport.park()、LockSupport.parkNanos()、LockSupport.parkUntil() 阻塞（Blocked）：与等待状态的区别是，“阻塞状态”在等待获取到一个排它锁，这个时间将在另外一个线程放弃这个锁的时候方式，而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在 synchronized 的时候，线程进入同步区域，就会进入阻塞状态； 结束（Terminated）：已终止线程的线程状态，线程已经结束执行。 第十三章 线程安全与锁优化1. 线程安全线程安全的定义： 当多个线程访问一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替执行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，（单次）调用这个对象的行为就能够获取到正确的结果，那么这个对象就是线程安全的。 按照线程安全的“安全程度”，由强至弱来排序，可以将 Java 语言中各种操作共享的数据分为以下五类：不可变、绝对线程安全、相对线程安全、线程兼容和线程独立。 1.1 不可变即可看不可改的数据，final 修饰。 1.2 绝对线程安全集合中的 Vector、ConcurrentHashMap并非绝对的线程安全，因为如果不使用迭代器去删除元素的话，遍历的时候依然会出现问题。 1.3 相对的线程安全单次调用不需要额外的加同步措施，这就是相对的线程安全，类似于 HasTable、Vector、Collections 的 synchronizedCollection()等等。 1.4 线程兼容线程兼容是指对象本身不是线程安全的，但是可以通过调用端正确的使用同步手段来保证对象在并发环境中安全的使用，如 ArrayList、HashMap。 1.5 线程对立线程对立是不管调用端是否采取了同步措施，都无法在多线程环境下并发使用的代码。这种代码就不要写了…比如 System.setIn()、System.setOut()。 2. 实现线程安全的方法2.1 互斥同步最基本的手段有 synchronized、ReentrantLock。这里也提到了 ReentrantLock几个特性： 有等待可中断，当持有锁长期不释放时，等待的线程可以选择放弃等待，改为处理其他事情； 有公平锁和非公平锁的选择； 可以绑定多个Condition，也就是可以有多个条件队列。 2.2 非阻塞同步基于冲突检测的乐观并发策略，通俗的来说就是先进行操作，如果没问题就成功了，如果有其他线程争用数据，产生了冲突，就再进行其他措施，这种乐观的并发策略不需要将线程挂起，因此这种同步操作称为非阻塞同步。 2.3 无同步方案ThreadLocal —- 见我的 多线程（四）—- ThreadLocal 一文 3. 锁优化 自旋锁和自适应自旋锁； 锁消除。可以不用用到锁的地方，可以不加锁； 锁粗化。如果有一系列操作反复的加锁和解锁，就会锁同步范围扩大； 轻量级锁。本意是在没有多线程竞争的前提下，比如两个线程交替执行，不会发生竞争，此时就可以使用轻量级锁。 偏向锁。可以提高带有同步但无竞争的程序性能，适合单线程。如果变量总是被多个线程访问，那么偏向模式是没有意义的并且会浪费开销和时间。 要理解轻量级锁和偏向锁，就需要谈到虚拟机中对象的内存布局。虚拟机中的对象头由两部分组成，第一部分是用来存储对象自身的运行时数据，例如哈希码（HashCode）、GC 分代年龄（GC Age）等等，一般是64位，官方称之为 Mark Word，它是实现轻量级锁和偏向锁的关键，对象头除了 Mark Word，还有一部分是用来存储指向方法区对象类型数据的指针，如果是数组对象的话，还有额外的一部分用来存储数组长度。 如图所示，是 32 位的虚拟机的 Mark Word。 现在讲一下 synchronized 的锁升级过程： 最开始的对象是无锁模式，Mark Word 的前 25 位记录对象的HashCode，锁标志位是01，是否偏向锁那一位是0。 当对象被当做同步锁并有一个线程A抢到了锁时，锁标志位还是01，但是否偏向锁那一位改成1，前23bit记录抢到锁的线程id，剩下的 2 位 是 epoch，代表偏向的时间戳，表示进入偏向锁状态。 当线程A再次试图来获得锁时，JVM发现同步锁对象的标志位是01，是否偏向锁是1，也就是偏向状态，Mark Word中记录的线程id就是线程A自己的id，表示线程A已经获得了这个偏向锁，可以执行同步锁的代码。 当线程B试图获得这个锁时，JVM发现同步锁处于偏向状态，但是Mark Word中的线程id记录的不是B，那么线程B会先用CAS操作试图获得锁，这里的获得锁操作是有可能成功的，因为线程A一般不会自动释放偏向锁。如果抢锁成功，就把Mark Word里的线程id改为线程B的id，代表线程B获得了这个偏向锁，可以执行同步锁代码。如果抢锁失败，则继续执行步骤5。 偏向锁状态抢锁失败，代表当前锁有一定的竞争，偏向锁将升级为轻量级锁。JVM会在当前线程的线程栈中开辟一块单独的空间，称之为 Lock Record，里面保存指向对象锁Mark Word的副本，同时在对象锁Mark Word中保存指向这片空间的指针。上述两个保存操作都是CAS操作，如果保存成功，代表线程抢到了同步锁，就把Mark Word中的锁标志位改成00，可以执行同步锁代码。如果保存失败，表示抢锁失败，竞争太激烈，继续执行步骤6。 轻量级锁抢锁失败，JVM会使用自旋锁，自旋锁不是一个锁状态，只是代表不断的重试，尝试抢锁。从JDK1.7开始，自旋锁默认启用，自旋次数由JVM决定。如果抢锁成功则执行同步锁代码，解轻量级锁锁也是用 CAS，将栈帧上的Lock Record替换回去，替换成刚就同步完成了。如果失败则继续执行步骤7。 自旋锁重试之后如果抢锁依然失败，同步锁会升级至重量级锁，锁标志位改为10。在这个状态下，未抢到锁的线程都会被阻塞。 https://blog.csdn.net/lkforce/article/details/81128115 总结至此，周志华先生的这本《深入理解 Java 虚拟机》第三版 就已经全部复习完一遍了，收获很多，大概满打满算花了 3 天「2020.03.05 ~ 2020.03.08」的时间啃完，接下来还得再好好看看，有些地方有点囫囵吞枣，比如说第六章的 Class 文件结构，并没有自己去利用 idea 中的 Jclasslib 认真去分析，还有很多地方虽然都是自己敲字敲出来的，但是敲出来了就给忘了…时间还是太仓促了…总而言之，继续加油吧！]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>jvm笔记</tag>
        <tag>oom</tag>
        <tag>垃圾回收</tag>
        <tag>Class 文件组成</tag>
        <tag>类加载</tag>
        <tag>字节码执行引擎</tag>
        <tag>Java 内存模型</tag>
        <tag>线程安全</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[十大内部排序]]></title>
    <url>%2F2019%2F10%2F27%2F%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F.html</url>
    <content type="text"><![CDATA[前言最近一直比较忙，马上就要考试了，所以LeetCode稍微暂时放了放，集中精力回顾了下内部排序算法，本篇文章会对常见的十大内部排序算法进行系统的回顾。 概述算法分类常见排序算法可以分为两大类： 比较排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 算法复杂度 在这里要说明的是，这个图里的快排空间复杂度错了，快排的平均空间复杂度应该是 O(logn)，就是栈的深度，但是最差的情况下，比如全是倒序，则空间复杂度为栈深度 O(n)。归并排序每次递归需要用到一个辅助表，长度与待排序的表相等，虽然递归次数是O(log2n)，但每次递归都会释放掉所占的辅助空间，所以下次递归的栈空间和辅助空间与这部分释放的空间就不相关了，因而空间复杂度还是 O(n)。 部分补充 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 不稳定排序算法：谐音记忆法：快（快排） 些（希尔）选（选择）对（堆）。 总共4个不稳定，6个稳定。 冒泡排序基本思想冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述 一次比较两个数字，两两比较，如果前者比后者大，则交换，以此类推，一轮下来，就会产生一个最大的数； 当某一轮没有产生任何一次交换，则说明冒泡完成。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344package 排序;import JAVA_I_O.Buffer.BufferedInputStream;import java.util.ArrayList;import java.util.Arrays;import java.util.Scanner;//测试类public class SortTest &#123; public static void main(String[] args) throws Exception &#123; System.out.println("请输入一个数组，用逗号分隔："); Scanner sc = new Scanner(System.in); String next = sc.nextLine(); String[] net = next.split(",");// String[] arr = Arrays.copyOf(net,net.length);// int[] arr = new int[0]; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); for (int i = 0 ; i &lt; net.length ; i++) &#123; arrayList.add(Integer.parseInt(net[i]));// System.out.println(arr[i]); &#125; int size = arrayList.size(); Integer[] arrInt = arrayList.toArray(new Integer[size]); int[] arr = new int[size]; for(int i = 0 ; i &lt; arrInt.length;i++)&#123; arr[i] = arrInt[i]; &#125;// System.out.println(arr[i]);// int[] InsertSortArray = Sort.InsertSort(arr);// int[] SelectSortArray = Sort.SelectSort(arr); Sort.BubbleSort(arr);// int[] ShellSortArray = Sort.ShellSort2(arr);// Sort.quickSort(arr,0,arr.length-1);// Sort.heapSort(arr,arr.length-1);// Sort.courtSort(arr);// Sort.BucketSort(arr);// Sort.radixSort(arr);// for (int i = 0 ; i &lt; ShellSortArray.length ; i++) &#123;// System.out.print(ShellSortArray[i] + " ");// &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829 /** * 冒泡 * * @param sortArray 原始数组 * @return 排序数组 */public static void BubbleSort(int[] sortArray) &#123; //复制一份原始数组，用于排序操作 int[] arr = Arrays.copyOf(sortArray, sortArray.length); //循环n-1轮，剩最后一个就不需要交换了 for (int i = 1; i &lt; arr.length; i++) &#123; //标志位，当一轮下来没有任何变化时，说明冒泡结束，需要结束循环 boolean flag = true; //每一轮都要两两比较，注意每一轮结束都会确认一个值在正确的位置上，所以j的遍历次数得稍加注意 for (int j = 0; j &lt; arr.length - i; j++) &#123; //位置变化的同时说明冒泡没有结束 if (arr[j] &gt; arr[j + 1]) &#123; int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; flag = false; &#125; &#125; if (flag) &#123; break; &#125; &#125; System.out.println(Arrays.toString(arr));&#125; 算法效率 时间复杂度：很明显，看代码就知道，冒泡有两个for循环，所以一般情况下和最坏情况下的复杂度都为O(n²)，最好的时间复杂度就是一次冒泡就完成了，即为O(n); 空间复杂度：在冒泡中，只用了temp来存储中间变量交换，没有用到其他空间(我的代码中的复制初始数组的容量其实可以省去)，所以空间复杂度是O(1); 稳定性：交换过程中，如果相邻两者的值一样，则不会交换，所以明显冒泡排序是稳定的。 tips 外循环 n-1次； 注意添加标志位，判断冒泡是否结束； 注意内部循环的次数，每外部循环结束一次，就确定一个数的最终位置。 快速排序基本思想快速排序（Quicksort）是对冒泡排序的一种改进，借用了分治的思想，由C. A. R. Hoare在1962年提出。它的基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。 算法描述 首先选定一个基准值（pivot），我一般的做法是选定最左边的作为基准； 定义两个指针，一个指向最右边，一个指向最左边，然后注意让右边的先动，等右边指针所指的数值小于pivot时，右边指针不再往左走，不准动了，然后移动左边指针，等左边指针所指向的数值大于pivot时，左边指针停下，不再向右移动，然后二者交换； 交换完毕后，重复第二步，右边指针继续向左，重复； 直到左右两个指针相遇，两个都停下，将指向的数值和pivot交换，至此数组就分成了两个子数组，pivot左侧比其小，右边比其大； 然后递归子数组，直到子数组为0（low &gt; high）。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 快速排序 * 后续所有测试都跟冒泡那章写的一样 * @param sortArray 原始数组 * @param low 最左侧 * @param high 最右侧 */public static void quickSort(int[] sortArray, int low, int high) &#123; //当low = high 时，说明子数组只剩一个数了，还排个锤子啊 if (low &lt; high) &#123; //拿到基准值，这样就可以愉快的递归了 int pivot = partition(sortArray, low, high); //注意哦，递归时可没有把pivot放入其中 quickSort(sortArray, low, pivot - 1); quickSort(sortArray, pivot + 1, high); System.out.println(Arrays.toString(sortArray)); &#125;&#125;/** * 快排主思路 * @param sortArray * @param low * @param high * @return */private static int partition(int[] sortArray, int low, int high) &#123; //我个人习惯是把最左边定为基准值，具体方法有很多啦，随便你，爱咋咋的 int pivot = low; //定义左右两指针 int p = low; int q = high; //相遇就结束了哈，然后直接交换基准和指向的数值就行咯 while (p &lt; q) &#123; //没相遇并且右指针指向的值比基准值大，就向左移动，若比基准值小，那就得停下 while (p &lt; q &amp;&amp; sortArray[pivot] &lt;= sortArray[q]) q--; while (p &lt; q &amp;&amp; sortArray[pivot] &gt;= sortArray[p]) p++; //都停下了就交换叭！ swap(sortArray, p, q); &#125; swap(sortArray, pivot, p); //不要忘记最后要返回这个基准值 return p;&#125;public static void swap(int[] sum, int i, int j) &#123; int temp = sum[i]; sum[i] = sum[j]; sum[j] = temp;&#125; 算法效率快排嘛，用了分治的思想咯，所以复杂度肯定是比冒泡低的，至于是多少，我们稍加分析一下： 时间复杂度：分治思想，但是绝壁不是线性的，因为它一轮下来搞来搞去，交换这交换那的，所以一轮是O(n),综合前面的分治就是nlogN了。我说的是最好情况和平均情况哈，最坏那肯定还是O(n²)，因为可能你很不幸，基准值就是最大的，然后就得疯狂交换。 空间复杂度：不瞎的都看得出来，就用了一个temp中间变量用来存值，所以是O(1) 。 稳定性：哎呀肯定是不稳定的啊，两个指针在那换来换去，能稳定嘛… tips 注意递归时的子串是不包括pivot的，一个是(low,pivot-1)，一个是(pivot+1,high)； 两个指针相遇，不用判断和pivot值的大小，直接交换即可，因为两个指针只会有一个在移动，那么此时此刻另一个指针一定是静止的，假如此时是右边的指针移动，那么相遇的位置即左指针停下的位置，而左指针停下说明之前右边指针也停下来过并且和左边指针指向的数进行了交换(因为只有右边停下，左边才能移动)，那么此时左边指向的数是一定比pivot小的，所以必然可以直接交换，换个角度考虑也是一样的。 还有，我一直强调必须右指针先动(其实只要是和基准相反方向的指针先动就行)，为什么捏，因为嘛，给你举个例子吧，假如有这样一个数组 9,3,4,5,6,7,10 ,左边的先动那就凉了啊，因为最后你都定在10，一叫换就变成了10,3,4,5,6,7,9，这还玩个锤子啊———–&gt; 快排从右边开始的原因 至于快排的优化 采用三数取中法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package 排序.二刷排序;import java.util.Arrays;import java.util.Scanner;public class quickSort &#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.print("请输入数组的大小： "); int len = scanner.nextInt(); int[] nums = new int[len]; for (int i = 0;i &lt; len;i++)&#123; nums[i] = scanner.nextInt(); &#125; quick(nums,0,len-1); System.out.println(Arrays.toString(nums)); &#125; public static void quick(int[] nums,int left,int right)&#123; if(left &lt; right)&#123; int partition = partition(nums,left,right); quick(nums,left,partition-1); quick(nums,partition+1,right); &#125; &#125; private static int partition(int[] nums, int left, int right) &#123; // 基准值 // 优化，就是尽量能做到分治，也就是选取的基准值尽量在中间是性能最好的 int mid = (left + right)/2; dealPivot(nums,left,mid,right); int pivot = left; // 定义两个指针 int p = left; int q = right; while(p &lt; q)&#123; while(p &lt; q &amp;&amp; nums[q] &gt;= nums[pivot])&#123; q--; &#125; while(p &lt; q &amp;&amp; nums[p] &lt;= nums[pivot])&#123; p++; &#125; swap(nums,p,q); &#125; swap(nums,p,pivot); return p; &#125; private static void dealPivot(int[] nums, int left, int mid, int right) &#123; if((left - mid) * (mid - right) &gt; 0)&#123; // left &lt; mid &lt; right swap(nums,left,mid); return; &#125; else if((mid - left) * (left - right) &gt; 0)&#123; // left 是中间值 return; &#125; swap(nums,left,right); return; &#125; private static void swap(int[] nums, int p, int q) &#123; int temp = nums[p]; nums[p] = nums[q]; nums[q] = temp; &#125;&#125; 三路快排，这个适用于有大量相同数字的数组 「LeetCode 第 75 题」 https://www.bilibili.com/video/av73100790?from=search&amp;seid=4070762078682607758 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class quickSortByThree&#123; public static void main(String[] args) &#123; Scanner scanner = new Scanner(System.in); System.out.print("请输入要输入的排序数组的长度：" ); int len = scanner.nextInt(); int[] nums = new int[len]; for(int i = 0; i &lt; len;i++)&#123; nums[i] = scanner.nextInt(); &#125; quickSortByThreeWays(nums,0,len-1); System.out.println(Arrays.toString(nums)); &#125; private static void quickSortByThreeWays(int[] nums, int left, int right) &#123; if(left &lt; right)&#123; // 此时需要返回的值是两个，一个是基准值的最终位置，还有一个是与基准值相同的最后一个数的位置 // 适用于很多相同数字的数组排序，官方的 Arrays.sort() 也是采用的三路快排 int[] par = quickThreePartition(nums,left,right); int lt = par[0]; int gt = par[1]; quickSortByThreeWays(nums,left,lt-1); quickSortByThreeWays(nums,gt+1,right); &#125; &#125; private static int[] quickThreePartition(int[] nums, int left, int right) &#123; int pivot = left; // 定义三个指针 int cur = left + 1; int lt = left + 1; // nums[left...lt-1] 都是 小于 pivot 的 int gt = right; // nums[gt...right] 都是 大于 pivot 的 while(cur &lt;= gt)&#123; if(nums[cur] &lt; nums[pivot])&#123; swap(nums,cur,lt); cur++; lt++; &#125; else if(nums[cur] &gt; nums[pivot])&#123; swap(nums,gt,cur); gt--; &#125; else cur++; &#125; swap(nums,lt-1,pivot); return new int[]&#123;lt-1,gt&#125;; &#125; private static void swap(int[] nums, int p, int q) &#123; int temp = nums[p]; nums[p] = nums[q]; nums[q] = temp; &#125;&#125; 简单插入排序基本思想看名字叭，插入，还尼玛简单插入，顾名思义，就是把这个待排序数组看成是一步步插好的数组，用的是最简单的方法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 算法描述一般来说，插入排序都采用 in-place 在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 注：啥是in-place知道不：原地算法) 动图演示 代码实现1234567891011121314151617181920212223/** * 直接插入排序 * 我没有in-place哈，但是是可以的 * @param SortArray 原始数组 * @return 排序数组 */ public static void InsertSort(int[] SortArray) &#123; int[] arr = Arrays.copyOf(SortArray, SortArray.length); //从1开始哦，因为第一个数默认是排好序的啦 for (int i = 1; i &lt; arr.length; i++) &#123; int temp = arr[i]; int j = i; for (; (j &gt; 0 &amp;&amp; arr[j - 1] &gt; temp); j--) &#123; //这一步最关键，因为要注意每当插入一个数字，其后面都要往后顺移一位，不然的话就空不出要插入的位置！！！ arr[j] = arr[j - 1]; &#125; //如果没有变化，则temp依然等于arr[i],如果变化，则将arr[i]移动过来就行 //注意不能直接arr[j] = arr[i]哦，因为此时的i索引值已经可能变化啦！！！！temp非常的关键！！ arr[j] = temp; &#125; System.out.println(Arrays.toString(arr)); &#125; 算法效率 时间复杂度：这种直接插入麻烦的一笔，如果是原数组是倒序，那每一轮都得疯狂插，毫无疑问最坏情况下的时间复杂度是O(n²)，最好的时间复杂度就是一次都不用给后来者让位，那么就是线性复杂度了O(n)，平均下来依旧是O(²)。 空间复杂度：in-place,O(1) 。 稳定性：当然是稳定的啦！ tips 注意插入的时候是将比插入的数大的已经排序好的数全部后移一位哦！！！给朕让位的赶脚… 用temp来保存当前需要插入的数据是非常重要的，这也是插入排序唯二需要注意的地方，还要一个需要注意的地方就是全部后移一位。 希尔排序基本思想希尔排序又叫缩小增量排序，1959年Shell发明。是插入排序的一种高速而稳定的改进版本。 希尔排序是先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。 算法描述希尔排序有n种步长方式，这里介绍两种，一种是循环折半，另一种是 gap = gap * 3 + 1。 具体：希尔排序增量序列介绍 。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 动图演示 代码实现1234567891011121314151617181920212223/*** 其实就是将直接插入排序的间隔1变为更大，也就是其实就多了一个循环，就是多次缩小间隔的循环，其他跟直接插入是一样的** @param sortArray* @return*/ public static int[] ShellSort2(int[] sortArray) &#123; int[] arr = Arrays.copyOf(sortArray, sortArray.length); int size = sortArray.length; for (int increment = size / 2; increment &gt; 0; increment /= 2) &#123; for (int i = increment; i &lt; size; i++) &#123; int temp = arr[i]; int j = i; for (; (j &gt; 0 &amp;&amp; arr[j - increment] &gt; temp); j -= increment) &#123; //这一步最关键，因为要注意每当插入一个数字，其后面都要往后顺移increment位，不然的话就空不出要插入的位置！！！ arr[j] = arr[j - increment]; &#125; //如果没有变化，则temp依然等于arr[i],如果变化，则将arr[i]移动过来就行 arr[j] = temp; &#125; &#125; return arr; &#125; 12345678910111213141516171819202122232425262728293031 /** * 不是我写的 * * @param sourceArray * @return * @throws Exception */public static int[] ShellSort1(int[] sourceArray) throws Exception &#123; // 对 arr 进行拷贝，不改变参数内容 int[] arr = Arrays.copyOf(sourceArray, sourceArray.length); int gap = 1; while (gap &lt; arr.length) &#123; gap = gap * 3 + 1; &#125; while (gap &gt; 0) &#123; for (int i = gap; i &lt; arr.length; i++) &#123; int tmp = arr[i]; int j = i; for (; j &gt;= 0 &amp;&amp; arr[j - gap] &gt; tmp; j -= gap) &#123; arr[j] = arr[j - gap]; &#125; arr[j] = tmp; &#125; gap = (int) Math.floor(gap / 3); &#125; return arr;&#125; 算法效率 时间复杂度：最坏和最好我们都知道，这个就跟直接插入没区别，最好最坏都是取决于原数组序列，所以最好的时间复杂度依旧是O(n)，最坏的时间复杂度依旧是O(n²)，平均复杂度？我tm也不知道为啥，记住就是啦，O(n^1.3)。 空间复杂度：这个简单的一批，还不就是O(1)。 稳定性：这种跳跃性的当然是不稳定了。例如arr = {3,2,2}。 tips简单的一批…不就是把直接插入的1变成 increment 嘛 …… 没啥需要注意的！ 啊！！！不对，其实还是有一个需要注意的！！！ 这里是三重循环，因为那个步长要变化，然后不同步长都要进行排序，一次排序是两重循环 over 简单选择排序基本思想选择排序，意在突出选择 二字，所以我们要做的就是有一个选择的过程，至于简单选择，简而言之就是非常非常简单的选择排序，那不就是每次都找个最小值往前扔嘛… 选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 算法描述n个记录的直接选择排序可经过 n-1 趟直接选择排序得到有序结果。具体算法描述如下：(这么官方肯定是复制的啦哈哈哈哈哈) 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；(这种屁话读起来简直就是浪费生命) n-1趟结束，数组有序化了。 注：就是说 ——– 每次遍历数组都选一个最小的扔到最前面（跟要扔的位置的数进行交换），然后从剩下的还没排序的继续遍历，继续扔，扔了n-1次就结束了叭。 动图演示 代码实现1234567891011121314151617181920212223242526272829/** * 简单选择排序 * * @param SortArray 原始数组 * @return 排序数组 */public static int[] SelectSort(int[] SortArray) &#123; int[] arr = Arrays.copyOf(SortArray, SortArray.length); //注意是n-1轮，一轮一个最小值，最后那个不用轮了 for (int i = 0; i &lt; arr.length - 1; i++) &#123; //最开始默认第一个是最小的，然后进入内层循环，分别进行比较，一定要记录下min的下标，这是最后比较完交换的关键 //每一次循环的目的就是找到一个最小值，然后与当前外部循环的值进行一个交换 int min = i; for (int j = i + 1; j &lt; arr.length; j++) &#123; //min = Math.min(min,arr[j]); if (arr[min] &gt; arr[j]) &#123; min = j; &#125; &#125; //判断是否需要交换 if (i != min) &#123; int temp = arr[i]; arr[i] = arr[min]; arr[min] = temp; &#125; &#125; return arr;&#125; 算法效率必须要吐槽的是，这个直接选择排序是垃圾中的战斗机，因为它无视任何数组，干啥都是废物的O(n²)时间复杂度！！！！！！！ 时间复杂度：O(n²) ； 空间复杂度：O(1) ； 稳定性：当然是不稳定的了，涉及到瞎鸡儿交换（不临近）的，就肯定不稳定的啦。 tips没啥注意点，就是非常的垃圾，不要用… 堆排序基本思想这个牛逼勒，我个人最欣赏这个，因为java中的PriorityQueue用了堆排序…在做k个有序链表合并等问题时，用PriorityQueue简直就是神器啊… 堆排序的基本思想是：将待排序序列构造成一个大顶堆，此时，整个序列的最大值就是堆顶的根节点。将其与末尾元素进行交换，此时末尾就为最大值。然后将剩余n-1个元素重新构造成一个堆，这样会得到n个元素的次小值。如此反复执行，便能得到一个有序序列了。 算法描述 将无需序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆; 将堆顶元素与末尾元素交换，将最大元素”沉”到数组末端; 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序。 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475 /** * 堆排序 * * @param sortArray * @param len 注意这里是arr.length，跟之前不一样，堆排序是从1开始的 */ public static void heapSort(int[] sortArray, int len) &#123; //建立大根堆 BuildMaxHeap(sortArray, len); //每次都将根取出，然后将最底下的放到根，然后进行堆调整 for (int i = len; i &gt; 0; i--) &#123; swap(sortArray, i, 0);// AdjustDown(sortArray,0,i-1); AdjustDown2(sortArray, 0, i - 1); &#125; System.out.println(Arrays.toString(sortArray)); &#125; /** * 先写一个递归的，比较好理解 * * @param sortArray * @param i * @param len */ private static void AdjustDown(int[] sortArray, int i, int len) &#123; //注意这里的len是length-1，然后根的序号是1，所以left是2i+1 int left = 2 * i + 1; int right = 2 * i + 2; int largest = i; if (left &lt;= len &amp;&amp; sortArray[left] &gt; sortArray[largest]) largest = left; if (right &lt;= len &amp;&amp; sortArray[right] &gt; sortArray[largest]) largest = right; if (largest != i) &#123; swap(sortArray, largest, i); AdjustDown(sortArray, largest, len); &#125; &#125; /** * 写一个非递归的吧，注意哈，这里的len是length-1，跟上面的一样 * * @param sortArray * @param i * @param len */ private static void AdjustDown2(int[] sortArray, int i, int len) &#123; int t = i; for (int m = 2 * i + 1; m &lt;= len; m = t * 2 + 1) &#123; if (sortArray[m] &gt; sortArray[t]) t = m; if (m + 1 &lt;= len &amp;&amp; sortArray[m + 1] &gt; sortArray[t]) t = m + 1; if (t == i) break; else &#123; swap(sortArray, t, (m - 1) / 2); i = (m - 1) / 2; &#125; &#125; &#125; private static void BuildMaxHeap(int[] sortArray, int len) &#123; double lens = len; //注意是从lens/2开始 for (int i = (int) Math.round((lens / 2)) - 1; i &gt;= 0; i--) &#123;// System.out.println(i);// AdjustDown(sortArray,i,len); AdjustDown2(sortArray, i, len); &#125; &#125; 算法效率 时间复杂度：平均时间、最坏和最好情况下都一样，都是O(nlogn) 。 建立堆的过程, 从length/2 一直处理到0, 时间复杂度为O(n); 调整堆的过程是沿着堆的父子节点进行调整, 执行次数为堆的深度, 时间复杂度为O(lgn); 堆排序的过程由n次第②步完成, 时间复杂度为O(nlgn)。 空间复杂度：O(1)。 稳定性：由于堆排序中初始化堆的过程比较次数较多, 因此它不太适用于小序列。同时由于多次任意下标相互交换位置, 相同元素之间原本相对的顺序被破坏了, 因此, 它是不稳定的排序。 tips 堆排序，主要就是把根拿出来，然后剩下的调整成最大堆，所以第一个拿出来的最大的就应该放到最好一个位置，一共循环 n-1 次即可； 注意代码中的 len 是 length-1。 还有一个容易出错的地方在建立大根堆的时候，i 应该是从最后一个节点的父亲节点开始调整。 写的非递归还是不太好…虽然好像能通过，非递归中代码中的m是指节点的左孩子，如果有交换，切勿忘记更新新的父亲节点(i = (m-1)/2)，然后进行下一步和孩子节点的比较。 归并排序基本思想归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用 分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为 2-路归并。 算法描述 把长度为n的输入序列分成两个长度为n/2的子序列，然后子序列首端各维护一个指针，互相比较； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 动图演示 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 /** * 自己写一个 底层是数组 */ public static void mergeSort(int[] sortArray, int low, int high) &#123; int mid = (low + high) / 2; if (low &lt; high) &#123; //注意和快排区分一下，这里的mid是要在递归里面的 mergeSort(sortArray, low, mid); mergeSort(sortArray, mid + 1, high); //思路就是先一分为二再合二为一 merge(sortArray, low, mid, high); System.out.println(Arrays.toString(sortArray)); &#125; &#125; private static void merge(int[] sortArray, int low, int mid, int high) &#123; //牺牲了空间，引入了一个新的空间来存储排好序的数组 int[] temp = new int[high - low + 1]; int i = low; int j = mid + 1; int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (sortArray[i] &lt; sortArray[j]) &#123; temp[k++] = sortArray[i++]; &#125; else &#123; temp[k++] = sortArray[j++]; &#125; &#125; //非常严重的错误，if中的j++后会影响下一个if，所以会超出索引值。数组越界// while(i &gt; mid)&#123;// temp[k++] = sortArray[j++];// &#125;// while(j &gt; high)&#123;// temp[k++] = sortArray[i++];// &#125; while (i &lt;= mid) &#123; temp[k++] = sortArray[i++]; &#125; while (j &lt;= high) &#123; temp[k++] = sortArray[j++]; &#125; for (int t = 0; t &lt; temp.length; t++) &#123; //这里也是易错点，注意sortArray的下标是从low开始的 sortArray[low + t] = temp[t]; &#125; &#125; 算法效率 时间复杂度：归并排序可算是排序算法中的”佼佼者”。假设数组长度为n，那么拆分数组共需logn, 又每步都是一个普通的合并子数组的过程，时间复杂度为O(n)， 故其综合时间复杂度为O(nlogn)。注意，平均时间复杂度、最好时间复杂度、最坏时间复杂度都是O(nlogn)。 空间复杂度：需要一个额外的数组来存储排序好的数组，所以是O(N)。 稳定性：稳定。 tips 首先注意递归部分，跟快排有所区别，快排是排序一轮结束后才能返回中间节点，而归并是直接算出中间节点，并且归并的mid是要参与递归的，而快排中的基准值不需要参加递归。 合并过程。首先是要new一个新的数组来存储排序好的数组，两个指针轮流比较，如果底层是链表，两个头结点互相比较，最后合并之后返回一个头结点即可。 注意最后将合并排序好的数组放回到原数组时，下标是从 low 开始的。 计数排序计数排序、基数排序、桶排序属于非比较排序。非比较排序是通过确定每个元素之前，应该有多少个元素来排序。针对数组arr，计算arr[i]之前有多少个元素，则唯一确定了arr[i]在排序后数组中的位置。 非比较排序只要确定每个元素之前的已有的元素个数即可，所有一次遍历即可解决。算法时间复杂度O(n)。非比较排序时间复杂度低，但由于非比较排序需要占用空间来确定唯一位置。所以对数据规模和数据分布有一定的要求。 基本思想计数排序需要占用大量空间，它仅适用于数据比较集中的情况。比如 [0~100]，[10000~19999] 这样的数据。 计数排序的基本思想是： 对每一个输入的元素arr[i]，确定小于 arr[i] 的元素个数。所以可以把 arr[i] 放到它输出数组中的位置上。 假设有5个数小于 arr[i]，所以 arr[i] 应该放在数组的第6个位置上。 其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中 arr[i] 的元素出现的次数，将 arr[i] - min 作为辅助数组helper的索引值； 对所有的计数累加，helper 索引值对应的值即为该 arr[i] 出现的次数； 反向填充目标数组：将 helper 中的元素根据索引依次放回原数组，索引对应的数值即为该数出现的次数，每放一个元素就将 helper[arr[i]-min] 减去1。 动图演示 代码实现12345678910111213141516171819202122232425/** * 计数排序 * 要有两个数组，分别是原数组，中间数组用来排序的 * * @param sortArray */public static void courtSort(int[] sortArray) &#123; int max = Integer.MIN_VALUE; int min = Integer.MAX_VALUE; for (int i = 0; i &lt; sortArray.length; i++) &#123; max = Math.max(max, sortArray[i]); min = Math.min(min, sortArray[i]); &#125; int[] helparr = new int[max - min + 1]; for (int i = 0; i &lt; sortArray.length; i++) &#123; helparr[sortArray[i] - min]++; &#125; int index = 0; for (int i = 0; i &lt; helparr.length; i++) &#123; while (helparr[i]-- &gt; 0) &#123; sortArray[index++] = i + min; &#125; &#125; System.out.println(Arrays.toString(sortArray));&#125; 算法效率 时间复杂度：当输入元素是 n 个 0到 k 之间的整数时，平均时间、最坏和最好情况下都一样，都是O(n+k) 。其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 空间复杂度：O(n+k) 。 稳定性：稳定。 tips 注意：为了节省空间，helper 必须是 arr[i]-min作为索引值。 在将排序好的数组放回原数组时，别忘了把中间数组索引对应的值减去1，当为0时说明该索引代表的值不存在。 桶排序基本思想 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。 桶排序 (Bucket sort) 的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 桶排序可用于最大最小值相差较大的数据情况，比如[9012,19702,39867,68957,83556,102456]。但桶排序要求数据的分布必须均匀，否则可能导致数据都集中到一个桶中。比如[104,150,123,132,20000], 这种数据会导致前4个数都集中到同一个桶中。导致桶排序失效。 桶排序的基本思想是：把数组 arr 划分为n个大小相同子区间（桶），每个子区间各自排序，最后合并。计数排序是桶排序的一种特殊情况，可以把计数排序当成每个桶里只有一个元素的情况。 算法描述 找出待排序数组中的最大值 max、最小值 min； 我们使用 动态数组 ArrayList 作为桶，桶里放的元素也用 ArrayList 存储。桶的数量为 (max-min)/BucketSize+1； 遍历数组 arr，计算每个元素 arr[i] 放的桶； 每个桶各自排序； 遍历桶数组，把排序好的元素放进输出数组。 图片演示 代码实现1234567891011121314151617181920212223242526272829303132/** * 桶排序 * * @param sortArray */public static void BucketSort(int[] sortArray) &#123; //1.设置固定数量的空桶 int max = Integer.MIN_VALUE; int BucketSize = 5; int min = Integer.MAX_VALUE; for (int i = 0; i &lt; sortArray.length; i++) &#123; max = Math.max(max, sortArray[i]); min = Math.min(min, sortArray[i]); &#125; int BucketCount = (max - min) / BucketSize + 1; //2.把数据放到对应桶中 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; buckets = new ArrayList&lt;&gt;(BucketCount); for (int i = 0; i &lt; BucketCount; i++) &#123; buckets.add(new ArrayList&lt;Integer&gt;()); &#125; for (int i = 0; i &lt; sortArray.length; i++) &#123; int num = (sortArray[i] - min) / BucketSize; buckets.get(num).add(sortArray[i]); &#125; //3.排序 for (int i = 0; i &lt; buckets.size(); i++) &#123; //直接用的是集合自带的排序，可以选择插入或者递归使用桶排序 Collections.sort(buckets.get(i)); &#125; System.out.println(buckets.toString()); &#125; 算法效率 时间复杂度：桶排序最好情况下使用线性时间 O(n)，桶排序的时间复杂度，取决与对各个桶中数据进行排序的时间复杂度，因为其它部分的时间复杂度都为 O(n)。很显然，桶的范围划分的越小，桶越多，各个桶之间的数据越少，排序所用的时间也会越少。所以最差的时间复杂度为 O(n²)，平均时间复杂度为 O(n+k)，最好的时间复杂度为 O(n)。 空间复杂度：O(n+k) 。 稳定性：稳定。 tips 桶最好是用ArrayList，里面的数据最好也是ArrayList装载，因为这样方便动态的添加数据到桶中。 桶内排序可以随便选择一种排序算法，这里采用的是java集成好的集合排序的方法。 基数排序基本思想基数排序（Radix Sort）是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 算法描述 取得数组中的最大数，并取得位数； arr 为原始数组，从最低位开始取每个位组成 radix 数组； 对 radix 进行计数排序（利用计数排序适用于小范围数的特点）； 动图演示 代码实现123456789101112131415161718192021222324252627282930313233343536373839/** 基数排序 * 可以采用桶的思想，也可以采用队列的思想 * * @param array */ public static void radixSort(int[] array) &#123; if (array == null || array.length &lt; 2) return; // 1.先算出最大数的位数； int max = array[0]; for (int i = 1; i &lt; array.length; i++) &#123; max = Math.max(max, array[i]); &#125; int maxDigit = 0; while (max != 0) &#123; max /= 10; maxDigit++; &#125; int mod = 10, div = 1; //桶可以用二维数组实现，也可以用ArrayList实现，推荐列表，因为是动态的 ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucketList = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; bucketList.add(new ArrayList&lt;Integer&gt;()); &#125; for (int i = 0; i &lt; maxDigit; i++, mod *= 10, div *= 10) &#123; for (int j = 0; j &lt; array.length; j++) &#123; int num = (array[j] % mod) / div; bucketList.get(num).add(array[j]); &#125; int index = 0; for (int j = 0; j &lt; bucketList.size(); j++) &#123; for (int k = 0; k &lt; bucketList.get(j).size(); k++) &#123; array[index++] = bucketList.get(j).get(k); &#125; bucketList.get(j).clear(); &#125; &#125; System.out.println(Arrays.toString(array)); &#125; 算法效率 时间复杂度：基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要 O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要 O(n)的时间复杂度。假如待排数据可以分为 d个关键字，则基数排序的时间复杂度将是 O(d*2n) ，当然 d 要远远小于 n，因此基本上还是线性级别的。最好、最差、平均时间复杂度都是O(n*k)级别的。 空间复杂度：O(n+k)，其中k为桶的数量。一般来说 n&gt;&gt;k，因此额外空间需要大概 n 个左右。 稳定性：稳定。 tips 注意每个桶内数据排序好然后返回给原数组后，记得clear； 点睛之笔是 将基数排序转换为计数排序，分别对其个位、十位…进行计数排序。 参考文献 十大经典排序算法 十大经典排序算法之JAVA实现 计数排序和桶排序之JAVA实现 十大经典排序算法动画与解析，看我就够了！（配代码完全版） 面试必备：八种排序算法原理及Java实现]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[darkmode for mac]]></title>
    <url>%2F2019%2F10%2F24%2Fdarkmode%E6%95%99%E7%A8%8B.html</url>
    <content type="text"><![CDATA[Mac端darkmode设置Mac端外观具体操作方式 打开 系统偏好设置 点击 通用 修改 外观 修改完之后，部分软件会自动适配系统的环境色，例如chrome，网易云等等。 修改chrome网页颜色在谷歌插件商店中查找一款Dark Reader的插件，可以完美配合暗黑模式的使用 自动切换白天黑夜的模式下载一款叫做DarkNight的软件，可以根据当地的日落日出时间自动切换屏保，配合暗黑模式的使用。]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java实现单链表]]></title>
    <url>%2F2019%2F10%2F16%2Fjava%E5%AE%9E%E7%8E%B0%E5%8D%95%E9%93%BE%E8%A1%A8.html</url>
    <content type="text"><![CDATA[Java实现单向链表基本功能先说一下，参考借鉴的博客有： Java实现单向链表基本功能 Java单链表反转 详细过程 从链表中删除重复数据（三种方法) 补充写的没人家好，这部分主要是对第一个博客的补充，其单向链表中的链表反转和从链表中删除重复数据解释的不够清晰，后面两个博客是扩展这两个知识点的，在看这个知识点的同时，发现用到了部分HashTable的知识，过段时间需要把这个知识点也完善一下。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[transient关键字]]></title>
    <url>%2F2019%2F10%2F12%2Ftransient%E5%85%B3%E9%94%AE%E5%AD%97.html</url>
    <content type="text"><![CDATA[transient关键字作用(转载)作用及使用方法​ 我们都知道一个对象只要实现了Serilizable接口，这个对象就可以被序列化，java的这种序列化模式为开发者提供了很多便利，我们可以不必关系具体序列化的过程，只要这个类实现了Serilizable接口，这个类的所有属性和方法都会自动序列化。 ​ 然而在实际开发过程中，我们常常会遇到这样的问题，这个类的有些属性需要序列化，而其他属性不需要被序列化，打个比方，如果一个用户有一些敏感信息（如密码，银行卡号等），为了安全起见，不希望在网络操作（主要涉及到序列化操作，本地序列化缓存也适用）中被传输，这些信息对应的变量就可以加上transient关键字。换句话说，这个字段的生命周期仅存于调用者的内存中而不会写到磁盘里持久化。 ​ 总之，java 的transient关键字为我们提供了便利，你只需要实现Serilizable接口，将不需要序列化的属性前添加关键字transient，序列化对象的时候，这个属性就不会序列化到指定的目的地中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;/** * @description 使用transient关键字不序列化某个变量 * 注意读取的时候，读取数据的顺序一定要和存放数据的顺序保持一致 * * @author Alexia * @date 2013-10-15 */public class TransientTest &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername("Alexia"); user.setPasswd("123456"); System.out.println("read before Serializable: "); System.out.println("username: " + user.getUsername()); System.err.println("password: " + user.getPasswd()); try &#123; ObjectOutputStream os = new ObjectOutputStream( new FileOutputStream("C:/user.txt")); os.writeObject(user); // 将User对象写进文件 os.flush(); os.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; ObjectInputStream is = new ObjectInputStream(new FileInputStream( "C:/user.txt")); user = (User) is.readObject(); // 从流中读取User的数据 is.close(); System.out.println("\nread after Serializable: "); System.out.println("username: " + user.getUsername()); System.err.println("password: " + user.getPasswd()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class User implements Serializable &#123; private static final long serialVersionUID = 8294180014912103005L; private String username; private transient String passwd; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPasswd() &#123; return passwd; &#125; public void setPasswd(String passwd) &#123; this.passwd = passwd; &#125;&#125; 1234567read before Serializable: username: Alexiapassword: 123456read after Serializable: username: Alexiapassword: null 密码字段为null，说明反序列化时根本没有从文件中获取到信息。 transient使用小结 一旦变量被transient修饰，变量将不再是对象持久化的一部分，该变量内容在序列化后无法获得访问。 transient关键字只能修饰变量，而不能修饰方法和类。注意，本地变量是不能被transient关键字修饰的。变量如果是用户自定义类变量，则该类需要实现Serializable接口。 被transient关键字修饰的变量不再能被序列化，一个静态变量不管是否被transient修饰，均不能被序列化。 第三点可能有些人很迷惑，因为发现在User类中的username字段前加上static关键字后，程序运行结果依然不变，即static类型的username也读出来为“Alexia”了，这不与第三点说的矛盾吗？实际上是这样的：第三点确实没错（一个静态变量不管是否被transient修饰，均不能被序列化），反序列化后类中static型变量username的值为当前JVM中对应static变量的值，这个值是JVM中的不是反序列化得出的，不相信？好吧，下面我来证明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;/** * @description 使用transient关键字不序列化某个变量 * 注意读取的时候，读取数据的顺序一定要和存放数据的顺序保持一致 * * @author Alexia * @date 2013-10-15 */public class TransientTest &#123; public static void main(String[] args) &#123; User user = new User(); user.setUsername("Alexia"); user.setPasswd("123456"); System.out.println("read before Serializable: "); System.out.println("username: " + user.getUsername()); System.err.println("password: " + user.getPasswd()); try &#123; ObjectOutputStream os = new ObjectOutputStream( new FileOutputStream("C:/user.txt")); os.writeObject(user); // 将User对象写进文件 os.flush(); os.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; // 在反序列化之前改变username的值 User.username = "jmwang"; ObjectInputStream is = new ObjectInputStream(new FileInputStream( "C:/user.txt")); user = (User) is.readObject(); // 从流中读取User的数据 is.close(); System.out.println("\nread after Serializable: "); System.out.println("username: " + user.getUsername()); System.err.println("password: " + user.getPasswd()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125;class User implements Serializable &#123; private static final long serialVersionUID = 8294180014912103005L; public static String username; private transient String passwd; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public String getPasswd() &#123; return passwd; &#125; public void setPasswd(String passwd) &#123; this.passwd = passwd; &#125;&#125; 1234567read before Serializable: username: Alexiapassword: 123456read after Serializable: username: jmwangpassword: null 这说明反序列化后类中static型变量username的值为当前JVM中对应static变量的值，为修改后jmwang，而不是序列化时的值Alexia。 3. transient使用细节——被transient关键字修饰的变量真的不能被序列化吗？123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.io.Externalizable;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInput;import java.io.ObjectInputStream;import java.io.ObjectOutput;import java.io.ObjectOutputStream;/** * @descripiton Externalizable接口的使用 * * @author Alexia * @date 2013-10-15 * */public class ExternalizableTest implements Externalizable &#123; private transient String content = "是的，我将会被序列化，不管我是否被transient关键字修饰"; @Override public void writeExternal(ObjectOutput out) throws IOException &#123; out.writeObject(content); &#125; @Override public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &#123; content = (String) in.readObject(); &#125; public static void main(String[] args) throws Exception &#123; ExternalizableTest et = new ExternalizableTest(); ObjectOutput out = new ObjectOutputStream(new FileOutputStream( new File("test"))); out.writeObject(et); ObjectInput in = new ObjectInputStream(new FileInputStream(new File( "test"))); et = (ExternalizableTest) in.readObject(); System.out.println(et.content); out.close(); in.close(); &#125;&#125; content变量会被序列化吗？ 好吧，我把答案都输出来了，是的，运行结果就是：是的，我将会被序列化，不管我是否被transient关键字修饰 这是为什么呢，不是说类的变量被transient关键字修饰以后将不能序列化了吗？ ​ 我们知道在Java中，对象的序列化可以通过实现两种接口来实现，若实现的是Serializable接口，则所有的序列化将会自动进行，若实现的是Externalizable接口，则没有任何东西可以自动序列化，需要在writeExternal方法中进行手工指定所要序列化的变量，这与是否被transient修饰无关。因此第二个例子输出的是变量content初始化的内容，而不是null。 转载于：Java transient关键字使用小记]]></content>
      <categories>
        <category>Java 基础</category>
      </categories>
      <tags>
        <tag>关键字</tag>
        <tag>transient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode学习]]></title>
    <url>%2F2019%2F10%2F12%2Fleetcode%E7%83%AD%E9%A2%98100.html</url>
    <content type="text"><![CDATA[第1题：两数之和题目给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 我的思路 先排序 排除比target大的数 从最靠近target的数开始遍历 记录找到的数的下标输出即可 我的代码（未AC）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.util.Arrays;public class twoNumbersSum &#123; public int[] twoSum(int[] nums, int target) &#123; //先排序 int[] nums_copy = Arrays.copyOf(nums,nums.length); Arrays.sort(nums_copy); int i; //排除比target大的数 //从最靠近target的数开始遍历 //记录找到的数的下标，输出即可 int t; int k; for(i = nums_copy.length-1; i &gt; 0 ;i--)&#123; if(nums_copy[i] &lt;= target)&#123; if(Arrays.binarySearch(nums_copy,0,i,target-nums_copy[i]) &gt;= 0)&#123; if(nums_copy[i] * 2 == target) &#123; int[] twobro = new int[2]; int d=0; for(int h=0;h&lt;nums.length;h++)&#123; if(nums[h] == nums_copy[i])&#123; twobro[d] = h; d++; &#125; &#125; return twobro; &#125; else&#123; for(int m=0;m &lt; nums.length;m++)&#123; if(nums[m] == nums_copy[i])&#123; t = m; &#125; if(nums[m] == (target-nums_copy[i]))&#123; k = m; &#125; &#125; k = Arrays.binarySearch(nums,target-nums_copy[i]); t = Arrays.binarySearch(nums,nums_copy[i]); &#125; return new int[]&#123;k,t&#125;; &#125; &#125; &#125; return null; &#125; public static void main(String[] args) &#123; twoNumbersSum tws = new twoNumbersSum(); int[] receive = new int[20]; receive = tws.twoSum(new int[]&#123;0,4,3,0&#125;,0); if(receive == null) &#123; System.out.println("不存在"); &#125; else&#123; System.out.println(receive[0] + "\n" + receive[1]); &#125; &#125;&#125; 正确思路利用 HashMap 记录数组元素值和对应的下标，对于一个数 nums[i]，判断 target - nums[i] 是否存在 HashMap 中，存在的话，返回两个下标组成的数组。注意，已存在的元素下标在前，当前元素下标在后。 正确代码123456789101112class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; nums.length; ++i) &#123; if (map.containsKey(target - nums[i])) &#123; return new int[] &#123;map.get(target - nums[i]), i&#125;; &#125; map.put(nums[i], i); &#125; return null; &#125;&#125; 涉及的知识点数组内容 一维和二维数组一维数组：int[] a = new int[4];二维数组： 1234//第一行4个元素，第二行5个元素int[][] a = new int[2][];a[0] = new int[4];a[1] = new int[5]; Arrays类java.util.Arrays类能够方便的操作数组，提供的所有方法都是静态的： 12345678910import java.util.Arrays;public int[] twoSum(int[] nums, int target) &#123; int[] nums_copy = Arrays.copyOf(nums,nums.length); Arrays.fill(nums,2); Integer index = Arrays.binarySearch(nums,target); Arrays.equals(nums,nums_copy); Arrays.sort(nums); return new int[]&#123;0,0,0&#125;;&#125; 总共5个方法，分别是copyOf,fill,binarySearch,equals,sort。第一个用来复制原始数组，方便后续排序的操作，fill是用来初始化数组比较方便，可以将所有数组中的值全部初始化为同一个值，binarySearch是二分查找，返回的是该数的索引值，sort是用来排序的。 binarySearch()自己写代码的时候用到了这个方法，首先该方法需要数组排好序才能调用，其次很特别的是，如果要找的值在数组中，则会返回搜索键的索引，但是，注意：值不存在于数组的话会返回-1或者是目标值需要插入的位置，从1开始数起，不是0哦！！！这个写的贼好哈哈哈哈：数组查询Arrays类的binarySearch()方法详解 map.containsKey和map.get()区别hashmap判断是否存在key时，使用get(key)==null判断还是containsKey？key值可能为null，若此时Map集合值对象为null，并且没有个数限制，所以当get()方法的返回值为null时，可能有两种情况，一种是在集合中没有该键对象，另一种是该键对象没有映射任何值对象，即值对象为null。因此，在Map集合中不应该利用get(Object key)方法来判断是否存在某个键，而应该利用containsKey()方法来判断,containsKey方法用来判断Map集合对象中是否包含指定的键名。一句话概括:get()如果得到null，可能这是键对应的值对象为null也可能是不存在该键，而containsKey则是false或true，不存在这种疑问。 扩充:map.containsKey()、map.containsValue()、map.get() get的过程是先计算hash，然后通过hash与table.length取摸计算index值，然后遍历table[index]上的链表，直到找到key，然后返回； 1234567891011121314public V get(Object key) &#123; if (key == null) return getForNullKey();//处理null值 int hash = hash(key.hashCode());//计算hash //在table[index]遍历查找key，若找到则返回value，找不到返回null for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; return null; &#125; containsKey方法也是先计算hash，然后使用hash和table.length取摸得到index值，遍历table[index]元素查找是否包含key相同的值； 123456789101112131415public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; 看代码能看到区别，一个是返回key对应的value值，一个是返回是否有该key的boolean变量。 * containsValue方法就比较粗暴了，就是直接遍历所有元素直到找到value，由此可见HashMap的containsValue方法本质上和普通数组和list的contains方法没什么区别，你别指望它会像containsKey那么高效。 1234567891011 public boolean containsValue(Object value) &#123; if (value == null) return containsNullValue(); Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) for (Entry e = tab[i]; e != null; e = e.next) if (value.equals(e.value)) return true; return false;&#125; 知识缺陷 压根没想到用Map去做 自己只考虑到了全是正数的情况，所以用了排序和跟0判断，如果含有负数的话就做不了了…… 收获在遇到数组问题时，可以考虑用map，因为索引和数值就是一个天生的map集合，如果我知道数值，我就可以通过map找到其索引，本题思路就是这样，当已知一个值是我所需要的，直接从map中拿出就行。 插入知识点既然复习到了map，那就给list、set和map来一个全部的复习吧！！！嘿嘿开始吧！！！先来上个链接，主要是看的这个写的：Java集合中List,Set以及Map等集合体系详解(史上最全) collection先上个图：这个图画的好啊哈哈哈哈哈哈哈不瞎的都看得到，Collection这个接口下有三个接口继承，分别是Set、List、Queue(我他妈好像没怎么用过Queue啊，以后要多用点了)，Set有三个实现类，分别是HashSet、LinkedHashSet、TreeSet，List有三个实现类，分别是ArrayList、Vector(感觉现在是不是用的比较少啊…)、LinkedList，咦这个LinkedList牛逼啊，竟然还是Queue的实现类，不过看别人博客好像是说继承Queue部分的LinkedList是被阉割了的实现类，也就是Queue不能访问到LinkedList的所有方法(管它呢我都没用过…)，还有一个PriorityQueue,看名字就知道是优先级队列啦！(妈呀看的资料太多，想单独开一篇来总结集合源码和Map源码了…算了先这样写着吧) ####先列个提纲： 1.先综述一下collection中三个儿子接口得各个实现类的特点，比如底层实现，优缺点等等； 还有要考虑的就是base case，由于是最短递归子序列至少为1，所以base case为1，所以dp数组全部初始化为1即可。 3.面试常问到的点 综述— List 有序,可重复 ArrayList优点: 底层数据结构是数组，查询快，增删慢。缺点: 线程不安全，效率高特点: 允许null，不同步Vector优点: 底层数据结构是数组，查询快，增删慢。缺点: 线程安全，效率低tips:所谓的线程安全，是相对的，在vector内部内部内部，其所有方法不会被多线程所访问，单个方法的原子性（注：原子性，程序的原子性即不会被线程调度机制打断），并不能保证复合操作也具有原子性，所以如果是复合操作，同样线程不安全！！如果要保证真正的线程安全，还需要以vector对象为锁，来进行操作，但这样就跟ArrayList没啥区别了…———–&gt; Vector是线程安全吗特点：允许null，不同步LinkedList优点: 底层数据结构是双向链表，查询慢，增删快。故既可以做Queue，又可以做Stack。缺点: 线程不安全，效率高特点：允许null，不同步 —Set 无序,唯一 HashSet（不同步，允许null）底层数据结构是哈希表(无序,唯一)，其实就是HashMap的实例，只不过值是key，value是一个固定的对象。如何来保证元素唯一性?1.依赖两个方法：hashCode()和equals() LinkedHashSet（不同步，允许null）底层数据结构是双向链表和哈希表。(FIFO插入有序,唯一)，实际上依旧是LinkedHashMap的实例，待会源码分析看看1.由链表保证元素有序2.由哈希表保证元素唯一 TreeSet（允许null，不同步）底层数据结构是红黑树。(唯一，有序，这里的有序指的是排序好的，不是说FIFO之类的)，实际上依旧是TreeMap的实例 如何保证元素排序的呢?自然排序(重写):1.Student类中实现 Comparable接口 2.重写Comparable接口中的Comparetor方法比较器排序:1.单独创建一个比较类，这里以MyComparator为例，并且要让其继承Comparator接口 2.重写Comparator接口中的Compare方法2.如何保证元素唯一性的呢?根据比较的返回值是否是0来决定 -Queue PriorityQueue：优先级队列，按照大小排序好了的队列，并不遵循先进先出，不允许null元素，头部是最小元素，底层采用的数组和堆。 PriorityQueue不是线程安全的。如果多个线程中的任意线程从结构上修改了列表， 则这些线程不应同时访问PriorityQueue 实例，这时请使用线程安全的PriorityBlockingQueue 类。 不允许插入 null 元素。 PriorityQueue实现插入方法（offer、poll、remove() 和 add 方法） 的时间复杂度是O(log(n)) ；实现 remove(Object) 和 contains(Object) 方法的时间复杂度是O(n) ；实现检索方法（peek、element 和 size）的时间复杂度是O(1)。所以在遍历时，若不需要删除元素，则以peek的方式遍历每个元素。 方法iterator()中提供的迭代器并不保证以有序的方式遍历PriorityQueue中的元素。剩余有关Queue队列看—–&gt; Java集合（七） Queue详解 分别阐述这个还是另开一篇吧…内容太多了…够写两星期了！！还是不放在这喧宾夺主了!! map 具体展开见另一篇博客：Map源码分析 第2题：两数相加题目给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例123输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 我的思路 先讲输入输出。要输入两个链表，首先就要构造Node实体类，注意构造函数有多个，根据参数的不同进行选择，输出同输入，将组合好的链表的头结点(有数据的)返回后，就可以循环将整个链表打印出来了； 再讲实现。见代码注释。 我的代码（AC）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package 链表实现_java.leecode第二题;public class leecode_second_Link &#123; public static void main(String[] args) &#123; Node head = new Node(2); Node node1 = new Node(5); Node node2 = new Node(8); Node node3 = new Node(7); head.setNext(node1); node1.setNext(node2); node2.setNext(node3); // 打印链表1 Node temp = head; while (temp != null) &#123; if(temp.next == null) &#123; System.out.println(temp.getData()); &#125; else &#123; System.out.print(temp.getData() + "--&gt;"); &#125; temp = temp.getNext(); &#125; System.out.println("+"); Node head_sec = new Node(2); Node node1_sec = new Node(4); Node node2_sec = new Node(6); Node node3_sec = new Node(7); head_sec.setNext(node1_sec); node1_sec.setNext(node2_sec); node2_sec.setNext(node3_sec); // 打印链表2 Node temp1 = head_sec; while (temp1 != null) &#123; if(temp1.next == null) &#123; System.out.println(temp1.getData()); &#125; else &#123; System.out.print(temp1.getData() + "--&gt;"); &#125; temp1 = temp1.getNext(); &#125; System.out.println("="); Node head3 = new leecode_second_Link().addTwoNums(head,head_sec); //打印总和链表 Node temp3 = head3; while(temp3 != null)&#123; if(temp3.next == null) &#123; System.out.println(temp3.getData()); &#125; else &#123; System.out.print(temp3.getData() + "--&gt;"); &#125; temp3 = temp3.getNext(); &#125; &#125; public Node addTwoNums(Node head1,Node head2)&#123; //没啥用，初始化头结点(可以不带数据) Node res = new Node(0); //temp是后面用来遍历链表的 Node temp = res; //这个是商，用来进位给下一位的 int quo = 0; //循环，只有当第一个链表中的值为空且第二个链表中的值为空且没有了进位，计算才算结束 while(head1 != null || head2 != null || quo != 0)&#123; //t为两链表相同位置的和加上前面一位的进位 int t = (head1 == null ? 0 : head1.data ) + (head2 == null ? 0 : head2.data) + quo; //商 quo = t/10; //新链表在该位的值 Node head3 = new Node(t % 10); //新链表的第一个值 temp.next = head3; //只有这样，才能让链表next下去，这两步要学会，以后链表经常要用 temp = head3; //分别看两个链表是否为空，如果为空说明该位已经没了，否则就next下去 head1 = (head1 == null ? head1 : head1.next); head2 = (head2 == null ? head2 : head2.next); &#125; //返回链表第一个有值的节点，相当于有值的头结点 return res.next; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940package 链表实现_java.leecode第二题;public class Node &#123; //数据域 public Integer data; //指针域，指向下一个节点 public Node next; public Node() &#123; &#125; public Node(int data) &#123; this.data = data; &#125; public Node(int data, Node next) &#123; this.data = data; this.next = next; &#125; public int getData() &#123; return data; &#125; public void setData(int Data) &#123; this.data = Data; &#125; public Node getNext() &#123; return next; &#125; public void setNext(Node Next) &#123; this.next = Next; &#125;&#125; 正确思路同时遍历两个链表，对应值相加(还有 quotient)求余数得到值并赋给新创建的结点。而商则用quotient存储，供下次相加。 正确代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152//复杂版/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode res = new ListNode(-1); ListNode cur = res; int quotient = 0; int t = 0; while (l1 != null &amp;&amp; l2 != null) &#123; t = l1.val + l2.val + quotient; quotient = t / 10; ListNode node = new ListNode(t % 10); cur.next = node; l1 = l1.next; l2 = l2.next; cur = node; &#125; while (l1 != null) &#123; t = l1.val + quotient; quotient = t / 10; ListNode node = new ListNode(t % 10); cur.next = node; l1 = l1.next; cur = node; &#125; while (l2 != null) &#123; t = l2.val + quotient; quotient = t / 10; ListNode node = new ListNode(t % 10); cur.next = node; l2 = l2.next; cur = node; &#125; if (quotient != 0) &#123; cur.next = new ListNode(quotient); cur = cur.next; &#125; return res.next; &#125;&#125; 1234567891011121314151617181920212223242526//简化版/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode res = new ListNode(-1); ListNode cur = res; int quotient = 0; while (l1 != null || l2 != null || quotient != 0) &#123; int t = (l1 == null ? 0 : l1.val) + (l2 == null ? 0 : l2.val) + quotient; quotient = t / 10; ListNode node = new ListNode(t % 10); cur.next = node; cur = node; l1 = (l1 == null) ? l1 : l1.next; l2 = (l2 == null) ? l2 : l2.next; &#125; return res.next; &#125;&#125; 涉及的知识点java中的单链表见我写的另外一篇博文 ——&gt; Java实现单向链表 第3题：无重复字符的最长子串题目给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 示例示例 1: 123输入: "abcabcbb"输出: 3 解释: 因为无重复字符的最长子串是 "abc"，所以其长度为 3。 示例 2: 123输入: "bbbbb"输出: 1解释: 因为无重复字符的最长子串是 "b"，所以其长度为 1。 示例 3: 1234输入: "pwwkew"输出: 3解释: 因为无重复字符的最长子串是 "wke"，所以其长度为 3。请注意，你的答案必须是 子串 的长度，"pwke" 是一个子序列，不是子串。 我的思路要找到一个最长子串，就必须有一头一尾，所以就必须有两个指针，然后又是字符串对应索引值，所以肯定是需要用map来操作的，key为索引值，value为索引所在位置的值。于是设定两个指针p、q，最开始同时指定在最开始的位置，然后q向后移动，每移动一次，只要q对应的值没有在map中，就将其值放入map，并且记录下串的大小，当碰到了map中相同的值时，就将p向后移到map中出现该值的索引后一位，同时注意！！！此时p可能会回溯，所以此时要用max函数判断一下，然后继续判断串的大小和继续遍历的最长子串的长度，最后返回最长子串长度即可。 我的代码（AC）1234567891011121314151617181920212223242526272829303132package 第三题;import java.util.HashMap;import java.util.Scanner;public class lengthOfLongestSubstring &#123; public static void main(String[] args) &#123; Scanner a = new Scanner(System.in); System.out.println("请输入串: "); String string = a.nextLine(); int num = new lengthOfLongestSubstring().lengthofSubstring(string); System.out.println("最长子串的长度为：" + num); &#125; private int lengthofSubstring(String string) &#123; int max = 0; char[] chars = string.toCharArray(); int p=0,q=0; int len = chars.length; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); while(q &lt; len)&#123; if(map.containsKey(chars[q]))&#123; p = Math.max(p,map.get(chars[q])+1); &#125; map.put(chars[q],q); max = Math.max(max,q-p+1); q++; &#125; return max; &#125;&#125; 正确思路利用指针 p, q，初始指向字符串开头。遍历字符串，q 向右移动，若指向的字符在 map 中，说明出现了重复字符，此时，p 要在出现重复字符的下一个位置 map.get(chars[q]) + 1 和当前位置 p 之间取较大值，防止 p 指针回溯。循环的过程中，要将 chars[q] 及对应位置放入 map 中，也需要不断计算出max 与 q - p + 1 的较大值，赋给 max。最后输出 max 即可。 正确代码123456789101112131415161718192021222324class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s == null || s.length() == 0) &#123; return 0; &#125; char[] chars = s.toCharArray(); int len = chars.length; int p = 0, q = 0; int max = 0; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); while (q &lt; len) &#123; if (map.containsKey(chars[q])) &#123; // 防止p指针回溯，导致计算到重复字符的长度 // eg. abba,当q指向最右的a时，若简单把p赋为map.get(chars[q] + 1)，则出现指针回溯 p = Math.max(p, map.get(chars[q]) + 1); &#125; map.put(chars[q], q); max = Math.max(max, q - p + 1); ++q; &#125; return max; &#125;&#125; 涉及的知识点好像并没有啥新知识点，其实就是用map代替指针的作用。 第237题： 删除链表中的节点题目请编写一个函数，使其可以删除某个链表中给定的（非末尾）节点，你将只被给定要求被删除的节点。 示例示例 1 123输入: head = [4,5,1,9], node = 5输出: [4,1,9]解释: 给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 1 -&gt; 9 示例 2: 123输入: head = [4,5,1,9], node = 1输出: [4,5,9]解释: 给定你链表中值为 1 的第三个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 5 -&gt; 9 说明: 链表至少包含两个节点。 链表中所有节点的值都是唯一的。 给定的节点为非末尾节点并且一定是链表中的一个有效节点。 不要从你的函数中返回任何结果。 我的思路本题题干个人觉得没有交代的很清楚，让人感觉有点摸不着头脑，正常来说应该是给定两个参数，但是要写的函数只有一个参数，所以最开始会让人感觉很突兀，但是实际上这道题设计的很巧妙，不需要给定头结点，可以采用替身攻击，给定的node其实就可以当做头结点来处理，因为不可能是最后一个节点，所以后面一定有节点，故可以将node后节点牺牲掉，这样就相当于将node本身干掉了。 个人遇到的困难主要是在输入输出，尤其是构造单链表的过程花费了比较长的时间，总而言之还是对单链表的操作不够熟练，接下来会重点攻克单链表这一块的知识点！ ##我的代码（AC） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package 第237题_删除链表中的节点;import java.util.ArrayList;import java.util.Scanner;public class deleteNode &#123; public static void main(String[] args) &#123; System.out.println("请输入一串数字代表链表："); Scanner sc = new Scanner(System.in); String head = sc.nextLine(); char[] chars = head.toCharArray();// System.out.println(chars); ArrayList list = new ArrayList(); Node head_first = new Node(-1); Node temp = head_first; for(int i = 0 ; i &lt; chars.length ; i++)&#123; temp.next = new Node(Integer.parseInt(String.valueOf(chars[i]))); temp = temp.next; list.add(chars[i]); &#125; System.out.println("您输入的链表为： "+ list); System.out.println("请您输入要删除的数字："); String data = sc.nextLine(); int data_int = Integer.parseInt(data); Node temp1 = head_first; for(int i = 0 ; i &lt; chars.length ; i++)&#123; temp1 = temp1.next; if(data_int == temp1.data)&#123; deleteNode(temp1); break; &#125; &#125; System.out.println("删除后的链表为："); Node temp3 = head_first.next; for(int i = 0 ; i &lt; chars.length-1 ; i++) &#123; System.out.print(temp3.data + " "); temp3 = temp3.next; &#125; System.out.println("\n"); &#125; public static void deleteNode(Node node) &#123; Node tmp = node.next; node.data = node.next.data; node.next = node.next.next; tmp = null; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637package 第237题_删除链表中的节点;public class Node &#123; //数据域 public Integer data; //指针域，指向下一个节点 public Node next; public Node() &#123; &#125; public Node(int data) &#123; this.data = data; &#125; public Node(int data, Node next) &#123; this.data = data; this.next = next; &#125; public int getData() &#123; return data; &#125; public void setData(int Data) &#123; this.data = Data; &#125; public Node getNext() &#123; return next; &#125; public void setNext(Node Next) &#123; this.next = Next; &#125;&#125; 正确思路只提供 node 依然可以解决此题。只要把下个结点的 值 &amp; next 赋给当前 node，然后删除下个结点，就可以搞定。 正确代码12345678910111213141516171819202122/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public void deleteNode(ListNode node) &#123; // 保存下一个结点 ListNode tmp = node.next; // 将下个结点的值赋给当前要删除的结点 node.val = node.next.val; node.next = node.next.next; // tmp 置为空，让 jvm 进行垃圾回收 tmp = null; &#125;&#125; 涉及的知识点 链表的构建 链表删除 不足对链表操作还不是很驾轻就熟，接下来会重点训练链表操作。 第19题：删除链表的倒数第N个节点题目给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。 示例123给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2.当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5. 说明： 给定的 n 保证是有效的。 我的思路删除倒数第n个节点，这个思路比较简单，就是运用两个指针 fast 和 slow ，一个指针比另外一个多n-1步，这样的话当fast指针到最后一个节点的时候，slow指针刚好到达要删除的节点的位置，此时就可以用上题用过的替身牺牲法，牺牲掉要删除节点的下一个节点，只需要将下一个节点的值赋值给当前节点并且将slow.next = slow.next.next即可。但是，有特殊情况 ： 当要删除的节点是最后一个时，无法做到用下一个节点替换，这个时候就要提前预判，不能等到slow到了最后一个节点才考虑删除，要在slow.next.next == null时就考虑！！！ 我的代码（AC）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package 第19题;//import 第237题_删除链表中的节点.Node;import java.util.ArrayList;import java.util.Scanner;public class removeNthFromEnd &#123; public static void main(String[] args) &#123; System.out.println("请输入一串数字代表链表："); Scanner sc = new Scanner(System.in); String head = sc.nextLine(); char[] chars = head.toCharArray();// System.out.println(chars); ArrayList list = new ArrayList(); ListNode head_first = new ListNode(-1); ListNode temp = head_first; for(int i = 0 ; i &lt; chars.length ; i++)&#123; temp.next = new ListNode(Integer.parseInt(String.valueOf(chars[i]))); temp = temp.next; list.add(chars[i]); &#125; System.out.println("您输入的链表为： "+ list); System.out.print("请您输入要删除的倒数第n个数的节点，n = "); String number = sc.nextLine(); int num = Integer.parseInt(number); ListNode result_node = removeNthFromEnd(head_first.next,num); ListNode temp1 = result_node; while(temp1 != null)&#123;// int data = Integer.parseInt(String.valueOf(temp1.val)); System.out.print(temp1.val); temp1 = temp1.next; &#125; &#125; public static ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode pre = new ListNode(-1); pre.next = head; ListNode fast = pre; ListNode slow = pre; // 快指针先走 n-1 步 for (int i = 0; i &lt; n-1; ++i) &#123; fast = fast.next; &#125; while (fast.next != null &amp;&amp; slow.next.next != null) &#123; fast = fast.next; slow = slow.next; &#125;// ListNode tmp = slow.next; if(slow.next.next == null &amp;&amp; (n == 1)) &#123; slow.next = null; &#125; else &#123; slow.val = slow.next.val; slow.next = slow.next.next;// tmp = null; &#125; return pre.next; &#125;&#125; 12345678910package 第19题;public class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; &#125; &#125; 正确思路快指针 fast 先走 n 步，接着快指针 fast 与慢指针 slow 同时前进，等到快指针指向链表最后一个结点时，停止前进。然后将 slow 的 next 指向 slow.next.next，即删除了第 n 个结点。最后返回头指针。 这里设置了 pre 虚拟结点(指向 head )是为了方便处理只有一个结点的情况。 正确代码12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode removeNthFromEnd(ListNode head, int n) &#123; ListNode pre = new ListNode(-1); pre.next = head; ListNode fast = pre; ListNode slow = pre; // 快指针先走 n 步 for (int i = 0; i &lt; n; ++i) &#123; fast = fast.next; &#125; while (fast.next != null) &#123; fast = fast.next; slow = slow.next; &#125; slow.next = slow.next.next; return pre.next; &#125;&#125; 涉及的知识点单链表的删除…比较简单啦 不足个人感觉其实答案的解法还是要比我的好一些，它是通过直接找到删除节点的前一个，这样就非常好处理了，而且还没有特殊情况…我就很笨了，还自以为是的用了一个替身攻击的方法…学到了！！！要记得用到删除节点的前一个节点，这才是单链表的关键。 第21题：合并两个有序链表题目将两个有序链表合并为一个新的有序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例12输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 我的思路创建一个新链表，然后比较两个链表，哪个小就让新链表的next指向他，如果有一个提前结束了，剩下的链表接上新链表的后半部分。 我的代码（AC）123456789101112131415161718192021public static ListNode merge(ListNode l1, ListNode l2)&#123; ListNode prehead = new ListNode(-1); ListNode prev = prehead; while (l1 != null &amp;&amp; l2 != null) &#123; if (l1.val &lt;= l2.val) &#123; prev.next = l1; l1 = l1.next; &#125; else &#123; prev.next = l2; l2 = l2.next; &#125; prev = prev.next; &#125; // exactly one of l1 and l2 can be non-null at this point, so connect // the non-null list to the end of the merged list. prev.next = l1 == null ? l2 : l1; return prehead.next; &#125; 1234567package 第21题;public class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; &#125;&#125; 正确思路利用链表天然的递归性。如果 l1 为空，返回 l2；如果 l2 为空，返回 l1。如果 l1.val &lt; l2.val，返回 l1-&gt;mergeTwoLists(l1.next, l2)；否则返回 l2-&gt;mergeTwoLists(l1, l2.next)。 正确代码123456789101112131415161718192021222324/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) &#123; return l2; &#125; if (l2 == null) &#123; return l1; &#125; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125;&#125; 涉及的知识点链表算法题面试必看必看必看！！！！！！！ 第23题：合并K个排序链表题目合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。 示例1234567输入:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 我的思路我没做出来，然后看了下讨论区，大概总结出三种思路： 运用优先级队列，将整个链表扔到优先级队列中，然后一个个取出来就可以了，这种思路代码实现比较简单，但是用了人家封装好的东西，总感觉有点投机取巧的感觉… 运用分治归并的思想，K个链表两两进行归并。 强行归并，两个归并完直接放到后者，然后后者再跟后面的排序，这样复杂度很高。 我的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode mergeKLists(ListNode[] lists) &#123; if (lists == null || lists.length == 0) &#123; return null; &#125; int len = lists.length; if (len == 1) &#123; return lists[0]; &#125; // 合并前后两个链表，结果放在后一个链表位置上，依次循环下去 for (int i = 0; i &lt; len - 1; ++i) &#123; lists[i + 1] = mergeTwoLists(lists[i], lists[i + 1]); &#125; return lists[len - 1]; &#125; /** * 合并两个有序链表 * @param l1 * @param l2 * @return listNode */ private ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) &#123; return l2; &#125; if (l2 == null) &#123; return l1; &#125; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; l2.next = mergeTwoLists(l1, l2.next); return l2; &#125;&#125; 正确思路 第一种，优先级队列，20ms, 复杂度 时间复杂度： O(Nlogk) ，其中 k 是链表的数目。弹出操作时，比较操作的代价会被优化到 O(logk) 。同时，找到最小值节点的时间开销仅仅为 O(1)。最后的链表中总共有 N 个节点。 空间复杂度：O(n) 。创造一个新的链表需要 O(n) 的开销。O(k) 。以上代码采用了重复利用原有节点，所以只要 O(1) 的空间。同时优先队列（通常用堆实现）需要 O(k) 的空间（远比大多数情况的 N要小）。 过程： 1.因为链表有序，所以用每个链表的首元素构建初试堆(小顶堆) – 的队列 2.首元素出队，该元素next指向元素入队 第二种，归并分治，典型的归并分治思想，自底向上，依次合并(可结合归并排序理解，将每个链表理解成排序的值)。 复杂度分析 时间复杂度： O(Nlogk) ，其中 k 是链表的数目。空间复杂度：O(1)，我们可以用 O(1) 的空间实现两个有序链表的合并。 第三种，强行做。见我的代码，170ms 用第一个链依次和后面的所有链进行双链合并，利用021的双顺序链合并，秒杀！但是效率极低， 时间复杂度是O(x(a+b) + (x-1)(a+b+c) + … + 1 * (a+b+…+z);[a-z]是各链表长度，x表示链表个数-1，可见时间复杂度是极大的。 正确代码 优先级队列 12345678910111213141516171819202122public ListNode mergeKLists(ListNode[] lists) &#123; int len = 0; if((len=lists.length)==0 || lists == null) return null; ListNode preHead = new ListNode(-1); ListNode preNode = preHead; PriorityQueue&lt;ListNode&gt; queue = new PriorityQueue&lt;&gt;(len, new Comparator&lt;ListNode&gt;() &#123; @Override public int compare(ListNode o1, ListNode o2) &#123; return o1.val - o2.val; &#125; &#125;); for (ListNode node : lists) &#123; if(node!=null) queue.add(node); &#125; while(!queue.isEmpty())&#123; ListNode small = queue.poll(); preNode.next = small; if(small.next!=null) queue.add(small.next); //将最小值节点后面的节点添加到队里中 preNode = preNode.next; &#125; return preHead.next; &#125; 归并分治 1234567891011121314151617181920212223242526class Solution &#123; public ListNode mergeKLists(ListNode[] lists) &#123; if (lists == null || lists.length == 0) return null; return merge(lists, 0, lists.length - 1); &#125; private ListNode merge(ListNode[] lists, int left, int right) &#123; if (left == right) return lists[left]; int mid = left + (right - left) / 2; ListNode l1 = merge(lists, left, mid); ListNode l2 = merge(lists, mid + 1, right); return mergeTwoLists(l1, l2); &#125; private ListNode mergeTwoLists(ListNode l1, ListNode l2) &#123; if (l1 == null) return l2; if (l2 == null) return l1; if (l1.val &lt; l2.val) &#123; l1.next = mergeTwoLists(l1.next, l2); return l1; &#125; else &#123; l2.next = mergeTwoLists(l1,l2.next); return l2; &#125; &#125; &#125; 涉及的知识点包括了优先级队列、最小堆、归并以及 分治的思想 优先级队列。java中的优先级队列是PriorityQueue，是通过最小堆实现的 最小堆 归并 分治 第5题 最长回文子串题目给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例示例 1： 123输入: "babad"输出: "bab"注意: "aba" 也是一个有效答案。 示例 2： 12输入: "cbbd"输出: "bb" 我的思路 最开始的想法是将源字符串翻转，然后判断翻转后的字符串和源字符串的最长公共子序列，但是貌似有点问题，例如accbbdcca翻转后变为accdbbcca,最长公共子序列为acc，但是最长回文子串为bb。 但是上面的思路有可取之处，其实遇到回文子串最核心的问题是从中间开始依次比较左右是否相等，直到不相等，返回左右相等的子串，当然还有一个问题，就是该回文子串可能是单数，也可能是双数，单数的话，直接比较该数的左右即可，双数则需要先判断最开始两数是否相等。 时间复杂度是O(n²)，空间复杂度O(1) 我的代码（AC）美其名曰 中心扩展算法 12345678910111213141516171819202122232425262728package Dynamic_Programming.最长回文子串;public class Solution &#123; public static String longestPalindrome(String s) &#123; String res = ""; for(int i = 0;i &lt; s.length(); i++)&#123; String single = palindrome(s,i,i); String dou = palindrome(s,i,i+1); res = res.length() &gt; single.length() ? res : single; res = res.length() &gt; dou.length() ? res : dou; &#125; return res; &#125; private static String palindrome(String s, int left, int right) &#123; char[] s1 = s.toCharArray(); while(left &gt;= 0 &amp;&amp; right &lt; s.length() &amp;&amp; s1[left] == s1[right])&#123; left--; right++; &#125; return s.substring(left + 1,right); &#125; public static void main(String[] args) &#123; String res = longestPalindrome("aaabaacc"); System.out.println(res); &#125;&#125; 正确思路和代码dp12345678910111213141516171819202122232425 private static String dp(String s)&#123; //每次都忘记了边界处理 if(s == null || s.length() &lt; 1)&#123; return ""; &#125; char[] s1 = s.toCharArray(); int len = s1.length; int max = 1;// int over = 1; int start = 0; boolean res[][] = new boolean[len][len]; //此时的j是最右边的数 //此时的i是最左边的数 for(int j = 0; j &lt; s1.length; j++)&#123; for(int i = 0; i &lt;= j ;i++ )&#123; res[i][j] = (j &lt;= i + 2) ? s1[i] == s1[j] : res[i+1][j-1] &amp;&amp; s1[i] == s1[j]; if(res[i][j] &amp;&amp; max &lt; j - i + 1)&#123; max = j - i + 1; start = i; &#125; &#125; &#125; return s.substring(start,start + max); &#125; 关键就是暴力法： res[i][j] = (j &lt;= i + 2) ? s1[i] == s1[j] : res[i+1][j-1] &amp;&amp; s1[i] == s1[j]; 上面就是dp最为关键的状态转移递推式，为什么在dp中不用考虑回文串长度的奇偶呢，因为我的方法中是从中间扩散，那么就必然需要分类，而dp是从两边向中间靠，要是回文串首尾必须相同，而当回文串小于等于3时，只要比较的首尾相等，则无需再比较，这样回文串的奇偶就不需要再考虑了。 细细想来，其实dp就是中心扩展方法的逆，一个是从中间向两边发散，一个是两边向中间靠拢！！ Tips： 注意边界处理，因为 str.substring 这个是不允许字符串为 null 的 时间复杂度为O(n²)，空间复杂度为O(n²) 注意二维数组的维度分别是代表首和尾，子串是否为回文串取决于子子串和首尾是否相等，要注意base case是子串为1个字符时，它必为回文子串 最核心的就是状态转移的条件，是分为两种小情况，一种是当源字符串长度&lt;=3时 当源字符串元素个数为3个，若左右边界相等，则去掉他们，只剩一个字符，必为回文串 当源字符串元素个数为2个，若左右边界相等，则必为回文串 此时该串是否为回文串就取决于首尾，另一种情况是当源字符串长度&gt;3时，则需要判断首尾是否相等并且去除首尾后的子串是否为回文串 当发现有回文串时，则判断一下长度是否比之前发现的长，如果是，则记录长度，并且将最长回文串的起始位置拿到，最后全部循环完一遍后截取最长回文串即可 部分知识点补充暂无 编辑距离(72)题目描述给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 123456输入：word1 = "horse", word2 = "ros"输出：3解释：horse -&gt; rorse (将 'h' 替换为 'r')rorse -&gt; rose (删除 'r')rose -&gt; ros (删除 'e') 示例 2： 12345678输入：word1 = &quot;intention&quot;, word2 = &quot;execution&quot;输出：5解释：intention -&gt; inention (删除 &apos;t&apos;)inention -&gt; enention (将 &apos;i&apos; 替换为 &apos;e&apos;)enention -&gt; exention (将 &apos;n&apos; 替换为 &apos;x&apos;)exention -&gt; exection (将 &apos;n&apos; 替换为 &apos;c&apos;)exection -&gt; execution (插入 &apos;u&apos;) 代码12345678910111213141516171819202122232425class Solution &#123; public int minDistance(String s1, String s2) &#123; int[][] dp = new int[s1.length()+1][s2.length()+1]; for(int i = 0;i&lt;= s1.length();i++)&#123; dp[i][0] = i; &#125; for(int j = 0;j&lt;=s2.length();j++)&#123; dp[0][j] = j; &#125; for(int i = 1;i&lt;= s1.length();i++)&#123; for(int j = 1;j &lt;= s2.length();j++)&#123; if(s1.charAt(i-1) == s2.charAt(j-1))&#123; dp[i][j] = dp[i-1][j-1]; &#125; else&#123; dp[i][j] = Math.min(dp[i-1][j-1] + 1,dp[i-1][j] + 1); dp[i][j] = Math.min(dp[i][j],dp[i][j-1] + 1); &#125; &#125; &#125; return dp[s1.length()][s2.length()]; &#125;&#125; 会议室 II(253)题目描述给定一个会议时间安排的数组，每个会议时间都会包括开始和结束的时间 [[s1,e1],[s2,e2],…] (si &lt; ei)，为避免会议冲突，同时要考虑充分利用会议室资源，请你计算至少需要多少间会议室，才能满足这些会议安排。 示例 1: 12输入: [[0, 30],[5, 10],[15, 20]]输出: 2 示例 2: 12输入: [[7,10],[2,4]]输出: 1 代码12345678910111213141516171819class Solution &#123; public int minMeetingRooms(int[][] inter) &#123; if(inter == null || inter.length == 0) return 0; PriorityQueue&lt;Integer&gt; queue = new PriorityQueue&lt;&gt;(); Arrays.sort(inter, Comparator.comparingInt(a -&gt; a[0])); queue.add(inter[0][1]); for(int i = 1;i != inter.length;i++)&#123; int last = queue.peek(); if(last &lt;= inter[i][0])&#123; queue.poll(); queue.add(inter[i][1]); &#125; else &#123; queue.add(inter[i][1]); &#125; &#125; return queue.size(); &#125;&#125; 第96题 不同的二叉搜索树题目描述给定一个整数 n，求以 1 ... n 为节点组成的二叉搜索树有多少种？ 示例: 12345678910输入: 3输出: 5解释:给定 n = 3, 一共有 5 种不同结构的二叉搜索树: 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 解法原问题可拆解为子问题的求解。 二叉搜索树，可以分别以 1/2/3..n 做为根节点。所有情况累加起来，也就得到了最终结果。 res[n] 表示整数n组成的二叉搜索树个数。它的左子树可以有0/1/2...n-1 个节点，右子树可以有n-1/n-2...0 个节点。res[n] 是所有这些情况的加和。 时间复杂度分析：状态总共有 n 个，状态转移的复杂度是 O(n)，所以总时间复杂度是 O(n²)。 普通的dp 123456789101112class Solution &#123; public int numTrees(int n) &#123; int[] dp = new int[n+1]; dp[0] = 1; for(int i=1;i&lt;=n;i++)&#123; for(int j =1;j&lt;=i;j++)&#123; dp[i] += dp[j-1] * dp[i-j]; &#125; &#125; return dp[n]; &#125;&#125; 上面的解法明显还可以得到改进，因为左-右子树的节点个数为0,n-1,和左右子树节点个数为n-1,0。这两者的二叉搜索树的结果肯定是一致的，所以我们就没有必要算两遍。但是同时要考虑到奇偶的问题。 如果n=4，那么G(4) = G(0) G(3) + G(1) G(2) + G(2) G(1) + G(3) G(0) = 2[G(0) G(3) + G(1) G(2)] 如果n=5，那么G(5) = G(0) G(4) + G(1) G(3) + G(2) G(2) + G(3) G(1) + G(4) G(0) = 2[G(0) G(4) + G(1) G(3) ] + G(2) G(2) 12345678910111213141516171819202122232425262728class Solution &#123; public int numsTrees2(int n)&#123; if (n == 0) &#123; return 0; &#125; int[] dp = new int[n + 1]; dp[0] = 1; dp[1] = 1; // 长度为 1 到 n for (int i = 2; i &lt;= n; i++) &#123; // 将不同的数字作为根节点，只需要考虑到 len for (int root = 1; root &lt;= i / 2; root++) &#123; int left = root - 1; // 左子树的长度 int right = i - root; // 右子树的长度 dp[i] += dp[left] * dp[right]; &#125; dp[i] *= 2;// 利用对称性乘 2 // 考虑奇数的情况 if ((i &amp; 1) == 1) &#123; int root = (i &gt;&gt; 1) + 1; int left = root - 1; // 左子树的长度 int right = i - root; // 右子树的长度 dp[i] += dp[left] * dp[right]; &#125; &#125; return dp[n]; &#125;&#125; 知识点补充 LeetCode二叉树专题——&gt;DFS和BFS 第95题 不同的二叉搜索树II题目描述给定一个整数 n，生成所有由 1 … n 为节点所组成的二叉搜索树。 示例123456789输入: 3输出:[ [1,null,3,2], [3,2,null,1], [3,1,null,null,2], [2,1,3], [1,null,2,null,3]] 解释1234567以上的输出对应以下 5 种不同结构的二叉搜索树： 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 解法这题就是典型的运用递归去做，明确三个点： 递归出口 递归返回值 一级递归需要做什么 递归出口当树没有节点了，递归结束，怎么表示树没有节点呢，所以就需新建一个函数，参数包括节点的起始和终止。 递归返回值返回值很明显就是符合条件的各种二叉树,是一个含根节点的列表(根据题目最终需要得到的)。 一级递归需要做什么这个是比较难的地方，我们来缕缕现在有什么，我们现在有三个节点，根节点，左子树根节点，右子树根节点，这三个节点我们可以随意将任意一个节点当做根节点，然后去组合得到新的搜索二叉树。注意！！！我们只需要关注一级递归就可以了，无需关注太多，我们现在手上假设就三个节点分别是1,2,3，首先要做的就是遍历1，2，3，分别将其作为根节点，假设以2为根节点，1代表就是左子树返回的根节点列表，3代表的是右子树返回的根节点列表，我们要做的就是遍历左右子树的根节点列表，分别将其添加到根节点的左右子树，然后将该根节点添加至列表，返回列表即可。 代码1234567891011121314151617181920212223242526272829303132333435/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if (n &lt;= 0) return new ArrayList&lt;&gt;(); return generateTrees(1, n); &#125; private List&lt;TreeNode&gt; generateTrees(int left, int right) &#123; List&lt;TreeNode&gt; list = new ArrayList&lt;&gt;(); if (left &gt; right) &#123; list.add(null); &#125; else &#123; for (int i = left; i &lt;= right; i++) &#123; List&lt;TreeNode&gt; leftTrees = generateTrees(left, i - 1); List&lt;TreeNode&gt; rightTrees = generateTrees(i + 1, right); for (TreeNode l : leftTrees) &#123; for (TreeNode r : rightTrees) &#123; TreeNode root = new TreeNode(i); root.left = l; root.right = r; list.add(root); &#125; &#125; &#125; &#125; return list; &#125;&#125; 部分知识点补充明天继续进军二叉树部分 同时复习并且捡回来原先要完成的集合那部分的源码分析！（2020.1.1） 杨辉三角(118)Tip:今天第一次写题解，还是非常开心的！！！！！ 今天重点就是掌握了一下递归的思想，最重要的三点！！！！！ 递归思想 找整个递归的终止条件 找返回值 本地递归需要如何操作 主要参考：递归 题目描述给定一个非负整数 numRows，生成杨辉三角的前 numRows 行。 在杨辉三角中，每个数是它左上方和右上方的数的和。 示例: 123456789输入: 5输出:[ [1], [1,1], [1,2,1], [1,3,3,1], [1,4,6,4,1]] 思路方法一：递归递归方法总而言之就是抓住三点： 找整个递归的终止条件 找返回值 一次递归需要如何操作 找整个递归的终止条件咱来分析一下题目，递归到numRows = 0 时或者numRows = 1时都可以终止，因为第一行比较特殊，只有一个1,所以我们可以将其当成整个递归的终止条件，当numRows = 1时，我们就可以终止递归向下返回值了。 找返回值找返回值，我们也需要分析下，题目要我们求的是整个杨辉三角的所有数，那最后递归得到的应该就是 List&lt;List&lt;Integer&gt;&gt; (题目给定)，也就是每递归完一层，我们就更新完List并返回即可，最后递归完成就是我们要的答案。 一次递归需要如何操作递归的难点就在这里，很多童靴刚学递归时，总是在这里搞晕，其实我们只需要关注一次递归即可，因为每一层递归的过程都是一样的，我们只需要找到最上层的递归的规律，就可以了。 如图所示，我们只需要分析第二行到第三行这级递归即可！先上代码！ 递归 代码12345678910111213141516171819202122232425262728293031class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; generate(int numRows) &#123; //存储要返回的杨辉三角 List&lt;List&lt;Integer&gt;&gt; dg = new ArrayList&lt;&gt;(); //若0行，则返回空 if(numRows == 0)&#123; return dg; &#125; //递归出口，这是第一步！找到出口 if(numRows == 1)&#123; dg.add(new ArrayList&lt;&gt;()); dg.get(0).add(1); return dg; &#125; //递归，注意返回值！！！这是第二步 dg = generate(numRows-1); //一级递归要做啥，我们可以看第二行到第三行需要做啥 //首先是要申请一个list来存储第三行，然后通过第二行得到第三行 //第三行的首尾为1是确定了的，然后就是中间的数如何得到 //通过观察很容易拿到for循环里面的式子 //最后别忘了返回值！！！ List&lt;Integer&gt; row = new ArrayList&lt;&gt;(); row.add(1); for(int j = 1;j &lt; numRows - 1;j++)&#123; row.add(dg.get(numRows-2).get(j-1) + dg.get(numRows-2).get(j)); &#125; row.add(1); dg.add(row); return dg; &#125;&#125; 方法二：动态规划思路其实差不多，只是一个递归，一个变成了迭代而，仅此而已！ 1234567891011121314151617181920212223class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; generate(int numRows) &#123; List&lt;List&lt;Integer&gt;&gt; dp = new ArrayList&lt;&gt;(); if(numRows == 0)&#123; return dp; &#125; dp.add(new ArrayList&lt;&gt;()); dp.get(0).add(1); //注意这里的 i 是指行数，但是dp是从0开始的 //所以preRow是i-2 for(int i = 2;i &lt;= numRows;i++)&#123; List&lt;Integer&gt; row = new ArrayList&lt;&gt;(); List&lt;Integer&gt; preRow = dp.get(i-2); row.add(1); for(int j = 1;j &lt; i-1;j++)&#123; row.add(preRow.get(j) + preRow.get(j-1)); &#125; row.add(1); dp.add(row); &#125; return dp; &#125;&#125; 单词拆分(139)题目给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。 说明： 拆分时可以重复使用字典中的单词。 你可以假设字典中没有重复的单词。 示例示例 1： 123输入: s = "leetcode", wordDict = ["leet", "code"]输出: true解释: 返回 true 因为 "leetcode" 可以被拆分成 "leet code"。 示例 2： 1234输入: s = "applepenapple", wordDict = ["apple", "pen"]输出: true解释: 返回 true 因为 "applepenapple" 可以被拆分成 "apple pen apple"。注意你可以重复使用字典中的单词。 1234示例 3：输入: s = "catsandog", wordDict = ["cats", "dog", "sand", "and", "cat"]输出: false 代码123456789101112131415class Solution &#123; public boolean wordBreak(String s, List&lt;String&gt; wordDict) &#123; boolean[] dp = new boolean[s.length()+1]; dp[0] = true; for(int i = 1;i &lt;= s.length();i++)&#123; for(int j = 0;j &lt; i;j++)&#123; if(dp[j] == true &amp;&amp; wordDict.contains(s.substring(j,i)))&#123; dp[i] = true; break; &#125; &#125; &#125; return dp[s.length()]; &#125;&#125; 解码方法(91)题目描述1234567一条包含字母 A-Z 的消息通过以下方式进行了编码：'A' -&gt; 1'B' -&gt; 2...'Z' -&gt; 26给定一个只包含数字的非空字符串，请计算解码方法的总数。 示例1234567891011示例 1:输入: "12"输出: 2解释: 它可以解码为 "AB"（1 2）或者 "L"（12）。示例 2:输入: "226"输出: 3解释: 它可以解码为 "BZ" (2 26), "VF" (22 6), 或者 "BBF" (2 2 6) 。 思路所有可以用dp的，基本都有三个方法： 递归 ——-&gt; 带备忘录的自顶向下 ——-&gt; dp 递归1234567891011121314151617181920212223242526272829class Solution &#123; public int numDecodings(String s) &#123; if(s.length() == 0 || s == null)&#123; return 0; &#125; return numDecodingsByRecrusion(s,0,s.length()-1); &#125; private int numDecodingsByRecrusion(String s,int start,int end)&#123; //1.递归出口：数字为最后一个数字时即结束递归 //2.返回值：返回解码方法数 //3.一次递归的过程 if(start &gt; end)&#123; return 1; &#125; if(s.charAt(start) == '0')&#123; return 0; &#125; if(start == end)&#123; return 1; &#125; int num1 = numDecodingsByRecrusion(s,start+1,end); int num2 = 0; if((s.charAt(start) - '0') * 10 + (s.charAt(start+1)-'0') &lt;= 26)&#123; num2 = numDecodingsByRecrusion(s,start+2,end); &#125; return num1+num2; &#125;&#125; 带备忘录的自顶向下123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public int numDecodings(String s) &#123; if(s.length() == 0 || s == null)&#123; return 0; &#125; // int res = numDecodingsByRecrusion(s,0,s.length()-1); // int res = numDecodingsByDp(s); HashMap&lt;Integer,Integer&gt; memo = new HashMap&lt;&gt;(); return numDecodingsByMemo(s,0,s.length()-1,memo); &#125; private int numDecodingsByMemo(String s, int start, int end, HashMap&lt;Integer, Integer&gt; memo)&#123; //1.递归出口：数字为最后一个数字时即结束递归 //2.返回值：返回解码方法数 //3.一次递归的过程 if(start &gt; end)&#123; return 1; &#125; if(s.charAt(start) == '0')&#123; return 0; &#125; if(start == end)&#123; return 1; &#125; if(memo.containsKey(start))&#123; return memo.get(start); &#125; int num1 = numDecodingsByMemo(s,start+1,end,memo); int num2 = 0; if((s.charAt(start) - '0') * 10 + (s.charAt(start+1)-'0') &lt;= 26)&#123; num2 = numDecodingsByMemo(s,start+2,end,memo); &#125; memo.put(start,num1+num2); return num1+num2; &#125;&#125; dp注意：难点在处理&#39;0&#39;,&#39;00&#39;等边界问题！尤其是在dp[n]、dp[n-1]的赋值问题上有一点难度，而且，这个由于是倒序的，跟平时处理的dp问题略微有些许不同，以前是dp[1]对应第一个字符，而这里是dp[0]对应第一个字符。 123456789101112131415161718192021222324252627class Solution &#123; public int numDecodings(String s) &#123; int len = s.length(); int[] dp = new int[len + 1]; dp[len] = 1; //将递归法的结束条件初始化为 1 //最后一个数字不等于 0 就初始化为 1 if (s.charAt(len - 1) != '0') &#123; dp[len - 1] = 1; &#125; for (int i = len - 2; i &gt;= 0; i--) &#123; //当前数字时 0 ，直接跳过，0 不代表任何字母 if (s.charAt(i) == '0') &#123; continue; &#125; int ans1 = dp[i + 1]; //判断两个字母组成的数字是否小于等于 26 int ans2 = 0; int ten = (s.charAt(i) - '0') * 10; int one = s.charAt(i + 1) - '0'; if (ten + one &lt;= 26) &#123; ans2 = dp[i + 2]; &#125; dp[i] = ans1 + ans2; &#125; return dp[0]; &#125;&#125; 零钱兑换(322)一维dp数组，简单的一批… 12345678910111213141516class Solution &#123; public int coinChange(int[] coins, int amount) &#123; //非常明显的使用dp int[] dp = new int[amount+1]; //代表的是对应钱数所需的最少的硬币个数 Arrays.fill(dp,amount+1); dp[0] = 0; for(int i = 0;i &lt;= amount;i++)&#123; for(int j = 0;j &lt; coins.length;j++)&#123; if(coins[j] &lt;= i)&#123; dp[i] = Math.min(dp[(i-coins[j])]+1,dp[i]); &#125; &#125; &#125; return dp[amount] == amount+1 ? -1:dp[amount]; &#125;&#125; 回文子串(647)思路同第5题！！！！ 12345678910111213141516class Solution &#123; public int countSubstrings(String s) &#123; int res = 0; boolean dp[][] = new boolean[s.length()][s.length()]; for (int i = 0; i &lt; s.length(); i++) &#123; for (int j = 0; j &lt;= i; j++) &#123; //dp的出口就是长度在3以内(包括3),注意j为开始，i为结束 if (s.charAt(j) == s.charAt(i) &amp;&amp; ((i - j &lt;= 2) || dp[j + 1][i - 1])) &#123; dp[j][i] = true; res++; &#125; &#125; &#125; return res; &#125;&#125; 环形链表I(141)题目给定一个链表，判断链表中是否有环。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。 思路 方法一：使用Set集合存储已经出现过的节点，如果再次出现，则直接返回true，如果没有再出现，则一定会有null节点的出现。 方法二：运用快慢指针的方法，快指针走得快，如果遍历到null节点，则说明不存在环，如果快指针和慢指针会相遇，则说明存在环。 代码 方法一 123456789101112131415public class Solution &#123; public boolean hasCycle(ListNode head) &#123; HashSet&lt;ListNode&gt; set = new HashSet&lt;&gt;(); while(head != null)&#123; if(!set.contains(head))&#123; set.add(head); &#125; else&#123; return true; &#125; head = head.next; &#125; return false; &#125;&#125; 方法二 1234567891011121314151617public class Solution &#123; public boolean hasCycle(ListNode head) &#123; if(head == null || head.next == null)&#123; return false; &#125; ListNode slow = head.next; ListNode fast = head.next.next; while(slow != fast)&#123; if(fast == null || fast.next == null)&#123; return false; &#125; slow = slow.next; fast = fast.next.next; &#125; return true; &#125;&#125; 环形链表 II(142)这个题翻译的一坨屎…题目整来整去不知道在说什么，真的是服了耶！ 其实题目就一个意思，给定一个有环链表，要你返回环的入口！ 思路这种思路都要讲烂了吧..就是使用快慢指针，第一次相遇后，快指针继续走，慢指针回到起点，第二次相遇的地方就是环的起点。 代码12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; ListNode fast = head, slow = head; while (true) &#123; if (fast == null || fast.next == null) return null; fast = fast.next.next; slow = slow.next; if (fast == slow) break; &#125; fast = head; while (slow != fast) &#123; slow = slow.next; fast = fast.next; &#125; return fast; &#125;&#125; 4. 寻找两个有序数组的中位数题目给定两个大小为 m 和 n 的有序数组 nums1 和 nums2。 请你找出这两个有序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。 你可以假设 nums1 和 nums2 不会同时为空。 示例 1: 1234nums1 = [1, 3]nums2 = [2]则中位数是 2.0 示例 2: 1234nums1 = [1, 2]nums2 = [3, 4]则中位数是 (2 + 3)/2 = 2.5 思路 如果没有时间复杂度的话，那这道题有非常多的思路可以做，可以先排序再取中位数，可以采用两个有序数组进行归并排序，这样的时间复杂度为 O(m+n) ,达不到 O(log(m+n)) 看到log(m+n)，就应该想到二分查找，这道题也的确可以用二分去做。首先我们明确一下这里的中位数是什么意思？这里说的是求两个有序数组(m + n)的中位数，如果 m + n 为奇数，则中位数的下标为 (m + n + 1)/2 [下标从1开始！]，如果 m + n 为偶数，则中位数的值为下标为 (m + n + 1)/2 、(m + n + 2)/2的数的平均值，所以其实可以统一一下，也就是不论奇数还是偶数，中位数的值 = 下标为 (m + n + 1)/2 、(m + n + 2)/2的数的平均值！！！所以问题就转换为求第 (m + n + 1)/2、 (m + n + 2)/2大的数了，至于求第K大的数问题，这个方法就很多了，从远古的快排到堆排序，待会会拓展一下！回到正题，如何在两个有序数组中间取到第K大的数还必须是log级别的时间复杂度，那只能选择二分了，这里的二分比较特殊，是对两个数组取第K/2大的数，有人会问了，为何是这样取呢？因为在每次划分的时候，我们都要确保第K大的数未被去除，所以两个数组分别取K/2，这样能确保有 m + n - k个数是一定大于我们要取的数的，也就是说，我们的二分，就是一步步去除比中位数小的数，直到遍历到中位数为止，而什么时候能遍历到中位数呢？难点就在于边界比较复杂，比如：数组长度过短导致取不到K/2、K如果为1的话是不能取K/2的(下标从1开始，这样会越界)、单个数组可能已经遍历完另外一个还没遍历完等等。 代码实现方法一见归并排序的做法(时间复杂度其实是不符合要求的) 方法二123456789101112131415161718192021222324252627282930313233343536/** * 方法二 * @param nums1 * @param nums2 * @return */public static double findMedianSortedArrays(int[] nums1, int[] nums2) &#123; return (findMedianSortedArraysByRecursion(nums1,0,nums2,0,(nums1.length + nums2.length + 1)/2) + findMedianSortedArraysByRecursion(nums1,0,nums2,0,(nums1.length + nums2.length + 2)/2))/2.0;&#125;/** * 递归三部曲 * 1、找到递归出口，当遍历的数的下标大于等于数组本身长度，说明可以结束了，注意这里是大于等于哦，因为下标是从0开始的！ * 还要一个递归出口就是，当k为1时，即找最小的那个数，则直接比较数组第一个元素即可 * 2、找到返回值，返回第K大的数的值即可 * 3、每一轮递归需要做的事情，我们需要从起始点开始，寻找第K大的数，即分别在两个数组中找到第K/2大的数 * 若一个数组中找不到第K/2个数，则说明另外一个数组的前K/2的数对我们没有意义 * @param nums1 * @param nums1_start * @param nums2 * @param nums2_start * @param k * @return */private static double findMedianSortedArraysByRecursion(int[] nums1, int nums1_start, int[] nums2, int nums2_start, int k) &#123; if(nums1_start &gt;= nums1.length) return nums2[nums2_start+k-1]; if(nums2_start &gt;= nums2.length) return nums1[nums1_start+k-1]; if(k == 1)&#123; return Math.min(nums1[nums1_start],nums2[nums2_start]); &#125; int mid_num1 = (nums1_start + k/2 - 1) &lt; nums1.length ? nums1[nums1_start + k/2 -1] : Integer.MAX_VALUE; int mid_num2 = (nums2_start + k/2 - 1) &lt; nums2.length ? nums2[nums2_start + k/2 -1] : Integer.MAX_VALUE; return mid_num1 &gt; mid_num2 ? findMedianSortedArraysByRecursion(nums1,nums1_start,nums2,nums2_start+k/2,k-k/2) : findMedianSortedArraysByRecursion(nums1,nums1_start+k/2,nums2,nums2_start,k-k/2);&#125; 17. 电话号码的字母组合题目给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。 给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。 示例: 12输入："23"输出：["ad", "ae", "af", "bd", "be", "bf", "cd", "ce", "cf"]. 思路队列先定义一个String字符串数组，然后遍历输入的数字，拿到每一个数字，然后利用队列，先将前一次循环的字符串遍历(这里非常巧妙，因为 i 从 0 开始，那么第 n 次循环的 i 为 n-1 ,即是前一次循环的字符串的长度)，每次遍历前一次循环的字符串，就将其出队列，然后将新的字符串拼接好直接放入队列，类似于树的层序遍历，只是这里非常巧妙地运用了队头元素的长度和 i 的关系。 回溯回溯是一种通过穷举所有可能情况来找到所有解的算法。如果一个候选解最后被发现并不是可行解，回溯算法会舍弃它，并在前面的一些步骤做出一些修改，并重新尝试找到可行解。 给出如下回溯函数 backtrack(combination, next_digits) ，它将一个目前已经产生的组合 combination 和接下来准备要输入的数字 next_digits 作为参数。 如果没有更多的数字需要被输入，那意味着当前的组合已经产生好了。如果还有数字需要被输入：遍历下一个数字所对应的所有映射的字母。将当前的字母添加到组合最后，也就是 combination = combination + letter。重复这个过程，输入剩下的数字： backtrack(combination + letter, next_digits[1:])。 代码实现队列实现1234567891011121314151617class Solution &#123; public List&lt;String&gt; letterCombinations(String digits) &#123; LinkedList&lt;String&gt; ans = new LinkedList&lt;String&gt;(); if(digits.isEmpty()) return ans; String[] mapping = new String[] &#123;"0", "1", "abc", "def", "ghi", "jkl", "mno", "pqrs", "tuv", "wxyz"&#125;; ans.add(""); for(int i =0; i&lt;digits.length();i++)&#123; int x = Character.getNumericValue(digits.charAt(i)); while(ans.peek().length()==i)&#123; String t = ans.remove(); for(char s : mapping[x].toCharArray()) ans.add(t+s); &#125; &#125; return ans; &#125;&#125; 回溯实现1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; Map&lt;String, String&gt; phone = new HashMap&lt;String, String&gt;() &#123;&#123; put("2", "abc"); put("3", "def"); put("4", "ghi"); put("5", "jkl"); put("6", "mno"); put("7", "pqrs"); put("8", "tuv"); put("9", "wxyz"); &#125;&#125;; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); public void backtrack(String combination, String next_digits) &#123; // if there is no more digits to check if (next_digits.length() == 0) &#123; // the combination is done output.add(combination); &#125; // if there are still digits to check else &#123; // iterate over all letters which map // the next available digit String digit = next_digits.substring(0, 1); String letters = phone.get(digit); for (int i = 0; i &lt; letters.length(); i++) &#123; String letter = phone.get(digit).substring(i, i + 1); // append the current letter to the combination // and proceed to the next digits backtrack(combination + letter, next_digits.substring(1)); &#125; &#125; &#125; public List&lt;String&gt; letterCombinations(String digits) &#123; if (digits.length() != 0) backtrack("", digits); return output; &#125;&#125; 22. 括号生成##题目 给出 n 代表生成括号的对数，请你写出一个函数，使其能够生成所有可能的并且有效的括号组合。 例如，给出 n = 3，生成结果为： 1234567[ "((()))", "(()())", "(())()", "()(())", "()()()"] 思路dp简单来说，在求N个括号的排列组合时，把第N种情况（也就是N个括号排列组合）视为单独拿一个括号E出来，剩下的N-1个括号分为两部分，P个括号和Q个括号，P+Q=N-1，然后这两部分分别处于括号E内和括号E的右边，各自进行括号的排列组合。由于我们是一步步计算得到N个括号的情况的，所以小于等于N-1个括号的排列组合方式我们是已知的（用合适的数据结构存储，方便后续调用，且在存储时可利用特定数据结构实现题目某些要求，如排序，去重等），且P+Q=N-1，P和Q是小于等于N-1的，所以我们能直接得到P个和Q个括号的情况，进而得到N个括号的结果！ 这个算法主要的基点就是将排列组合的情况分为了括号内和括号外这两种情况，且仅存在两种情况！至于为什么，原因在于楼主的算法的前提是单独拿出来的括号E的左边在N个括号所有排列组合情况中都是处于最左边，所以不存在括号位于括号E的左边的情况。因此，N-1个括号（拿出了括号E）仅可能分布于括号E内和括号E外，分为两种子情况讨论！ 这种思想还可以应用于其他类似的题的求解中，即怎样合理高效的利用前面步骤的计算结果得出当前步骤结果，从而得出最终结果。 递归只有在我们知道序列仍然保持有效时才添加 ‘(‘ or ‘)’，我们可以通过跟踪到目前为止放置的左括号和右括号的数目来做到这一点，如果我们还剩一个位置，我们可以开始放一个左括号。 如果它不超过左括号的数量，我们可以放一个右括号。 代码实现dp123456789101112131415161718192021222324252627class Solution &#123; public List&lt;String&gt; generateParenthesis(int n) &#123; // dp.get(i)表示我们要求的List，dp.get(0)即表示由0个括号组成的字符串的列表 List&lt;LinkedList&lt;String&gt;&gt; dp = new LinkedList&lt;&gt;(); if( n == 0) return dp.get(0); LinkedList&lt;String&gt; zero = new LinkedList&lt;&gt;(); zero.add(""); dp.add(zero); //计算dp数组的值 for(int i = 1;i &lt;= n;i++)&#123; LinkedList&lt;String&gt; single = new LinkedList&lt;&gt;(); //每个dp的值都是由两部分组成的 for(int j = 0;j &lt; i;j++)&#123; LinkedList&lt;String&gt; str1 = dp.get(j); LinkedList&lt;String&gt; str2 = dp.get(i-j-1); for(String s1:str1)&#123; for(String s2:str2)&#123; String str = "(" + s1 + ")" + s2; single.add(str); &#125; &#125; &#125; dp.add(single); &#125; return dp.get(n); &#125;&#125; 递归1234567891011121314151617181920212223/** * 剪枝 + 回溯 * 递归三部曲 * 1、找到递归出口，当当前字符串的长度为2n时，即该字符串添加完成 * 2、返回值，无需返回任何东西，只需要在每次递归完成时将拼接好的字符串加到要返回的列表即可 * 3、一次递归需要做的事：如果左括号，小于n，则需要open+1，如果右括号小于左括号，则需要添加右括号 * @param n * @return */public static List&lt;String&gt; generateParenthesis(int n) &#123; List&lt;String&gt; res = new LinkedList&lt;&gt;(); backtrack(res,"",0,0, n); return res;&#125;public static void backtrack(List&lt;String&gt; ans, String cur, int open, int close, int max)&#123; if (cur.length() == max * 2) &#123; ans.add(cur); return; &#125; if (open &lt; max) backtrack(ans, cur+"(", open+1, close, max); if (close &lt; open) backtrack(ans, cur+")", open, close+1, max);&#125; 31. 下一个排列题目实现获取下一个排列的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列。 如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。 必须 原地 修改，只允许使用额外常数空间。 以下是一些例子，输入位于左侧列，其相应输出位于右侧列。 12341,2,3 → 1,3,23,2,1 → 1,2,31,1,5 → 1,5,1158476531 -&gt; 158513467 思路 从右向左，找到第一个非倒序的数字，例如上面的158476531，从右向左的第一个非倒序的数字是4 然后再从右向左遍历一次，找到第一个比4大的数，这里是右边数第三个 5，交换 4 和 5,数字变为 158576431,这显然不是我们要的答案 然后将 5 后面的数字翻转，也就是将 76431 翻转,于是数字变为 158513467 代码实现12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public void nextPermutation(int[] nums) &#123; int i = nums.length - 2; for(; i &gt;= 0; i--)&#123; if(nums[i] &lt; nums[i+1])&#123; break; &#125; &#125; if(i &gt;= 0)&#123; int j = nums.length -1; for(; j &gt; i; j--)&#123; if(nums[j] &gt; nums[i])&#123; break; &#125; &#125; swap(nums,i,j); &#125; reverse(nums,i+1); System.out.println(Arrays.toString(nums)); &#125; private void reverse(int[] nums, int start) &#123; int i = start, j = nums.length - 1; while (i &lt; j) &#123; swap(nums, i, j); i++; j--; &#125; &#125; private void swap(int[] nums, int i, int j) &#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; &#125;&#125; 32. 最长有效括号题目给定一个只包含 ‘(‘ 和 ‘)’ 的字符串，找出最长的包含有效括号的子串的长度。 示例 1: 123输入: "(()"输出: 2解释: 最长有效括号子串为 "()" 示例 2: 123输入: ")()())"输出: 4解释: 最长有效括号子串为 "()()" 思路 思路一：栈 + dp 前文第20题 有效的括号 ，就是采用 栈 去做的，那么这个很自然的也同样是采用栈去做，凡是碰到了括号，基本都是用栈去做处理，因为符合后进先出的原则。 然后又是寻找子串长度，那自然是要用到dp，故采用的方法很明显就是栈 + dp。 思路二：栈 与找到每个可能的子字符串后再判断它的有效性不同，我们可以用栈在遍历给定字符串的过程中去判断到目前为止扫描的子字符串的有效性，同时都是最长有效字符串的长度。我们首先将 −1 放入栈顶。对于遇到的每个 ‘(’ ，我们将它的下标放入栈中。对于遇到的每个 ‘)’ ，我们弹出栈顶的元素并将当前元素的下标与弹出元素下标作差，得出当前有效括号字符串的长度。通过这种方法，我们继续计算有效子字符串的长度，并最终返回最长有效子字符串的长度 我认为这种思路最精妙的地方在于他提前压栈了-1，这是这个方法最妙的地方，而在遍历元素时，每次都会有压栈或者弹栈的操作，这样就能知道最长有效字符串的长度，因为一旦不是有效字符串了，栈就会变空，下一个填入的一定是有效字符串的前一位…不过我还是觉得我的方法更容易让人理解，这个方法只能欣赏了。 作者：LeetCode链接：https://leetcode-cn.com/problems/longest-valid-parentheses/solution/zui-chang-you-xiao-gua-hao-by-leetcode/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 代码 思路一 1234567891011121314151617181920212223242526272829class Solution &#123; public int longestValidParentheses(String s) &#123; Stack&lt;Integer&gt; stack=new Stack&lt;Integer&gt;(); int max = 0; // 保存上一个有效括号的长度 int last_num; int[] num = new int[s.length()+1]; boolean[] dp = new boolean[s.length()+1]; for(int i = 0;i &lt; s.length();i++)&#123; if(s.charAt(i) == '(')&#123; stack.push(i); &#125; else if(!stack.isEmpty() &amp;&amp; s.charAt(i) == ')')&#123; //这里的是i是从0开始的，所以这其实是第 i+1 个数 dp[i+1] = true; //如果取出来的数对应的索引的前一位也是有效括号，注意哦，这里的dp是从1开始的！ // 所以不用减1了，直接就是dp[stack.peek()],num数组同理 // dp数组和num数组，索引值为 i+1 if(dp[stack.peek()] == true)&#123; last_num = num[stack.peek()]; &#125; else last_num = 0; num[i+1] = i - stack.pop() + 1 + last_num; max = Math.max(max,num[i+1]); &#125; &#125; return max; &#125;&#125; 思路二 【精妙无比，适当记忆】 123456789101112131415161718192021public class Solution &#123; public int longestValidParentheses(String s) &#123; int maxans = 0; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); stack.push(-1); for (int i = 0; i &lt; s.length(); i++) &#123; if (s.charAt(i) == '(') &#123; stack.push(i); &#125; else &#123; stack.pop(); if (stack.empty()) &#123; stack.push(i); &#125; else &#123; maxans = Math.max(maxans, i - stack.peek()); &#125; &#125; &#125; return maxans; &#125;&#125; 5152. 将矩阵按对角线排序题目给你一个 m * n 的整数矩阵 mat ，请你将同一条对角线上的元素（从左上到右下）按升序排序后，返回排好序的矩阵。 示例 1： 12输入：mat = [[3,3,1,1],[2,2,1,2],[1,1,1,2]]输出：[[1,1,1,1],[1,2,2,2],[1,2,3,3]] 思路 使用的是 N皇后 问题的编码技巧：主对角线上元素的特点是：纵坐标 - 横坐标 = 定值 【难点】 为了能够放进数组中，加上偏移 m - 1 。【难点】 两次遍历：第一次遍历把数据拷贝到对角线数组中，然后排序；第二次遍历把对角线数组写回原始数组（或者新开一个数组）均可。 作者：liweiwei1419链接：https://leetcode-cn.com/problems/sort-the-matrix-diagonally/solution/bao-li-jie-fa-by-liweiwei1419/来源：力扣（LeetCode） 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.ArrayList;import java.util.Arrays;import java.util.Collections;public class Solution &#123; public int[][] diagonalSort(int[][] mat) &#123; // 行数 int m = mat.length; // 列数 int n = mat[0].length; // 主对角线的条数 int dLen = m + n - 1; // 每一条对角线都创建一个动态数组 ArrayList&lt;Integer&gt;[] diagonal = new ArrayList[dLen]; for (int i = 0; i &lt; dLen; i++) &#123; diagonal[i] = new ArrayList&lt;&gt;(m); &#125; // 遍历原始矩阵，把原始矩阵中的元素放进对应的动态数组中 // 主对角线上元素的特点是：纵坐标 - 横坐标 = 定值 // 加上偏移 m - 1 是为了能够放进数组中 for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; diagonal[j - i + (m - 1)].add(mat[i][j]); &#125; &#125; // 对每一个对角线上的动态数组分别进行升序排序 for (int i = 0; i &lt; dLen; i++) &#123; Collections.sort(diagonal[i]); &#125; int[][] res = new int[m][n]; // 对角线数组上还未取出的元素的下标，初始化的时候均为 0 int[] next = new int[dLen]; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; // 对角线的坐标 int index = j - i + (m - 1); // 记录结果 res[i][j] = diagonal[index].get(next[index]); // 维护 next 数组的值 next[index]++; &#125; &#125; return res; &#125;&#125; 作者：liweiwei1419链接：https://leetcode-cn.com/problems/sort-the-matrix-diagonally/solution/bao-li-jie-fa-by-liweiwei1419/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 51. N皇后回溯算法这篇文章是很久之前的一篇《回溯算法详解》的进阶版，之前那篇不够清楚，就不必看了，看这篇就行。把框架给你讲清楚，你会发现回溯算法问题都是一个套路。 废话不多说，直接上回溯算法框架。解决一个回溯问题，实际上就是一个决策树的遍历过程。你只需要思考 3 个问题： 1、路径：也就是已经做出的选择。 2、选择列表：也就是你当前可以做的选择。 3、结束条件：也就是到达决策树底层，无法再做选择的条件。 如果你不理解这三个词语的解释，没关系，我们后面会用「全排列」和「N 皇后问题」这两个经典的回溯算法问题来帮你理解这些词语是什么意思，现在你先留着印象。 代码方面，回溯算法的框架： 12345678910result = []def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) returnfor 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择 其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」，特别简单。 什么叫做选择和撤销选择呢，这个框架的底层原理是什么呢？下面我们就通过「全排列」这个问题来解开之前的疑惑，详细探究一下其中的奥妙！ 一、全排列问题我们在高中的时候就做过排列组合的数学题，我们也知道 n 个不重复的数，全排列共有 n! 个。 PS：为了简单清晰起见，我们这次讨论的全排列问题不包含重复的数字。 那么我们当时是怎么穷举全排列的呢？比方说给三个数 [1,2,3]，你肯定不会无规律地乱穷举，一般是这样： 先固定第一位为 1，然后第二位可以是 2，那么第三位只能是 3；然后可以把第二位变成 3，第三位就只能是 2 了；然后就只能变化第一位，变成 2，然后再穷举后两位…… 其实这就是回溯算法，我们高中无师自通就会用，或者有的同学直接画出如下这棵回溯树： 只要从根遍历这棵树，记录路径上的数字，其实就是所有的全排列。我们不妨把这棵树称为回溯算法的「决策树」。 为啥说这是决策树呢，因为你在每个节点上其实都在做决策。比如说你站在下图的红色节点上： 你现在就在做决策，可以选择 1 那条树枝，也可以选择 3 那条树枝。为啥只能在 1 和 3 之中选择呢？因为 2 这个树枝在你身后，这个选择你之前做过了，而全排列是不允许重复使用数字的。 现在可以解答开头的几个名词：[2] 就是「路径」，记录你已经做过的选择；[1,3] 就是「选择列表」，表示你当前可以做出的选择；「结束条件」就是遍历到树的底层，在这里就是选择列表为空的时候。 如果明白了这几个名词，可以把「路径」和「选择」列表作为决策树上每个节点的属性，比如下图列出了几个节点的属性： 我们定义的 backtrack 函数其实就像一个指针，在这棵树上游走，同时要正确维护每个节点的属性，每当走到树的底层，其「路径」就是一个全排列。 再进一步，如何遍历一棵树？这个应该不难吧。回忆一下之前「学习数据结构的框架思维」写过，各种搜索问题其实都是树的遍历问题，而多叉树的遍历框架就是这样： 123456void traverse(TreeNode root) &#123; for (TreeNode child : root.childern) // 前序遍历需要的操作 traverse(child); // 后序遍历需要的操作&#125; 而所谓的前序遍历和后序遍历，他们只是两个很有用的时间点，我给你画张图你就明白了： 前序遍历的代码在进入某一个节点之前的那个时间点执行，后序遍历代码在离开某个节点之后的那个时间点执行。 回想我们刚才说的，「路径」和「选择」是每个节点的属性，函数在树上游走要正确维护节点的属性，那么就要在这两个特殊时间点搞点动作： 现在，你是否理解了回溯算法的这段核心框架？ 12345678for 选择 in 选择列表: # 做选择 将该选择从选择列表移除 路径.add(选择) backtrack(路径, 选择列表) # 撤销选择 路径.remove(选择) 将该选择再加入选择列表 我们只要在递归之前做出选择，在递归之后撤销刚才的选择，就能正确得到每个节点的选择列表和路径。 下面，直接看全排列代码： 1234567891011121314151617181920212223242526272829303132List&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;&gt;();/* 主函数，输入一组不重复的数字，返回它们的全排列 */List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; // 记录「路径」 LinkedList&lt;Integer&gt; track = new LinkedList&lt;&gt;(); backtrack(nums, track); return res;&#125;// 路径：记录在 track 中// 选择列表：nums 中不存在于 track 的那些元素// 结束条件：nums 中的元素全都在 track 中出现void backtrack(int[] nums, LinkedList&lt;Integer&gt; track) &#123; // 触发结束条件 if (track.size() == nums.length) &#123; res.add(new LinkedList(track)); return; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; // 排除不合法的选择 if (track.contains(nums[i])) continue; // 做选择 track.add(nums[i]); // 进入下一层决策树 backtrack(nums, track); // 取消选择 track.removeLast(); &#125;&#125; 我们这里稍微做了些变通，没有显式记录「选择列表」，而是通过 nums 和 track 推导出当前的选择列表： 至此，我们就通过全排列问题详解了回溯算法的底层原理。当然，这个算法解决全排列不是很高效，应为对链表使用 contains 方法需要 O(N) 的时间复杂度。有更好的方法通过交换元素达到目的，但是难理解一些，这里就不写了，有兴趣可以自行搜索一下。 但是必须说明的是，不管怎么优化，都符合回溯框架，而且时间复杂度都不可能低于 O(N!)，因为穷举整棵决策树是无法避免的。这也是回溯算法的一个特点，不像动态规划存在重叠子问题可以优化，回溯算法就是纯暴力穷举，复杂度一般都很高。 明白了全排列问题，就可以直接套回溯算法框架了，下面简单看看 N 皇后问题。 二、N 皇后问题这个问题很经典了，简单解释一下：给你一个 N×N 的棋盘，让你放置 N 个皇后，使得它们不能互相攻击。 PS：皇后可以攻击同一行、同一列、左上左下右上右下四个方向的任意单位。 这个问题本质上跟全排列问题差不多，决策树的每一层表示棋盘上的每一行；每个节点可以做出的选择是，在该行的任意一列放置一个皇后。 直接套用框架: 123456789101112131415161718192021222324252627282930313233vector&lt;vector&lt;string&gt;&gt; res;/* 输入棋盘边长 n，返回所有合法的放置 */vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n) &#123; // '.' 表示空，'Q' 表示皇后，初始化空棋盘。 vector&lt;string&gt; board(n, string(n, '.')); backtrack(board, 0); return res;&#125;// 路径：board 中小于 row 的那些行都已经成功放置了皇后// 选择列表：第 row 行的所有列都是放置皇后的选择// 结束条件：row 超过 board 的最后一行void backtrack(vector&lt;string&gt;&amp; board, int row) &#123; // 触发结束条件 if (row == board.size()) &#123; res.push_back(board); return; &#125; int n = board[row].size(); for (int col = 0; col &lt; n; col++) &#123; // 排除不合法选择 if (!isValid(board, row, col)) continue; // 做选择 board[row][col] = 'Q'; // 进入下一行决策 backtrack(board, row + 1); // 撤销选择 board[row][col] = '.'; &#125;&#125; 这部分主要代码，其实跟全排列问题差不多，isValid 函数的实现也很简单： 12345678910111213141516171819202122/* 是否可以在 board[row][col] 放置皇后？ */bool isValid(vector&lt;string&gt;&amp; board, int row, int col) &#123; int n = board.size(); // 检查列是否有皇后互相冲突 for (int i = 0; i &lt; n; i++) &#123; if (board[i][col] == 'Q') return false; &#125; // 检查右上方是否有皇后互相冲突 for (int i = row - 1, j = col + 1; i &gt;= 0 &amp;&amp; j &lt; n; i--, j++) &#123; if (board[i][j] == 'Q') return false; &#125; // 检查左上方是否有皇后互相冲突 for (int i = row - 1, j = col - 1; i &gt;= 0 &amp;&amp; j &gt;= 0; i--, j--) &#123; if (board[i][j] == 'Q') return false; &#125; return true;&#125; 函数 backtrack 依然像个在决策树上游走的指针，通过 row 和 col 就可以表示函数遍历到的位置，通过 isValid 函数可以将不符合条件的情况剪枝： 如果直接给你这么一大段解法代码，可能是懵逼的。但是现在明白了回溯算法的框架套路，还有啥难理解的呢？无非是改改做选择的方式，排除不合法选择的方式而已，只要框架存于心，你面对的只剩下小问题了。 当 N = 8 时，就是八皇后问题，数学大佬高斯穷尽一生都没有数清楚八皇后问题到底有几种可能的放置方法，但是我们的算法只需要一秒就可以算出来所有可能的结果。 不过真的不怪高斯。这个问题的复杂度确实非常高，看看我们的决策树，虽然有 isValid 函数剪枝，但是最坏时间复杂度仍然是 O(N^(N+1))，而且无法优化。如果 N = 10 的时候，计算就已经很耗时了。 有的时候，我们并不想得到所有合法的答案，只想要一个答案，怎么办呢？比如解数独的算法，找所有解法复杂度太高，只要找到一种解法就可以。 其实特别简单，只要稍微修改一下回溯算法的代码即可： 1234567891011121314151617181920// 函数找到一个答案后就返回 truebool backtrack(vector&lt;string&gt;&amp; board, int row) &#123; // 触发结束条件 if (row == board.size()) &#123; res.push_back(board); return true; &#125; ... for (int col = 0; col &lt; n; col++) &#123; ... board[row][col] = 'Q'; if (backtrack(board, row + 1)) return true; board[row][col] = '.'; &#125; return false;&#125; 这样修改后，只要找到一个答案，for 循环的后续递归穷举都会被阻断。也许你可以在 N 皇后问题的代码框架上，稍加修改，写一个解数独的算法？ 三、最后总结回溯算法就是个多叉树的遍历问题，关键就是在前序遍历和后序遍历的位置做一些操作，算法框架如下： 12345def backtrack(...): for 选择 in 选择列表: 做选择 backtrack(...) 撤销选择 写 backtrack 函数时，需要维护走过的「路径」和当前可以做的「选择列表」，当触发「结束条件」时，将「路径」记入结果集。 其实想想看，回溯算法和动态规划是不是有点像呢？我们在动态规划系列文章中多次强调，动态规划的三个需要明确的点就是「状态」「选择」和「base case」，是不是就对应着走过的「路径」，当前的「选择列表」和「结束条件」？ 某种程度上说，动态规划的暴力求解阶段就是回溯算法。只是有的问题具有重叠子问题性质，可以用 dp table 或者备忘录优化，将递归树大幅剪枝，这就变成了动态规划。而今天的两个问题，都没有重叠子问题，也就是回溯算法问题了，复杂度非常高是不可避免的。 作者：labuladong链接：https://leetcode-cn.com/problems/n-queens/solution/hui-su-suan-fa-xiang-jie-by-labuladong/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 Tip: Java写法 【N皇后】 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Solution &#123; public List&lt;List&lt;String&gt;&gt; res; public List&lt;List&lt;String&gt;&gt; solveNQueens(int n) &#123; if (n &lt;= 0) return null; res = new LinkedList&lt;&gt;(); char[][] board = new char[n][n]; for (char[] chars : board) Arrays.fill(chars, '.'); backtrack(board, 0); return res; &#125; private static List&lt;String&gt; charToString(char[][] array) &#123; List&lt;String&gt; result = new LinkedList&lt;&gt;(); for (char[] chars : array) &#123; result.add(String.valueOf(chars)); &#125; return result; &#125; /** * 路径：board中小于row的那些行都已经成功放置了皇后 * 可选择列表: 第row行的所有列都是放置Q的选择 * 结束条件: row超过board的最后一行 * * @param board * @param row */ private void backtrack(char[][] board, int row) &#123; if (row == board.length) &#123; res.add(charToString(board)); return; &#125; int n = board[row].length; for (int col = 0; col &lt; n; col++) &#123; if (!isValid(board, row, col)) continue; board[row][col] = 'Q'; backtrack(board, row + 1); board[row][col] = '.'; &#125; &#125; private boolean isValid(char[][] board, int row, int col) &#123; int rows = board.length; // check is valid in col for (char[] chars : board) if (chars[col] == 'Q') return false; // check is valide upright for (int i = row - 1, j = col + 1; i &gt;= 0 &amp;&amp; j &lt; rows; i--, j++) &#123; if (board[i][j] == 'Q') return false; &#125; // check is valide upleft for (int i = row - 1, j = col - 1; i &gt;= 0 &amp;&amp; j &gt;= 0; i--, j--) &#123; if (board[i][j] == 'Q') return false; &#125; return true; &#125;&#125; 代码来自： https://leetcode-cn.com/u/kpcoding/ 42. 接雨水题目给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 示例: 12输入: [0,1,0,2,1,0,1,3,2,1,2,1]输出: 6 思路见代码 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * 暴力法 * 就是找左右柱子的最大值的最小值，即Math.min(L_max,R_max),然后减去自身高度就是能接的雨水 * 值得注意的是，求左右柱子最大值时需要把自己计算在内！！！ * @param height * @return */ public int trapByViolence(int[] height)&#123; int res = 0; for(int i = 0;i &lt; height.length;i++)&#123; int L_max = 0; int R_max = 0; for(int j = i;j &gt;= 0;j--)&#123; L_max = Math.max(L_max,height[j]); &#125; for(int j = i;j &lt; height.length;j++)&#123; R_max = Math.max(R_max,height[j]); &#125; res = res + Math.min(L_max,R_max) - height[i]; &#125; return res; &#125; /** * dp大法 * 用空间换时间，把 L_max、R_max 记录下来，就不用重复计算了 * @param height * @return */ public int trapByDp(int[] height)&#123; int res = 0; int len = height.length; if(len == 0) return 0; int[] L_max = new int[len]; int[] R_max = new int[len]; L_max[0] = height[0]; R_max[len-1] = height[len-1]; for(int i = 1;i &lt; len;i++)&#123; L_max[i] = Math.max(L_max[i-1],height[i]); &#125; for(int i = len - 2;i &gt;= 0;i--)&#123; R_max[i] = Math.max(R_max[i+1],height[i]); &#125; for(int i = 0;i &lt; height.length;i++)&#123; res = res + Math.min(L_max[i],R_max[i]) - height[i]; &#125; return res; &#125; /** * 双指针之固定最高点 * 先找到最高点，然后左边向最高点靠近，右边也向最高点靠近，左边只需要考虑左边，右边只需要考虑右边 * @param height * @return */ public int trapByDouble(int[] height)&#123; int res = 0; int len = height.length; if(len == 0) return 0; int max = 0; int max_index = -1; for(int i = 0;i &lt; len;i++)&#123; if(height[i] &gt; max)&#123; max = height[i]; max_index = i; &#125; &#125; int L_max = height[0]; for(int i = 0;i &lt; max_index;i++)&#123; if(height[i] &gt; L_max) L_max = height[i]; else res = res + L_max - height[i]; &#125; int R_max = height[len-1]; for(int i = len - 1;i &gt; max_index;i--)&#123; if(height[i] &gt; R_max) R_max = height[i]; else res = res + R_max - height[i]; &#125; return res; &#125; /** * 双指针法二 * 整体思路其实是一样的，只是这个更简便一点而已，无需找最高点，只要确保遍历的数只需要考虑一边即可 * @param height * @return */ public int trapByDouble2(int[] height)&#123; int res = 0; int len = height.length; int left = 0; int right = len - 1; if(len == 0) return 0; int L_max = height[0]; int R_max = height[len-1]; //一定在最高点相遇，所以无需 left &lt;= right while(left &lt; right)&#123; L_max = Math.max(L_max, height[left]); R_max = Math.max(R_max, height[right]); if (L_max &lt; R_max) &#123; res = res + L_max - height[left]; left++; &#125; else &#123; res = res + R_max - height[right]; right--; &#125; &#125; return res; &#125; 33. 搜索旋转排序数组题目假设按照升序排序的数组在预先未知的某个点上进行了旋转。 ( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。 搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。 你可以假设数组中不存在重复的元素。 你的算法时间复杂度必须是 O(log n) 级别。 123456789示例 1:输入: nums = [4,5,6,7,0,1,2], target = 0输出: 4示例 2:输入: nums = [4,5,6,7,0,1,2], target = 3输出: -1 思路题目要求 O(logN) 的时间复杂度，基本可以断定本题是需要使用二分查找，怎么分是关键。由于题目说数字了无重复，举个例子：1 2 3 4 5 6 7 可以大致分为两类，第一类 2 3 4 5 6 7 1 这种，也就是 nums[start] &lt;= nums[mid]。此例子中就是 2 &lt;= 5。这种情况下，前半部分有序。因此如果 nums[start] &lt;=target&lt;nums[mid]，则在前半部分找，否则去后半部分找。第二类 6 7 1 2 3 4 5 这种，也就是 nums[start] &gt; nums[mid]。此例子中就是 6 &gt; 2。这种情况下，后半部分有序。因此如果 nums[mid] &lt;target&lt;=nums[end]，则在后半部分找，否则去前半部分找。 此题有个存在重复数字的变形题，可参考 此题解 。 作者：reedfan链接：https://leetcode-cn.com/problems/search-in-rotated-sorted-array/solution/ji-bai-liao-9983de-javayong-hu-by-reedfan/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 代码实现123456789101112131415161718192021222324252627282930class Solution &#123; public int search(int[] nums, int target) &#123; int start = 0; int end = nums.length - 1; while(start &lt;= end)&#123; int mid = start + (end - start)/2; if(nums[mid] == target) return mid; //最容易错的点，就是列表只有两个数字时，mid和start是同一个数，此时必须是前半部分有序 if(nums[mid] &gt;= nums[start])&#123; // 说明前半部分有序 if(nums[start] &lt;= target &amp;&amp; target &lt; nums[mid])&#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; else &#123; // 说明后半部分有序 if(nums[mid] &lt; target &amp;&amp; target &lt;= nums[end])&#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; &#125; return -1; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public int search(int[] nums, int target) &#123; int start = 0; int end = nums.length - 1; while(start &lt;= end)&#123; int mid = start + (end - start)/2; if(nums[mid] == target) return mid; if (nums[start] == nums[mid]) &#123; start++; continue; &#125; //最容易错的点，就是列表只有两个数字时，mid和start是同一个数，此时必须是前半部分有序 // num[mid] == nums[start] 只会在列表只有两个数时才相等，所以才可以上面那样处理 // 正常其实是不应该那样处理，而应该是当num[mid] == nums[start]，直接start++，然后进行下一次循环 if(nums[mid] &gt; nums[start])&#123; // 说明前半部分有序 if(nums[start] &lt;= target &amp;&amp; target &lt; nums[mid])&#123; end = mid - 1; &#125; else &#123; start = mid + 1; &#125; &#125; else &#123; // 说明后半部分有序 if(nums[mid] &lt; target &amp;&amp; target &lt;= nums[end])&#123; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; &#125; return -1; &#125;&#125; Tip: 81题思路一样 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public boolean search(int[] nums, int target) &#123; if (nums == null || nums.length == 0) &#123; return false; &#125; int start = 0; int end = nums.length - 1; int mid; while (start &lt;= end) &#123; mid = start + (end - start) / 2; if (nums[mid] == target) &#123; return true; &#125; if (nums[start] == nums[mid]) &#123; start++; continue; &#125; // 前半部分有序 // [1,3,1,1,1] if (nums[start] &lt; nums[mid]) &#123; // target在前半部分 if (nums[mid] &gt; target &amp;&amp; nums[start] &lt;= target) &#123; end = mid - 1; &#125; else &#123; // 否则，去后半部分找 start = mid + 1; &#125; &#125; else &#123; // 后半部分有序 // target在后半部分 if (nums[mid] &lt; target &amp;&amp; nums[end] &gt;= target) &#123; start = mid + 1; &#125; else &#123; // 否则，去后半部分找 end = mid - 1; &#125; &#125; &#125; // 一直没找到，返回false return false; &#125;&#125; 34. 在排序数组中查找元素的第一个和最后一个位置题目给定一个按照升序排列的整数数组 nums，和一个目标值 target。找出给定目标值在数组中的开始位置和结束位置。 你的算法时间复杂度必须是 O(log n) 级别。 如果数组中不存在目标值，返回 [-1, -1]。 12345678示例 1:输入: nums = [5,7,7,8,8,10], target = 8输出: [3,4]示例 2:输入: nums = [5,7,7,8,8,10], target = 6输出: [-1,-1] 思路见 labuladong 公众号笔记 实现代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123; public int[] searchRange(int[] nums, int target) &#123; if(nums.length == 0) return new int[]&#123;-1,-1&#125;; return new int[]&#123;searchLeft(nums,target),searchRight(nums,target)&#125;; &#125; public int searchLeft(int[] nums,int target)&#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; // 防止溢出 int mid = left + (right - left)/2; if(nums[mid] &gt;= target) right = mid - 1; else left = mid + 1; &#125; // 上面的while循环后，最后一次循环 left == mid, right = left - 1 = mid - 1 // 若有符合要求的target，则会有两种情况： // 1. left 和 right 在 target 值的最左索引处的前一位相遇，此时 left 会向右一位，到达最左 target 处,而 right 会停在原地，弹出 while // 2. left 和 right 在 target 值的最左索引处相遇，此时 left 不动，到达最左 target 处,而 right 会向左一位，弹出 while // 注意：此时的 left 都是处在 target 最左索引处 // 如果没有符合要求的target，则有三种情况： // 1.所有值均小于target，此时，left会在 nums[nums.length-1]处与right相遇，然后left 加 1，跳出循环，此时left == nums.length // 2.所有值均大于target，此时right会不断向左，直至 left = right = 0 相遇，此时 right 减 1，跳出循环，此时 left == 0 // 3.target位于值的中间，但是没有值取到，此时跟有target情况是类似的，最终 left 会停留在比 target 大的第一个数上 // 总结上面 5 种情况，left 为 nums.length 时，另其为 nums.length - 1,此时直接判断 nums[left] 即可，若为target则直接返回 left // 否则返回 -1 int pos = (left == nums.length) ? nums.length - 1 : left; if(nums[pos] != target) return -1; return left; &#125; public int searchRight(int[] nums,int target)&#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; // 防止溢出 int mid = left + (right - left)/2; if(nums[mid] &lt;= target) left = mid + 1; else right = mid - 1; &#125; // 同上分析 // 最左和最右只有一个区别，就是 left 换成了 right ，nums.length 换成了 -1 int pos = (right == -1)? 0 : right; if(nums[pos] != target) return -1; return right; &#125;&#125; 39. 组合总和题目给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。 candidates 中的数字可以无限制重复被选取。 说明： 所有数字（包括 target）都是正整数。 解集不能包含重复的组合。 示例 1: 123456输入: candidates = [2,3,6,7], target = 7,所求解集为:[ [7], [2,2,3]] 示例 2: 1234567输入: candidates = [2,3,5], target = 8,所求解集为:[ [2,2,2,2], [2,3,3], [3,5]] 思路直接上回溯算法框架。解决一个回溯问题，实际上就是一个决策树的遍历过程。你只需要思考 3 个问题： 1、路径：也就是已经做出的选择。 2、选择列表：也就是你当前可以做的选择。 3、结束条件：也就是到达决策树底层，无法再做选择的条件。 代码方面，回溯算法的框架： 12345678910result = []def backtrack(路径, 选择列表): if 满足结束条件: result.add(路径) returnfor 选择 in 选择列表: 做选择 backtrack(路径, 选择列表) 撤销选择 其核心就是 for 循环里面的递归，在递归调用之前「做选择」，在递归调用之后「撤销选择」，特别简单。 labuladong 回溯框架！！！老哥写的文章真是干净利落！ demo(全排列)123456789101112131415161718192021222324252627282930313233List&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;&gt;();/* 主函数，输入一组不重复的数字，返回它们的全排列 */List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; // 记录「路径」 // 这里的 选择列表 即包含在nums中 LinkedList&lt;Integer&gt; track = new LinkedList&lt;&gt;(); backtrack(nums, track); return res;&#125;// 路径：记录在 track 中// 选择列表：nums 中的元素// 结束条件：nums 中的元素全都在 track 中出现void backtrack(int[] nums, LinkedList&lt;Integer&gt; track) &#123; // 触发结束条件 if (track.size() == nums.length) &#123; res.add(new LinkedList(track)); return; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; // 排除不合法的选择 if (track.contains(nums[i])) continue; // 做选择 track.add(nums[i]); // 进入下一层决策树 backtrack(nums, track); // 取消选择，返回上一层决策树 track.removeLast(); &#125;&#125; 代码实现先按照demo写一个差不多的，这个暂时无法做到去重！ 1234567891011121314151617181920212223242526272829303132333435363738public class combinationSum_39 &#123; public static void main(String[] args) &#123; combinationSum(new int[]&#123;2,3,6,7&#125;,7); System.out.println(res); &#125; public static List&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;&gt;(); public static List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; LinkedList&lt;Integer&gt; track = new LinkedList&lt;&gt;(); // 排序的原因是在回溯的时候比较容易剪枝 Arrays.sort(candidates); // 套用上面的公式，candidates是指选择列表，target用来判断是否结束以及用于剪枝 // track则是路径，即已经做出的选择 backtrack(candidates, target, track); return res; &#125; private static void backtrack(int[] candidates, int target, LinkedList&lt;Integer&gt; track) &#123; //先判断结束条件 if (target &lt; 0) return; if (target == 0)&#123; // 当target等于0的时候，将路径加入到结果列表中 res.add(new LinkedList&lt;&gt;(track)); return; &#125; // 遍历选择列表，这里没有去重 //下面会设置start，每次递归的时候只在candidates中当前及之后的数字中选择。 for(int i = 0;i &lt; candidates.length;i++)&#123; // 这就是排序的好处，可以直接这样剪枝，否则还得遍历 if(target &lt; candidates[i]) break; track.add(candidates[i]); backtrack(candidates,target-candidates[i],track); track.removeLast(); &#125; &#125;&#125; 1输出：[[2, 2, 3], [2, 3, 2], [3, 2, 2], [7]] 去重之后的代码： 123456789101112131415161718192021222324252627282930public class combinationSum_39 &#123; public static void main(String[] args) &#123; combinationSum(new int[]&#123;2,3,6,7&#125;,7); System.out.println(res); &#125; public static List&lt;List&lt;Integer&gt;&gt; res = new LinkedList&lt;&gt;(); public static List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; LinkedList&lt;Integer&gt; track = new LinkedList&lt;&gt;(); Arrays.sort(candidates); backtrack(candidates, 0, target, track); return res; &#125; private static void backtrack(int[] candidates, int start, int target, LinkedList&lt;Integer&gt; track) &#123; if (target &lt; 0) return; if (target == 0)&#123; res.add(new LinkedList&lt;&gt;(track)); return; &#125; for(int i = start;i &lt; candidates.length;i++)&#123; if(target &lt; candidates[i]) break; track.add(candidates[i]); backtrack(candidates,i,target-candidates[i],track); track.removeLast(); &#125; &#125;&#125; 1输出：[[2, 2, 3], [7]] 146. LRU缓存机制题目运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。写入数据 put(key, value) - 如果密钥不存在，则写入其数据值。当缓存容量达到上限时，它应该在写入新数据之前删除最近最少使用的数据值，从而为新的数据值留出空间。 相信如果有认真看过 LinkedHashMap 源码的小伙伴，一定会很快的跟官方题解写的一模一样！ 简单介绍LinkedHashMap（跟题目有关的知识点）HashMap 大家都清楚，底层是 数组 + 红黑树 + 链表 （不清楚也没有关系），同时其是无序的，而 LinkedHashMap 刚好就比 HashMap 多这一个功能，就是其提供 有序，并且，LinkedHashMap的有序可以按两种顺序排列，一种是按照插入的顺序，一种是按照读取的顺序（这个题目的示例就是告诉我们要按照读取的顺序进行排序），而其内部是靠 建立一个双向链表 来维护这个顺序的，在每次插入、删除后，都会调用一个函数来进行 双向链表的维护 ，准备的来说，是有三个函数来做这件事，这三个函数都统称为 回调函数 ，这三个函数分别是： void afterNodeAccess(Node p) { } 其作用就是在访问元素之后，将该元素放到双向链表的尾巴处(所以这个函数只有在按照读取的顺序的时候才会执行)，之所以提这个，是建议大家去看看，如何优美的实现在双向链表中将指定元素放入链尾！ void afterNodeRemoval(Node p) { } 其作用就是在删除元素之后，将元素从双向链表中删除，还是非常建议大家去看看这个函数的，很优美的方式在双向链表中删除节点！ void afterNodeInsertion(boolean evict) { } 这个才是我们题目中会用到的，在插入新元素之后，需要回调函数判断是否需要移除一直不用的某些元素！ 其次，我再介绍一下 LinkedHashMap 的构造函数！ 其主要是两个构造方法，一个是继承 HashMap ，一个是可以选择 accessOrder 的值(默认 false，代表按照插入顺序排序)来确定是按插入顺序还是读取顺序排序。 1234567891011121314151617181920212223/** //调用父类HashMap的构造方法。 */public LinkedHashMap() &#123; super(); accessOrder = false;&#125;// 这里的 accessOrder 默认是为false，如果要按读取顺序排序需要将其设为 true// initialCapacity 代表 map 的 容量，loadFactor 代表加载因子 (默认即可)public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; 思路 &amp; 代码下面是我自己在分析 LinkedHashMap 源码时做的一些笔记，应该是比较清楚的，主体意思就是我们要继承 LinkedHashMap，然后复写 removeEldestEntry()函数，就能拥有我们自己的缓存策略！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 在插入一个新元素之后，如果是按插入顺序排序，即调用newNode()中的linkNodeLast()完成// 如果是按照读取顺序排序，即调用afterNodeAccess()完成// 那么这个方法是干嘛的呢，这个就是著名的 LRU 算法啦// 在插入完成之后，需要回调函数判断是否需要移除某些元素！// LinkedHashMap 函数部分源码/** \* 插入新节点才会触发该方法，因为只有插入新节点才需要内存 \* 根据 HashMap 的 putVal 方法, evict 一直是 true \* removeEldestEntry 方法表示移除规则, 在 LinkedHashMap 里一直返回 false \* 所以在 LinkedHashMap 里这个方法相当于什么都不做 */void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 根据条件判断是否移除最近最少被访问的节点 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125;// 移除最近最少被访问条件之一，通过覆盖此方法可实现不同策略的缓存// LinkedHashMap是默认返回false的，我们可以继承LinkedHashMap然后复写该方法即可// 例如 LeetCode 第 146 题就是采用该种方法，直接 return size() &gt; capacity;protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false; 通过上述代码，我们就已经知道了只要复写 removeEldestEntry() 即可，而条件就是 map 的大小不超过 给定的容量，超过了就得使用 LRU 了！然后根据题目给定的语句构造和调用： 123456789101112/** \* LRUCache 对象会以如下语句构造和调用: \* LRUCache obj = new LRUCache(capacity); \* int param_1 = obj.get(key); \* obj.put(key,value); */ 很明显我们只需要直接继承父类的put函数即可，因为题目没有特殊要求，故可以不写！至于 get() 函数，题目是有要求的！ 获取数据 get(key) - 如果密钥 (key) 存在于缓存中，则获取密钥的值（总是正数），否则返回 -1。 所以我们可以调用 LinkedHashMap 中的 getOrDefault()，完美符合这个要求，即当key不存在时会返回默认值 -1。 至此，我们就基本完成了本题的要求，只要写一个构造函数即可，答案的 super(capacity, 0.75F, true);，没看过源码的小伙伴可能不太清楚这个构造函数，这就是我上文讲的 LinkedHashMap 中的常用的第二个构造方法，具体大家可以看我上面代码的注释！ 至此，大功告成！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344class LRUCache extends LinkedHashMap&lt;Integer, Integer&gt;&#123; private int capacity; public LRUCache(int capacity) &#123; super(capacity, 0.75F, true); this.capacity = capacity; &#125; public int get(int key) &#123; return super.getOrDefault(key, -1); &#125; // 这个可不写 public void put(int key, int value) &#123; super.put(key, value); &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; capacity; &#125;&#125; 148. 排序链表题目在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。 示例 1: 12输入: 4-&gt;2-&gt;1-&gt;3输出: 1-&gt;2-&gt;3-&gt;4 示例 2: 12输入: -1-&gt;5-&gt;3-&gt;4-&gt;0输出: -1-&gt;0-&gt;3-&gt;4-&gt;5 思路 题目要求时间空间复杂度分别为 O(nlogn) 和 O(1)，根据时间复杂度我们自然想到二分法，从而联想到归并排序； 对数组做归并排序的空间复杂度为 O(n) ，分别由新开辟数组 O(n) 和递归函数调用 O(logn) 组成，而根据链表特性： 数组额外空间：链表可以通过修改引用来更改节点顺序，无需像数组一样开辟额外空间； 递归额外空间：递归调用函数将带来 O(logn) 的空间复杂度，因此若希望达到 O(1) 空间复杂度，则不能使用递归。 【这里咱还是用下递归，降低难度！】 通过递归实现链表归并排序，有以下两个环节： 分割 cut 环节： 找到当前链表中点，并从中点将链表断开（以便在下次递归 cut 时，链表片段拥有正确边界）； 我们使用 fast,slow 快慢双指针法，奇数个节点找到中点，偶数个节点找到中心左边的节点。 找到中点 slow 后，执行 slow.next = None 将链表切断。 递归分割时，输入当前链表左端点 head 和中心节点 slow 的下一个节点 tmp(因为链表是从 slow 切断的)。 cut 递归终止条件： 当head.next == None时，说明只有一个节点了，直接返回此节点。 合并 merge 环节： 将两个排序链表合并，转化为一个排序链表。 双指针法合并，建立辅助ListNode h 作为头部。 设置两指针 left, right 分别指向两链表头部，比较两指针处节点值大小，由小到大加入合并链表头部，指针交替前进，直至添加完两个链表。 返回辅助ListNode h 作为头部的下个节点 h.next。 时间复杂度 O(l + r)，l, r 分别代表两个链表长度。 当题目输入的 head == None 时，直接返回None。 代码1234567891011121314151617181920212223242526272829class Solution &#123; public ListNode sortList(ListNode head) &#123; if (head == null || head.next == null) return head; ListNode fast = head.next, slow = head; while (fast != null &amp;&amp; fast.next != null) &#123; slow = slow.next; fast = fast.next.next; &#125; ListNode tmp = slow.next; slow.next = null; ListNode left = sortList(head); ListNode right = sortList(tmp); ListNode h = new ListNode(0); ListNode res = h; while (left != null &amp;&amp; right != null) &#123; if (left.val &lt; right.val) &#123; h.next = left; left = left.next; &#125; else &#123; h.next = right; right = right.next; &#125; h = h.next; &#125; h.next = left != null ? left : right; return res.next; &#125;&#125; 作者：jyd链接：https://leetcode-cn.com/problems/sort-list/solution/sort-list-gui-bing-pai-xu-lian-biao-by-jyd/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 不符合时间复杂度的路过一下： 123456789101112131415161718public ListNode sortList(ListNode head) &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (head !=null) &#123; list.add(head.val); head = head.next; &#125; Collections.sort(list); return createLinkedList(list); &#125; // list递归转链表 private ListNode createLinkedList(List&lt;Integer&gt; data) &#123; if(data.isEmpty()) &#123; //如果为空返回null return null; &#125; ListNode firstNode = new ListNode(data.get(0)); //每次取第一个元素 firstNode.next = createLinkedList(data.subList(1, data.size()));//第二个元素从下标为1开始取余下list return firstNode; &#125; 437. 路径总和 III题目给定一个二叉树，它的每个结点都存放着一个整数值。 找出路径和等于给定数值的路径总数。 路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。 二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。 示例： 123456789101112131415root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 10 / \ 5 -3 / \ \ 3 2 11 / \ \3 -2 1返回 3。和等于 8 的路径有:1. 5 -&gt; 32. 5 -&gt; 2 -&gt; 13. -3 -&gt; 11 思路回溯，只不过这里的选择条件，比较特殊，并且 做选择 和 撤销选择 得稍微注意一下，这里可以直接用 if else。 代码12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public int res = 0; public int pathSum(TreeNode root, int sum) &#123; if (root == null) return res; if (root != null) &#123; res = path(root,sum,0) + res; &#125; if(root.left != null) pathSum(root.left,sum); if(root.right != null) pathSum(root.right,sum); return res; &#125; public int path(TreeNode root, int sum,int count) &#123; if (root == null) return count; sum = sum - root.val; if (sum == 0) &#123; count++; &#125; if (root.left != null) &#123; count = path(root.left, sum, count); &#125; if (root.right != null) &#123; count = path(root.right, sum, count); &#125; return count; &#125;&#125; 124. 二叉树中的最大路径和题目给定一个非空二叉树，返回其最大路径和。 本题中，路径被定义为一条从树中任意节点出发，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。 示例 1: 1234567输入: [1,2,3] 1 / \ 2 3输出: 6 示例 2: 123456789输入: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7输出: 42 思路这题还是挺难的，要求最大路径和，可以采用递归，递归就是三部曲： 1、确定递归出口，这个简单，root == null 即退出 2、确定返回值，这个是本题最难的，返回的是以该节点结尾的最大路径和！！！ 3、一级递归需要做的事，其实就是去算最大的路径和，这个很简单，在二叉树中，一级递归其实也就是三个节点，分别是根节点，左子树节点，右子树节点，既然每个节点返回的是 以该节点结尾的最大路径和，则我们可以在每级递归时去更新一下最大的路径和，即 左子树节点返回来的以其节点结尾的最大路径和 + 根节点的值 + 右子树节点返回的以该节点结尾的最大路径和。 代码1234567891011121314151617181920212223242526class Solution &#123; int max_sum = Integer.MIN_VALUE; public int max_gain(TreeNode node) &#123; if (node == null) return 0; // max sum on the left and right sub-trees of node int left_gain = Math.max(max_gain(node.left), 0); int right_gain = Math.max(max_gain(node.right), 0); // the price to start a new path where `node` is a highest node int price_newpath = node.val + left_gain + right_gain; // update max_sum if it's better to start a new path max_sum = Math.max(max_sum, price_newpath); // for recursion : // return the max gain if continue the same path return node.val + Math.max(left_gain, right_gain); &#125; public int maxPathSum(TreeNode root) &#123; max_gain(root); return max_sum; &#125;&#125; 48. 旋转图像题目给定一个 n × n 的二维矩阵表示一个图像。 将图像顺时针旋转 90 度。 说明： 你必须在原地旋转图像，这意味着你需要直接修改输入的二维矩阵。请不要使用另一个矩阵来旋转图像。 示例 1: 12345678910111213给定 matrix = [ [1,2,3], [4,5,6], [7,8,9]],原地旋转输入矩阵，使其变为:[ [7,4,1], [8,5,2], [9,6,3]] 示例 2: 123456789101112131415给定 matrix =[ [ 5, 1, 9,11], [ 2, 4, 8,10], [13, 3, 6, 7], [15,14,12,16]], 原地旋转输入矩阵，使其变为:[ [15,13, 2, 5], [14, 3, 4, 1], [12, 6, 8, 9], [16, 7,10,11]] 思路翻转 + 转置 代码12345678910111213141516171819202122232425//1.先转置再翻转，注意这里翻转是 行 翻转，比如 第一行 1 2 3，行翻转变为 3 2 1//2.先翻转再转置，这里的翻转是 列 翻转，比如第一列 1 4 7，翻转为 7 4 1//3.如果是逆时针，则是按照副对角线进行转置class Solution &#123; public void rotate(int[][] matrix) &#123; int n = matrix.length; // 这里是先转置再翻转 for (int i = 0; i &lt; n; i++) &#123; for (int j = i; j &lt; n; j++) &#123; int tmp = matrix[j][i]; matrix[j][i] = matrix[i][j]; matrix[i][j] = tmp; &#125; &#125; // 行翻转 1 2 3变为 3 2 1 for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; n / 2; j++) &#123; int tmp = matrix[i][j]; matrix[i][j] = matrix[i][n - j - 1]; matrix[i][n - j - 1] = tmp; &#125; &#125; &#125;&#125; 49. 字母异位词分组题目给定一个字符串数组，将字母异位词组合在一起。字母异位词指字母相同，但排列不同的字符串。 示例: 123456输入: ["eat", "tea", "tan", "ate", "nat", "bat"],输出:[ ["ate","eat","tea"], ["nat","tan"], ["bat"] 说明： 所有输入均为小写字母。 不考虑答案输出的顺序。 思路 每个单词进行字母排序，排完序后存入map中，key相同的存入同一个list中即可。 每个单词都是由 26 个字母组成的，这个方法无需对每个单词的字母进行排序，类似于桶的概念，每个单词不同的字符放入 26 个桶中，字母异位词对应的桶中的数值应该是一样的，将桶中数据相同的字母当成一个key，存入map中，然后key相同的存入同一个list中，我个人认为这种方法对字母很多很多的单词是非常有用的，时间复杂度上更小。 代码思路一 1234567891011121314class Solution &#123; public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) &#123; if (strs.length == 0) return new ArrayList(); Map&lt;String, List&gt; ans = new HashMap&lt;String, List&gt;(); for (String s : strs) &#123; char[] ca = s.toCharArray(); Arrays.sort(ca); String key = String.valueOf(ca); if (!ans.containsKey(key)) ans.put(key, new ArrayList()); ans.get(key).add(s); &#125; return new ArrayList(ans.values()); &#125;&#125; 思路二 123456789101112131415161718192021class Solution &#123; public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) &#123; if (strs.length == 0) return new ArrayList(); Map&lt;String, List&gt; ans = new HashMap&lt;String, List&gt;(); int[] count = new int[26]; for (String s : strs) &#123; Arrays.fill(count, 0); for (char c : s.toCharArray()) count[c - 'a']++; StringBuilder sb = new StringBuilder(""); for (int i = 0; i &lt; 26; i++) &#123; sb.append('#'); sb.append(count[i]); &#125; String key = sb.toString(); if (!ans.containsKey(key)) ans.put(key, new ArrayList()); ans.get(key).add(s); &#125; return new ArrayList(ans.values()); &#125;&#125; 136. 只出现一次的数字题目给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。 说明： 你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 示例 1: 12输入: [2,2,1]输出: 1 示例 2: 12输入: [4,1,2,1,2]输出: 4 思路 拿到手，要求时间复杂度在 O( n )，第一反应是使用哈希表 HashMap 来完成，遍历一遍数组，然后找到 value == 1的即可，时间复杂度 O(n)，空间复杂度为O(n)。 但是题目要求不使用额外空间，即空间复杂度 O(1)，这个就很难了，只能暴力法，就是每查一个数字，我们就去剩下的数字中去找，如果找不到，即使我们需要的，但是这个时间复杂度是O(n²)。 最后一个是最骚的，直接异或就行了… 代码123456789class Solution &#123; public int singleNumber(int[] nums) &#123; int a = nums[0]; for(int i = 1;i &lt; nums.length;i++)&#123; a = a ^ nums[i]; &#125; return a; &#125;&#125; 55. 跳跃游戏题目给定一个非负整数数组，你最初位于数组的第一个位置。数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个位置。 示例 1: 123输入: [2,3,1,1,4]输出: true解释: 我们可以先跳 1 步，从位置 0 到达 位置 1, 然后再从位置 1 跳 3 步到达最后一个位置。 示例 2: 123输入: [3,2,1,0,4]输出: false解释: 无论怎样，你总会到达索引为 3 的位置。但该位置的最大跳跃长度是 0 ， 所以你永远不可能到达最后一个位置。 思路 回溯 贪心算法，每次都找到能跳到的最远距离，然后在原地到最远距离之间遍历，看能否继续跳到更远，如果可以，就更新最远距离值，如果最远距离值能够不小于最后一个位置，说明可以跳到，否则不行。 评论区看到的，其实跟贪心算法思想类似，但是又有点不一样，就是记录每个节点能跳到的最远距离，不断更新最远距离，如果有某个节点的下标比k值大，说明到不了该节点，即有个挡板挡在了这个节点之前，过不来，此时就是无法到达最后一个节点，如果全程都没有被挡板挡住，且挡板的值过了最后一个位置，即可以到达最后一个位置。 代码 回溯【超出时间限制】 123456789101112131415161718192021class Solution &#123; public boolean canJump(int[] nums) &#123; if(nums.length == 0) return true; return canJumpByRecursion_In(0, nums); &#125; private boolean canJumpByRecursion_In(int position,int[] nums) &#123; if (position == nums.length - 1) &#123; return true; &#125; int furthestJump = Math.min(position + nums[position], nums.length - 1); for (int nextPosition = position + 1; nextPosition &lt;= furthestJump; nextPosition++) &#123; if (canJumpByRecursion_In(nextPosition, nums)) &#123; return true; &#125; &#125; return false; &#125;&#125; 贪心 12345678910class Solution &#123; public boolean canJump(int[] nums) &#123; int furthest = 0; for(int i = 0;i &lt;= furthest;i++)&#123; if(furthest &gt;= nums.length-1) return true; if(i + nums[i] &gt; furthest) furthest = i + nums[i]; &#125; return false; &#125;&#125; 评论区神仙 1234567891011class Solution &#123; public boolean canJump(int[] nums) &#123; int furthest = 0; for(int i = 0;i &lt;= nums.length-1;i++)&#123; if(furthest &gt;= nums.length-1) break; if(i &gt; furthest) return false; if(i + nums[i] &gt; furthest) furthest = i + nums[i]; &#125; return true; &#125;&#125; 56. 合并区间题目给出一个区间的集合，请合并所有重叠的区间。 示例 1: 123输入: [[1,3],[2,6],[8,10],[15,18]]输出: [[1,6],[8,10],[15,18]]解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2: 123输入: [[1,4],[4,5]]输出: [[1,5]]解释: 区间 [1,4] 和 [4,5] 可被视为重叠区间。 思路先按首位置进行排序; 接下来,如何判断两个区间是否重叠呢?比如 a = [1,4],b = [2,3] 当 a[1] &gt;= b[0] 说明两个区间有重叠 但是如何把这个区间找出来呢? 左边位置一定是确定，就是 a[0]，而右边位置是 max(a[1], b[1]) 所以,我们就能找出整个区间为:[1,4] 代码12345678910111213141516171819202122232425262728class Solution &#123; public int[][] merge(int[][] intervals) &#123; LinkedList&lt;int[]&gt; res = new LinkedList&lt;&gt;(); if (intervals == null || intervals.length == 0) &#123; return res.toArray(new int[0][]); &#125; // 按数组首元素排序 Arrays.sort(intervals, new Comparator&lt;int[]&gt;() &#123; @Override public int compare(int[] o1, int[] o2) &#123; return o1[0] - o2[0]; &#125; &#125;); // 如果遍历到的数组的初始值 比 res 的最新添加的数组的末尾值还大，说明没有重叠，可以直接添加 // 否则，直接比较两个数组的末尾值就行，因为是按数组首元素排好序的，所以只需要考虑末尾值替换就行 for (int i = 0; i &lt; intervals.length; i++) &#123; if (res.isEmpty() || res.getLast()[1] &lt; intervals[i][0]) &#123; res.add(intervals[i]); &#125; else &#123; res.getLast()[1] = Math.max(res.getLast()[1], intervals[i][1]); &#125; &#125; // list.toArray(new int[0][])，这是数组集体强转类型的方法 // new int[0][]是告诉程序需要转成这样的类型 // 长度多大是无所谓的，因为转换后的长度是按照你的list长度来定的 return res.toArray(new int[0][0]); &#125;&#125; 75. 颜色分类题目给定一个包含红色、白色和蓝色，一共 n 个元素的数组，原地对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。 此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。 注意:不能使用代码库中的排序函数来解决这道题。 示例: 12输入: [2,0,2,1,1,0]输出: [0,0,1,1,2,2] 进阶： 一个直观的解决方案是使用计数排序的两趟扫描算法。首先，迭代计算出0、1 和 2 元素的个数，然后按照0、1、2的排序，重写当前数组。 你能想出一个仅使用常数空间的一趟扫描算法吗？ 思路三路归并，其实也是 Arrays.sort() 这里采用的方法，也是三色旗的解决方案。 三个指针，分别是 left、cur、right。left指向数组最左侧，right指向数组最右侧，cur代表当前正在遍历的数组元素，当 cur 遍历的元素是 0 时，将 cur 指向的元素 与 p0 指向的元素交换，然后 cur++，p0++。当 cur 遍历的元素是 1 时，cur++。当 cur 遍历的元素是 2 时，将 cur 指向的元素与 p2 指向的元素交换，然后 p2–，cur不动！！！直到 cur &gt; p2 ,循环结束（即全部扫描完毕）。 对于以上，有一个难点！ 为何 cur 在与 p0 交换时需要 p0++，cur++；而在 cur 与 p2 交换时，却只需要 p2–？ 对于上面这个问题，有两种解释思路：1.cur 与 p0 交换需要自加，是因为其左边已经扫描过了，交换过来的值也是之前就扫描过了的，而右边不是， p2 d交换过来的值 cur 并没有扫描过；2.当 cur 与 p0 不是一个指向同一个索引值时，那 cur 指向的索引值如果发生交换，那交换过来的一定是 1（原因是只有当遍历过的节点有1，p0 和 cur 才不会同步），而 如果索引是 1 刚好也就不用有任何操作，所以可以直接继续向右扫描，当 cur 和 p0 指向的是同一个索引，那交换就等于没交换，故也是直接可以向右扫描，右边的就不行。 代码12345678910111213141516171819202122class Solution &#123; public void sortColors(int[] nums) &#123; int left = 0, cur = 0; int right = nums.length - 1; while (cur &lt;= right) &#123; if (nums[cur] == 0) &#123; swap(nums, left, cur); left++; cur++; &#125; else if (nums[cur] == 2) &#123; swap(nums, right, cur); right--; &#125; else cur++; &#125; &#125; public void swap(int[] nums, int a, int b) &#123; int temp = nums[a]; nums[a] = nums[b]; nums[b] = temp; &#125;&#125; 78. 子集题目给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 说明：解集不能包含重复的子集。 示例: 123456789101112输入: nums = [1,2,3]输出:[ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], []] 思路很明显，一看就是回溯，思路跟 全排列、N 皇后问题一样 代码1234567891011121314151617181920class Solution &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList(); public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums)&#123; LinkedList track = new LinkedList(); return subsets_in(nums,0, track); &#125; public List&lt;List&lt;Integer&gt;&gt; subsets_in(int[] nums,int start,LinkedList track) &#123; // 注意，这里必须new一个新的对象，否则的话每次添加都是指向同一个对象 // 最后track会变空，所有的添加的列表都会变空 res.add(new LinkedList(track)); for(int i = start;i &lt; nums.length;i++)&#123; track.add(nums[i]); subsets_in(nums,i+1,track); track.removeLast(); &#125; return res; &#125;&#125; 79. 单词搜索题目给定一个二维网格和一个单词，找出该单词是否存在于网格中。 单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。 示例: 12345678910board =[ ['A','B','C','E'], ['S','F','C','S'], ['A','D','E','E']]给定 word = "ABCCED", 返回 true.给定 word = "SEE", 返回 true.给定 word = "ABCB", 返回 false. 思路依旧是… 回溯 代码1234567891011121314151617181920212223242526272829303132class Solution &#123; public boolean exist(char[][] board, String word) &#123; boolean[][] visited = new boolean[board.length][board[0].length]; for (int i = 0; i &lt; board.length; i++) &#123; for (int j = 0; j &lt; board[0].length; j++) &#123; if (word.charAt(0) == board[i][j] &amp;&amp; backtrack(i, j, 0, word, visited, board)) return true; &#125; &#125; return false; &#125; private boolean backtrack(int i, int j, int index, String word, boolean[][] visited, char[][] board) &#123; // 触发结束条件 if (index == word.length()) return true; if (i &gt;= board.length || i &lt; 0 || j &gt;= board[0].length || j &lt; 0 || board[i][j] != word.charAt(index) || visited[i][j]) return false; // 做选择 visited[i][j] = true; // 进入下一层决策 if (backtrack(i + 1, j, index + 1, word, visited, board) || backtrack(i - 1, j, index + 1, word, visited, board) || backtrack(i, j + 1, index + 1, word, visited, board) || backtrack(i, j - 1, index + 1, word, visited, board)) return true; // 撤销选择 visited[i][j] = false; // 回溯 return false; &#125;&#125; 84. 柱状图中最大的矩形题目给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。 求在该柱状图中，能够勾勒出来的矩形的最大面积。 以上是柱状图的示例，其中每个柱子的宽度为 1，给定的高度为 [2,1,5,6,2,3]。 思路 暴力法，每个高度都计算一遍最长的连续的底，然后取最大值。 其实这道题的本质就是转化为求每个高度对应的最长的连续的底，即对两边分别找第一个小于遍历的数的高度的索引值（即求矩形的最长的底），这点和接雨水 有点相似，同时，采用单调递增栈的方法 跟 求最大有效括号 这道题有异曲同工之处，都是采用了栈存取数组索引值的方法！ 为何说单调递增栈（严格递增）能非常轻松的找到 height[i] 的 两边刚好比它的高度小的第一个数 呢？ 这里我们先假设所有的高度都是不会相同的。 首先由于栈是递增的，当 height[i] 比栈顶的索引值对应的高度 大时，直接压入栈即可，否则说明 height[i] 比栈顶的索引值对应的高度小，则栈顶对应的右边第一个小于它的高度的数找到了，就是 height[i]，然后把栈顶元素弹出，新栈顶的元素即是刚才弹栈元素左边第一个小于它的高度的数，这样就很轻松的找到了两边分别小于 栈顶元素的数，这样取更新最大值就行了。 所以，我们现在来考虑一下取消开始那个前提条件，现在有的高度是会相同的，这个条件我们怎么处理呢？当面临栈顶元素和遍历的元素对应的高度相同时，我们只需要更新栈顶元素的值（即将其存入的索引变为新的我们正在遍历的元素的索引值），我举个例子，比如说 2 5 6 7 5 6 3，当遍历到最后一个数 5 时，此时栈顶值为 1（索引 1 对应的高度是 5 ），我们只要把栈顶值变为 4 即可。 当然这样还有一个问题，就是假如是 2 5 6 7，高度一直递增，四个值全部入栈了，此时最后一个元素 7 的 右边第一个小于它的高度其实没有，我们可以令其为 height.length。 代码 暴力法 1234567891011121314151617181920212223242526272829class Solution &#123; public int largestRectangleArea(int[] heights) &#123; HashSet&lt;Integer&gt; heightsSet = new HashSet&lt;Integer&gt;(); //得到所有的高度，也就是去重。 for (int i = 0; i &lt; heights.length; i++) &#123; heightsSet.add(heights[i]); &#125; int maxArea = 0; //遍历每一个高度 for (int h : heightsSet) &#123; int width = 0; int maxWidth = 1; //找出连续的大于等于当前高度的柱形个数的最大值 for (int i = 0; i &lt; heights.length; i++) &#123; if (heights[i] &gt;= h) &#123; width++; //出现小于当前高度的就归零，并且更新最大宽度 &#125; else &#123; maxWidth = Math.max(width, maxWidth); width = 0; &#125; &#125; maxWidth = Math.max(width, maxWidth); //更新最大区域的面积 maxArea = Math.max(maxArea, h * maxWidth); &#125; return maxArea; &#125;&#125; 作者：windliang链接：https://leetcode-cn.com/problems/largest-rectangle-in-histogram/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-1-7/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 单调递增栈法 123456789101112131415161718192021222324class Solution &#123; public int largestRectangleArea(int[] heights)&#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int max = 0; stack.push(-1); for(int i = 0; i &lt; heights.length; i++)&#123; // 1. 栈中值不等于 -1 且 栈顶元素对应的高度 大于 正在遍历的元素的高度 while(stack.peek() != -1 &amp;&amp; heights[stack.peek()] &gt; heights[i])&#123; max = Math.max(max,heights[stack.pop()] * (i - stack.peek() - 1)); &#125; // 2. 栈中值不等于 -1 且 栈顶元素对应的高度 == 正在遍历的元素的高度 // 直接把当前栈顶弹栈即可 if(stack.peek() != -1 &amp;&amp; heights[stack.peek()] == heights[i])&#123; stack.pop(); &#125; stack.push(i); &#125; // 遍历完了，但是没计算完 while(stack.peek() != -1)&#123; max = Math.max(max,heights[stack.pop()] * (heights.length - stack.peek() - 1)); &#125; return max; &#125;&#125; 85. 最大矩形题目给定一个仅包含 0 和 1 的二维二进制矩阵，找出只包含 1 的最大矩形，并返回其面积。 示例: 12345678输入:[ ["1","0","1","0","0"], ["1","0","1","1","1"], ["1","1","1","1","1"], ["1","0","0","1","0"]]输出: 6 思路思路同84题，每一行都调用84题的算法即可。 代码1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public int maximalRectangle(char[][] matrix) &#123; if (matrix.length == 0) return 0; int maxarea = 0; int[] dp = new int[matrix[0].length]; for(int i = 0; i &lt; matrix.length; i++) &#123; for(int j = 0; j &lt; matrix[0].length; j++) &#123; // update the state of this row's histogram using the last row's histogram // by keeping track of the number of consecutive ones dp[j] = matrix[i][j] == '1' ? dp[j] + 1 : 0; &#125; // update maxarea with the maximum area from this row's histogram maxarea = Math.max(maxarea, largestRectangleArea(dp)); &#125; return maxarea; &#125; public int largestRectangleArea(int[] heights)&#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int max = 0; stack.push(-1); for(int i = 0; i &lt; heights.length; i++)&#123; while(stack.peek() != -1 &amp;&amp; heights[stack.peek()] &gt; heights[i])&#123; max = Math.max(max,heights[stack.pop()] * (i - stack.peek() - 1)); &#125; if(stack.peek() != -1 &amp;&amp; heights[stack.peek()] == heights[i])&#123; stack.pop(); &#125; stack.push(i); &#125; while(stack.peek() != -1)&#123; max = Math.max(max,heights[stack.pop()] * (heights.length - stack.peek() - 1)); &#125; return max; &#125;&#125; 121. 买卖股票的最佳时机题目给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。 注意你不能在买入股票前卖出股票。 示例 1: 1234输入: [7,1,5,3,6,4]输出: 5解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 示例 2: 123输入: [7,6,4,3,1]输出: 0解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 思路假设当前在第 i 天，令 minPrice 表示前 i-1 天的最低价格；令 maxProfit 表示前 i-1 天的最大收益。那么考虑第 i 天的收益时，存在两种情况： 在第 i 天卖出。很显然，想要获得最大收益，应该在前 i-1 天中价格最低的时候买入，即此时的收益为：prices[i] - minPrice。（可能会出现负数，但是没关系） 不在第 i 天卖出。那么第 i 天的最大收益就等于前 i -1 天中的最大收益 状态转移方程为：第 i 天最大收益 = max( 在第 i 天卖出的所得收益 , 前 i-1 天的最大收益) 代码1234567891011class Solution &#123; public int maxProfit(int[] prices) &#123; int minPrice = Integer.MAX_VALUE; int maxProfit = 0; for (int i = 0; i &lt; prices.length; i++) &#123; minPrice = Math.min(minPrice, prices[i]); maxProfit = Math.max(maxProfit, prices[i] - minPrice); &#125; return maxProfit; &#125;&#125; 122. 买卖股票的最佳时机 II题目给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票） 示例 1: 1234输入: [7,1,5,3,6,4]输出: 7解释: 在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6-3 = 3 。 示例 2: 12345输入: [1,2,3,4,5]输出: 4解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。 因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 123输入: [7,6,4,3,1]输出: 0解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 思路扫描一遍，只要后一天比前一天大，就把这两天的差值加一下。 代码123456789101112class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length == 0) return 0; int ans = 0; for(int i = 0;i &lt; prices.length-1;i++)&#123; if(prices[i+1] &gt; prices[i])&#123; ans+=(prices[i+1] - prices[i]); &#125; &#125; return ans; &#125; &#125; 123. 买卖股票的最佳时机 III题目给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 1234输入: [3,3,5,0,0,3,1,4]输出: 6解释: 在第 4 天（股票价格 = 0）的时候买入，在第 6 天（股票价格 = 3）的时候卖出，这笔交易所能获得利润 = 3-0 = 3 。 随后，在第 7 天（股票价格 = 1）的时候买入，在第 8 天 （股票价格 = 4）的时候卖出，这笔交易所能获得利润 = 4-1 = 3 。 示例 2: 12345输入: [1,2,3,4,5]输出: 4解释: 在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。 因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3: 123输入: [7,6,4,3,1] 输出: 0 解释: 在这个情况下, 没有交易完成, 所以最大利润为 0。 思路 https://mp.weixin.qq.com/s/CWGKl0Ctfc6wStcvBJsD3Q 代码12345678910111213141516171819202122class Solution &#123; public int maxProfit(int[] prices) &#123; /** 对于任意一天考虑四个变量: fstBuy: 在该天第一次买入股票可获得的最大收益 fstSell: 在该天第一次卖出股票可获得的最大收益 secBuy: 在该天第二次买入股票可获得的最大收益 secSell: 在该天第二次卖出股票可获得的最大收益 分别对四个变量进行相应的更新, 最后secSell就是最大 收益值(secSell &gt;= fstSell) **/ int fstBuy = Integer.MIN_VALUE, fstSell = 0; int secBuy = Integer.MIN_VALUE, secSell = 0; for(int p : prices) &#123; fstBuy = Math.max(fstBuy, -p); fstSell = Math.max(fstSell, fstBuy + p); secBuy = Math.max(secBuy, fstSell - p); secSell = Math.max(secSell, secBuy + p); &#125; return secSell; &#125;&#125; 188. 买卖股票的最佳时机 IV题目给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。 注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 123输入: [2,4,1], k = 2输出: 2解释: 在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2 。 示例 2: 1234输入: [3,2,6,5,0,3], k = 2输出: 7解释: 在第 2 天 (股票价格 = 2) 的时候买入，在第 3 天 (股票价格 = 6) 的时候卖出, 这笔交易所能获得利润 = 6-2 = 4 。 随后，在第 5 天 (股票价格 = 0) 的时候买入，在第 6 天 (股票价格 = 3) 的时候卖出, 这笔交易所能获得利润 = 3-0 = 3 。 思路思路跟上题一样，这里最多可以有 k 笔交易，所以就存在 s(2k+1) 个状态，思路一模一样，但是注意有个坑，就是这里的 k 如果超过数组长度的一半，就说明可以随便交易了，就是股票 II 一样的贪心思想就可以了。 代码12345678910111213141516171819202122232425262728293031323334class Solution &#123; //依旧是使用状态机模型，只不过需要用数组来保存buy和sell了 //因为内存有限制，所以不能用数组 //而我们观察到除了buy1 和 sell1，其实后面的buy[i]都是在之前的buy[i-1]上继续做事 public int maxProfit(int k, int[] p) &#123; if(k == 0 || p == null || p.length &lt;= 1) return 0; if(k &gt;= p.length/2) return greedy(p); //如果k大于p长度的一半，那么其实就是随便买了多少次数了，就是问题II //k = Math.min(k, p.length / 2); //k最大只能是p的长度的一半，防止数组太大 int[] buy = new int[k]; int[] sell = new int[k]; for(int i = 0;i &lt; k;i++)&#123; buy[i] = Integer.MIN_VALUE; sell[i] = 0; &#125; for(int i = 0;i &lt; p.length;i++)&#123; buy[0] = Math.max(buy[0], 0 - p[i]); sell[0] = Math.max(sell[0], buy[0] + p[i]); for(int j = 1;j &lt; k;j++)&#123; buy[j] = Math.max(buy[j], sell[j-1] - p[i]); sell[j] = Math.max(sell[j], buy[j] + p[i]); &#125; &#125; return sell[k-1]; &#125; public int greedy(int[] p)&#123; int max = 0; for(int i = 1;i &lt; p.length;i++)&#123; if(p[i] &gt; p[i-1]) max += p[i] - p[i-1]; &#125; return max; &#125; &#125; 309. 最佳买卖股票时机含冷冻期题目给定一个整数数组，其中第 i 个元素代表了第 i 天的股票价格 。​ 设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 示例: 123输入: [1,2,3,0,2]输出: 3 解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] 思路 代码12345678910111213141516171819202122class Solution &#123; public int maxProfit(int[] prices) &#123; if (prices.length == 0) &#123; return 0; &#125; int n = prices.length; // 这个相比 III 来说，没有了交易次数的限制，那么就不需要那么多状态机了， //这里只需要三个状态，待卖出状态「持股」、待买入且冷冻状态「非持股」、待买入且非冷冻状态「非持股」 // f[i][0]: 手上持有股票的最大收益 // f[i][1]: 手上不持有股票，并且处于冷冻期中的累计最大收益 // f[i][2]: 手上不持有股票，并且不在冷冻期中的累计最大收益 int[][] f = new int[n][3]; f[0][0] = -prices[0]; for (int i = 1; i &lt; n; ++i) &#123; f[i][0] = Math.max(f[i - 1][0], f[i - 1][2] - prices[i]); f[i][1] = f[i - 1][0] + prices[i]; f[i][2] = Math.max(f[i - 1][1], f[i - 1][2]); &#125; return Math.max(f[n - 1][1], f[n - 1][2]); &#125;&#125; 714. 买卖股票的最佳时机含手续费题目给定一个整数数组 prices，其中第 i 个元素代表了第 i 天的股票价格 ；非负整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 注意：这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。 示例 1: 12345678输入: prices = [1, 3, 2, 8, 4, 9], fee = 2输出: 8解释: 能够达到的最大利润: 在此处买入 prices[0] = 1在此处卖出 prices[3] = 8在此处买入 prices[4] = 4在此处卖出 prices[5] = 9总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8. 思路只有两个状态机，持股和不持股 代码1234567891011class Solution &#123; public int maxProfit(int[] prices, int fee) &#123; int cash = 0, hold = -prices[0]; for (int i = 1; i &lt; prices.length; i++) &#123; // 这里可以不用临时变量存储 cash，因为不可能在一天先卖出再买入，这样血亏 cash = Math.max(cash, hold + prices[i] - fee); hold = Math.max(hold, cash - prices[i]); &#125; return cash; &#125;&#125; 128. 最长连续序列题目给定一个未排序的整数数组，找出最长连续序列的长度。 要求算法的时间复杂度为 O(n)。 示例: 123输入: [100, 4, 200, 1, 3, 2]输出: 4解释: 最长连续序列是 [1, 2, 3, 4]。它的长度为 4。 思路 暴力法。从头到尾遍历每个数，然后对每个数去找数组中是否存在下一个数，如果存在，就找是否存在下一个再下一个的数，以此类推。 排序之后再进行判断，这个很简单…不讲了。 用 Set 存储数组，这样查询是否有该数的时候，直接就是 O(1) 的复杂度，同时不需要数组从头到尾遍历，只有在（遍历的元素值 - 1） 不在 Set 中，才开始判断其下一个是否在 Set 中，这样就可以减少遍历的次数。 代码 暴力 12345678910111213141516171819202122232425262728class Solution &#123; private boolean arrayContains(int[] arr, int num) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; if (arr[i] == num) &#123; return true; &#125; &#125; return false; &#125; public int longestConsecutive(int[] nums) &#123; int longestStreak = 0; for (int num : nums) &#123; int currentNum = num; int currentStreak = 1; while (arrayContains(nums, currentNum + 1)) &#123; currentNum += 1; currentStreak += 1; &#125; longestStreak = Math.max(longestStreak, currentStreak); &#125; return longestStreak; &#125;&#125; 排序 1234567891011121314151617181920212223242526class Solution &#123; public int longestConsecutive(int[] nums) &#123; if (nums.length == 0) &#123; return 0; &#125; Arrays.sort(nums); int longestStreak = 1; int currentStreak = 1; for (int i = 1; i &lt; nums.length; i++) &#123; if (nums[i] != nums[i-1]) &#123; if (nums[i] == nums[i-1]+1) &#123; currentStreak += 1; &#125; else &#123; longestStreak = Math.max(longestStreak, currentStreak); currentStreak = 1; &#125; &#125; &#125; return Math.max(longestStreak, currentStreak); &#125;&#125; Set 1234567891011121314151617181920212223242526class Solution &#123; public int longestConsecutive(int[] nums) &#123; Set&lt;Integer&gt; num_set = new HashSet&lt;Integer&gt;(); for (int num : nums) &#123; num_set.add(num); &#125; int longestStreak = 0; for (int num : num_set) &#123; if (!num_set.contains(num-1)) &#123; int currentNum = num; int currentStreak = 1; while (num_set.contains(currentNum+1)) &#123; currentNum += 1; currentStreak += 1; &#125; longestStreak = Math.max(longestStreak, currentStreak); &#125; &#125; return longestStreak; &#125;&#125; 具体见 官方题解：https://leetcode-cn.com/problems/longest-consecutive-sequence/solution/zui-chang-lian-xu-xu-lie-by-leetcode/ 152. 乘积最大子序列题目给定一个整数数组 nums ，找出一个序列中乘积最大的连续子序列（该序列至少包含一个数）。 示例 1: 123输入: [2,3,-2,4]输出: 6解释: 子数组 [2,3] 有最大乘积 6。 示例 2: 123输入: [-2,0,-1]输出: 0解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 思路这其实说白了就是子串的题目，所以必须使用动态规划去做。做 dp 的题目，我觉得首先最重要的不是状态转移方程，而是dp数组的含义是什么，只有这个确定对了，状态方程才能很好的列出来！！！ 这里的 dp 数组指的是以第 i 个数 结尾的 连续子序列，由于存在负数，所以必须维护两个 dp 数组，其实这里根本用不到数组，但是为了更加清晰的看到 dp 的思想，我还是用数组来表达吧。 我们先考虑都是正数的情况。dp_max[i] 的含义我们已经讲过了，dp_max[i] = Math.max(nums[i-1],dp_max[i-1]*nums[i-1])，即 dp_max[i] 这个值只会在这两者产生，要么 乘上之前的会更大，要么 舍弃前面的。 接下来考虑负数的情况，所以我们有必要维护一个 dp_min，思路是一模一样的，当遍历的元素为负数时，我们只需要把 dp_max[i-1]，dp_min[i-1]交换即可。 最后，只要找到所有dp_max中的数值最大的那个，就是我们需要的值了。 代码1234567891011121314151617181920212223242526class Solution &#123; public int maxProduct(int[] nums) &#123; int[] dp_max = new int[nums.length+1]; int[] dp_min = new int[nums.length+1]; if(nums.length == 0) return 0; int max = Integer.MIN_VALUE; // 由于存在负数，所以需要维护两个数组 // dp_max[i] 指的是以第 i 个数结尾的 乘积最大 的连续子序列 // dp_min[i] 指的是以第 i 个数结尾的 乘积最小 的连续子序列 dp_max[0] = 1; dp_min[0] = 1; for (int i = 1;i &lt;= nums.length;i++)&#123; // 如果数组的数是负数，那么会导致 max 变成 min，min 变成 max // 故需要交换dp if(nums[i-1] &lt; 0)&#123; int temp = dp_min[i-1]; dp_min[i-1] = dp_max[i-1]; dp_max[i-1] = temp; &#125; dp_min[i] = Math.min(nums[i-1],dp_min[i-1]*nums[i-1]); dp_max[i] = Math.max(nums[i-1],dp_max[i-1]*nums[i-1]); max = Math.max(max,dp_max[i]); &#125; return max; &#125;&#125; 155. 最小栈题目设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。 push(x) – 将元素 x 推入栈中。 pop() – 删除栈顶的元素。 top() – 获取栈顶元素。 getMin() – 检索栈中的最小元素。 示例: 12345678MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.getMin(); --&gt; 返回 -3.minStack.pop();minStack.top(); --&gt; 返回 0.minStack.getMin(); --&gt; 返回 -2. 思路https://leetcode-cn.com/problems/min-stack/solution/xiang-xi-tong-su-de-si-lu-fen-xi-duo-jie-fa-by-38/ 用两个栈，一个栈专门存最小值，主要就是入栈和出栈做到同步就行。存最小值的栈的具体操作流程如下： 将第一个元素入栈。 新加入的元素如果大于栈顶元素，那么新加入的元素就不处理。 新加入的元素如果小于等于栈顶元素，那么就将新元素入栈。 出栈元素不等于栈顶元素，不操作。 出栈元素等于栈顶元素，那么就将栈顶元素出栈。 用一个栈，当有更小的值来的时候，我们只需要把之前的最小值入栈，当前更小的值再入栈即可。当这个最小值要出栈的时候，下一个值便是之前的最小值了。 栈中存储链表，其中设定一个节点包括其 val，和当前最小值。 代码 两个栈 123456789101112131415161718192021222324252627282930313233343536373839404142class MinStack &#123; /** initialize your data structure here. */ private Stack&lt;Integer&gt; stack; private Stack&lt;Integer&gt; minStack; public MinStack() &#123; stack = new Stack&lt;&gt;(); minStack = new Stack&lt;&gt;(); &#125; public void push(int x) &#123; stack.push(x); if (!minStack.isEmpty()) &#123; int top = minStack.peek(); //小于的时候才入栈 if (x &lt;= top) &#123; minStack.push(x); &#125; &#125;else&#123; minStack.push(x); &#125; &#125; public void pop() &#123; int pop = stack.pop(); int top = minStack.peek(); //等于的时候再出栈 if (pop == top) &#123; minStack.pop(); &#125; &#125; public int top() &#123; return stack.peek(); &#125; public int getMin() &#123; return minStack.peek(); &#125;&#125; 单个栈 1234567891011121314151617181920212223242526272829class MinStack &#123; int min = Integer.MAX_VALUE; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); public void push(int x) &#123; //当前值更小 if(x &lt;= min)&#123; //将之前的最小值保存 stack.push(min); //更新最小值 min=x; &#125; stack.push(x); &#125; public void pop() &#123; //如果弹出的值是最小值，那么将下一个元素更新为最小值 if(stack.pop() == min) &#123; min=stack.pop(); &#125; &#125; public int top() &#123; return stack.peek(); &#125; public int getMin() &#123; return min; &#125;&#125; 存储链表 123456789101112131415161718192021222324252627282930313233343536373839404142class MinStack &#123; class Node&#123; int value; int min; Node next; Node(int x, int min)&#123; this.value=x; this.min=min; next = null; &#125; &#125; Node head; //每次加入的节点放到头部 public void push(int x) &#123; if(null==head)&#123; head = new Node(x,x); &#125;else&#123; //当前值和之前头结点的最小值较小的做为当前的 min Node n = new Node(x, Math.min(x,head.min)); n.next=head; head=n; &#125; &#125; public void pop() &#123; if(head!=null) head =head.next; &#125; public int top() &#123; if(head!=null) return head.value; return -1; &#125; public int getMin() &#123; if(null!=head) return head.min; return -1; &#125;&#125; 5330. 分裂二叉树的最大乘积题目给你一棵二叉树，它的根为 root 。请你删除 1 条边，使二叉树分裂成两棵子树，且它们子树和的乘积尽可能大。 由于答案可能会很大，请你将结果对 10^9 + 7 取模后再返回。 示例 1： 123输入：root = [1,2,3,4,5,6]输出：110解释：删除红色的边，得到 2 棵子树，和分别为 11 和 10 。它们的乘积是 110 （11*10 示例 2： 123输入：root = [1,null,2,3,4,null,null,5,6]输出：90解释：移除红色的边，得到 2 棵子树，和分别是 15 和 6 。它们的乘积为 90 （15*6） 代码123456789101112131415161718192021222324252627282930313233/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; private static final long MOD = (long) (1e9 + 7); public int maxProduct(TreeNode root) &#123; Set&lt;Long&gt; ss = new HashSet&lt;&gt;(); long sum = dfs(root, ss); long ans = 0; for (Long s : ss) &#123; ans = Math.max(ans, s * (sum - s)); &#125; return (int) (ans % MOD); &#125; private long dfs(TreeNode root, Set&lt;Long&gt; ss) &#123; if (root == null) &#123; return 0; &#125; long l = dfs(root.left, ss); long r = dfs(root.right, ss); ss.add(l); ss.add(r); return l + r + root.val; &#125;&#125; 206. 反转链表题目反转一个单链表。 示例: 12输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 思路迭代设置三个节点pre、cur、next （1）每次查看cur节点是否为NULL，如果是，则结束循环，获得结果 （2）如果cur节点不是为NULL，则先设置临时变量next为cur的下一个节点 （3）让cur的下一个节点变成指向pre，而后pre移动cur，cur移动到next （4）重复（1）（2）（3） 递归拿到手之后，是直接使用的递归的做法，看评论区大家好像对递归的过程都觉得很绕，其实我个人觉得大家把这个想复杂了，下面我来试着帮大家一起理解一下！递归，就是三部曲： 1、找到递归出口 2、确定返回值 3、分析单次递归需要做的事情 下面，我们来具体分析一下： 首先，找到递归出口，这个还是非常简单的，就是当前即将反转的节点为 null 或者是 反转链表 为 null 时（一轮递归其实就只有两个节点，后面会讲），说明已经全部反转完毕了，即递归出口； 其次，确定返回值，我们只需要返回反转链表的头结点即可； 最后，分析单次递归需要做的事情，我觉得大家觉得递归比较难理解的地方就是在这，其实是大家把递归复杂化了，递归其实每一轮做的事情都是一样的，我们不需要去重复考虑，这样反而会很乱，只需要考虑单轮递归需要做什么就可以了。在这里，我们就只有两个节点，一个是即将反转的节点元素，一个是已经反转完毕的链表头结点。 我们要做的一轮递归只是 将当前节点加入到反转链表中，仅此而已。 代码 迭代 123456789101112131415161718192021/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; ListNode prev = null; ListNode curr = head; while (curr != null) &#123; ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; &#125; return prev; &#125;&#125; 递归 123456789101112131415161718192021222324/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode reverseList(ListNode head) &#123; // 如果当前要反转的节点为 null 或者反转链表为 null // head.next 为 null，即反转链表的尾结点不存在，即反转链表不存在 if (head == null || head.next == null) return head; // 节点 p 其实就是反转链表的头节点 ListNode p = reverseList(head.next); // 我们将反转链表的尾结点（head.next）的 lianext 指向当前即将反转的节点 head.next.next = head; // 然后让当前节点变成反转链表的尾结点 head.next = null; // 返回反转链表的头结点 return p; &#125;&#125; 160. 相交链表题目编写一个程序，找到两个单链表相交的起始节点。 【注意：这里相交节点并不是看链表的值相等就代表相交，得是两个节点直接相等才代表相交，指向同一块内存。】 如下面的两个链表： 在节点 c1 开始相交。 示例 1： 123输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3输出：Reference of the node with value = 8输入解释：相交节点的值为 8 （注意，如果两个列表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2： 123输入：intersectVal = 2, listA = [0,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1输出：Reference of the node with value = 2输入解释：相交节点的值为 2 （注意，如果两个列表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [0,9,1,2,4]，链表 B 为 [3,2,4]。在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3： 1234输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2输出：null输入解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。解释：这两个链表不相交，因此返回 null。 注意： 如果两个链表没有交点，返回 null。 在返回结果后，两个链表仍须保持原有的结构。 可假定整个链表结构中没有循环。 程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。 思路 最开始的思路就是，找到 长链表和短链表的长度差 c ，这样第二次遍历的时候，长链表从第 c+1 个节点出发，短链表从第一个节点出发，这样最后二者必是同时到达终点的，而二者如果有相交，则在遍历的时候节点必相等，第一个相等的节点就是相交的起始节点。 还有一个思路可以找到相交的起始节点，我们无需去计算长链表和短链表的长度差 c，只需要让两个指针 p1、p2 同时从链表头结点处出发，假设 长链表长度为 a，短链表的长度为 b，当短链表指针p2遍历到 null，即遍历完了短链表，将其移到长链表头结点继续遍历，同时p1也继续向前遍历，当p1遍历到 null 时，将其移到短链表头节点继续遍历，这样 如果长链表和短链表相交，则 p1 和 p2 必会在相交起始处相遇，如果两个链表不相交，p1 和 p2 会在 null 处相遇。 代码 Code I 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * Definition for singly-linked list. * public class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if (headA == null || headB == null) return null; ListNode p1 = headA; ListNode p2 = headB; while(p1 != null &amp;&amp; p2 != null)&#123; p1 = p1.next; p2 = p2.next; &#125; // 说明 headA 是短链表 if(p1 == null)&#123; // 计算差值 int difference = 0; while(p2 != null)&#123; p2 = p2.next; difference++; &#125; p1 = headA; p2 = headB; for(int i = 0;i &lt; difference;i++)&#123; p2 = p2.next; &#125; while (p1 != p2)&#123; p1 = p1.next; p2 = p2.next; &#125; return p1; &#125; // 说明 headB 是短链表 else&#123; int difference = 0; while(p1 != null)&#123; p1 = p1.next; difference++; &#125; p1 = headA; p2 = headB; for(int i = 0;i &lt; difference;i++)&#123; p1 = p1.next; &#125; while (p1 != p2)&#123; p1 = p1.next; p2 = p2.next; &#125; return p1; &#125; &#125;&#125; Code II 123456789public ListNode getIntersectionNode(ListNode headA, ListNode headB) &#123; if (headA == null || headB == null) return null; ListNode pA = headA, pB = headB; while (pA != pB) &#123; pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; &#125; return pA;&#125; 234. 回文链表题目请判断一个链表是否为回文链表。 示例 1: 12输入: 1-&gt;2输出: false 示例 2: 12输入: 1-&gt;2-&gt;2-&gt;1输出: true 思路 链表转列表 1.快慢指针找到链表的中点2.翻转链表前半部分3.回文校验 一边翻转一边快慢指针遍历 代码 Case I 1234567891011121314151617181920212223242526class Solution &#123; public boolean isPalindrome(ListNode head) &#123; List&lt;Integer&gt; vals = new ArrayList&lt;&gt;(); // Convert LinkedList into ArrayList. ListNode currentNode = head; while (currentNode != null) &#123; vals.add(currentNode.val); currentNode = currentNode.next; &#125; // Use two-pointer technique to check for palindrome. int front = 0; int back = vals.size() - 1; while (front &lt; back) &#123; // Note that we must use ! .equals instead of != // because we are comparing Integer, not int. if (!vals.get(front).equals(vals.get(back))) &#123; return false; &#125; front++; back--; &#125; return true; &#125;&#125; Case II 123456789101112131415161718192021222324252627282930313233343536public boolean isPalindrome(ListNode head) &#123; if (head == null || head.next == null) &#123; return true; &#125; //快慢指针找到链表的中点 ListNode fast = head.next.next; ListNode slow = head.next; while (fast != null &amp;&amp; fast.next != null) &#123; fast = fast.next.next; slow = slow.next; &#125; //翻转链表前半部分 ListNode pre = null; ListNode next = null; while (head != slow) &#123; next = head.next; head.next = pre; pre = head; head = next; &#125; //如果是奇数个节点，去掉后半部分的第一个节点。 if (fast != null) &#123; slow = slow.next; &#125; //回文校验 while (pre != null) &#123; if (pre.val != slow.val) &#123; return false; &#125; pre = pre.next; slow = slow.next; &#125; return true; &#125; Case III 12345678910111213141516171819202122232425public boolean isPalindrome(ListNode head) &#123; if(head == null || head.next == null) &#123; return true; &#125; ListNode slow = head, fast = head; ListNode pre = head, prepre = null; while(fast != null &amp;&amp; fast.next != null) &#123; pre = slow; slow = slow.next; fast = fast.next.next; pre.next = prepre; prepre = pre; &#125; if(fast != null) &#123; slow = slow.next; &#125; while(pre != null &amp;&amp; slow != null) &#123; if(pre.val != slow.val) &#123; return false; &#125; pre = pre.next; slow = slow.next; &#125; return true;&#125; 169. 多数元素题目给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。 你可以假设数组是非空的，并且给定的数组总是存在多数元素。 示例 1: 12输入: [3,2,3]输出: 3 示例 2: 12输入: [2,2,1,1,1,2,2]输出: 2 思路 哈希表存储。 投票算法，如果我们把众数记为 +1 ，把其他数记为 −1 ，将它们全部加起来，显然和大于 0 ，从结果本身我们可以看出众数比其他数多。本质上， Boyer-Moore 算法就是找 nums 的一个后缀 suf ，其中 suf[0] 就是后缀中的众数。我们维护一个计数器，如果遇到一个我们目前的候选众数，就将计数器加一，否则减一。只要计数器等于 0 ，我们就将 nums 中之前访问的数字全部 忘记 ，并把下一个数字当做候选的众数。 代码 Case I 123456789101112class Solution &#123; public int majorityElement(int[] nums) &#123; int majority = nums.length / 2; Map&lt;Integer, Integer&gt; numCount = new HashMap&lt;&gt;(); for (int num : nums) &#123; int count = numCount.getOrDefault(num, 0) + 1; if (count &gt; majority) return num; numCount.put(num, count); &#125; return -1; &#125;&#125; Case II 123456789101112131415class Solution &#123; public int majorityElement(int[] nums) &#123; int count = 0; Integer candidate = null; for (int num : nums) &#123; if (count == 0) &#123; candidate = num; &#125; count += (num == candidate) ? 1 : -1; &#125; return candidate; &#125;&#125; 200. 岛屿数量题目给定一个由 ‘1’（陆地）和 ‘0’（水）组成的的二维网格，计算岛屿的数量。一个岛被水包围，并且它是通过水平方向或垂直方向上相邻的陆地连接而成的。你可以假设网格的四个边均被水包围。 示例 1: 1234567输入:11110110101100000000输出: 1 示例 2: 1234567输入:11000110000010000011输出: 3 思路思路一：深度优先遍历DFS 目标是找到矩阵中 “岛屿的数量” ，上下左右相连的 1 都被认为是连续岛屿。 dfs方法： 设目前指针指向一个岛屿中的某一点 (i, j)，寻找包括此点的岛屿边界。 从 (i, j) 向此点的上下左右 (i+1,j),(i-1,j),(i,j+1),(i,j-1) 做深度搜索。 终止条件： (i, j) 越过矩阵边界; grid[i][j]== 0，代表此分支已越过岛屿边界。 搜索岛屿的同时，执行 grid[i][j] = ‘0’，即将岛屿所有节点删除，以免之后重复搜索相同岛屿。 主循环： 遍历整个矩阵，当遇到 grid[i][j] == ‘1’ 时，从此点开始做深度优先搜索 dfs，岛屿数 count + 1 且在深度优先搜索中删除此岛屿。 最终返回岛屿数 count 即可。 思路二：广度优先遍历BFS 主循环和思路一类似，不同点是在于搜索某岛屿边界的方法不同。 bfs 方法： 借用一个队列 queue，判断队列首部节点 (i, j) 是否未越界且为 1： 若是则置零（删除岛屿节点），并将此节点上下左右节点 (i+1,j),(i-1,j),(i,j+1),(i,j-1) 加入队列； 若不是则跳过此节点； 循环 pop 队列首节点，直到整个队列为空，此时已经遍历完此岛屿。 代码 Case I 12345678910111213141516171819202122class Solution &#123; public int numIslands(char[][] grid) &#123; int count = 0; for(int i = 0; i &lt; grid.length; i++) &#123; for(int j = 0; j &lt; grid[0].length; j++) &#123; if(grid[i][j] == '1')&#123; dfs(grid, i, j); count++; &#125; &#125; &#125; return count; &#125; private void dfs(char[][] grid, int i, int j)&#123; if(i &lt; 0 || j &lt; 0 || i &gt;= grid.length || j &gt;= grid[0].length || grid[i][j] == '0') return; grid[i][j] = '0'; dfs(grid, i + 1, j); dfs(grid, i, j + 1); dfs(grid, i - 1, j); dfs(grid, i, j - 1); &#125;&#125; Case II 1234567891011121314151617181920212223242526272829class Solution &#123; public int numIslands(char[][] grid) &#123; int count = 0; for(int i = 0; i &lt; grid.length; i++) &#123; for(int j = 0; j &lt; grid[0].length; j++) &#123; if(grid[i][j] == '1')&#123; bfs(grid, i, j); count++; &#125; &#125; &#125; return count; &#125; private void bfs(char[][] grid, int i, int j)&#123; Queue&lt;int[]&gt; list = new LinkedList&lt;&gt;(); list.add(new int[] &#123; i, j &#125;); while(!list.isEmpty())&#123; int[] cur = list.remove(); i = cur[0]; j = cur[1]; if(0 &lt;= i &amp;&amp; i &lt; grid.length &amp;&amp; 0 &lt;= j &amp;&amp; j &lt; grid[0].length &amp;&amp; grid[i][j] == '1') &#123; grid[i][j] = '0'; list.add(new int[] &#123; i + 1, j &#125;); list.add(new int[] &#123; i - 1, j &#125;); list.add(new int[] &#123; i, j + 1 &#125;); list.add(new int[] &#123; i, j - 1 &#125;); &#125; &#125; &#125;&#125; 作者：jyd链接：https://leetcode-cn.com/problems/number-of-islands/solution/number-of-islands-shen-du-you-xian-bian-li-dfs-or-/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 并查集 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class Solution &#123; class UnionFind &#123; int count; // # of connected components int[] parent; int[] rank; public UnionFind(char[][] grid) &#123; // for problem 200 count = 0; int m = grid.length; int n = grid[0].length; parent = new int[m * n]; rank = new int[m * n]; for (int i = 0; i &lt; m; ++i) &#123; for (int j = 0; j &lt; n; ++j) &#123; if (grid[i][j] == '1') &#123; parent[i * n + j] = i * n + j; ++count; &#125; rank[i * n + j] = 0; &#125; &#125; &#125; public int find(int i) &#123; // path compression if (parent[i] != i) parent[i] = find(parent[i]); return parent[i]; &#125; public void union(int x, int y) &#123; // union with rank int rootx = find(x); int rooty = find(y); if (rootx != rooty) &#123; if (rank[rootx] &gt; rank[rooty]) &#123; parent[rooty] = rootx; &#125; else if (rank[rootx] &lt; rank[rooty]) &#123; parent[rootx] = rooty; &#125; else &#123; parent[rooty] = rootx; rank[rootx] += 1; &#125; --count; &#125; &#125; public int getCount() &#123; return count; &#125; &#125; public int numIslands(char[][] grid) &#123; if (grid == null || grid.length == 0) &#123; return 0; &#125; int nr = grid.length; int nc = grid[0].length; int num_islands = 0; UnionFind uf = new UnionFind(grid); for (int r = 0; r &lt; nr; ++r) &#123; for (int c = 0; c &lt; nc; ++c) &#123; if (grid[r][c] == '1') &#123; grid[r][c] = '0'; if (r - 1 &gt;= 0 &amp;&amp; grid[r-1][c] == '1') &#123; uf.union(r * nc + c, (r-1) * nc + c); &#125; if (r + 1 &lt; nr &amp;&amp; grid[r+1][c] == '1') &#123; uf.union(r * nc + c, (r+1) * nc + c); &#125; if (c - 1 &gt;= 0 &amp;&amp; grid[r][c-1] == '1') &#123; uf.union(r * nc + c, r * nc + c - 1); &#125; if (c + 1 &lt; nc &amp;&amp; grid[r][c+1] == '1') &#123; uf.union(r * nc + c, r * nc + c + 1); &#125; &#125; &#125; &#125; return uf.getCount(); &#125;&#125; 221. 最大正方形题目在一个由 0 和 1 组成的二维矩阵内，找到只包含 1 的最大正方形，并返回其面积。 示例: 12345678输入: 1 0 1 0 01 0 1 1 11 1 1 1 11 0 0 1 0输出: 4 思路前面做了一题求 最大长方形的题目，当时的思路是用的第 84 题 柱状图中最大的矩形 ，分别求每一行的最大矩形，最后得到 整个的最大长方形。这里求正方形，就不能用那种方法了，必须得另辟蹊径。 这里采用的是 dp，既然是找最大正方形，其实找对 dp数组代表什么 和 状态转移方程 如何写，就完成了，这里的 dp 数组代表以 该元素为右下角的正方形边长，故 dp(i, j)=min(dp(i−1, j), dp(i−1, j−1), dp(i, j−1))+1 代码12345678910111213141516public class Solution &#123; public int maximalSquare(char[][] matrix) &#123; int rows = matrix.length, cols = rows &gt; 0 ? matrix[0].length : 0; int[][] dp = new int[rows + 1][cols + 1]; int maxsqlen = 0; for (int i = 1; i &lt;= rows; i++) &#123; for (int j = 1; j &lt;= cols; j++) &#123; if (matrix[i-1][j-1] == '1')&#123; dp[i][j] = Math.min(Math.min(dp[i][j - 1], dp[i - 1][j]), dp[i - 1][j - 1]) + 1; maxsqlen = Math.max(maxsqlen, dp[i][j]); &#125; &#125; &#125; return maxsqlen * maxsqlen; &#125;&#125; 由于当前 dp[i][j] 只用到了左上、左边、上边三个元素，所以不需要建立一个二维数组去操作，只需要用一个一维的数组去存每一行对应的列的数就行了，可以复用，至于 左上角 的数，可以用一个变量单独记录一下（这个是真的牛皮） 1234567891011121314151617181920public class Solution &#123; public int maximalSquare(char[][] matrix) &#123; int rows = matrix.length, cols = rows &gt; 0 ? matrix[0].length : 0; int[] dp = new int[cols + 1]; int maxsqlen = 0, prev = 0; for (int i = 1; i &lt;= rows; i++) &#123; for (int j = 1; j &lt;= cols; j++) &#123; int temp = dp[j]; if (matrix[i - 1][j - 1] == '1') &#123; dp[j] = Math.min(Math.min(dp[j - 1], prev), dp[j]) + 1; maxsqlen = Math.max(maxsqlen, dp[j]); &#125; else &#123; dp[j] = 0; &#125; prev = temp; &#125; &#125; return maxsqlen * maxsqlen; &#125;&#125; 207. 课程表题目现在你总共有 n 门课需要选，记为 0 到 n-1。 在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们: [0,1] 给定课程总量以及它们的先决条件，判断是否可能完成所有课程的学习？ 示例 1: 123输入: 2, [[1,0]] 输出: true解释: 总共有 2 门课程。学习课程 1 之前，你需要完成课程 0。所以这是可能的。 示例 2: 123输入: 2, [[1,0],[0,1]]输出: false解释: 总共有 2 门课程。学习课程 1 之前，你需要先完成课程 0；并且学习课程 0 之前，你还应先完成课程 1。这是不可能的。 思路这题本质就是拓扑排序，解决拓扑排序，一般就是两个方法：BFS 和 DFS。其核心都是能拓扑排序的都是有向无环图。BFS 主要是从 入度出度考虑，而 DFS 主要从有无环考虑。 详细思路见： https://leetcode-cn.com/problems/course-schedule/solution/course-schedule-tuo-bu-pai-xu-bfsdfsliang-chong-fa/ 代码 BFS 12345678910111213141516171819202122232425262728293031class Solution &#123; // BFS public boolean canFinish(int numCourses, int[][] prerequisites) &#123; // 记录每个结点的入度，类似于邻接表 int[] indegrees = new int[numCourses]; // 统计每个结点的入度数 // 这个题目的好处是课程是从0开始的，所以我们申请空间很方便 for(int[] cp : prerequisites) indegrees[cp[0]]++; // 队列用来存储入度为0的结点 LinkedList&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); // 如果找到入度为0的，放入队尾 for(int i = 0; i &lt; numCourses; i++)&#123; if(indegrees[i] == 0) queue.addLast(i); &#125; // 只要队列不为空，就一直循环 while(!queue.isEmpty()) &#123; // 取出队首元素 Integer pre = queue.removeFirst(); // 相当于删除操作，说明此门课程已经学完 numCourses--; // 然后把这门课程的出度删除 // 并把新的入度为0的课程加入队列 for(int[] req : prerequisites) &#123; if(req[1] != pre) continue; if(--indegrees[req[0]] == 0) queue.add(req[0]); &#125; &#125; // 队列为空，如果能全部学习完，numCourses == 0，否则不能全部学习完 return numCourses == 0; &#125;&#125; DFS 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution &#123; /** * DFS 方法 * @param numCourses * @param prerequisites * @return */ public boolean canFinish(int numCourses, int[][] prerequisites) &#123; // 用来表示节点之间是否有边,类似于 邻接矩阵 int[][] arc = new int[numCourses][numCourses]; for(int[] cp:prerequisites)&#123; arc[cp[1]][cp[0]] = 1; &#125; // 对每个节点分别进行dfs // dfs 主要是判断是否有环的存在，在这里我们引入 flag // flag == 1，代表该节点本轮dfs已经访问过了,即正在访问的节点 // flag == -1，代表以前的dfs访问过了 // flag == 0，代表该节点从未访问过 // 只有当我们访问到了 flag == 1的节点，说明存在环 int[] flag = new int[numCourses]; for(int i = 0;i &lt; numCourses;i++)&#123; if(dfs(arc,i,flag) == false) return false; &#125; return true; &#125; /** * dfs 具体流程 * @param arc * @param i * @param flag * @return */ private boolean dfs(int[][] arc, int i, int[] flag) &#123; // 发现是本轮 dfs 正在访问的节点，故存在环 if(flag[i] == 1) return false; // 是其他结点发起的 dfs，没有关系，不能判断是否有环 if(flag[i] == -1) return true; // 标记一下本轮dfs正在访问该节点 flag[i] = 1; // 进行 dfs，发现有环，返回 false for(int j = 0;j &lt; arc.length;j++)&#123; if(arc[i][j] == 1 &amp;&amp; dfs(arc,j,flag) == false) return false; &#125; // 该节点的 dfs 结束，标志位变为 -1 flag[i] = -1; return true; &#125;&#125; 208. 实现 Trie (前缀树)题目实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。 示例: 12345678Trie trie = new Trie();trie.insert("apple");trie.search("apple"); // 返回 truetrie.search("app"); // 返回 falsetrie.startsWith("app"); // 返回 truetrie.insert("app"); trie.search("app"); // 返回 true 说明: 你可以假设所有的输入都是由小写字母 a-z 构成的。 保证所有输入均为非空字符串。 思路 https://blog.csdn.net/qq_43152052/article/details/101109415 大佬对 leetcode 前缀树的习题总结 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package LeetCode_100_hotest;class Trie_208 &#123; public static void main(String[] args) &#123; Trie obj = new Trie(); String word = "apple"; String prefix = "app"; obj.insert(word); boolean param_2 = obj.search(word); boolean param_3 = obj.startsWith(prefix); System.out.println("param_2:" + param_2); System.out.println("param_3:" + param_3); &#125;&#125;class Trie &#123; private TrieNode root; public Trie() &#123; root = new TrieNode(); &#125; // Inserts a word into the trie. public void insert(String word) &#123; TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) &#123; char currentChar = word.charAt(i); if (!node.containsKey(currentChar)) &#123; node.put(currentChar, new TrieNode()); &#125; node = node.get(currentChar); &#125; node.setEnd(); &#125; private TrieNode searchPrefix(String word) &#123; TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) &#123; char curLetter = word.charAt(i); if (node.containsKey(curLetter)) &#123; node = node.get(curLetter); &#125; else &#123; return null; &#125; &#125; return node; &#125; // Returns if the word is in the trie. public boolean search(String word) &#123; TrieNode node = searchPrefix(word); return node != null &amp;&amp; node.isEnd(); &#125; public boolean startsWith(String prefix) &#123; TrieNode node = searchPrefix(prefix); return node != null; &#125;&#125;class TrieNode &#123; // R links to node children // 这里不止是 left、right，所以必须是一个数组组成 private TrieNode[] links; private final int R = 26; private boolean isEnd; public TrieNode() &#123; links = new TrieNode[R]; &#125; public boolean containsKey(char ch) &#123; return links[ch -'a'] != null; &#125; public TrieNode get(char ch) &#123; return links[ch -'a']; &#125; public void put(char ch, TrieNode node) &#123; links[ch -'a'] = node; &#125; public void setEnd() &#123; isEnd = true; &#125; public boolean isEnd() &#123; return isEnd; &#125;&#125;/** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * boolean param_2 = obj.search(word); * boolean param_3 = obj.startsWith(prefix); */ 739. 每日温度题目根据每日 气温 列表，请重新生成一个列表，对应位置的输入是你需要再等待多久温度才会升高超过该日的天数。如果之后都不会升高，请在该位置用 0 来代替。 例如，给定一个列表 temperatures = [73, 74, 75, 71, 69, 72, 76, 73]，你的输出应该是 [1, 1, 4, 2, 1, 1, 0, 0]。 提示：气温 列表长度的范围是 [1, 30000]。每个气温的值的均为华氏度，都是在 [30, 100] 范围内的整数。 思路很明显是采用单调栈的方法，具体见 https://labuladong.gitbook.io/algo/shu-ju-jie-gou-xi-lie/dan-tiao-zhan 代码1234567891011121314class Solution &#123; public int[] dailyTemperatures(int[] T) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int[] ans = new int[T.length]; for(int i = T.length - 1; i &gt;= 0;i--)&#123; while(!stack.isEmpty() &amp;&amp; T[i] &gt;= T[stack.peek()])&#123; stack.pop(); &#125; ans[i] = stack.isEmpty() == true ? 0 : (stack.peek() - i); stack.push(i); &#125; return ans; &#125;&#125; 621. 任务调度器题目给定一个用字符数组表示的 CPU 需要执行的任务列表。其中包含使用大写的 A - Z 字母表示的26 种不同种类的任务。任务可以以任意顺序执行，并且每个任务都可以在 1 个单位时间内执行完。CPU 在任何一个单位时间内都可以执行一个任务，或者在待命状态。 然而，两个相同种类的任务之间必须有长度为 n 的冷却时间，因此至少有连续 n 个单位时间内 CPU 在执行不同的任务，或者在待命状态。 你需要计算完成所有任务所需要的最短时间。 示例 1 123输入: tasks = ["A","A","A","B","B","B"], n = 2输出: 8执行顺序: A -&gt; B -&gt; (待命) -&gt; A -&gt; B -&gt; (待命) -&gt; A -&gt; B. 思路 桶思想，每个桶固定大小为 n+1（除最后一个桶之外），这样可以确保相同的任务可以分在不同的桶中 当然，每个任务在桶中的次序是固定的，比如说 A 在桶底，那么在每个桶中 A 都在底部，这样可以确保相同任务的间隔时间都不小于 n 桶的数量由 拥有最多任务数的那个任务决定，只要他保证了冷却时间，其他的一定可以 结果就是 (n+1) \ (count - 1) + 最后一个桶的大小*，count 为桶的数量，因为最后一个桶无需固定大小 count 很好求，那最后一个桶大小如何求呢，很明显就是 拥有最多数任务的个数，比如AAABBBCCCDDEE，那最后一个桶的大小就是 3，因为 A B C 都是拥有 3 个任务数 如果冷却时间过短，任务数过多，也就是说桶不够用了，比如说 AAABBBCCCDDEE 且 n = 2 这种情况，此时 桶的大小为 3，桶的数量为 3。第一个桶 ABC ，第二个 ABC，第三个 ABC，此时的 D 和 E 我们可以理解为按照一定次序放在桶之上就行 ，也就是不用放到桶中，这样不会影响桶内元素 由于 D 和 E 的出现次数是一定小于桶的数量的，所以最多每个桶上放一个相同任务，这样 D 和 E 按次序排布是一定符合要求的 此时的答案就是 任务总数 了，因为所有的桶都满了，并且多出来的也是任务，没有待命时间 故答案就是 两个时间 的最大值 代码1234567891011121314151617181920class Solution &#123; public int leastInterval(char[] tasks, int n) &#123; HashMap&lt;Character, Integer&gt; task_map = new HashMap&lt;&gt;(); // 记录 单个任务出现的最多的次数 int max_count = 0; // 记录 有最多任务数的 任务个数 int difference = 0; for (Character task : tasks) &#123; int count = task_map.getOrDefault(task, 0) + 1; task_map.put(task, count); max_count = Math.max(max_count,count); &#125; for(Map.Entry&lt;Character, Integer&gt; entry:task_map.entrySet())&#123; if(entry.getValue() == max_count) difference++; &#125; int number1 = (n + 1) * (max_count - 1) + difference; int number2 = tasks.length; return Math.max(number1,number2); &#125;&#125; 581. 最短无序连续子数组题目给定一个整数数组，你需要寻找一个连续的子数组，如果对这个子数组进行升序排序，那么整个数组都会变为升序排序。 你找到的子数组应是最短的，请输出它的长度。 示例 1: 123输入: [2, 6, 4, 8, 10, 9, 15]输出: 5解释: 你只需要对 [6, 4, 8, 10, 9] 进行升序排序，那么整个表都会变为升序排序。 思路我采用的是排序 代码12345678910111213141516171819202122232425class Solution &#123; public int findUnsortedSubarray(int[] nums) &#123; int[] copy = Arrays.copyOf(nums,nums.length); Arrays.sort(copy); int start = 0; int end = 0; for(int i = 0;i &lt; nums.length;i++)&#123; if(nums[i] != copy[i])&#123; start = i; break; &#125; &#125; for(int j = nums.length-1;j &gt;= 0;j--)&#123; if(nums[j] != copy[j])&#123; end = j; // 其实这步应该放到最后 end - start + 1 // 但是为了防止num.length == 0 以及 正序的数组 结果正确 // 就放到了这里 end++; break; &#125; &#125; return end - start; &#125;&#125; 560. 和为K的子数组题目给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。 示例 1 : 12输入:nums = [1,1,1], k = 2输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。 思路 暴力法，两次 for 循环，首先是 start，然后 end 从第一个数开始，当碰到 sum = k 时 count 就 + 1 dp。dp[i] 表示从0到第 i 个数的总和，则 dp[j] - dp[i] = k，这个 dp[i] 的个数就是 连续子数组的个数，这里可以用 HashMap 直接存储 和 以及 和 出现的次数，这样就可以非常方便的求得 dp[i] 的个数。 代码 暴力法 12345678910111213141516class Solution &#123; public int subarraySum(int[] nums, int k) &#123; if (nums.length == 0) return 0; int res = 0; for (int i = 0; i &lt; nums.length; i++) &#123; int sum = 0; for (int j = i; j &lt; nums.length; j++) &#123; sum += nums[j]; if (sum == k) &#123; res++; &#125; &#125; &#125; return res; &#125;&#125; dp 123456789101112131415161718192021222324import java.util.HashMap;class Solution &#123; public static void main(String[] args) &#123; new Solution().subarraySum(new int[]&#123;0, 0, 0, 0, 0, 0, 0, 0, 0, 0&#125;, 0); &#125; public int subarraySum(int[] nums, int k) &#123; if (nums == null || nums.length == 0) return 0; //dp[i]表示前i个数的和 int[] dp = new int[nums.length + 1]; for (int i = 1; i &lt;= nums.length; i++) &#123; dp[i] = dp[i - 1] + nums[i - 1]; &#125; int ret = 0; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; dp.length; i++) &#123; if (map.containsKey(dp[i] - k)) ret += map.get(dp[i] - k); map.put(dp[i], map.getOrDefault(dp[i], 0) + 1); &#125; return ret; &#125;&#125; dp 优化 123456789101112131415public class Solution &#123; public int subarraySum(int[] nums, int k) &#123; int count = 0, sum = 0; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); // 和为0的总和，出现了一次 map.put(0, 1); for (int i = 0; i &lt; nums.length; i++) &#123; sum += nums[i]; if (map.containsKey(sum - k)) count += map.get(sum - k); map.put(sum, map.getOrDefault(sum, 0) + 1); &#125; return count; &#125;&#125; 238. 除自身以外数组的乘积题目给定长度为 n 的整数数组 nums，其中 n &gt; 1，返回输出数组 output ，其中 output[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积。 示例: 12输入: [1,2,3,4]输出: [24,12,8,6] 说明: 请不要使用除法，且在 O(n) 时间复杂度内完成此题。 进阶：你可以在常数空间复杂度内完成这个题目吗？（ 出于对空间复杂度分析的目的，输出数组不被视为额外空间。） 思路 左边乘积 \ 右边乘积*，题目中要求使用 O(1) 的空间复杂度，但是 输出数组 不被视为额外空间，于是可以用左边乘积 * 右边乘积，具体见代码！ 代码123456789101112131415161718class Solution &#123; public int[] productExceptSelf(int[] nums) &#123; int[] res = new int[nums.length]; res[0] = 1; for(int i = 1;i &lt; nums.length;i++)&#123; // 此时是计算每个值的左边乘积 res[i] = res[i-1] * nums[i-1]; &#125; // 此时需要一个变量来表示右边乘积 // 但是不需要数组，因为从右边开始算，算完一个就可以清除它 int right = 1; for(int j = nums.length - 1;j &gt;= 0;j--)&#123; res[j] = res[j] * right; right = right * nums[j]; &#125; return res; &#125;&#125; 239. 滑动窗口最大值题目给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。 返回滑动窗口中的最大值。 示例: 123456789101112输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3输出: [3,3,5,5,6,7] 解释: 滑动窗口的位置 最大值--------------- -----[1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7 提示： 1你可以假设 k 总是有效的，在输入数组不为空的情况下，1 ≤ k ≤ 输入数组的大小。 进阶： 1你能在线性时间复杂度内解决此题吗？ 思路 暴力法 双端队列法。遍历数组，将数存放在双向队列中，并用L,R来标记窗口的左边界和右边界。队列中保存的并不是真的数，而是该数值对应的数组下标位置，并且数组中的数要从大到小排序。如果当前遍历的数比队尾的值大，则需要弹出队尾值，直到队列重新满足从大到小的要求。刚开始遍历时，L和R都为0，有一个形成窗口的过程，此过程没有最大值，L不动，R向右移。当窗口大小形成时，L和R一起向右移，每次移动时，判断队首的值的数组下标是否在[L,R]中，如果不在则需要弹出队首的值，当前窗口的最大值即为队首的数。 【有点难，适当记忆步骤】 作者：hanyuhuang链接：https://leetcode-cn.com/problems/sliding-window-maximum/solution/shuang-xiang-dui-lie-jie-jue-hua-dong-chuang-kou-2/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 dp。这个就更骚了…见官方题解方法三 代码 暴力法 1234567891011121314class Solution &#123; public int[] maxSlidingWindow(int[] nums, int k) &#123; if(nums.length == 0) return new int[]&#123;&#125;; int[] res = new int[nums.length+1-k]; for(int i = 0;i &lt;= nums.length-k;i++)&#123; int max = Integer.MIN_VALUE; for(int j = 0;j &lt; k;j++)&#123; max = Math.max(max,nums[i+j]); &#125; res[i] = max; &#125; return res; &#125;&#125; 双端队列法 123456789101112131415161718192021222324252627class Solution &#123; public int[] maxSlidingWindow(int[] nums, int k) &#123; if(nums == null || nums.length &lt; 2) return nums; // 双向队列 保存当前窗口最大值的数组位置 保证队列中数组位置的数值按从大到小排序 LinkedList&lt;Integer&gt; queue = new LinkedList(); // 结果数组 int[] result = new int[nums.length-k+1]; // 遍历nums数组 for(int i = 0;i &lt; nums.length;i++)&#123; // 保证从大到小 如果前面数小则需要依次弹出，直至满足要求 while(!queue.isEmpty() &amp;&amp; nums[queue.peekLast()] &lt;= nums[i])&#123; queue.pollLast(); &#125; // 添加当前值对应的数组下标 queue.addLast(i); // 判断当前队列中队首的值是否有效 if(queue.peek() &lt;= i-k)&#123; queue.poll(); &#125; // 当窗口长度为k时 保存当前窗口中最大值 if(i+1 &gt;= k)&#123; result[i+1-k] = nums[queue.peek()]; &#125; &#125; return result; &#125;&#125; dp 12345678910111213141516171819202122232425262728class Solution &#123; public int[] maxSlidingWindow(int[] nums, int k) &#123; int n = nums.length; if (n * k == 0) return new int[0]; if (k == 1) return nums; int [] left = new int[n]; left[0] = nums[0]; int [] right = new int[n]; right[n - 1] = nums[n - 1]; for (int i = 1; i &lt; n; i++) &#123; // from left to right if (i % k == 0) left[i] = nums[i]; // block_start else left[i] = Math.max(left[i - 1], nums[i]); // from right to left int j = n - i - 1; if ((j + 1) % k == 0) right[j] = nums[j]; // block_end else right[j] = Math.max(right[j + 1], nums[j]); &#125; int [] output = new int[n - k + 1]; for (int i = 0; i &lt; n - k + 1; i++) output[i] = Math.max(left[i + k - 1], right[i]); return output; &#125;&#125; 240. 搜索二维矩阵 II题目编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性： 每行的元素从左到右升序排列。 每列的元素从上到下升序排列。 示例: 现有矩阵 matrix 如下： 1234567[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] 给定 target = 5，返回 true。 给定 target = 20，返回 false。 思路暴力法直接先行后列遍历，遍历到了 target 退出。 二分查找法以对角线为界限，对角线之上的进行 行二分查找，对角线之下的进行 列二分查找。 减治法其实就是选定一个特殊的出发点。 选左上角，往右走和往下走都增大，不能选 选右下角，往上走和往左走都减小，不能选 选左下角，往右走增大，往上走减小，可选 选右上角，往下走增大，往左走减小，可选 这里我们选定左下角元素！！具体操作如下： * 设矩阵左下角元素 matrix\[i][j] ，它是第 i 行最小值，同时也是第 j 列最大值 若 target &lt; matrix[i][j] (小于第 i 行最小值)，则排除第 i 行，令 i– 若 target &gt; matrix[i][j] (大于第 j 列最大值)，则排除第 j 列，令 j++ 循环 2~3 直到找到 target，或所有行列均被排除 代码 暴力法 123456789101112class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; for (int i = 0; i &lt; matrix.length; i++) &#123; for (int j = 0; j &lt; matrix[0].length; j++) &#123; if (matrix[i][j] == target) &#123; return true; &#125; &#125; &#125; return false; &#125;&#125; 二分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; private boolean binarySearch(int[][] matrix, int target, int start, boolean vertical) &#123; int lo = start; int hi = vertical ? matrix[0].length-1 : matrix.length-1; while (hi &gt;= lo) &#123; int mid = (lo + hi)/2; if (vertical) &#123; // searching a column if (matrix[start][mid] &lt; target) &#123; lo = mid + 1; &#125; else if (matrix[start][mid] &gt; target) &#123; hi = mid - 1; &#125; else &#123; return true; &#125; &#125; else &#123; // searching a row if (matrix[mid][start] &lt; target) &#123; lo = mid + 1; &#125; else if (matrix[mid][start] &gt; target) &#123; hi = mid - 1; &#125; else &#123; return true; &#125; &#125; &#125; return false; &#125; public boolean searchMatrix(int[][] matrix, int target) &#123; // an empty matrix obviously does not contain `target` if (matrix == null || matrix.length == 0) &#123; return false; &#125; // iterate over matrix diagonals int shorterDim = Math.min(matrix.length, matrix[0].length); for (int i = 0; i &lt; shorterDim; i++) &#123; boolean verticalFound = binarySearch(matrix, target, i, true); boolean horizontalFound = binarySearch(matrix, target, i, false); if (verticalFound || horizontalFound) &#123; return true; &#125; &#125; return false; &#125;&#125; 减治法 12345678910111213141516171819class Solution &#123; public boolean searchMatrix(int[][] matrix, int target) &#123; // start our "pointer" in the bottom-left int row = matrix.length-1; int col = 0; while (row &gt;= 0 &amp;&amp; col &lt; matrix[0].length) &#123; if (matrix[row][col] &gt; target) &#123; row--; &#125; else if (matrix[row][col] &lt; target) &#123; col++; &#125; else &#123; // found it return true; &#125; &#125; return false; &#125;&#125; 279. 完全平方数题目给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, ...）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。 示例 1: 123输入: n = 12输出: 3 解释: 12 = 4 + 4 + 4. 示例 2: 123输入: n = 13输出: 2解释: 13 = 4 + 9. 思路dp 代码123456789101112class Solution &#123; public int numSquares(int n) &#123; int[] dp = new int[n + 1]; // 默认初始化值都为0 for (int i = 1; i &lt;= n; i++) &#123; dp[i] = i; // 最坏的情况就是每次+1 for (int j = 1; i - j * j &gt;= 0; j++) &#123; dp[i] = Math.min(dp[i], dp[i - j * j] + 1); // 动态转移方程 &#125; &#125; return dp[n]; &#125;&#125; 283. 移动零题目给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。 示例: 12输入: [0,1,0,3,12]输出: [1,3,12,0,0] 说明: 必须在原数组上操作，不能拷贝额外的数组。 尽量减少操作次数。 思路先复制后补0 代码1234567891011121314151617class Solution &#123; public void moveZeroes(int[] nums) &#123; int index = 0; for(int i = 0;i &lt; nums.length;i++)&#123; if(nums[i] != 0)&#123; // 先复制 nums[index] = nums[i]; index++; &#125; &#125; // 补0 while(index &lt; nums.length)&#123; nums[index] = 0; index++; &#125; &#125;&#125; 287. 寻找重复数题目给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。 示例 1: 12输入: [1,3,4,2,2]输出: 2 示例 2: 12输入: [3,1,3,4,2]输出: 3 说明： 不能更改原数组（假设数组是只读的）。 只能使用额外的 O(1) 的空间。 时间复杂度小于 O(n2) 。 数组中只有一个重复的数字，但它可能不止重复出现一次。 思路 排序后，相邻元素如果相等，则 return 用 set 存储，一旦发现 key 已经存在，直接返回，否则存入 map 中 其实这是一个链表中非常常见的问题，就是寻找环的入口问题，n 个不同的数，相同的那个数其实就是形成一个环。即元素索引下标为节点 node，而 node 的 next 指针则是 元素值 所对应的索引下标值。 Tip: 例如[2,1,2,3,4]，这个是符合题目要求且符合自循环的情况，在nums[2]处，索引值和元素值相等，在这里的确，快慢指针会在2的位置不停指向自己，但是只要发生这种自循环的情况，那么重复的数字就是这个，我们依旧可以使用快慢指针的方法去做，因为我们的做法就是先求一次相遇，然后慢指针回到原点，第二次相遇就是环的起点，在这里，既然是自循环的，那么当然最后第二次相遇还是在这里，答案是一样的。 二分法 代码 排序 1234567891011class Solution &#123; public int findDuplicate(int[] nums) &#123; Arrays.sort(nums); for (int i = 1; i &lt; nums.length; i++) &#123; if (nums[i] == nums[i-1]) &#123; return nums[i]; &#125; &#125; return -1; &#125;&#125; Set 存储 123456789101112class Solution &#123; public int findDuplicate(int[] nums) &#123; Set&lt;Integer&gt; seen = new HashSet&lt;Integer&gt;(); for (int num : nums) &#123; if (seen.contains(num)) &#123; return num; &#125; seen.add(num); &#125; return -1; &#125;&#125; 快慢指针 12345678910111213141516171819class Solution &#123; public int findDuplicate(int[] nums) &#123; // Find the intersection point of the two runners. int slow = nums[0]; int fast = nums[0]; do &#123; slow = nums[slow]; fast = nums[nums[fast]]; &#125; while (slow != fast); // Find the "entrance" to the cycle. slow = nums[0]; while (fast != slow) &#123; fast = nums[fast]; slow = nums[slow]; &#125; return fast; &#125;&#125; 二分 1234567891011121314151617181920212223class Solution &#123; public int findDuplicate(int[] nums) &#123; int left = 0; int right = nums.length - 1; while(left &lt;= right)&#123; int mid = left + (right - left)/2; int count = 0; int mid_count = 0; for(int i = 0;i &lt; nums.length;i++)&#123; if(nums[i] &lt;= mid) count++; if(nums[i] == mid) mid_count++; &#125; // 如果 [left,mid] 没有出现重复数字，count &lt;= mid // 否则说明在这个区间出现了重复的 // 这里跟常见的 二分有一点点不同，这里 right = mid，不是 right = mid - 1 // 因为我在上面计算 count 的时候 是有计算 mid 的 if(mid_count &gt; 1) return mid; if(count &gt; mid) right = mid - 1; else left = mid + 1; &#125; return -1; &#125;&#125; 301. 删除无效的括号（暂时未做哈~ 先把代码贴一下 第二遍写） 题目删除最小数量的无效括号，使得输入的字符串有效，返回所有可能的结果。 说明: 输入可能包含了除 ( 和 ) 以外的字符。 示例 1: 12输入: "()())()"输出: ["()()()", "(())()"] 示例 2: 12输入: "(a)())()"输出: ["(a)()()", "(a())()"] 示例 3: 12输入: ")("输出: [""] 思路代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788class Solution &#123; public List&lt;String&gt; removeInvalidParentheses(String s) &#123; // 统计需要删除的左括号和右括号数量 int left = 0, right = 0; for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); if (c == '(') &#123; left++; &#125; else if (c == ')') &#123; if (left &gt; 0) left--; else right++; &#125; &#125; // 开始删除 List&lt;String&gt; res = new ArrayList&lt;&gt;(); List&lt;String&gt; array = new ArrayList&lt;&gt;(); Set&lt;String&gt; set = new HashSet&lt;&gt;(); array.add(s); // 先删左括号 while (left-- &gt; 0) &#123; List&lt;String&gt; tempArray = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; array.size(); i++) &#123; String ss = array.get(i); for (int j = 0; j &lt; ss.length(); j++) &#123; if (ss.charAt(j) == '(') &#123; String sss = ss.substring(0, j) + ss.substring(j + 1); if (!set.contains(sss)) &#123; set.add(sss); tempArray.add(sss); &#125; &#125; &#125; &#125; array = tempArray; &#125; // 删右括号 while (right-- &gt; 0) &#123; List&lt;String&gt; tempArray = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; array.size(); i++) &#123; String ss = array.get(i); for (int j = 0; j &lt; ss.length(); j++) &#123; if (ss.charAt(j) == ')') &#123; String sss = ss.substring(0, j) + ss.substring(j + 1); if (!set.contains(sss)) &#123; set.add(sss); tempArray.add(sss); &#125; &#125; &#125; &#125; array = tempArray; &#125; // 对删除后字符串进行验证 for (int i = 0; i &lt; array.size(); i++) &#123; String str = array.get(i); if (this.checkVaild(str)) &#123; res.add(str); &#125; &#125; return res; &#125; private boolean checkVaild(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); if (c == '(' ) &#123; stack.push(c); &#125; else if (c == ')') &#123; if (stack.empty() || stack.peek() == ')') &#123; stack.push(c); &#125; else &#123; stack.pop(); &#125; &#125; &#125; if (stack.empty()) return true; return false; &#125;&#125; 312. 戳气球【跟上面一样，这两题没做呢！】 题目有 n 个气球，编号为0 到 n-1，每个气球上都标有一个数字，这些数字存在数组 nums 中。 现在要求你戳破所有的气球。每当你戳破一个气球 i 时，你可以获得 nums[left] * nums[i] * nums[right] 个硬币。 这里的 left 和 right 代表和 i 相邻的两个气球的序号。注意当你戳破了气球 i 后，气球 left 和气球 right 就变成了相邻的气球 思路超详细回溯到dp 代码123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public static int maxCoins(int[] nums) &#123; //避免空指针异常 if (nums == null) &#123; return 0; &#125; //创建虚拟边界 int length = nums.length; int[] nums2 = new int[length + 2]; System.arraycopy(nums, 0, nums2, 1, length); nums2[0] = 1; nums2[length + 1] = 1; length = nums2.length; //创建dp表 length = nums2.length; int[][] dp = new int[length][length]; //开始dp：i为begin，j为end，k为在i、j区间划分子问题时的边界 for (int i = length - 2; i &gt; -1; i--) &#123; for (int j = i + 2; j &lt; length; j++) &#123; //维护一个最大值；如果i、j相邻，值为0 int max = 0; for (int k = i + 1; k &lt; j; k++) &#123; int temp = dp[i][k] + dp[k][j] + nums2[i] * nums2[k] * nums2[j]; if (temp &gt; max) &#123; max = temp; &#125; &#125; dp[i][j] = max; &#125; &#125; return dp[0][length-1]; &#125;&#125; 338. 比特位计数题目给定一个非负整数 num。对于 0 ≤ i ≤ num 范围中的每个数字 i ，计算其二进制数中的 1 的数目并将它们作为数组返回。 示例 1: 12输入: 2输出: [0,1,1] 示例 2: 12输入: 5输出: [0,1,1,2,1,2] 进阶: 给出时间复杂度为O(n*sizeof(integer))的解答非常容易。但你可以在线性时间O(n)内用一趟扫描做到吗？ 要求算法的空间复杂度为O(n)。 你能进一步完善解法吗？要求在C++或任何其他语言中不使用任何内置函数来执行此操作。 思路 从 191. 位1的个数 中启发而来，n &amp;（n-1）这种秀的一批的操作 对上述方法有一个改进版，动态规划版本的 代码 Case I 123456789101112131415161718class Solution &#123; public int[] countBits(int num) &#123; int[] res = new int[num+1]; for(int i = 0;i &lt;= num;i++)&#123; res[i] = hammingWeight(i); &#125; return res; &#125; public int hammingWeight(int n) &#123; int sum = 0; while(n != 0)&#123; sum++; n = n &amp; (n-1); &#125; return sum; &#125;&#125; Case II 123456789class Solution &#123; public int[] countBits(int num) &#123; int[] dp = new int[num+1]; for(int i = 1;i &lt;= num;i++)&#123; dp[i] = dp[i&amp;(i-1)] + 1; &#125; return dp; &#125;&#125; 347. 前 K 个高频元素题目给定一个非空的整数数组，返回其中出现频率前 k 高的元素。 示例 1: 12输入: nums = [1,1,1,2,2,3], k = 2输出: [1,2] 示例 2: 12输入: nums = [1], k = 1输出: [1] 说明： 你可以假设给定的 k 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。 你的算法的时间复杂度必须优于 O(n log n) , n 是数组的大小。 思路 借助 2020-02-02 周赛的一道题的思路，将其存入 map，然后用 list 存储 map，然后利用集合的排序方法，对其进行排序，再转成 map，然后用 iterator 遍历即可。 思路基本一致，但是其实这里排序之后根本用不到 value 了，所以根本没必要再转成 map，所以可以不用 list 去存储 entry 对象，直接用 PriorityQueue 去对 value 排序，然后存储 key 值就可以了，因为 PriorityQueue 是最小堆实现的，时间复杂度只有 O(nlogn) 这个方法不用排序，用 map 存储完之后，直接放到桶中（value值作为桶的序号），存入 key。 代码 Case I 12345678910111213141516171819202122232425262728293031323334class Solution &#123; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); for (int i = 0; i &lt; nums.length; i++) &#123; if (map.get(nums[i]) != null) &#123; map.put(nums[i], map.get(nums[i]) + 1); &#125; else &#123; map.put(nums[i], 1); &#125; &#125; List&lt;Map.Entry&lt;Integer, Integer&gt;&gt; list = new ArrayList(map.entrySet()); // 按 value 排序，降序排序 Collections.sort(list, (o1, o2) -&gt; &#123; int compare = (o1.getValue()).compareTo(o2.getValue()); return -compare; &#125;); Map&lt;Integer, Integer&gt; returnMap = new LinkedHashMap&lt;Integer, Integer&gt;(); for (Map.Entry&lt;Integer, Integer&gt; entry : list) &#123; returnMap.put(entry.getKey(), entry.getValue()); &#125; int i = 0; List&lt;Integer&gt; res = new ArrayList(); for(Map.Entry&lt;Integer, Integer&gt; entry:returnMap.entrySet())&#123; if(i == k)&#123; break; &#125; else &#123; res.add(entry.getKey()); i++; &#125; &#125; return res; &#125;&#125; Case II 12345678910111213141516171819202122232425262728class Solution &#123; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; // build hash map : character and how often it appears HashMap&lt;Integer, Integer&gt; count = new HashMap(); for (int n: nums) &#123; count.put(n, count.getOrDefault(n, 0) + 1); &#125; // init heap 'the less frequent element first' PriorityQueue&lt;Integer&gt; heap = new PriorityQueue&lt;Integer&gt;((n1, n2) -&gt; count.get(n1) - count.get(n2)); // keep k top frequent elements in the heap for (int n: count.keySet()) &#123; heap.add(n); if (heap.size() &gt; k) // 是 堆 实现的，然后上述构造的是升序排列，最小的在第一个，最大的在最后 heap.poll(); &#125; // build output list List&lt;Integer&gt; top_k = new LinkedList(); while (!heap.isEmpty()) top_k.add(heap.poll()); Collections.reverse(top_k); return top_k; &#125;&#125; Case III 12345678910111213141516171819202122232425262728293031323334//基于桶排序求解「前 K 个高频元素」class Solution &#123; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; List&lt;Integer&gt; res = new ArrayList(); // 使用字典，统计每个元素出现的次数，元素为键，元素出现的次数为值 HashMap&lt;Integer,Integer&gt; map = new HashMap(); for(int num : nums)&#123; if (map.containsKey(num)) &#123; map.put(num, map.get(num) + 1); &#125; else &#123; map.put(num, 1); &#125; &#125; //桶排序 //将频率作为数组下标，对于出现频率不同的数字集合，存入对应的数组下标 List&lt;Integer&gt;[] list = new List[nums.length+1]; for(int key : map.keySet())&#123; // 获取出现的次数作为下标 int i = map.get(key); if(list[i] == null)&#123; list[i] = new ArrayList(); &#125; list[i].add(key); &#125; // 倒序遍历数组获取出现顺序从大到小的排列 for(int i = list.length - 1;i &gt;= 0 &amp;&amp; res.size() &lt; k;i--)&#123; if(list[i] == null) continue; res.addAll(list[i]); &#125; return res; &#125;&#125; 394. 字符串解码题目给定一个经过编码的字符串，返回它解码后的字符串。 编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。 你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。 此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。 示例: 123s = "3[a]2[bc]", 返回 "aaabcbc".s = "3[a2[c]]", 返回 "accaccacc".s = "2[abc]3[cd]ef", 返回 "abcabccdcdcdef". 思路 栈 递归 代码 栈 12345678910111213141516171819202122232425262728293031public String decodeString(String s) &#123; StringBuilder res = new StringBuilder(); int multi = 0; // 存储数字 Stack&lt;Integer&gt; stack_multi = new Stack&lt;&gt;(); // 存储字符串，字符串分为两类，一类是在'[' 之前的，这个需要压栈，然后拼接的 // 一类是在']'之前的，这类是用来在括号内部乘以倍数的 Stack&lt;String&gt; stack_res = new Stack&lt;&gt;(); for(Character c : s.toCharArray()) &#123; if(c == '[') &#123; // 将倍数压栈 stack_multi.push(multi); // 将左括号之前的字符串压栈，方便后续拼接 stack_res.push(res.toString()); multi = 0; // 清空的原因是因为要便于下一次左括号之前的字母的保存 res = new StringBuilder(); &#125; else if(c == ']') &#123; // 用来 拼接字符串 StringBuilder tmp = new StringBuilder(); int cur_multi = stack_multi.pop(); for(int i = 0; i &lt; cur_multi; i++) tmp.append(res); res = new StringBuilder(stack_res.pop() + tmp); &#125; // 记得处理 multi 可能是多位数的情况！！！ else if(c &gt;= '0' &amp;&amp; c &lt;= '9') multi = multi * 10 + Integer.parseInt(c + ""); else res.append(c); &#125; return res.toString();&#125; 递归 12345678910111213141516171819202122232425262728293031323334class Solution &#123; public String decodeString(String s) &#123; return dfs(s, 0)[0]; &#125; /** * 递归三部曲 * 1、递归出口，当遍历完了最后一个字符，递归结束 * 2、递归返回值，返回括号内的字符串，并且为了方便在一轮递归中进行拼接，也将右括号的索引返回 * 3、一轮递归做的事情，就是将返回的字符串进行扩展，然后拼接前者和后者的字符串 * @param s * @param i * @return */ private String[] dfs(String s, int i) &#123; StringBuilder res = new StringBuilder(); int multi = 0; while (i &lt; s.length()) &#123; if (s.charAt(i) &gt;= '0' &amp;&amp; s.charAt(i) &lt;= '9') multi = multi * 10 + Integer.parseInt(String.valueOf(s.charAt(i))); else if (s.charAt(i) == '[') &#123; String[] tmp = dfs(s, i + 1); i = Integer.parseInt(tmp[0]); while (multi &gt; 0) &#123; res.append(tmp[1]); multi--; &#125; &#125; else if (s.charAt(i) == ']') return new String[]&#123;String.valueOf(i), res.toString()&#125;; else res.append(s.charAt(i)); i++; &#125; return new String[]&#123;res.toString()&#125;; &#125;&#125; 448. 找到所有数组中消失的数字题目给定一个范围在 1 ≤ a[i] ≤ n ( n = 数组大小 ) 的 整型数组，数组中的元素一些出现了两次，另一些只出现一次。 找到所有在 [1, n] 范围之间没有出现在数组中的数字。 您能在不使用额外空间且时间复杂度为O(n)的情况下完成这个任务吗? 你可以假定返回的数组不算在额外空间内。 示例: 12345输入:[4,3,2,7,8,2,3,1]输出:[5,6] 思路 https://leetcode-cn.com/problems/find-all-numbers-disappeared-in-an-array/solution/ti-jie-bu-shi-yong-e-wai-kong-jian-by-gehui1007/ 就是用正负号去维护一个简易的 map，标记是否出现该数。题目限制是数是在 1 ~ n 之间的，而一共有 n 个数，我们这么想，用数组元素的值代表 数组中出现的数的索引，比如说[4,3,2,7,8,2,3,1]，第一个数是 4，就代表了第 4个数，也就是 7，将其标志位 -1，以此类推，通过这种方式数组变为 [-4,-3,-2,-7,8,2,-3,-1]，故再遍历一遍，为正数的即未出现的数字。 用这种思路还可以解决 442. 数组中重复的数据 ，思路一模一样，当遍历时发现元素（代表数组下标）所指向的数是负数，说明那个数已经出现了一次，此时此数就出现了两次，记录下来即可。 代码1234567891011121314class Solution &#123; public List&lt;Integer&gt; findDisappearedNumbers(int[] nums) &#123; List&lt;Integer&gt; res = new ArrayList(); for(int num:nums)&#123; num = Math.abs(num); if(nums[num-1] &lt; 0) continue; else nums[num-1] = -nums[num-1]; &#125; for(int i =0;i &lt; nums.length;i++)&#123; if(nums[i] &gt; 0) res.add(i+1); &#125; return res; &#125;&#125; 442题代码 123456789public List&lt;Integer&gt; findDuplicates(int[] nums) &#123; List&lt;Integer&gt; res = new ArrayList(); for(int num:nums)&#123; num = Math.abs(num); if(nums[num-1] &lt; 0) res.add(num); else nums[num-1] = -nums[num-1]; &#125; return res;&#125; 399. 除法求值【图论知识，暂时不太会，从 leetcode英文版的 高分区 溜了几个高赞答案过来，以后欣赏一下】 题目给出方程式 A / B = k, 其中 A 和 B 均为代表字符串的变量， k 是一个浮点型数字。根据已知方程式求解问题，并返回计算结果。如果结果不存在，则返回 -1.0。 示例 : 123给定 a / b = 2.0, b / c = 3.0问题: a / c = ?, b / a = ?, a / e = ?, a / a = ?, x / x = ? 返回 [6.0, 0.5, -1.0, 1.0, -1.0 ] 123给定 a / b = 2.0, b / c = 3.0问题: a / c = ?, b / a = ?, a / e = ?, a / a = ?, x / x = ? 返回 [6.0, 0.5, -1.0, 1.0, -1.0 ] 思路 dfs 并查集 代码 dfs 第一个版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123; public double[] calcEquation(List&lt;List&lt;String&gt;&gt; equations, double[] values, List&lt;List&lt;String&gt;&gt; queries) &#123; List&lt;String[]&gt; arr = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:equations)&#123; List&lt;String&gt; tmp; tmp = a; arr.add(tmp.toArray(new String[0])); &#125; String[][] eq = arr.toArray(new String[0][0]); List&lt;String[]&gt; arr1 = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:queries)&#123; List&lt;String&gt; tmp; tmp = a; arr1.add(tmp.toArray(new String[0])); &#125; String[][] q = arr1.toArray(new String[0][0]); return calcEquation_dfs(eq,values,q); &#125; public double[] calcEquation_dfs(String[][] eq, double[] vals, String[][] q) &#123; Map&lt;String, Map&lt;String, Double&gt;&gt; m = new HashMap&lt;&gt;(); for (int i = 0; i &lt; vals.length; i++) &#123; m.putIfAbsent(eq[i][0], new HashMap&lt;&gt;()); m.putIfAbsent(eq[i][1], new HashMap&lt;&gt;()); m.get(eq[i][0]).put(eq[i][1], vals[i]); m.get(eq[i][1]).put(eq[i][0], 1 / vals[i]); &#125; double[] r = new double[q.length]; for (int i = 0; i &lt; q.length; i++) r[i] = dfs(q[i][0], q[i][1], 1, m, new HashSet&lt;&gt;()); return r; &#125; double dfs(String s, String t, double r, Map&lt;String, Map&lt;String, Double&gt;&gt; m, Set&lt;String&gt; seen) &#123; if (!m.containsKey(s) || !seen.add(s)) return -1; if (s.equals(t)) return r; Map&lt;String, Double&gt; next = m.get(s); for (String c : next.keySet()) &#123; double result = dfs(c, t, r * next.get(c), m, seen); if (result != -1) return result; &#125; return -1; &#125;&#125; dfs 第二个版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123; public double[] calcEquation(List&lt;List&lt;String&gt;&gt; equations, double[] values, List&lt;List&lt;String&gt;&gt; queries) &#123; List&lt;String[]&gt; arr = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:equations)&#123; List&lt;String&gt; tmp; tmp = a; arr.add(tmp.toArray(new String[0])); &#125; String[][] eq = arr.toArray(new String[0][0]); List&lt;String[]&gt; arr1 = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:queries)&#123; List&lt;String&gt; tmp; tmp = a; arr1.add(tmp.toArray(new String[0])); &#125; String[][] q = arr1.toArray(new String[0][0]); return calcEquation_dfs(eq,values,q); &#125; HashSet&lt;String&gt; seen = new HashSet&lt;&gt;(); HashMap&lt;String, String&gt; root = new HashMap&lt;&gt;(); HashMap&lt;String, Double&gt; vals = new HashMap&lt;&gt;(); HashMap&lt;String, HashMap&lt;String, Double&gt;&gt; edges = new HashMap&lt;&gt;(); public double[] calcEquation_dfs(String[][] equations, double[] values, String[][] queries) &#123; int n = equations.length, m = queries.length; for (int i = 0; i &lt; n; ++i) &#123; String x = equations[i][0], y = equations[i][1]; if (!edges.containsKey(x)) edges.put(x, new HashMap&lt;String, Double&gt;()); if (!edges.containsKey(y) ) edges.put(y, new HashMap&lt;String, Double&gt;()); edges.get(x).put(y, values[i]); edges.get(y).put(x, 1 / values[i]); &#125; for (String x : edges.keySet()) &#123; if (!seen.contains(x)) dfs(x, x, 1); &#125; double[] res = new double[m]; for (int i = 0; i &lt; m; ++i) &#123; String x = queries[i][0], y = queries[i][1]; String px = root.getOrDefault(x, x), py = root.getOrDefault(y, y); if (px != py) res[i] = -1.0; else res[i] = vals.get(x) / vals.get(y); &#125; return res; &#125; public void dfs(String x, String p, double v) &#123; vals.put(x, v); root.put(x, p); seen.add(x); for (String y : edges.get(x).keySet()) &#123; if (!seen.contains(y)) dfs(y, p, v * edges.get(y).get(x)); &#125; &#125;&#125; Dfs 第三个版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class Solution &#123; public double[] calcEquation(List&lt;List&lt;String&gt;&gt; equations, double[] values, List&lt;List&lt;String&gt;&gt; queries) &#123; List&lt;String[]&gt; arr = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:equations)&#123; List&lt;String&gt; tmp; tmp = a; arr.add(tmp.toArray(new String[0])); &#125; String[][] eq = arr.toArray(new String[0][0]); List&lt;String[]&gt; arr1 = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:queries)&#123; List&lt;String&gt; tmp; tmp = a; arr1.add(tmp.toArray(new String[0])); &#125; String[][] q = arr1.toArray(new String[0][0]); return calcEquation_dfs(eq,values,q); &#125; public double[] calcEquation_dfs(String[][] equations, double[] values, String[][] queries) &#123; /* Build graph. */ Map&lt;String, Map&lt;String, Double&gt;&gt; graph = buildGraph(equations, values); double[] result = new double[queries.length]; for (int i = 0; i &lt; queries.length; i++) &#123; result[i] = getPathWeight(queries[i][0], queries[i][1], new HashSet&lt;&gt;(), graph); &#125; return result; &#125; private double getPathWeight(String start, String end, Set&lt;String&gt; visited, Map&lt;String, Map&lt;String, Double&gt;&gt; graph) &#123; /* Rejection case. */ if (!graph.containsKey(start)) return -1.0; /* Accepting case. */ if (graph.get(start).containsKey(end)) return graph.get(start).get(end); visited.add(start); for (Map.Entry&lt;String, Double&gt; neighbour : graph.get(start).entrySet()) &#123; if (!visited.contains(neighbour.getKey())) &#123; double productWeight = getPathWeight(neighbour.getKey(), end, visited, graph); if (productWeight != -1.0) return neighbour.getValue() * productWeight; &#125; &#125; return -1.0; &#125; private Map&lt;String, Map&lt;String, Double&gt;&gt; buildGraph(String[][] equations, double[] values) &#123; Map&lt;String, Map&lt;String, Double&gt;&gt; graph = new HashMap&lt;&gt;(); String u, v; for (int i = 0; i &lt; equations.length; i++) &#123; u = equations[i][0]; v = equations[i][1]; graph.putIfAbsent(u, new HashMap&lt;&gt;()); graph.get(u).put(v, values[i]); graph.putIfAbsent(v, new HashMap&lt;&gt;()); graph.get(v).put(u, 1 / values[i]); &#125; return graph; &#125;&#125; 并查集 I 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677class Solution &#123; public double[] calcEquation(List&lt;List&lt;String&gt;&gt; equations, double[] values, List&lt;List&lt;String&gt;&gt; queries) &#123; List&lt;String[]&gt; arr = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:equations)&#123; List&lt;String&gt; tmp; tmp = a; arr.add(tmp.toArray(new String[0])); &#125; String[][] eq = arr.toArray(new String[0][0]); List&lt;String[]&gt; arr1 = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:queries)&#123; List&lt;String&gt; tmp; tmp = a; arr1.add(tmp.toArray(new String[0])); &#125; String[][] q = arr1.toArray(new String[0][0]); return calcEquation_union(eq,values,q); &#125; /** 1. Thoughts - check if we have enough info to get the result - if yes, calculate; if not, return -1.0 - Method: union find - a/b = 2.0 --&gt; b is the root of a; the distance from a to b is 1/2.0 - if two nums have the same root, we can get the result; a/b=2.0, b/c=3.0 index a b c root b c c dist 2 3 1 - if we want to know a/c = ?: a = 2 * b = 2 * 3 * c =&gt; a/c = 6.0 2. Corner case - if any input is null, return null - no enough info, return -1.0 3. Steps - go through equations to union elements with the same root and update root map and distance map - go through each query: check if has the same root; find relative dist*/ public double[] calcEquation_union(String[][] e, double[] values, String[][] q) &#123; double[] res = new double[q.length]; Map&lt;String, String&gt; root = new HashMap&lt;&gt;(); Map&lt;String, Double&gt; dist = new HashMap&lt;&gt;(); for (int i = 0; i &lt; e.length; i++) &#123; String r1 = find(root, dist, e[i][0]); String r2 = find(root, dist, e[i][1]); root.put(r1, r2); dist.put(r1, dist.get(e[i][1]) * values[i] / dist.get(e[i][0])); &#125; for (int i = 0; i &lt; q.length; i++) &#123; if (!root.containsKey(q[i][0]) || !root.containsKey(q[i][1])) &#123; res[i] = -1.0; continue; &#125; String r1 = find(root, dist, q[i][0]); String r2 = find(root, dist, q[i][1]); if (!r1.equals(r2)) &#123; res[i] = -1.0; continue; &#125; res[i] = (double) dist.get(q[i][0]) / dist.get(q[i][1]); &#125; return res; &#125; private String find(Map&lt;String, String&gt; root, Map&lt;String, Double&gt; dist, String s) &#123; if (!root.containsKey(s)) &#123; root.put(s, s); dist.put(s, 1.0); return s; &#125; if (root.get(s).equals(s)) return s; String lastP = root.get(s); String p = find(root, dist, lastP); root.put(s, p); dist.put(s, dist.get(s) * dist.get(lastP)); return p; &#125;&#125; 并查集 ——&gt; 大佬的并查集和DFS 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solution &#123; public double[] calcEquation(List&lt;List&lt;String&gt;&gt; equations, double[] values, List&lt;List&lt;String&gt;&gt; queries) &#123; List&lt;String[]&gt; arr = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:equations)&#123; List&lt;String&gt; tmp; tmp = a; arr.add(tmp.toArray(new String[0])); &#125; String[][] eq = arr.toArray(new String[0][0]); List&lt;String[]&gt; arr1 = new ArrayList&lt;&gt;(); for(List&lt;String&gt; a:queries)&#123; List&lt;String&gt; tmp; tmp = a; arr1.add(tmp.toArray(new String[0])); &#125; String[][] q = arr1.toArray(new String[0][0]); return calcEquation_union(eq,values,q); &#125; Map&lt;String, String&gt;parents = new HashMap&lt;&gt;(); Map&lt;String, Double&gt;vals = new HashMap&lt;&gt;(); public double[] calcEquation_union(String[][] equs, double[] values, String[][] queries) &#123; double[] res = new double[queries.length]; for (int i = 0; i &lt; values.length ; ++i ) union(equs[i][0], equs[i][1], values[i]); for (int i = 0; i &lt; queries.length; ++i) &#123; String x = queries[i][0], y = queries[i][1]; res[i] = (parents.containsKey(x) &amp;&amp; parents.containsKey(y) &amp;&amp; find(x) == find(y)) ? vals.get(x) / vals.get(y) : -1.0; &#125; return res; &#125; public void add(String x) &#123; if (parents.containsKey(x)) return; parents.put(x, x); vals.put(x, 1.0); &#125; public String find(String x) &#123; String p = parents.getOrDefault(x, x); if (x != p) &#123; String pp = find(p); vals.put(x, vals.get(x) * vals.get(p)); parents.put(x, pp); &#125; return parents.getOrDefault(x, x); &#125; public void union(String x, String y, double v) &#123; add(x); add(y); String px = find(x), py = find(y); parents.put(px, py); vals.put(px, v * vals.get(y) / vals.get(x)); &#125;&#125; 406. 根据身高重建队列题目假设有打乱顺序的一群人站成一个队列。 每个人由一个整数对(h, k)表示，其中h是这个人的身高，k是排在这个人前面且身高大于或等于h的人数。 编写一个算法来重建这个队列。 示例 12345输入:[[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]]输出:[[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]] 思路官方题解讲的不错，高个子的眼中，矮个子相当于不存在，所以高个子先按规矩站好，后面矮个子的插入不会影响其顺序，所以我们先把高个子安排完，矮个子可以直接按 k 值从小到大插入，k 值即他们当时插入的下标值。 代码12345678910111213class Solution &#123; public int[][] reconstructQueue(int[][] people) &#123; // 前面身高应该降序，后面的k应该升序排列 Arrays.sort(people, (o1, o2) -&gt; o1[0] == o2[0] ? o1[1] - o2[1] : o2[0] - o1[0]); ArrayList&lt;int[]&gt; res = new ArrayList(); for(int[] p:people)&#123; // 高个子无视矮个子，所以矮个子的插入对之前插入的高个子没有影响 // 故矮个子可以直接按 k 当成序号插入 res.add(p[1],p); &#125; return res.toArray(new int[0][0]); &#125;&#125; 416. 分割等和子集题目给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 注意: 每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1: 12345输入: [1, 5, 11, 5]输出: true解释: 数组可以分割成 [1, 5, 5] 和 [11]. 示例 2: 12345输入: [1, 2, 3, 5]输出: false解释: 数组不能分割成两个元素和相等的子集. 思路见 liwei大佬思路，这个优化的套路，值得学习。 https://leetcode-cn.com/problems/partition-equal-subset-sum/solution/0-1-bei-bao-wen-ti-xiang-jie-zhen-dui-ben-ti-de-yo/ 代码1234567891011121314151617181920212223242526272829public class Solution &#123; public boolean canPartition(int[] nums) &#123; int len = nums.length; int sum = 0; for (int num : nums) &#123; sum += num; &#125; if ((sum &amp; 1) == 1) &#123; return false; &#125; int target = sum / 2; boolean[] dp = new boolean[target + 1]; dp[0] = true; if (nums[0] &lt;= target) &#123; dp[nums[0]] = true; &#125; for (int i = 1; i &lt;= len; i++) &#123; for (int j = target; j &gt;= nums[i]; j--) &#123; if (dp[target]) &#123; return true; &#125; dp[j] = dp[j] || dp[j - nums[i]]; &#125; &#125; return dp[target]; &#125;&#125; 438. 找到字符串中所有字母异位词题目给定一个字符串 s 和一个非空字符串 p，找到 s 中所有是 p 的字母异位词的子串，返回这些子串的起始索引。 字符串只包含小写英文字母，并且字符串 s 和 p 的长度都不超过 20100 示例 1: 123456789输入:s: "cbaebabacd" p: "abc"输出:[0, 6]解释:起始索引等于 0 的子串是 "cba", 它是 "abc" 的字母异位词。起始索引等于 6 的子串是 "bac", 它是 "abc" 的字母异位词。 示例 2: 12345678910输入:s: "abab" p: "ab"输出:[0, 1, 2]解释:起始索引等于 0 的子串是 "ab", 它是 "ab" 的字母异位词。起始索引等于 1 的子串是 "ba", 它是 "ab" 的字母异位词。起始索引等于 2 的子串是 "ab", 它是 "ab" 的字母异位词。 思路滑动窗口算法，见 另外一篇文章：滑动窗口技巧总结（假装有链接） 代码12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public List&lt;Integer&gt; findAnagrams(String s, String p) &#123; // 用于返回字母异位词的起始索引 List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); // 用 map 存储目标值中各个单词出现的次数 HashMap&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); for (Character c : p.toCharArray()) map.put(c, map.getOrDefault(c, 0)+1); // 用另外一个 map 存储滑动窗口中有效字符出现的次数 HashMap&lt;Character, Integer&gt; window = new HashMap&lt;&gt;(); int left = 0; // 左指针 int right = 0; // 右指针 int valid = p.length(); // 只有当 valid == 0 时，才说明 window 中包含了目标子串 while (right &lt; s.length()) &#123; // 如果目标子串中包含了该字符，才存入 window 中 if (map.containsKey(s.charAt(right))) &#123; window.put(s.charAt(right), window.getOrDefault(s.charAt(right), 0)+1); // 只有当 window 中该有效字符数量不大于map中该字符数量，才能算一次有效包含 if (window.get(s.charAt(right)) &lt;= map.get(s.charAt(right))) &#123; valid--; &#125; &#125; // 如果 window 符合要求，即两个 map 存储的有效字符相同，就可以移动左指针了 // 但是只有二个map存储的数据完全相同，才可以记录当前的起始索引，也就是left指针所在位置 while (valid == 0) &#123; if (right - left + 1 == p.length()) res.add(left); // 如果左指针指的是有效字符,需要更改 window 中的 key 对应的 value // 如果 有效字符对应的数量比目标子串少，说明无法匹配了 if (map.containsKey(s.charAt(left))) &#123; window.put(s.charAt(left), window.get(s.charAt(left))-1); if (window.get(s.charAt(left)) &lt; map.get(s.charAt(left))) &#123; valid++; &#125; &#125; left++; &#125; right++; &#125; return res; &#125;&#125; 用数组代替 map 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public List&lt;Integer&gt; findAnagrams(String s, String p) &#123; if(s == null || s.length() == 0) return new ArrayList&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); int[] needs = new int[26]; //由于都是小写字母，因此直接用26个长度的数组代替原来的HashMap int[] window = new int[26]; int left = 0, right = 0, total = p.length(); //用total检测窗口中是否已经涵盖了p中的字符 for(char ch : p.toCharArray())&#123; needs[ch - 'a'] ++; &#125; while(right &lt; s.length())&#123; char chr = s.charAt(right); if(needs[chr - 'a'] &gt; 0)&#123; window[chr - 'a'] ++; if(window[chr - 'a'] &lt;= needs[chr - 'a'])&#123; total --; &#125; &#125; while(total == 0)&#123; if(right-left+1 == p.length())&#123; res.add(left); &#125; char chl = s.charAt(left); if(needs[chl - 'a'] &gt; 0)&#123; window[chl - 'a'] --; if(window[chl - 'a'] &lt; needs[chl - 'a'])&#123; total ++; &#125; &#125; left ++; &#125; right ++; &#125; return res; &#125;&#125; 494. 目标和题目给定一个非负整数数组，a1, a2, …, an, 和一个目标数，S。现在你有两个符号 + 和 -。对于数组中的任意一个整数，你都可以从 + 或 -中选择一个符号添加在前面。 返回可以使最终数组和为目标数 S 的所有添加符号的方法数。 示例 1: 1234567891011输入: nums: [1, 1, 1, 1, 1], S: 3输出: 5解释: -1+1+1+1+1 = 3+1-1+1+1+1 = 3+1+1-1+1+1 = 3+1+1+1-1+1 = 3+1+1+1+1-1 = 3一共有5种方法让最终目标和为3。 注意: 数组非空，且长度不会超过20。 初始的数组的和不会超过1000。 保证返回的最终结果能被32位整数存下。 思路 暴力法 ，见官方题解 https://leetcode-cn.com/problems/target-sum/solution/mu-biao-he-by-leetcode/ Dp，我做的时候第一反应是用的 dp，但是这里有个困难，dp不能用二维数组表示，因为可能越界，下标为负数，所以可以用 hashmap，dp[i][j] 转化为 i 当 key，j 当 value。 转化为 0-1 背包问题。 https://leetcode-cn.com/problems/target-sum/solution/python-dfs-xiang-jie-by-jimmy00745/ https://leetcode-cn.com/problems/target-sum/solution/dong-tai-gui-hua-ji-bai-liao-98de-javayong-hu-by-r/ 代码123456789101112131415161718public class Solution &#123; public int findTargetSumWays(int[] nums, int S) &#123; int[] dp = new int[2001]; dp[nums[0] + 1000] = 1; dp[-nums[0] + 1000] += 1; for (int i = 1; i &lt; nums.length; i++) &#123; int[] next = new int[2001]; for (int sum = -1000; sum &lt;= 1000; sum++) &#123; if (dp[sum + 1000] &gt; 0) &#123; next[sum + nums[i] + 1000] += dp[sum + 1000]; next[sum - nums[i] + 1000] += dp[sum + 1000]; &#125; &#125; dp = next; &#125; return S &gt; 1000 ? 0 : dp[S + 1000]; &#125;&#125; 215. 数组中的第K个最大元素题目无序数组第K大的数 示例 1 12输入: [3,2,1,5,6,4] 和 k = 2输出: 5 思路 快排 大顶堆 小顶堆 优先级队列实现 == 小顶堆实现 代码 快排 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 快速选择的思想 */public int findKthLargest(int[] nums, int k) &#123; int left = 0, right = nums.length - 1; while (true)&#123; // 第一次做 partition 得到的下标位置 int index = partition(nums, left, right); if (index == k - 1) &#123; return nums[index]; &#125; else if (index &lt; k - 1) &#123; left = index + 1; &#125; else &#123; right = index - 1; &#125; &#125;&#125;private int partition(int[] nums, int left, int right) &#123; int pivot = nums[right]; int pivotIndex = right; while (left &lt; right) &#123; while (left &lt; right &amp;&amp; nums[left] &gt;= pivot) &#123; left++; &#125; while (left &lt; right &amp;&amp; nums[right] &lt;= pivot) &#123; right--; &#125; // 交换 pivot 和 swap(nums, left, right); &#125; nums[pivotIndex] = nums[left]; nums[left] = pivot; return left;&#125;public void swap(int[] nums, int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp;&#125;作者：kelly2018链接：https://leetcode-cn.com/problems/kth-largest-element-in-an-array/solution/java-liang-chong-fang-fa-xiao-ding-dui-kuai-su-xua/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 大根堆 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; int heapSize = nums.length; buildMaxHeap(nums, heapSize); for (int i = nums.length - 1; i &gt;= nums.length - k + 1; --i) &#123; swap(nums, 0, i); --heapSize; maxHeapify(nums, 0, heapSize); &#125; return nums[0]; &#125; public void buildMaxHeap(int[] a, int heapSize) &#123; for (int i = heapSize / 2; i &gt;= 0; --i) &#123; maxHeapify(a, i, heapSize); &#125; &#125; public void maxHeapify(int[] a, int i, int heapSize) &#123; int l = i * 2 + 1, r = i * 2 + 2, largest = i; if (l &lt; heapSize &amp;&amp; a[l] &gt; a[largest]) &#123; largest = l; &#125; if (r &lt; heapSize &amp;&amp; a[r] &gt; a[largest]) &#123; largest = r; &#125; if (largest != i) &#123; swap(a, i, largest); maxHeapify(a, largest, heapSize); &#125; &#125; public void swap(int[] a, int i, int j) &#123; int temp = a[i]; a[i] = a[j]; a[j] = temp; &#125;&#125;作者：LeetCode-Solution链接：https://leetcode-cn.com/problems/kth-largest-element-in-an-array/solution/shu-zu-zhong-de-di-kge-zui-da-yuan-su-by-leetcode-/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 小根堆 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; // 先利用前 k 个节点构建一个堆, nums[0] 是小顶堆的堆顶元素 buildHeap(nums, k); for (int i = k; i &lt; nums.length; i++) &#123; if (nums[i] &lt; nums[0]) &#123; continue; &#125; swap(nums,i,0); heapify(nums, k, 0); &#125; return nums[0]; &#125; private void buildHeap(int[] nums, int k) &#123; for (int i = k / 2 - 1; i &gt;= 0 ; i--) &#123; heapify(nums, k, i); &#125; &#125; private void heapify(int[] nums, int k, int i) &#123; // 最小值的索引 int minPos = i; while (true) &#123; int left = i * 2 + 1; int right = i * 2 + 2; // 左子树比根小，将最小值的位置记录为左子树 if (left &lt; k &amp;&amp; nums[left] &lt; nums[minPos]) &#123; minPos = left; &#125; if (right &lt; k &amp;&amp; nums[right] &lt; nums[minPos]) &#123; minPos = right; &#125; if (minPos == i) &#123; break; &#125; // 交换最小值和根节点，继续进行调整 swap(nums, minPos, i); i = minPos; &#125; &#125; public void swap(int[] nums, int i, int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; &#125;&#125;作者：kelly2018链接：https://leetcode-cn.com/problems/kth-largest-element-in-an-array/solution/java-liang-chong-fang-fa-xiao-ding-dui-kuai-su-xua/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 优先级队列 1234567891011121314151617181920public int findKthLargest(int[] nums, int k) &#123; // 默认小顶堆 PriorityQueue&lt;Integer&gt; heap = new PriorityQueue&lt;Integer&gt;(k); for (int num : nums) &#123; if (heap.size() &lt; k)&#123; heap.offer(num); &#125; else &#123; if (num &gt; heap.peek())&#123; heap.poll(); heap.offer(num); &#125; &#125; &#125; return heap.peek();&#125;作者：kelly2018链接：https://leetcode-cn.com/problems/kth-largest-element-in-an-array/solution/java-liang-chong-fang-fa-xiao-ding-dui-kuai-su-xua/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 求根号n思路二分 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package 练手算法;import java.text.DecimalFormat;class Main &#123; public static double sqrt(double num) &#123; if (num &lt; 0) &#123; return -1; &#125; double low = 0; double high = num / 2; double precision = 0.000001; //格式化，保证输出位数 DecimalFormat df = new DecimalFormat("#.00"); double res = high; while (Math.abs(num - (res * res)) &gt; precision) &#123; if (high * high &gt; num) &#123; double n = high - (high - low) / 2; if (n * n &gt; num) &#123; high = n; &#125; else if (n * n &lt; num) &#123; low = n; &#125; else &#123; return Double.valueOf(df.format(n)); &#125; res = n; &#125; else if (high * high &lt; num) &#123; double m = high + (high - low) / 2; if (m * m &gt; num) &#123; low = high; high = m; &#125; else if (m * m &lt; num) &#123; low = high; high = m; &#125; else &#123; return Double.valueOf(df.format(m)); &#125; res = m; &#125; else &#123; return Double.valueOf(df.format(high)); &#125; &#125; return Double.valueOf(df.format(res)); &#125; public static void main(String[] args) &#123; System.out.println(sqrt(16)); &#125;&#125;public class SqrtN &#123; public static void main(String[] args) &#123; &#125;&#125; 343. 整数拆分 = 剪绳子题目给定一个正整数 n，将其拆分为至少两个正整数的和，并使这些整数的乘积最大化。 返回你可以获得的最大乘积。 示例 : 123输入: 10输出: 36解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36 思路第一想法就是 dp，dp[i] 为整数 i，将 i 拆分成至少两个整数的和，其整数的乘积的最大值。 则 dp[i] = Math.max(dp[i],Math.max(dp[i-j],i-j) * j) 代码1234567891011121314class Solution &#123; public int integerBreak(int n) &#123; // dp[i] 表示 i 被拆分成至少两个正整数的和 // dp[0] = dp[1] = 0 // dp[i] = j * Math.max(dp[i-j],i-j) int[] dp = new int[n+1]; for(int i = 2;i &lt;= n;i++)&#123; for(int j = 1;j &lt; i;j++)&#123; dp[i] = Math.max(j * Math.max(dp[i-j],i-j),dp[i]); &#125; &#125; return dp[n]; &#125;&#125; 440. 字典序的第K小数字题目给定整数 n 和 k，找到 1 到 n 中字典序第 k 小的数字。 注意：1 ≤ k ≤ n ≤ 109。 示例 : 12345678输入:n: 13 k: 2输出:10解释:字典序的排列是 [1, 10, 11, 12, 13, 2, 3, 4, 5, 6, 7, 8, 9]，所以第二小的数字是 10。 思路前缀树，所以思路就是： 确定指定前缀下所有子节点数； 若 k 属于当前前缀下，则去子树里面看； 第 k 个数不属于当前前缀下，就扩大前缀，往后看。 代码12345678910111213141516171819202122232425262728293031323334353637383940package 二刷LeetCode和剑指offer.链表.三刷链表;public class findKthNumber_440 &#123; public static void main(String[] args) &#123; findKthNumber_440 ff = new findKthNumber_440(); ff.findKthNumber(13,2); &#125; public int findKthNumber(int n, int k) &#123; int curr = 1; // 此时第一个确定节点是 1，此时我们需要去除第一个节点 1，当 k 为 0 时，就说明所有结点都找到了，此时需要的值就是 curr----当前确定的需要节点 k = k - 1; while (k &gt; 0) &#123; //计算前缀之前的step数 int steps = getSteps(n, curr, curr + 1); //前缀间距太大，需要深入一层 if (steps &gt; k) &#123; curr *= 10; //多了一个确定节点，继续-1 k -= 1; &#125; //间距太小，需要扩大前缀范围 else &#123; curr += 1; k -= steps; &#125; &#125; return curr; &#125; private int getSteps(int n, long curr, long next) &#123; int steps = 0; while (curr &lt;= n) &#123; steps += Math.min(n + 1, next) - curr; curr *= 10; next *= 10; &#125; return steps; &#125;&#125; 114. 二叉树展开为链表题目给定一个二叉树，原地将它展开为一个单链表。 例如，给定二叉树： 12345 1 / \ 2 5 / \ \3 4 6 将其展开为： 12345678910111 \ 2 \ 3 \ 4 \ 5 \ 6 思路其实就是先把根节点的右子树放置到根节点左边，然后将左子树放到右子树的位置，将左子树置为空。 代码1234567891011121314151617181920public void flatten(TreeNode root) &#123; while (root != null) &#123; //左子树为 null，直接考虑下一个节点 if (root.left == null) &#123; root = root.right; &#125; else &#123; // 找左子树最右边的节点 TreeNode pre = root.left; while (pre.right != null) &#123; pre = pre.right; &#125; //将原来的右子树接到左子树的最右边节点 pre.right = root.right; // 将左子树插入到右子树的地方 root.right = root.left; root.left = null; // 考虑下一个节点 root = root.right; &#125; &#125; 752. 打开转盘锁题目你有一个带有四个圆形拨轮的转盘锁。每个拨轮都有10个数字： &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39; 。每个拨轮可以自由旋转：例如把 &#39;9&#39; 变为 &#39;0&#39;，&#39;0&#39; 变为 &#39;9&#39; 。每次旋转都只能旋转一个拨轮的一位数字。 锁的初始数字为 &#39;0000&#39; ，一个代表四个拨轮的数字的字符串。 列表 deadends 包含了一组死亡数字，一旦拨轮的数字和列表里的任何一个元素相同，这个锁将会被永久锁定，无法再被旋转。 字符串 target 代表可以解锁的数字，你需要给出最小的旋转次数，如果无论如何不能解锁，返回 -1。 示例 1: 123456输入：deadends = ["0201","0101","0102","1212","2002"], target = "0202"输出：6解释：可能的移动序列为 "0000" -&gt; "1000" -&gt; "1100" -&gt; "1200" -&gt; "1201" -&gt; "1202" -&gt; "0202"。注意 "0000" -&gt; "0001" -&gt; "0002" -&gt; "0102" -&gt; "0202" 这样的序列是不能解锁的，因为当拨动到 "0102" 时这个锁就会被锁定。 示例 2: 1234输入: deadends = ["8888"], target = "0009"输出：1解释：把最后一位反向旋转一次即可 "0000" -&gt; "0009"。 示例 3: 1234输入: deadends = ["8887","8889","8878","8898","8788","8988","7888","9888"], target = "8888"输出：-1解释：无法旋转到目标数字且不被锁定。 示例 4: 12输入: deadends = [&quot;0000&quot;], target = &quot;8888&quot;输出：-1 思路使用 BFS，但是要注意几点： 会走回头路。比如说我们从 &quot;0000&quot; 拨到 &quot;1000&quot;，但是等从队列拿出 &quot;1000&quot; 时，还会拨出一个 &quot;0000&quot;，这样的话会产生死循环； 注意终止条件； 要对 deaddends 进行处理； 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123; public int openLock(String[] deadends, String target) &#123; // 存储当前需要广度搜索的节点 LinkedList&lt;String&gt; queue = new LinkedList(); // 存储当前遍历过的组合，防止走回头路,可以直接将死锁的数字组合放到已visited Set&lt;String&gt; visited = new HashSet(); // 存储死锁数字组合 // Set&lt;String&gt; deadSet = new HashSet(); for(String dead : deadends) visited.add(dead); if(visited.contains("0000")) return -1; queue.offer("0000"); visited.add("0000"); int count = 0; while(!queue.isEmpty())&#123; // 别忘了扩散节点 int sz = queue.size(); for(int i = 0;i &lt; sz;i++)&#123; String cur = queue.poll(); // 判断是否到达终点 // if(deadSet.contains(cur)) continue; if(target.equals(cur)) return count; for(int j = 0;j &lt; 4;j++)&#123; String up = plusOne(cur,j); if(!visited.contains(up))&#123; queue.offer(up); visited.add(up); &#125; String down = minusOne(cur,j); if(!visited.contains(down))&#123; queue.offer(down); visited.add(down); &#125; &#125; &#125; count++; &#125; return -1; &#125; // 将 s[j] 向上拨动一次 public String plusOne(String s, int j) &#123; char[] ch = s.toCharArray(); if (ch[j] == '9') ch[j] = '0'; else ch[j] += 1; return new String(ch); &#125; // 将 s[i] 向下拨动一次 public String minusOne(String s, int j) &#123; char[] ch = s.toCharArray(); if (ch[j] == '0') ch[j] = '9'; else ch[j] -= 1; return new String(ch); &#125;&#125; 415. 字符串相加题目给定两个字符串形式的非负整数 num1 和num2 ，计算它们的和。 注意 num1 和num2 的长度都小于 5100. num1 和num2 都只包含数字 0-9. num1 和num2 都不包含任何前导零。 你不能使用任何內建 BigInteger 库， 也不能直接将输入的字符串转换为整数形式。 代码12345678910111213class Solution &#123; public String addStrings(String num1, String num2) &#123; StringBuilder sb = new StringBuilder(); int carry = 0, i = num1.length()-1, j = num2.length()-1; while(i &gt;= 0 || j &gt;= 0 || carry != 0)&#123; if(i&gt;=0) carry += num1.charAt(i--)-'0'; if(j&gt;=0) carry += num2.charAt(j--)-'0'; sb.append(carry%10); carry /= 10; &#125; return sb.reverse().toString(); &#125;&#125; 198. 打家劫舍题目你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1： 1234输入：[1,2,3,1]输出：4解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2： 1234输入：[2,7,9,3,1]输出：12解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 思路dp[i] 代表在打劫到第i+1个房子时，偷窃到的最高金额，dp[i] = Math.max(dp[i-1],dp[i-2]+nums[i])。 代码1234567891011121314151617181920class Solution &#123; public int rob(int[] nums) &#123; // dp[i] = Math.min(dp[i-2]+nums[i],dp[i-1]); int[] dp = new int[3]; if(nums.length == 0) return 0; else if(nums.length == 1)&#123; return nums[0]; &#125; else&#123; dp[0] = nums[0]; dp[1] = Math.max(nums[0],nums[1]); for(int i = 2;i &lt; nums.length;i++)&#123; dp[2] = Math.max(dp[0] + nums[i],dp[1]); dp[0] = dp[1]; dp[1] = dp[2]; &#125; &#125; return Math.max(dp[1],dp[2]); &#125;&#125; 213. 打家劫舍 II题目你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都围成一圈，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 1: 123输入: [2,3,2]输出: 3解释: 你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 示例 2: 1234输入: [1,2,3,1]输出: 4解释: 你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 思路其实就两种情况比大小。 代码1234567891011121314151617181920212223242526class Solution &#123; public int rob(int[] nums) &#123; if(nums.length == 0) return 0; if(nums.length == 1) return nums[0]; return Math.max(myRob(Arrays.copyOfRange(nums, 0, nums.length - 1)), myRob(Arrays.copyOfRange(nums, 1, nums.length))); &#125; public int myRob(int[] nums) &#123; // dp[i] = Math.min(dp[i-2]+nums[i],dp[i-1]); int[] dp = new int[3]; if(nums.length == 0) return 0; else if(nums.length == 1)&#123; return nums[0]; &#125; else&#123; dp[0] = nums[0]; dp[1] = Math.max(nums[0],nums[1]); for(int i = 2;i &lt; nums.length;i++)&#123; dp[2] = Math.max(dp[0] + nums[i],dp[1]); dp[0] = dp[1]; dp[1] = dp[2]; &#125; &#125; return Math.max(dp[1],dp[2]); &#125;&#125; 337. 打家劫舍 III##题目在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 示例 1: 12345678910输入: [3,2,3,null,3,null,1] 3 / \ 2 3 \ \ 3 1输出: 7 解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7. 示例 2: 12345678910输入: [3,4,5,1,3,null,1] 3 / \ 4 5 / \ \ 1 3 1输出: 9解释: 小偷一晚能够盗取的最高金额 = 4 + 5 = 9. 思路递归 + dp。小偷只有偷根节点和不偷根节点两种选择。 代码1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */class Solution &#123; public int rob(TreeNode root) &#123; int[] res = robIn(root); return Math.max(res[0],res[1]); &#125; private static int[] robIn(TreeNode root)&#123; //用一个数组记录偷根节点和不偷根节点两种情况，这里很特殊用了dp同时还搭配了递归 int[] res = new int[2]; if(root == null)&#123; return res; &#125; int[] left = robIn(root.left); int[] right = robIn(root.right); //偷根节点 res[0] = root.val + left[1] + right[1]; res[1] = Math.max(left[0],left[1]) + Math.max(right[0],right[1]); return res; &#125;&#125; 时间复杂度：O(n)。上文中已分析。空间复杂度：O(n)。虽然优化过的版本省去了哈希映射的空间，但是栈空间的使用代价依旧是 O(n)O(n)，故空间复杂度不变。 99. 恢复二叉搜索树题目二叉搜索树中的两个节点被错误地交换。 请在不改变其结构的情况下，恢复这棵树。 示例 1: 123456789101112131415输入: [1,3,null,null,2] 1 / 3 \ 2输出: [3,1,null,null,2] 3 / 1 \ 2 示例 2: 123456789101112131415输入: [3,1,4,null,null,2] 3 / \1 4 / 2输出: [2,1,4,null,null,3] 2 / \1 4 / 3 思路BST 有个最为重要的性质就是中序遍历是递增的，所以这题意思就是：在一个数组中有两个数顺序错了，如何找到这两个数，将他们的值互换，所以思路就很清晰了： 如果当前遍历的数比上一个要小，说明前面这个数是有问题的，将该数记录下，然后将当前遍历的数也记录下，如果后面的数都是有序的，则说明就是这两个数顺序有问题，将这两个数互换即可，如果后面还出现了有问题的数，则将 errorTwo 给到这个数，互换两个数即可。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode() &#123;&#125; * TreeNode(int val) &#123; this.val = val; &#125; * TreeNode(int val, TreeNode left, TreeNode right) &#123; * this.val = val; * this.left = left; * this.right = right; * &#125; * &#125; */class Solution &#123; public void recoverTree(TreeNode root) &#123; Stack&lt;TreeNode&gt; stack = new Stack(); TreeNode errorOne = null; TreeNode errorTwo = null; TreeNode cur = root; TreeNode pre = null; while(cur != null || !stack.isEmpty())&#123; while(cur != null)&#123; stack.push(cur); cur = cur.left; &#125; cur = stack.pop(); if(pre != null &amp;&amp; pre.val &gt; cur.val)&#123; if(errorOne == null)&#123; errorOne = pre; errorTwo = cur; &#125; else if(errorOne != null)&#123; errorTwo = cur; &#125; &#125; pre = cur; cur = cur.right; &#125; int temp = errorOne.val; errorOne.val = errorTwo.val; errorTwo.val = temp; return; &#125;&#125; 93. 复原IP地址给定一个只包含数字的字符串，复原它并返回所有可能的 IP 地址格式。 有效的 IP 地址正好由四个整数（每个整数位于 0 到 255 之间组成），整数之间用 ‘.’ 分隔。 示例: 12输入: "25525511135"输出: ["255.255.11.135", "255.255.111.35"]]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Leetcode</tag>
        <tag>题解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo提交文章出现问题]]></title>
    <url>%2F2019%2F10%2F09%2F%E8%A7%A3%E5%86%B3hexo%E6%97%A0%E6%B3%95git%E4%B8%8A%E7%9A%84%E9%97%AE%E9%A2%98.html</url>
    <content type="text"><![CDATA[问题记录hexo发布文章报错记录：Unhandled rejection TypeError: Cannot set property ‘lastIndex’ of undefined发布文章时出现问题，同时报的错还有：xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun 解决措施发现是git出现问题！！！ 控制端输入： 1brew install git 1xcode-select --install 在xcode-select指令安装好之后，再命令检查一下git指令的帮助信息： 1git -h 可以正确输出相应的帮助信息，意味着git已经被修复好了。 原因更新了Catalina导致的，新系统辣鸡哈哈哈哈哈哈]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>问题汇总</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[喜大普奔]]></title>
    <url>%2F2019%2F10%2F09%2F%E5%96%9C%E5%A4%A7%E6%99%AE%E5%A5%94.html</url>
    <content type="text"><![CDATA[开心哇！！！终于写到10w字了！！！继续加油！！]]></content>
      <categories>
        <category>吐槽</category>
      </categories>
      <tags>
        <tag>happy day</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[List源码分析]]></title>
    <url>%2F2019%2F10%2F07%2FList%20%26%20Queue%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html</url>
    <content type="text"><![CDATA[ArrayList 源码分析（转载）不知道各位朋友，还记得开工前制定的学习目标么？ 有没有一直为了那个目标废寝忘食呢？继 搞懂 Java 内部类 后开始探索总结 Java 集合框架源码的知识，希望能给自己夯实基础，也希望能为自己实现目标更近一步。 ArrayList 源码分析思路ArrayList 是我们 App 开发中常用的 Java 集合类，从学习 Java 开始我们基本上就对它天天相见了，但是通过探索ArrayList 源码，我们将会把它从普通朋友变成知根知底的老朋友,本文将从以下几部分开始分析 ArrayList： ArrayList 概述 ArrayList 的构造函数，也就是我们创建一个 ArrayList 的方法 ArrayList 的添加元素的方法， 以及 ArrayList 的扩容机制 ArrayList 的删除元素的常用方法 ArrayList 的 改查常用方法 ArrayList 的 toArray 方法 ArrayList 的遍历方法，以及常见的错误操作即产生错误操作的原因 ArrayList 概述ArrayList的基本特点 ArrayList 底层是一个动态扩容的数组结构 允许存放（不止一个） null 元素 允许存放重复数据，存储顺序按照元素的添加顺序 ArrayList 并不是一个线程安全的集合。如果集合的增删操作需要保证线程的安全性，可以考虑使用 CopyOnWriteArrayList 『使用的是 ReentrantLock 保证同步，但是有弱一致性，因为写数据时是先拷贝再复制回去。』或者使用 collections.synchronizedList(List l) 函数返回一个线程安全的ArrayList类。当然了，没有绝对的线程安全，这里的线程安全只能保证单个方法的线程安全，如果复合操作，比如一个线程不断读取一个数，另外一个线程不断删除一个数，两个方法的复合操作，也会导致线程不安全，这块我在 jvm 的最后一节也有讲。 ArrayList 的继承关系12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 从 ArrayList 的继承关系来看， ArrayList 继承自 AbstractList ，实现了List\, RandomAccess, Cloneable, java.io.Serializable 接口。 其中 AbstractList和 List\ 是规定了 ArrayList 作为一个集合框架必须具备的一些属性和方法，ArrayList 本身覆写了基类和接口的大部分方法，这就包含我们要分析的增删改查操作。 ArrayList 实现 RandomAccess 接口标识着其支持随机快速访问，查看源码可以知道 RandomAccess 其实只是一个标识，标识某个类拥有随机快速访问的能力，针对 ArrayList 而言通过 get(index) 去访问元素可以达到 O(1) 的时间复杂度。有些集合类不拥有这种随机快速访问的能力，比如 LinkedList 就没有实现这个接口。 ArrayList 实现 Cloneable 接口标识着他可以被克隆/复制，其内部实现了 clone 方法供使用者调用来对 ArrayList 进行克隆，但其实现只通过 Arrays.copyOf 完成了对 ArrayList 进行「浅拷贝」，也就是你改变 ArrayList clone后的集合中的元素，源集合中的元素也会改变，对于深浅拷贝我已经单独整理一篇文章来讲述这里不再过多的说。 对于 java.io.Serializable 标识着集合可被被序列化。 我们发现了一些有趣的事情，除了 List\ 以外，ArrayList 实现的接口都是标识接口，标识着这个类具有怎样的特点，看起来更像是一个属性。 ArrayList 的构造方法在说构造方法之前我们要先看下与构造参数有关的几个全局变量： 1234567891011121314151617181920212223/*** ArrayList 默认的数组容量*/ private static final int DEFAULT_CAPACITY = 10;/*** 这是一个共享的空的数组实例，当使用 ArrayList(0) 或者 ArrayList(Collection&lt;? extends E&gt; c) * 并且 c.size() = 0 的时候讲 elementData 数组讲指向这个实例对象。*/ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;/*** 另一个共享空数组实例，第一次 add 元素的时候将使用它来判断数组大小是否设置为 DEFAULT_CAPACITY*/ private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;/*** 真正装载集合元素的底层数组 * 至于 transient 关键字这里简单说一句，被它修饰的成员变量无法被 Serializable 序列化 * 有兴趣的可以去网上查相关资料* https://www.jianshu.com/p/14876ef38721 这个写得好*/ transient Object[] elementData; // non-private to simplify nested class access 其中最需要关注的就是 transient 修饰了 elementData，我们知道 transient 修饰的变量是防止变量被序列化和反序列化，那为何 ArrayList 继承了 Serializable，却将其内部元素变为 transient 呢？因为 elementData 是一个缓存数组，会预留一些容量，等容量不足时再扩充容量，那么有些空间可能就没有实际存储元素，采用上面的方式来实现序列化时，就可以保证只序列化实际存储的那些元素，而不是整个数组，从而节省空间和时间。 还有就是，那 ArrayList 是如何实现序列化的呢？是因为其写了 writeObject() 和 readObject() ，虽然这两个方法都是使用的私有方法，但如何调用的呢？就是通过反射机制得以调用的。反射里面是有 setAccessible() 可以忽略方法前的 private。 具体见： 作者：汪和呆喵链接：https://www.jianshu.com/p/14876ef38721来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 对于上述几个成员变量，我们只是在注释中简单的说明，对于他们具体有什么作用，在下边分析构造方法和扩容机制的时候将会更详细的讲解。 ArrayList 一共三种构造方式，我们先从无参的构造方法来开始： 无参构造方法123456/*** 构造一个初始容量为10的空列表。*/public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 这是我们经常使用的一个构造方法，其内部实现只是将 elementData 指向了我们刚才讲得 DEFAULTCAPACITY_EMPTY_ELEMENTDATA 这个空数组，这个空数组的容量是 0， 但是源码注释却说这是构造一个初始容量为10的空列表。这是为什么？其实在集合调用 add 方法添加元素的时候将会调用 ensureCapacityInternal 方法，在这个方法内部判断了： 123if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);&#125; 可见，如果采用无参数构造方法的时候第一次添加元素肯定走进 if 判断中 minCapacity 将被赋值为 10，所以构造一个初始容量为10的空列表 也就是这个意思。 指定初始容量的构造方法12345678910111213141516/*** 构造一个具有指定初始容量的空列表。* @param 初始容量 * @throws 如果参数小于 0 将会抛出 IllegalArgumentException 参数不合法异常 */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125; 如果我们预先知道一个集合元素的容纳的个数的时候推荐使用这个构造方法，比如我们有个FragmentPagerAdapter 一共需要装 15 个 Fragment ，那么我们就可以在构造集合的时候生成一个初始容量为 15 的一个集合。有人会认为 ArrayList 自身具有动态扩容的机制，无需这么麻烦，下面我们讲解扩容机制的时候我们就会发现，每次扩容是需要有一定的内存开销的，而这个开销在预先知道容量的时候是可以避免的。 源代码中指定初始容量的构造方法实现，判断了如果 我们指定容量大于 0 ，将会直接 new 一个数组，赋值给 elementData 引用作为集合真正的存储数组，而指定容量等于 0 的时候使用成员变量 EMPTY_ELEMENTDATA 作为暂时的存储数组，这是 EMPTY_ELEMENTDATA 这个空数组的一个用处（不必太过于纠EMPTY_ELEMENTDATA 的作用，其实它的在源码中出现的频率并不高）。 使用另个一个集合 Collection 的构造方法12345678910111213141516/*** 构造一个包含指定集合元素的列表，元素的顺序由集合的迭代器返回。* @param 源集合，其元素将被放置到这个集合中。 * @如果参数为 null，将会抛出 NullPointerException 空指针异常*/public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray 可能(错误地)不返回 Object[]类型的数组 参见 jdk 的 bug 列表(6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 如果集合大小为空将赋值为 EMPTY_ELEMENTDATA 等同于 new ArrayList(0); this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; 看完这个代码我最疑惑的地方是 Collection.toArray() 和 Arrays.copyOf() 这两个方法的使用，看来想明白这个构造参数具体做了什么必须理解这两个方法了。 Object[] Collection.toArray() 方法我们都知道 Collection 是集合框架的超类，其实 Collection.toArray 是交给具体的集合子类去实现的，这就说明不同的集合可能有不同的实现。他用来将一个集合转化为一个 Object[] 数组，事实上的真的是这样的么？参见 jdk 的 bug 列表(6260652)又是什么意思呢 ？我们来看下下边的这个例子： 12345678910111213List&lt;String&gt; subClasses = Arrays.asList("abc","def");// class java.util.Arrays$ArrayList System.out.println(list.getClass()); Object[] objects = subClasses.toArray();// class java.lang.String; Object[] objArray = list.toArray(); //这里返回的是 String[]System.out.println(objects.getClass().getSimpleName()); objArray[0] = new Object(); // cause ArrayStoreException 咦？为啥这里并不是一个 Object 数组呢？其实我们注意到，list.getClass 得到的并不是我们使用的 ArrayList 而是 Arrays 的内部类 Arrays$ArrayList。 123456789ArrayList(E[] array) &#123; //这里只是检查了数组是否为空，不为空直接将原数组赋值给这个 ArrayList 的存储数组。 a = Objects.requireNonNull(array);&#125;@Overridepublic Object[] toArray()&#123; return a.clone();&#125; 而我们调用的 toArray 方法就是这个内部对于 Collection.toArray 的实现， a.clone() ,这里 clone 并不会改变一个数组的类型，所以当原始数组中放的 String 类型的时候就会出现上边的这种情况了。 其实我们可以认为这是 jdk 的一个 bug，早在 05年的时候被人提出来了，但是一直没修复，但是在新的 「jdk 1.9」 种这个 bug 被修复了。 有兴趣的可以追踪 bug 6260652 看下。 Arrays.copyOf 方法这个方法是在集合源码中常见的一个方法，他有很多重载方式,我们来看下最根本的方法： 1234567891011public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings("unchecked") //根据class的类型是否是 Object[] 来决定是 new 还是反射去构造一个泛型数组 T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); //使用 native 方法批量赋值元素至新数组中。 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;&#125; 上边的注释也看出来了，Arrays.copyOf 方法复制数组的时候先判断了指定的数组类型是否为 Object[] 类型，否则使用反射去构造一个指定类型的数组。最后使用 System.arraycopy 这个 native 方法，去实现最终的数组赋值，newLength 如果比 original.length 大的时候会将多余的空间赋值为 null 由下边的例子可见: 12345String[] arrString = &#123;"abc","def"&#125;;Object[] copyOf = Arrays.copyOf(arrString, 5, Object[].class);//[abc, def, null, null, null]System.out.println(Arrays.toString(copyOf)); 当然 ArrayList(Collection&lt;? extends E&gt; c) 复制的时候传递的是 c.size() 所以不会出现 null。 ex: 对于 System.arraycopy 该方法，本文不再展开讨论，有一篇对于其分析很好的文章大家可以去参考System：System.arraycopy方法详解 ok，绕了这么大的圈子终于明白了，ArrayList(Collection&lt;? extends E&gt; c)干了啥了，其实就是将一个集合中的元素塞到 ArrayList 底层的数组中。至此我们也将 ArrayList 的构造研究完了。 ArrayList的添加元素 &amp; 扩容机制敲黑板了！这块是面试的常客了，所以必须仔细研究下了。我们先看下如何给一个 ArrayList 添加一个元素: 在集合末尾添加一个元素的方法123456789101112131415//成员变量 size 标识集合当前元素个数初始为 0int size；/*** 将指定元素添加到集合（底层数组）末尾* @param 将要添加的元素* @return 返回 true 表示添加成功*/public boolean add(E e) &#123; //检查当前底层数组容量，如果容量不够则进行扩容 ensureCapacityInternal(size + 1); // Increments modCount!! //将数组添加一个元素，size 加 1 elementData[size++] = e; return true;&#125; 调用 add 方法的时候总会调用 ensureCapacityInternal 来判断是否需要进行数组扩容， ensureCapacityInternal 参数为当前集合长度 size + 1，这很好理解，是否需要扩充长度，需要看当前底层数组是否够放 size + 1个元素的。 扩容机制1234567891011121314151617//扩容检查private void ensureCapacityInternal(int minCapacity) &#123; //如果是无参构造方法构造的的集合，第一次添加元素的时候会满足这个条件 minCapacity 将会被赋值为 10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; // 将 size + 1 或 10 传入 ensureExplicitCapacity 进行扩容判断 ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; //操作数加 1 用于保证并发访问 modCount++; // 如果 当前数组的长度比添加元素后的长度要小则进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; 上边的源码主要做了扩容前的判断操作，注意参数为当前集合元素个数+1，第一次添加元素的时候 size + 1 = 1 ,而 elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA, 长度为 0 ，1 - 0 &gt; 0, 所以需要进行 grow 操作也就是扩容。 1234567891011121314151617181920212223242526272829303132333435/*** 集合的最大长度 Integer.MAX_VALUE - 8 是为了减少出错的几率 Integer 最大值已经很大了*/private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;/*** 增加容量，以确保它至少能容纳最小容量参数指定的元素个数。* @param 满足条件的最小容量*/private void grow(int minCapacity) &#123; //获取当前 elementData 的大小，也就是 List 中当前的容量 int oldCapacity = elementData.length; //oldCapacity &gt;&gt; 1 等价于 oldCapacity / 2 所以新容量为当前容量的 1.5 倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //如果扩大1.5倍后仍旧比 minCapacity 小那么直接等于 minCapacity if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //如果新数组大小比 MAX_ARRAY_SIZE 就需要进一步比较 minCapacity 和 MAX_ARRAY_SIZE 的大小 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity通常接近 size 大小 //使用 Arrays.copyOf 构建一个长度为 newCapacity 新数组 并将 elementData 指向新数组 elementData = Arrays.copyOf(elementData, newCapacity);&#125;/*** 比较 minCapacity 与 Integer.MAX_VALUE - 8 的大小如果大则放弃-8的设定，设置为 Integer.MAX_VALUE */private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 由此看来 ArrayList 的扩容机制的知识点一共又两个 每次扩容的大小为原来大小的 1.5 倍 （当然这里没有包含 1.5倍后大于 MAX_ARRAY_SIZE 的情况）扩容的过程其实是一个将原来元素拷贝到一个扩容后数组大小的长度新数组中。所以 ArrayList 的扩容其实是相对来说比较消耗性能的。 在指定角标位置添加元素的方法123456789101112131415161718/*** 将指定的元素插入该列表中的指定位置。将当前位置的元素(如果有)和任何后续元素移到右边(将一个元素添加到它们的索引中)。* @param 要插入的索引位置* @param 要添加的元素* @throws 如果 index 大于集合长度 小于 0 则抛出角标越界 IndexOutOfBoundsException 异常*/public void add(int index, E element) &#123; // 检查角标是否越界 rangeCheckForAdd(index); // 扩容检查 ensureCapacityInternal(size + 1); //调用 native 方法新型数组拷贝 System.arraycopy(elementData, index, elementData, index + 1,size - index); // 添加新元素 elementData[index] = element; size++;&#125; 我们知道一个数组是不能在角标位置直接插入元素的，ArrayList 通过数组拷贝的方法将指定角标位置以及其后续元素整体向后移动一个位置，空出 index 角标的位置，来赋值新的元素。 将一个数组 src 起始 srcPos 角标之后 length 长度间的元素，赋值到 dest 数组中 destPos 到 destPos + length -1长度角标位置上。只是在 add 方法中 src 和 dest 为同一个数组而已。 123public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 批量添加元素由于批量添加和添加一个元素逻辑大概相同则这里不详细说了，代码注释可以了解整个添加流程。 在数组末尾添加 12345678910111213public boolean addAll(Collection&lt;? extends E&gt; c) &#123; // 调用 c.toArray 将集合转化数组 Object[] a = c.toArray(); // 要添加的元素的个数 int numNew = a.length; //扩容检查以及扩容 ensureCapacityInternal(size + numNew); // Increments modCount //将参数集合中的元素添加到原来数组 [size，size + numNew -1] 的角标位置上。 System.arraycopy(a, 0, elementData, size, numNew); size += numNew; //与单一添加的 add 方法不同的是批量添加有返回值，如果 numNew == 0 表示没有要添加的元素则需要返回 false return numNew != 0;&#125; 在数组指定角标位置添加1234567891011121314151617181920public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; //同样检查要插入的位置是否会导致角标越界 rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); //这里做了判断，如果要numMoved &gt; 0 代表插入的位置在集合中间位置，和在 numMoved == 0最后位置 则表示要在数组末尾添加 如果 &lt; 0 rangeCheckForAdd 就跑出了角标越界 int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0;&#125;private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 两个方法不同的地方在于如果移动角标即之后的元素，addAll(int index, Collection&lt;? extends E&gt; c)里做了判断，如果要 numMoved &gt; 0 代表插入的位置在集合中间位置，和在 numMoved == 0 最后位置 则表示要在数组末尾添加 如果 numMoved &lt; 0 ，rangeCheckForAdd 就抛出了角标越界异常了。 与单一添加的 add 方法不同的是批量添加有返回值，如果 numNew == 0 表示没有要添加的元素则需要返回 false。 ArrayList 删除元素根据角标移除元素1234567891011121314151617181920212223242526/*** 将任何后续元素移到左边(从它们的索引中减去一个)。*/public E remove(int index) &#123; //检查 index 是否 &gt;= size rangeCheck(index); modCount++; //index 位置的元素 E oldValue = elementData(index); // 需要移动的元素个数 int numMoved = size - index - 1; if (numMoved &gt; 0) //采用拷贝赋值的方法将 index 之后所有的元素 向前移动一个位置 System.arraycopy(elementData, index+1, elementData, index, numMoved); // 将 element 末尾的元素位置设为 null elementData[--size] = null; // clear to let GC do its work // 返回 index 位置的元素 return oldValue;&#125;// 比较要移除的角标位置和当前 elementData 中元素的个数private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 根据角标移除元素的方法源码如上所示，值得注意的地方是： rangeCheck 和 rangeCheckForAdd 方法不同 ，rangeCheck 只检查了 index是否大于等于 size，因为我们知道 size 为 elementData 已存储数据的个数，我们只能移除 elementData 数组中 [0 , size -1] 的元素，否则应该抛出角标越界。 但是为什么没有和 rangeCheckForAdd 一样检查小于0的角标呢，是不是remove(-1) 不会抛异常呢？ 其实不是的，因为 rangeCheck(index); 后我们去调用 elementData(index) 的时候也会抛出 IndexOutOfBoundsException 的异常，这是数组本身抛出的，不是 ArrayList 抛出的。那为什么要检查&gt;= size 呢？ 数组本身不也会检查么？ 哈哈.. 细心的同学肯定知道 elementData.length 并不一定等于 size，比如： 123456ArrayList&lt;String&gt; testRemove = new ArrayList&lt;&gt;(10); testRemove.add("1"); testRemove.add("2"); // java.lang.IndexOutOfBoundsException: Index: 2, Size: 2 String remove = testRemove.remove(2); System.out.println("remove = " + remove + ""); new ArrayList&lt;&gt;(10) 表示 elementData 初始容量为10，所以 elementData.length = 10 而我们只给集合添加了两个元素所以 size = 2 这也就是为啥要 rangeCheck 的原因了。 移除指定元素1234567891011121314151617181920212223242526272829303132333435/*** 删除指定元素，如果它存在则反会 true，如果不存在返回 false。* 更准确地说是删除集合中第一出现 o 元素位置的元素 ，* 也就是说只会删除一个，并且如果有重复的话，只会删除第一个次出现的位置。*/public boolean remove(Object o) &#123; // 如果元素为空则只需判断 == 也就是内存地址 if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; //得到第一个等于 null 的元素角标并移除该元素 返回 ture fastRemove(index); return true; &#125; &#125; else &#123; // 如果元素不为空则需要用 equals 判断。 for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; //得到第一个等于 o 的元素角标并移除该元素 返回 ture fastRemove(index); return true; &#125; &#125; return false;&#125;//移除元素的逻辑和 remve(Index)一样 private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 由上边代码可以看出来，移除元素和移除指定角标元素一样最终都是通过 System.arraycopy 将 index 之后的元素前移一位，并释放原来位于 size 位置的元素。 还可以看出，如果数组中有指定多个与 o 相同的元素只会移除角标最小的那个，并且 null 和 非null 的时候判断方法不一样。至于 equals 和 == 的区别，还有 hashCode 方法，我会之后在总结一篇单独的文章。等不急的可以先去网上找找喽。 注意，以下几点： Integer 重写的 equals 方法有自动拆箱； equals如果不重写，那跟 == 是一样的，如果要重写，必须 equal 和 hashcode 都重写； 最好的例子就是 HashSet，如果key不重写 hashcode()，那可能equals相等但是会导致存入两个一样的。 批量移除/保留 removeAll/retainAllArrayList 提供了 removeAll/retainAll 操作，这两个操作分别是 批量删除与参数集合中共同享有的元素 和 批量删除与参数集合中不共同享有的元素，保留共同享有的元素，两个方法只有一个参数不同!!! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** 批量删除与参数集合中共同享有的元素*/public boolean removeAll(Collection&lt;?&gt; c) &#123; //判空 如果为空则抛出 NullPointerException 异常 Objects 的方法 Objects.requireNonNull(c); return batchRemove(c, false);&#125; /** 只保留与 c 中元素相同的元素相同的元素*/public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true);&#125; /** 批量删除的指定方法 */private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; // r w 两个角标 r 为 elementData 中元素的索引 // w 为删除元素后集合的长度 int r = 0, w = 0; boolean modified = false; try &#123; for (; r &lt; size; r++) // 如果 c 当前集合中不包含当前元素，那么则保留 if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; &#125; finally &#123; // 如果c.contains（o）可能会抛出异常，如果抛出异常后 r!=size 则将 r 之后的元素不在比较直接放入数组 if (r != size) &#123; System.arraycopy(elementData, r, elementData, w, size - r); // w 加上剩余元素的长度 w += size - r; &#125; // 如果集合移除过元素，则需要将 w 之后的元素设置为 null 释放内存 if (w != size) &#123; // clear to let GC do its work for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; size = w; modified = true; &#125; &#125; //返回是否成功移除过元素，哪怕一个 return modified;&#125; 可以看到移除指定集合中包含的元素的方法代码量是目前分析代码中最长的了，但是逻辑也很清晰： 从 0 开始遍历 elementData 如果 r 位置的元素不存在于指定集合 c 中，那么我们就将他复制给数组 w 位置， 整个遍历过程中 w &lt;= r。 由于 c.contains(o)可能会抛出异常ClassCastException/NullPointerException，如果因为异常而终止（这两个异常是可选操作，集合源码中并没有显示生命该方法一定会抛异常），那么我们将会产生一次错误操作，所以 finally 中执行了判断操作，如果 r!= size 那么肯定是发生了异常，那么则将 r 之后的元素不在比较直接放入数组。最终得到的结果并不一定正确是删除了所有与 c 中的元素。 批量删除和保存中，涉及高效的保存/删除两个集合公有元素的算法，是值得我们学习的地方,写的真好哈哈哈哈哈！！！ ArrayList 的改查对于一个ArrayList 的改查方法就很简单了，set 和 get 方法。下面我们看下源码吧： 修改指定角标位置的元素1234567891011121314public E set(int index, E element) &#123; //角标越界检查 rangeCheck(index); //下标取数据注意这里不是elementData[index] 而是 elementData(index) 方法 E oldValue = elementData(index); //将 index 位置设置为新的元素 elementData[index] = element; // 返回之前在 index 位置的元素 return oldValue;&#125;E elementData(int index) &#123; return (E) elementData[index];&#125; 查询指定角标的元素123456public E get(int index) &#123; //越界检查 rangeCheck(index); //下标取数据注意这里不是elementData[index] 而是 elementData(index) 方法 return elementData(index); &#125; 查询指定元素的角标或者集合是否包含某个元素12345678910111213141516171819202122232425262728293031323334353637//集合中是否包含元素 indexOf 返回 -1 表示不包含 return false 否则返回 truepublic boolean contains(Object o) &#123; return indexOf(o) &gt;= 0;&#125;/*** 返回集合中第一个与 o 元素相等的元素角标，返回 -1 表示集合中不存在这个元素* 这里还做了空元素直接判断 == 的操作*/public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125;/** * 从 elementData 末尾开始遍历遍历数组，所以返回的是集合中最后一个与 o 相等的元素的角标*/public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; ArrayList 集合的 toArry 方法其实 Object[] toArray(); 方法，以及其重载函数 \ T[] toArray(T[] a); 是接口 Collection 的方法，ArrayList 实现了这两个方法，很少见ArrayList 源码分析的文章分析这两个方法，顾名思义这两个方法的是用来，将一个集合转为数组的方法，那么两者的不同之处是，后者可以指定数组的类型，前者返回为一个 Object[] 超类数组。那么我们具体下源码实现： 1234567891011121314public Object[] toArray() &#123; return Arrays.copyOf(elementData, size);&#125;@SuppressWarnings("unchecked")public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // Make a new array of a's runtime type, but my contents: return (T[]) Arrays.copyOf(elementData, size, a.getClass()); System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a;&#125; 可以看到 Object[] toArray() 只是调用了一次 Arrays.copyOf() 将集合中元素拷贝到一个新的 Object[] 数组并返回。这个 Arrays.copyOf() 方法前边已经讲了。所以 toArray() 方法并没有什么疑问，有疑问的地方在于toArray(T[] a) 。 我们可以传入一个指定类型的标志数组作为参数，toArray(T[] a) 方法最终会返回这个类型的包含集合元素的新数组。但是源码判断了 ： 如果 a.length &lt; size 即当前集合元素的个数与参数 a 数组元素的大小的时候将和 toArray() 一样返回一个新的数组。 如果 a.length == size 将不会产生新的数组直接将集合中的元素调用 System.arraycopy() 方法将元素复制到参数数组中，返回 a。 a.length &gt; size 也不会产生新的数组,但是值得注意的是 a[size] = null; 这一句改变了原数组中 index = size 位置的元素，被重新设置为 null 了。 下面我们来看下第三种情况的例子： 12345678910111213141516171819202122232425SubClass[] sourceMore = new SubClass[4]; for (int i = 0; i &lt; sourceMore.length; i++) &#123; sourceMore[i] = new SubClass(i);&#125; //当 List.toArray(T[] a) 中 a.length == list.size 的时候使用 Array.copyOf 会将 list 中的内容赋值给 sourceMore 并将其返回//sourceMore[0,size-1] = list&#123;0, size-1&#125; 而 sourceMore[size] = nullSubClass[] sourceMore = new SubClass[4];for (int i = 0; i &lt; sourceMore.length; i++) &#123; sourceMore[i] = new SubClass(i);&#125;//list to Array 之前 sourceMore [SubClass&#123;test=0&#125;, SubClass&#123;test=1&#125;, SubClass&#123;test=2&#125;, SubClass&#123;test=3&#125;] sourceEqual.length:: 4System.out.println("list to Array 之前 sourceMore " + Arrays.toString(sourceMore) + " sourceEqual.length:: " + sourceMore.length);SubClass[] desSourceMore = tLists.toArray(sourceMore);//list to Array 之后 desSourceMore [SubClass&#123;test=1&#125;, SubClass&#123;test=2&#125;, null, SubClass&#123;test=3&#125;]desSourceMore.length:: 4System.out.println("list to Array 之后 desSourceMore " + Arrays.toString(desSourceMore) + "desSourceMore.length:: " + desSourceMore.length);//list to Array 之后 source [SubClass&#123;test=1&#125;, SubClass&#123;test=2&#125;, null, SubClass&#123;test=3&#125;]sourceEqual.length:: 4System.out.println("list to Array 之后 source " + Arrays.toString(sourceMore) + "sourceEqual.length:: " + sourceMore.length);//source == desSource trueSystem.out.println("source == desSource " + (sourceMore == desSourceMore)); ArrayList 的遍历ArrayList 的遍历方式 jdk 1.8 之前有三种 ：for 循环遍历， foreach 遍历，迭代器遍历,jdk 1.8 之后又引入了forEach 操作，我们先来看看迭代器的源码实现： 迭代器迭代器 Iterator 模式是用于遍历各种集合类的标准访问方法。它可以把访问逻辑从不同类型的集合类中抽象出来，从而避免向客户端暴露集合的内部结构。 ArrayList 作为集合类也不例外，迭代器本身只提供三个接口方法： 12345public interface Iterator &#123; boolean hasNext();//是否还有下一个元素 Object next();// 返回当前元素 可以理解为他相当于 fori 中 i 索引 void remove();// 移除一个当前的元素 也就是 next 元素。 &#125; ArrayList 中调用 iterator() 将会返回一个内部类对象 Itr 其实现了 Iterator 接口。 123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; 下面让我们看下其实现的源码： 正如我们的 for 循环遍历一样，数组角标总是从 0 开始的，所以 cursor 初始值为 0 ， hasNext 表示是否遍历到数组末尾，即 i &lt; size 。对于 modCount 变量之所以一直没有介绍是因为他集合并发访问有关系，用于标记当前集合被修改（增删）的次数，如果并发访问了集合那么将会导致这个 modCount 的变化，在遍历过程中不正确的操作集合将会抛出 ConcurrentModificationException ，这是 Java 「fail-fast 的机制」，对于如何正确的在遍历过程中操作集合稍后会有说明。 12345678private class Itr implements Iterator&lt;E&gt; &#123; int cursor; // 对照 hasNext 方法 cursor 应理解为下个调用 next 返回的元素 初始为 0 int lastRet = -1; // 上一个返回的角标 int expectedModCount = modCount;//初始化的时候将其赋值为当前集合中的操作数， // 是否还有下一个元素 cursor == size 表示当前集合已经遍历完了 所以只有当 cursor 不等于 size 的时候 才会有下一个元素 public boolean hasNext() &#123; return cursor != size; &#125; next 方法是我们获取集合中元素的方法，next 返回当前遍历位置的元素，如果在调用 next 之前集合被修改，并且迭代器中的期望操作数并没有改变，将会引发ConcurrentModificationException。next 方法多次调用 checkForComodification 来检验这个条件是否成立。 1234567891011121314151617@SuppressWarnings("unchecked") public E next() &#123; // 验证期望的操作数与当前集合中的操作数是否相同 如果不同将会抛出异常 checkForComodification(); // 如果迭代器的索引已经大于集合中元素的个数则抛出异常，这里不抛出角标越界 int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; // 由于多线程的问题这里再次判断是否越界，如果有异步线程修改了List（增删）这里就可能产生异常 if (i &gt;= elementData.length) throw new ConcurrentModificationException(); // cursor 移动 cursor = i + 1; //最终返回 集合中对应位置的元素，并将 lastRet 赋值为已经访问的元素的下标 return (E) elementData[lastRet = i]; &#125; 只有 Iterator 的 remove 方法会在调用集合的 remove 之后让 期望 操作数改变使expectedModCount与 modCount 再相等，所以是安全的。 12345678910111213141516171819202122// 实质调用了集合的 remove 方法移除元素public void remove() &#123; // 比如操作者没有调用 next 方法就调用了 remove 操作，lastRet 等于 -1的时候抛异常 if (lastRet &lt; 0) throw new IllegalStateException(); //检查操作数 checkForComodification(); try &#123; //移除上次调用 next 访问的元素 ArrayList.this.remove(lastRet); // 集合中少了一个元素，所以 cursor 向前移动一个位置（调用 next 时候 cursor = lastRet + 1） cursor = lastRet; //删除元素后赋值-1，确保先前 remove 时候的判断 lastRet = -1; //修改操作数期望值， modCount 在调用集合的 remove 的时候被修改过了。 expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; // 集合的 remove 会有可能抛出 rangeCheck 异常，catch 掉统一抛出 ConcurrentModificationException throw new ConcurrentModificationException(); &#125; &#125; 检查期望的操作数与当前集合的操作数是否相同。Java8 发布了很多函数式编程的特性包括 lamada 和Stream 操作。迭代器也因此添加了 forEachRemaining 方法，这个方法可以将当前迭代器访问的元素（next 方法）后的元素传递出去还没用到过，源码就不放出来了,大家有兴趣自己了解下。 1234567891011 @Override @SuppressWarnings("unchecked")public void forEachRemaining(Consumer&lt;? super E&gt; consumer) &#123; //... Java8 的新特性，可以将当前迭代器访问的元素（next 方法）后的元素传递出去还没用到过，源码就不放出来了,大家有兴趣自己了解下。 &#125; // 检查期望的操作数与当前集合的操作数是否相同 final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125;&#125; ListIterator 迭代器ArrayList 可以通过以下两种方式获取 ListIterator 迭代器，区别在于初始角标的位置。不带参数的迭代器默认的cursor = 0 123456789public ListIterator&lt;E&gt; listIterator(int index) &#123; if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException("Index: "+index); return new ListItr(index);&#125; public ListIterator&lt;E&gt; listIterator() &#123; return new ListItr(0);&#125; ListItr对象继承自前边分析的 Itr，也就是说他拥有 Itr 的所有方法，并在此基础上进行扩展，其扩展了访问当前角标前一个元素的方法。以及在遍历过程中添加元素和修改元素的方法。 ListItr 的构造方法如下： 12345private class ListItr extends Itr implements ListIterator&lt;E&gt; &#123; ListItr(int index) &#123; super(); cursor = index;&#125; ListItr 的 previous 方法： 123456789101112131415161718192021222324252627282930public boolean hasPrevious() &#123; // cursor = 0 表示游标在数组第一个元素的左边，此时 `hasPrevious` 返回false return cursor != 0;&#125;public int nextIndex() &#123; return cursor;//调用返回当前角标位置&#125;public int previousIndex() &#123; return cursor - 1;//调用返回上一个角标&#125;//返回当前角标的上一个元素，并前移移动角标@SuppressWarnings("unchecked")public E previous() &#123; // fast-fail 检查 checkForComodification(); int i = cursor - 1; // 如果前移角标 &lt;0 代表遍历到数组遍历完成，一般在调用 previous 要调用 hasPrevious 判断 if (i &lt; 0) throw new NoSuchElementException(); //获取元素 Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); //获取成功后修改角标位置和 lastRet 位置 cursor = i; return (E) elementData[lastRet = i];&#125; ListItr 的 add 方法 1234567891011121314151617public void add(E e) &#123; // fast-fail 检查 checkForComodification(); try &#123; // 获取当前角标位置，一般的是调用 previous 后，角标改变后后去 cursor int i = cursor; //添加元素在角标位置 ArrayList.this.add(i, e); //集合修改完成后要改变当前角标位置 cursor = i + 1; //重新置位 -1 如果使用迭代器修改了角标位置元素后不允许立刻使用 set 方法修改修改后角标未知的额元素 参考 set 的源代码 lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125;&#125; 可能对比两个迭代器后，会对 cursor 指向的位置有所疑惑，现在我们来看下一段示例代码对应的图： 1234567891011121314151617181920private void testListItr()&#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); list.add(4); ListIterator&lt;Integer&gt; listIterator = list.listIterator(list.size()); while (listIterator.hasPrevious())&#123; if (listIterator.previous() == 2)&#123; listIterator.add(0); // listIterator.set(10); //Exception in thread "main" java.lang.IllegalStateException &#125; &#125; System.out.println("list " + list.toString());&#125; 由此可以看 cursor 于 数组角标不同，它可以处的位置总比角标多一个，因为在我们使用 Iterator 操作集合的时候，总是要先操作 cursor 移动， listIterator.previous 也好 iterator.next() 也好，都是一样的道理，如果不按照规定去进行操作，带给使用者的只有异常。 java8 新增加的遍历方法 forEachjava8增加很多好用的 API，工作和学习中也在慢慢接触这些 API，forEach 操作可能是我继 lambda 后，第一个使用的 API 了（囧），jdk doc 对这个方法的解释是： 对此集合的每个条目执行给定操作，直到处理完所有条目或操作抛出异常为止。 除非实现类另有规定，否则按照条目集迭代的顺序执行操作（如果指定了迭代顺序）。操作抛出的异常需要调用者自己处理。 其实其内部实现也很简单，只是一个判断了操作数的 for 循环，所以在效率上不会有提升，但是在安全性上的确有提升，也少些很多代码不是么？ 1234567891011121314151617@Overridepublic void forEach(Consumer&lt;? super E&gt; action) &#123; //检查调用者传进来的操作函数是否为空 Objects.requireNonNull(action); //与迭代不同期望操作被赋值为 final 也就是 forEach 过程中不允许并发修改集合否则会抛出异常 final int expectedModCount = modCount; @SuppressWarnings("unchecked") final E[] elementData = (E[]) this.elementData; final int size = this.size; //每次取元素之前判断操作数，确保操作正常 for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123; action.accept(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; 对于高级 for 循环以及最普通的 fori 方法这里不再赘述。下面我们看下面试会问到一个问题，也是我们在单线程操作集合的时候需要注意的一个问题，如果正确的在遍历过程中修改集合。 错误操作 1 在 for循环修改集合后继续遍历第一个例子： 1234567891011121314List&lt;SubClass&gt; list2 = new ArrayList&lt;&gt;();list2.add(new SubClass(1));list2.add(new SubClass(2));list2.add(new SubClass(3));list2.add(new SubClass(3));for (int i = 0; i &lt; list2.size(); i++) &#123; if (list2.get(i).test == 3) &#123; list2.remove(i); &#125;&#125;System.out.println(list2);//[SubClass&#123;test=1&#125;, SubClass&#123;test=2&#125;, SubClass&#123;test=3&#125;] 这个例子我们会发现，程序并没有抛出异常，但是从运行经过上来看并不是我们想要的，因为还有 SubClass.test = 3的数据在，这是因为 remove 操作改变了list.size(),而 fori 中每次执行都会重新调用一次lists2.size()，当我们删除了倒数第二个元素后，list2.size() = 3,i = 3 &lt; 3 不成立则没有在进行 remove 操作，知道了为什么以后我们试着这样改变了循环方式： 123456789int size = list2.size();for (int i = 0; i &lt; size; i++) &#123; if (list2.get(i).test == 3) &#123; list2.remove(i);//remove 以后 list 内部将 size 重新改变了 for 循环下次调用的时候可能就不进去了 &#125;&#125;System.out.println(list2);//Exception in thread "main" java.lang.IndexOutOfBoundsException: Index: 3, Size: 3 果真程序抛出了角标越界的异常，因为这样每次 fori 的时候我们不去拿更新后的 list 元素的 size 大小，所以当我们删除一个元素后，size = 3 当我们 for 循环去list2.get(3)的时候就会被 rangeCheck方法抛出异常。 错误操作导致 ConcurrentModificationException 异常我们分析迭代器的时候，知道 ConcurrentModificationException是指因为迭代器调用 checkForComodification 方法比较 modCount 和 expectedModCount 方法大小的时候抛出异常。我们在分析 ArrayList 的时候在每次对集合进行修改， 即有 add 和 remove 操作的时候每次都会对 modCount ++。 modCount 这个变量主要用来记录 ArrayList 被修改的次数，那么为什么要记录这个次数呢？是为了防止多线程对同一集合进行修改产生错误，记录了这个变量，在对 ArrayList 进行迭代的过程中我们能很快的发现这个变量是否被修改过，如果被修改了 ConcurrentModificationException 将会产生。下面我们来看下例子，这个例子并不是在多线程下的，而是因为我们在同一线程中对 list 进行了错误操作导致的： 12345678910111213Iterator&lt;SubClass&gt; iterator = lists.iterator();while (iterator.hasNext()) &#123; SubClass next = iterator.next(); int index = next.test; if (index == 3) &#123; list2.remove(index);//操作1： 注意是 list2.remove 操作 //iterator.remove()；/操作2 注意是 iterator.remove 操作 &#125;&#125;//操作1： Exception in thread "main" java.util.ConcurrentModificationException//操作2： [SubClass&#123;test=1&#125;, SubClass&#123;test=2&#125;]System.out.println(list2); 我们对操作1，2分别运行程序，可以看到，操作1很快就抛出了 java.util.ConcurrentModificationException 异常，操作2 则顺利运行出正常结果，如果对 modCount 注意了的话，我们很容易理解，list.remove(index) 操作会修改List 的 modCount，而 iterator.next() 内部每次会检验 expectedModCount != modCount，所以当我们使用 list.remove 下一次再调用 iterator.next() 就会报错了，而iterator.remove为什么是安全的呢？因为其操作内部会在调用 list.remove 后重新将新的 modCount 赋值给 expectedModCount。所以我们直接调用 list.remove 操作是错误的。对于多线程的影响这里不在展开这里推荐有兴趣的朋友看下这个文章 Java ConcurrentModificationException异常原因和解决方法; 经过了一轮分析我们我们知道了错误产生原因了，但是大家是否能真的分辨出什么操作是错误的呢？我们来看下边这个面试题，这是我在网上无意中看到的一道大众点评的面试题： 123456789101112ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();for (int i = 0; i &lt; 10; i++) &#123; list.add("sh" + i);&#125;for (int i = 0; list.iterator().hasNext(); i++) &#123; // hasNext 只是判断了 cursor != size，并没有移动cursor，所以只要 size 不是 0 ，就不会退出循环 // 最后size == 5，list.remove(5)越界：IndexOutOfBoundsException // 这里并不会去报 ConcurrentModificationException(),因为list.iterator()每次都会去new一个新的Itr对象，每次初始化这个对象都会把最新的 modCount 赋值给 expectedCount，所以不可能做到 modeCount != expectedCount，而且只有 iterator.next() 会有这个错误。 list.remove(i); System.out.println("秘密" + list.get(i));&#125; 一道面试题相信大家肯定知道这样操作是会产生错误的，但是最终会抛出角标越界还是ConcurrentModificationException呢？ 其实这里会抛出角标越界异常，为什么呢，因为 for 循环的条件 list.iterator().hasNext()，我们知道 list.iterator() 将会new 一个新的 iterator 对象，而在 new 的过程中我们将 每次 list.remove 后的 modCount 赋值给了新的 iterator 的 expectedModCount，所以不会抛出 ConcurrentModificationException 异常，而 hasNext 内部只判断了 size 是否等于 cursor != size 当我们删除了一半元素以后，size 变成了 5 而新的 list.iterator() 的 cursor 等于 0 ，0!=5 for 循环继续，那么当执行到 list.remove（5）的时候就会抛出角标越界了。 总结 ArrayList 底层是一个动态扩容的数组结构,每次扩容需要增加1.5倍的容量 ArrayList 扩容底层是通过 Arrays.CopyOf 和 System.arraycopy 来实现的。每次都会产生新的数组，和数组中内容的拷贝，所以会耗费性能，所以在多增删的操作的情况可优先考虑 LinkList 而不是 ArrayList。 ArrayList 的 toArray 方法重载方法的使用。 允许存放（不止一个） null 元素 允许存放重复数据，存储顺序按照元素的添加顺序 ArrayList 并不是一个线程安全的集合。如果集合的增删操作需要保证线程的安全性，可以考虑使用 CopyOnWriteArrayList 或者使collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类. 不正确访问集合元素的时候 ConcurrentModificationException和 java.lang.IndexOutOfBoundsException 异常产生的时机和原理。 本文又长篇大论的分析了一波 ArrayList 的源码，对我个人而言这很有意义，在查看源码的过程中，注意到了平时很少有机会接触的知识点。当然这只是集合源码分析的开端，以后还会更细，其他常用集合源码的分析。如果大家感觉我写的还可以， 请留言 + 点赞 + 关注。 转载：搞懂 Java ArrayList 源码 LinkedList先吐槽一下，感觉LinkedList源码真的很简单啊哈哈哈哈哈，总共也才1000来行代码…而且连个扩容都没有，基本上就是对双向链表的一顿操作，所以它是为了凑代码量所以顺带利用双向链表的性质实现一下双向队列嘛…算了，还是耐着性质稍微分析一下吧…（就是尼玛方法有点多，名字我都看晕啦…） 话说回来，这些里面的函数就是 对双向链表做某些操作的标准答案吖！里面写的代码都挺优美的！可以欣赏一哈~ 算了……我懒得分析了，直接丢个链接吧…… 搞懂 JAVA LinkedList 源码 注意点 继承自 AbstrackSequentialList 并实现了 List 接口以及 Deque 双向队列接口，因此 LinkedList 不但拥有 List 相关的操作方法，也有队列的相关操作方法。 LinkedList 集合底层实现的数据结构为双向链表 LinkedList 集合中元素允许为 null LinkedList 允许存入重复的数据 LinkedList 中元素存放顺序为存入顺序。 LinkedList 是非线程安全的，如果想保证线程安全的前提下操作 LinkedList，可以使用 List list = Collections.synchronizedList(new LinkedList(...)); 来生成一个线程安全的 LinkedList 实现List，利用了其是两个单链表组成的，实现Deque，利用其是双向链表的性质，注意哈，Deque既可以当队列也可以当栈，所以队列和栈的方法LinkedList全有。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface Deque&lt;E&gt; extends Queue&lt;E&gt; &#123; //从头部插入（抛异常） void addFirst(E e); //从尾部插入（抛异常） void addLast(E e); //从头部插入（特殊值） boolean offerFirst(E e); //从尾部插入（特殊值） boolean offerLast(E e); //从头部移除（抛异常） E removeFirst(); //从尾部移除（抛异常） E removeLast(); //从头部移除（特殊值） E pollFirst(); //从尾部移除（特殊值） E pollLast(); //从头部查询（抛异常） E getFirst(); //从尾部查询（抛异常） E getLast(); //从头部查询（特殊值） E peekFirst(); //从尾部查询（特殊值） E peekLast(); //（从头到尾遍历列表时，移除列表中第一次出现的指定元素） boolean removeFirstOccurrence(Object o); //（从头到尾遍历列表时，移除列表中最后一次出现的指定元素） boolean removeLastOccurrence(Object o); //都没啥难度，不解释了 boolean add(E e); boolean offer(E e); E remove(); E poll(); E element(); E peek(); void push(E e); E pop(); boolean remove(Object o); boolean contains(Object o); public int size(); Iterator&lt;E&gt; iterator(); Iterator&lt;E&gt; descendingIterator();&#125; Vector简而言之，Vector 与 ArrayList 之间的关系，就是 HashMap 和 HashTable 之间的关系，Vector 的方法基本上都是 ArrayList 的方法 加个 synchronized 关键字组成的，这边讲下它与 ArrayList 的不同，就可以结束了！ 不同点有： Vector 线程安全，是同步的；ArrayList 非线程安全，非同步的 扩容策略不同，ArrayList 扩容是变成原来的 1.5 倍，而 Vector 和 ArrayDeque 扩容一样，都是变为原来的两倍 Vector 可以使用 Enumeration 和 Iterator 进行元素遍历，ArrayList 只提供了 Iterator 的方式 由于使用的线程同步，Vector 的效率比 ArrayList 低，但是 jdk8 之后，由于对 synchronized 的关键字的优化，引入了 偏向锁 等轻量级锁之后， Vector 和 ArrayList 的性能差异已经不大了 Queue没啥好分析的，实现这个接口的，接触的比较多的也就是 PriorityQueue、ArrayDeque。 直接放链接： Java集合（七） Queue详解 Java集合ArrayDeque类解读 死磕 java集合之ArrayDeque源码分析 注意点就其实现而言，ArrayDeque采用了循环数组的方式来完成双端队列的功能。 无限的扩展，自动扩展队列大小的。（当然在不会内存溢出的情况下） 非线程安全的，不支持并发访问和修改。 支持fast-fail。 作为栈使用的话比比栈要快。 当队列使用比 LinkedList 要快。 null元素被禁止使用，这个跟 ArrayList 和 LinkedList 也不一样 初始值跟 ArrayList 不一样，这里的初始值是 16，扩容也是直接扩容两倍，跟 ArrayList 的 1.5 倍不一样，扩容的时候利用了容量是2次幂这个性质，跟 HashMap 一样，直接与 length 相与即可。]]></content>
      <categories>
        <category>集合类</category>
      </categories>
      <tags>
        <tag>集合</tag>
        <tag>ArrayList</tag>
        <tag>LinkedList</tag>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA对数组的四种拷贝方式]]></title>
    <url>%2F2019%2F10%2F07%2FJAVA%E5%AF%B9%E6%95%B0%E7%BB%84%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%8B%B7%E8%B4%9D%E6%96%B9%E5%BC%8F.html</url>
    <content type="text"><![CDATA[数组拷贝的四种方式 for clone System.arraycopy arrays.copyof 数组拷贝方式之一——for()就是for循环咯！！！ 12345/** 1 for 循环拷贝*/int[] arr1 = new int[len];for (int i = 0; i &lt; len; i++) &#123; arr1[i] = arr0[i];&#125; 也是浅拷贝咯！ 数组拷贝方式之二——clone()（主要参考）细说 Java 的深拷贝和浅拷贝 在 Java 中，所有的 Class 都继承自 Object ，而在 Object 上，存在一个 clone() 方法，它被声明为了 protected ，所以我们可以在其子类中，使用它。而无论是浅拷贝还是深拷贝，都需要实现 clone() 方法，来完成操作。 首先需要说明的是，clone()是浅拷贝的方式！！！其次，我们应该如何实现深拷贝呢，有两种方法： 用多层浅拷贝达到深拷贝的目的，要知道，浅拷贝和深拷贝的区别就是在处理对象引用时，浅拷贝不会创造新的对象，而深拷贝会，但是如果一个对象的属性变成了基本类型数据，那么浅拷贝同样会创造新的对象，所以说，多层浅拷贝就能达到深拷贝的目的，例如：在对fatherClass和childClass进行两级浅拷贝之后，fatherB就有浅拷贝变成了深拷贝，因为此时基本数据类型和Child对象都是指向不同的地址的，也就是二者都是不同的对象，所以就是达到了深拷贝的目的了。 序列化（serialization）这个对象，再反序列化回来，就可以得到这个新的对象，无非就是序列化的规则需要我们自己来写。总结下来就是做了5件事： 确保对象图中的所有类都是可序列化的 创建输入输出流 使用这个输入输出流来创建对象输入和对象输出流 将你想要拷贝的对象传递给对象输出流 从对象输入流中读取新的对象并且转换回你所发送的对象的类 最后，推一个序列化实现深拷贝的例子：序列化的方式使得其变为深拷贝 数组拷贝方式之三——System.arraycopy()native方法，依旧是一个浅拷贝——–&gt; 详细传送门 数组拷贝方式之四——arrays.copyof()1234567891011public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings("unchecked") //根据class的类型是否是 Object[] 来决定是 new 还是反射去构造一个泛型数组 T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); //使用 native 方法批量赋值元素至新数组中。 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;&#125; 你看看，底层也在用System.arraycopy()这个native方法，所以就不再赘述了，这个在ArrayList源码分析中也有涉及哦！！！！ 深浅拷贝的区别： https://blog.csdn.net/baiye_xing/article/details/71788741?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task]]></content>
      <categories>
        <category>源码分析</category>
      </categories>
      <tags>
        <tag>Array</tag>
        <tag>数组拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10.2----10.7收获]]></title>
    <url>%2F2019%2F10%2F02%2F10-2-10-7.html</url>
    <content type="text"><![CDATA[10.2 学习完ajax异步调用； 完成添加然后数据库里有数据并且能正常跳转； b站上再看看springboot+mybatis的结合； 总结博客，分别总结分页排序还要mongodb在springboot中的相关操作； 收获 springboot启动很慢的原因：未修改host目录的文件，导致每次启动都很慢。步骤 在terminal终端输入: 1$ hostname 复制下来，然后修改host文件中的localhost： 1$ sudo vim /etc/hosts 然后将自己hostname替换掉localhost 123127.0.0.1&lt;两个tab&gt;localhost 替换为$hostname255.255.255.255&lt;两个tab&gt;broadcasthost::1&lt;两个tab&gt;localhost 替换为$hostname 在js传值给controller时，一直不能将json数据传过去，原因是没有给data的值加上JSON.stringify()处理，具体错误：springboot 接收参数，解析json出现错误：was expecting ‘null’, ‘true’, ‘false’ or NaN解决方案：见—-&gt;js与controller之间传值常见问题 JQuery.ajaxajax和springboot传值 jquery中字符串转日期，日期转字符串 MyBatis主键回填策略 表中部分属性自动生成，不需要额外传入参数 先是尝试了在bean中加@value，加载配置文件中的随机数，但是发现执行一次，随机数是唯一的，这样就不能做到插入一条数据，随机数就变化； 后面发现可以在mybatis直接配置结果集，好像也不行…因为如果你用结果集将id转换为taskId,那原本bean中的id属性就会取不到值… jQuery获取html中的值获取html中的值 10.3收获使用最大的id！！！mysql插入数据后返回自增ID的方法（AUTO_INCREMENT） 10.4收获 复杂json转成多层map 12345678910111213141516171819public Map&lt;String, Object&gt; parseJSON2Map(@RequestBody String jsonStr)&#123; Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); JSONObject json = JSONObject.parseObject(jsonStr); for(Object k : json.keySet())&#123; Object v = json.get(k); if(v instanceof JSONArray)&#123; List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList&lt;Map&lt;String,Object&gt;&gt;(); Iterator it = ((JSONArray)v).iterator(); while(it.hasNext())&#123; Object json2 = it.next(); list.add(parseJSON2Map(json2.toString())); &#125; map.put(k.toString(), list); &#125; else &#123; map.put(k.toString(), v); &#125; &#125; return map; &#125; mac强制刷新缓存：command + shift + R普通刷新：command + R Springboot使用多线程 最近遇到一个需求，就是当服务器接到请求并不需要任务执行完成才返回结果，可以立即返回结果，让任务异步的去执行。开始考虑是直接启一个新的线程去执行任务或者把任务提交到一个线程池去执行，这两种方法都是可以的。但是 Spring 这么强大，肯定有什么更简单的方法，就 google 了一下，还真有呢。就是使用 @EnableAsync 和 @Async 这两个注解就 ok 了。传送门：SpringBoot非官方教程 | 第二十三篇： 异步方法 首先声明一下自己用了比较长的时间才弄出来的原因是，自己把异步方法写到了Controller层，导致在controller的另一个方法调用时无法开启异步功能，所以对此我有三点见解： 在@SpringBootApplication启动类 添加注解@EnableAsync 异步方法使用注解@Async ,返回值为void或者Future 切记一点，异步方法和调用方法一定要 写在不同的类中,如果写在一个类中，是没有效果的，至于为什么—-&gt;因为Spring像@Transaction @Async等这些都是使用了动态代理，Spring容器在初始化的时候就会将含有AOP注解的类对象替换为代理对象，再由Proxy对象去调用被增强方法，重点来了：方法里想用增强方法(第三点)则需要得到当前的Proxy对象，但如果是同一个类的话，是不会经过spring容器的，此时是真正的对象本身去调用方法，并不是代理对象，这样的话被增强方法就失效了，详情请看 -&gt;Spring的 AopContext.currentProxy()方法 解决方案 我采用的方案是，将异步方法写入service层，然后再controller层中调用，这样就经过了spring容器，调用方法时就是采用的代理对象。 第二种方法，我没有成功！！！！真的是气的不行，因为我看网上博客别人都成功了……就是用AopContext.currentProxy()这个方法,步骤为： 添加依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 在启动类中添加注释 1@EnableAspectJAutoProxy(exposeProxy=true,proxyTargetClass=true) 然后在调用方法处写： 1((TaskController)AopContext.currentProxy()).testAsy("www.baidu.com"); 按理说应该可以了的，结果吧，我一跑程序就报错，说我的哦代理怎么没暴露啊，我真是卧槽了，那他妈不是已经配置了暴露了嘛！！！我服了！！！算了，懒得搭理了…甩个跟这个原理一样的链接，学习一下：JDK动态代理给Spring事务埋下的坑！再甩一个链接，三种方法配置这个exposeProxy(然而我一个都没成功…)—-&gt;springboot中如何配置aop动态代理模式 (原创)]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019-9-26]]></title>
    <url>%2F2019%2F09%2F26%2F2019-9-26.html</url>
    <content type="text"><![CDATA[今日任务 springboot + mongodb,完成mongo在springboot中的搭建，写一个小demo 最主要的是把repository这一层写完，业务逻辑可以放到明天来写 完成过程mongodb的安装macOS下mongodb的安装与启动 Springboot下的相关注解 @DataData注解使用 @Document标注在实体类上，类似于hibernate的entity注解，标明由mongo来维护该表，collection代表表的名称。SpringBoot中MongoDB中的相关注解 @Entity等一系列Spring Data JPA注解 @DynamicUpdate、@DynamicInsert 是hibernate里面的注解，这两个注解加上之后就不会为字段值不变的字段生成sql语句，这样sql的长度就减少了提高了传输效率和执行效率，在插入和修改数据的时候,语句中只包括要插入或者修改的字段。 @Entity 标识这个实体类是一个JPA实体，告诉JPA在程序运行的时候记得生成这个实体类所对应的表~！ GeneratedValue、GenericGenerator是用于主键生成策略的，具体见JPA注解主键生成策略-UUID @Column（name = “自定义字段名”，length = “自定义长度”，nullable = “是否可以空”，unique = “是否唯一”，columnDefinition = “自定义该字段的类型和长度”）。表示对这个变量所对应的字段名进行一些个性化的设置，例如字段的名字，字段的长度，是否为空和是否唯一等等设置。 剩余注释：Spring Data JPA中常用的注解详解 @Controller（Spring Boot之 Controller 接收参数和返回数据总结(包括上传、下载文件)）Controller层的注解大概有以下一些：（主要参考：Controller层主要注解） @Controller:标注 Controller 类，处理 http 请求 @RestController:标注 Controller 类，spring 4 新加注解，相当于@Controller + @ResponseBody ，主要是为了使 http 请求返回数据格式为 json 格式，正常情况下都是使用这个注解 @RequestMapping:配置 url 映射，可以作用于类上，也可以在方法上 123456789@RestController//处理http请求，返回json格式@RequestMapping(value = "/users")//配置url，让该类下的所有接口url都映射在/users下public class UserController &#123; @RequestMapping(value = "/myInfo", method = RequestMethod.GET) public String say() &#123; return "我是张少林"; &#125;&#125; @RequestMapping 定义在类上，指定该类下的所有接口 url 映射在 /users 下，定义在方法上，指定 请求方法，可以指定GET，POST，DELETE，PUT四种标准的 Restfulapi请求方法。那么此时的接口 url 为：http://127.0.0.1:8080/users/myInfo 请求方法：GET，类上也可以不用配置 url 映射的。 @PathVariable获取 url 中的数据，我们在 url 中拼接一个字符串 {username}，类似于地址占位符，由用户请求时添加，请求获取。注意注解中的参数必须与占位符参数一致 12345678@RestController//处理http请求，返回json格式@RequestMapping(value = "/users")//配置url，让该类下的所有接口url都映射在/users下public class UserController &#123; @RequestMapping(value = "/myInfo/&#123;username&#125;", method = RequestMethod.GET) public String say(@PathVariable("username") String username) &#123; return username; &#125;&#125; @RequestParam获取请求参数值，方法随意可以设置，但是通常需求都是使用 POST 请求处理表单提交。 12345678@RestController//处理http请求，返回json格式@RequestMapping(value = "/users")//配置url，让该类下的所有接口url都映射在/users下public class UserController &#123; @RequestMapping(value = "/myInfo", method = RequestMethod.POST) public String say(@RequestParam(value = "username") String username, @RequestParam(value = "password") String password) &#123; return username + password; &#125;&#125; 假如用户输入的的uri是:http://127.0.0.1:8080/users/myInfo?username=yangweijie&amp;password=123456，那么最后返回的是yangweijie123456，如果参数不带值且添加属性required=true，则会报错，此时就建议添加一个defaultValue属性： 12345678910@RestController//处理http请求，返回json格式@RequestMapping(value = "/users")//配置url，让该类下的所有接口url都映射在/users下public class UserController &#123; @RequestMapping(value = "/myInfo", method = RequestMethod.POST) public String say(@RequestParam(value = "username",required = false,defaultValue = "张少林") String username, @RequestParam(value = "password",required = false,defaultValue = "123456") String password) &#123; return username + password; &#125;&#125; @GetMapping、@PostMapping、@DeleteMapping、@PutMapping等是RequestMapping的组合注解，根据method的不同。 @RequestHeader可以把Request请求header部分的值绑定到方法的参数上。 @CookieValue可以把Request header中关于cookie的值绑定到方法的参数上。 RequestBody该注解常用来处理Content-Type: 不是application/x-www-form-urlencoded编码的内容，例如application/json, application/xml等；它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。因为配置有FormHttpMessageConverter，所以也可以用来处理 application/x-www-form-urlencoded的内容，处理完的结果放在一个MultiValueMap&lt;String, String&gt;里，这种情况在某些特殊需求下使用，详情查看FormHttpMessageConverter api; ModelAttribute(参考：Spring MVC @ModelAttribute详解)@ModelAttribute有三种用法： 可以标注在方法上； 可以标注在方法中的参数上； 还可以和@RequestMapping一起标注在方法上；目的都是在RequestMapping之前进行model属性的注入，对RestController好像没用，因为是返回json数据，并不能将model里面的数据直接返回给页面。如果没用@RestController，则@ModelAttribute方法通常被用来填充一些公共需要的属性或数据，比如一个下拉列表所预设的几种状态，或者宠物的几种类型，或者去取得一个HTML表单渲染所需要的命令对象，比如Account等。@ModelAttribute标注方法有两种风格： 12345678910111213141516// Add one attribute// The return value of the method is added to the model under the name "account"// You can customize the name via @ModelAttribute("myAccount")@ModelAttributepublic Account addAccount(@RequestParam String number) &#123; return accountManager.findAccount(number);&#125;// Add multiple attributes@ModelAttributepublic void populateModel(@RequestParam String number, Model model) &#123; model.addAttribute(accountManager.findAccount(number)); // add more ... &#125; 在第一种写法中，方法通过返回值的方式默认地将添加一个属性，在第二种写法中，方法接收一个Model对象，然后可以向其中添加任意数量的属性，可以在根据需要，在两种风格中选择合适的一种。model里的数据会被放入到request中,页面通过request域可以获取到。 @SessionAttributes和@SessionAttribute@SessionAttributes注解的使用就是将属性放入session域内，然后可以在session域内进行相关的操作。 SpringMVC中@ModelAttribute和@SessionAttributes注解的使用 @id（好像如果表中数据字段是id，会比较特殊一点，springboot会自动检索传过来的参数是否有id，如果没有会自动添加id） Repository相关注解 Springboot的分页和排序操作由于采用的是Mongo数据库，所以先后使用了MongoTemplate()和MongoRepository(),MongoTemplate在写法上比MongoRepository 更复杂一些，但是带来更多的灵活性。对于复杂的查询操作，我们一般使用MongoTemplate，对于一些简单的查询我们会使用MongoRepository 。可以这么理解，MongoRepository 只是作为一种对于简单查询的简便操作，而MongoTemplate才是我们在做一些复杂查询时的首选。 MongoRepository自己还借用了下@Query和@Modifying注解，方便自己摆脱关键字的查询，如果需要进行更新或者删除数据，则需要在@Query之上加上@Modifing。]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS学习]]></title>
    <url>%2F2019%2F09%2F21%2FJS%E5%AD%A6%E4%B9%A0.html</url>
    <content type="text"><![CDATA[学习过程：promise—&gt;箭头函数—&gt;作用域—&gt;闭包 js基础入门篇基本语法变量提升JavaScript 引擎的工作方式是，先解析代码，获取所有被声明的变量，然后再一行一行地运行。这造成的结果，就是所有的变量的声明语句，都会被提升到代码的头部，这就叫做变量提升（hoisting）。其中，函数名同样被视为变量名，所以一样会被提升到代码头部。 标识符 不能以数字开头，可以以 _ 和 $ 开头 JavaScript 有一些保留字，不能用作标识符：arguments、break、case、catch、class、const、continue、debugger、default、delete、do、else、enum、eval、export、extends、false、finally、for、function、if、implements、import、in、instanceof、interface、let、new、null、package、private、protected、public、return、static、super、switch、this、throw、true、try、typeof、var、void、while、with、yield。 标签js允许语句前面可以带有标签，方便跳出块区域和多重循环，常与for循环、continue、break连用。 数据类型##null，布尔值 boolean如果 JavaScript 预期某个位置应该是布尔值，会将该位置上现有的值自动转为布尔值。转换规则是除了下面六个值被转为false，其他值都视为true。 undefined null false 0 NaN “”或’’（空字符串） 注意，{}和[]都是true。 数值 js中所有数都是小数，64位，其中符号位1位，表示指数的11位，剩余的52位表示精度，但是由于表示精度的最高位1.xxxx中的1被省略，所以可以表示53位精度； parseInt即将字符串转换为整数型，不管是不是字符串，都会先转换为字符串,再转为整数型，其中，parseInt可以传入两个参数，第二个参数可以是转换进制值； 字符串 想输出多行字符串，有一种利用多行注释的变通方法 12345678(function () &#123; /*line 1line 2line 3*/&#125;).toString().split('\n').slice(1, -1).join('\n')// "line 1// line 2// line 3" Base64转码 Base64 就是一种编码方法，可以将任意值转成 0～9、A～Z、a-z、+和/这64个字符组成的可打印字符。使用它的主要目的，不是为了加密，而是为了不出现特殊字符，简化程序的处理。 JavaScript 原生提供两个 Base64 相关的方法。 btoa()：任意值转为 Base64 编码 atob()：Base64 编码转为原来的值 123var string = 'Hello World!';btoa(string) // "SGVsbG8gV29ybGQh"atob('SGVsbG8gV29ybGQh') // "Hello World!" * 要将非 ASCII 码字符转为 Base64 编码，必须中间插入一个转码环节，再使用这两个方法。 12345678910function b64Encode(str) &#123; return btoa(encodeURIComponent(str));&#125;function b64Decode(str) &#123; return decodeURIComponent(atob(str));&#125;b64Encode('你好') // "JUU0JUJEJUEwJUU1JUE1JUJE"b64Decode('JUU0JUJEJUEwJUU1JUE1JUJE') // "你好" 对象 对象的引用两个变量同时指向同一个对象，其中任何一个变量添加属性，另一个变量都可以读写该属性，但是如果取消某个变量对于原对象的引用，并不会影响到另一个变量；注意:仅限于对象，如果两个变量同时指向同一个原始类型的值，那么变量此时都是值的拷贝而已； 如果行首是一个大括号，js引擎一律将其解释为代码块，如果想要表示为对象，则必须在大括号前加上一个圆括号，这种差异在eval语句（作用是对字符串求值）中体现的尤为明显； 12eval('&#123;foo: 123&#125;') // 123eval('(&#123;foo: 123&#125;)') // &#123;foo: 123&#125; 属性读取：一种使用点运算符，另外一种采用方括号运算符，注意若使用方括号运算符，键名必须放在引号里面，否则会被当做变量处理： 123456789var foo = 'bar';var obj = &#123; foo: 1, bar: 2&#125;;obj.foo // 1obj[foo] // 2 查看一个对象本身所有属性，可以使用Object.keys方法 1234567var obj = &#123; key1: 1, key2: 2&#125;;Object.keys(obj);// ['key1', 'key2'] 属性的遍历：for…in 循环: 它遍历的是对象所有可遍历（enumerable）的属性，会跳过不可遍历的属性。 它不仅遍历对象自身的属性，还遍历继承的属性。 123456789101112var obj = &#123;a: 1, b: 2, c: 3&#125;;for (var i in obj) &#123; console.log('键名：', i); console.log('键值：', obj[i]);&#125;// 键名： a// 键值： 1// 键名： b// 键值： 2// 键名： c// 键值： 3 with语句：作用是操作同一个对象的多个属性时，提供方便，但是不建议使用，因为在with语句里面没有改变作用域，导致绑定对象不明确，如果with区块内部有变量的赋值操作，必须是当前对象已经存在的属性，否则会创造一个当前作用域的全局变量。 12345678var obj = &#123;&#125;;with (obj) &#123; p1 = 4; p2 = 5;&#125;obj.p1 // undefinedp1 // 4 函数函数声明 function命令 123function print(s)&#123; console.log(s);&#125; 函数表达式(如果function后带函数名，函数名也只在函数体内部有效) 1var print = function()&#123;console.log(s)&#125;; Function构造函数 12345678910var add = new Function( 'x', 'y', 'return x + y');// 等同于function add(x, y) &#123; return x + y;&#125; 函数ToString可以解析注释实现多行字符串： 12345678910111213var multiline = function (fn) &#123; var arr = fn.toString().split('\n'); return arr.slice(1, arr.length - 1).join('\n');&#125;;function f() &#123;/* 这是一个 多行注释*/&#125;multiline(f);// " 这是一个// 多行注释" 函数作用域在ES6中，有三种作用域：全局作用域、块作用域、函数作用域。函数本身也是一个值，也有自己的作用域。它的作用域与变量一样，就是其声明时所在的作用域，与其运行时所在的作用域无关。 1234567891011var a = 1;var x = function () &#123; console.log(a);&#125;;function f() &#123; var a = 2; x();&#125;f() // 1 上面代码中，函数x是在函数f的外部声明的，所以它的作用域绑定外层，内部变量a不会到函数f体内取值，所以输出1，而不是2。 总之，函数执行时所在的作用域，是定义时的作用域，而不是调用时所在的作用域。所以说，如果函数A调用函数B,函数B不会引用函数A的内容。 参数传值 函数参数如果是原始类型的值（数值、字符串、布尔值），传递方式是传值传递； 函数参数如果是复合类型的值（数组、对象、其他函数），传递方式是传址传递； 函数内部修改的如果是参数对象的某个属性，那么会影响到原始值，如果是替换掉整个参数，则不会影响到原始值： 12345678var obj = [1, 2, 3];function f(o) &#123; o = [2, 3, 4];&#125;f(obj);obj // [1, 2, 3] arguments arguments对象包含了函数运行时的所有参数，但是它并不是一个数组，而是一个对象，所以它不能用数组中的slice和forEach方法； 在正常模式下，arguments对象可以在运行时修改，但是在严格模式下，修改arguments对象的值并不会改变真实参数的值； 如果要让arguments对象使用数组方法，真正的解决办法就是让arguments对象变为数组，常用的转换方法有两种，分别是slice方法和逐一填入新数组： 1234567var args = Array.prototype.slice.call(arguments);// 或者var args = [];for (var i = 0; i &lt; arguments.length; i++) &#123; args.push(arguments[i]);&#125; 闭包闭包，简而言之就是函数中的函数，闭包最大的特点，就是它可以“记住”诞生的环境，用处有三个： 1. 可以读取函数内部的变量； 2. 让这些变量始终保持在内存中，即闭包可以使得它诞生环境一直存在; 3. 封装对象的私有属性和私有方法。 1234567891011function createIncrementor(start) &#123; return function () &#123; return start++; &#125;;&#125;var inc = createIncrementor(5);inc() // 5inc() // 6inc() // 7 12345678910111213141516171819function Person(name) &#123; var _age; function setAge(n) &#123; _age = n; &#125; function getAge() &#123; return _age; &#125; return &#123; name: name, getAge: getAge, setAge: setAge &#125;;&#125;var p1 = Person('张三');p1.setAge(25);p1.getAge() // 25 “立即调用的函数表达式”（Immediately-Invoked Function Expression）123(function()&#123; /* code */ &#125;());// 或者(function()&#123; /* code */ &#125;)(); 通常情况下，只对匿名函数使用这种“立即执行的函数表达式”。它的目的有两个：一是不必为函数命名，避免了污染全局变量；二是 IIFE 内部形成了一个单独的作用域，可以封装一些外部无法读取的私有变量。 eval 本质是在当前域中注入代码，可以接受一个字符串作为参数，并且将该字符串当做语句执行，如果识别不是字符串，则会原样返回。 同时，eval没有自己的作用域，会改变当前域原有变量的值，当然在严格模式下，eval内部自己声明的变量有自己的作用域，不会影响到外部作用域的值。 由于引擎无法分辨eval的别名调用，所以在eval使用别名时，一律都是全局作用域。 数组数组遍历 数组遍历可以考虑使用for循环、while循环、forEach循环，会返回键值（键名为整数），for…in…循环会同时遍历非整数键（返回的是键名）； 数组遍历空位时，需要格外注意，空位和undefined是不一样的，空位表示数组中没有这个元素，所以在for…in…和forEach循环中不会被遍历到，但是undefined代表数组有这个元素，是会遍历到的； 运算符算术运算符加法运算符 对象的相加如果运算子是对象，必须先转成原始类型的值，引擎会自动调用obj.valueof().toString(),所以我们可以重新valueOf()和toString()方法即可，其中有个特例，当运算子是一个Date对象的实例，那么会优先执行toString方法。 指数运算符** 为指数运算符，是右结合的，当然，三元条件运算符也是右结合的，当然还有赋值运算符（=）也是右结合的。 比较运算符 严格相等运算符JavaScript 提供两种相等运算符：== 和 === 。简单说，它们的区别是相等运算符（==）比较两个值是否相等，严格相等运算符（===）比较它们是否为“同一个值”。如果两个值不是同一类型，严格相等运算符（===）直接返回false，而相等运算符（==）会将它们转换成同一个类型，再用严格相等运算符进行比较。tip: undefined和null与自身严格相等布尔运算符略二进制位运算符略 其他运算符，运算顺序void 运算符void运算符的作用是执行一个表达式，然后不返回任何值，或者说返回undefined，主要用途是浏览器的书签工具，以及在超链接中插入代码防止网页跳转。 语法专题错误处理机制原生错误类型 SyntaxError ReferenceError RangeError TypeError URIError EvalError 自定义错误除了 JavaScript 原生提供的七种错误对象，还可以定义自己的错误对象 1234567function UserError(message) &#123; this.message = message || '默认信息'; this.name = 'UserError';&#125;UserError.prototype = new Error();UserError.prototype.constructor = UserError; 上面代码自定义一个错误对象UserError，让它继承Error对象。然后，就可以生成这种自定义类型的错误了 1new UserError('这是自定义的错误！'); 标准库Object四个用法： Object本身就是一个函数，可以将任意值转换为对象； 不仅可以当做工具函数使用，还可以当做构造函数使用，即前面可以加new命令； 1注意，通过var obj = new Object()的写法生成新对象，与字面量的写法var obj = &#123;&#125;是等价的。或者说，后者只是前者的一种简便写法。 Object的静态方法，例如Object.print,指部署在Object对象自身的方法； Object的实例方法，即定义在Object.prototype对象上的方法。 Object.prototype.valueof() Object.prototype.toString() Object.prototype.toLocaleString() Object.prototype.hasOwnProperty() Object.prototype.isPrototypeof() Object.prototype.propertyIsEnumerable() 属性描述对象js提供了一个内部数据结构，用来描述对象的属性，比如是否可写可读可遍历，这个内部数据结构就称之为“属性描述对象”。 tip:Object.getOwnPropertyNames方法返回一个数组，成员是参数对象自身的全部属性的属性名，不管该属性是否可遍历。而这跟Object.keys的行为不同，Object.keys只返回对象自身的可遍历属性的全部属性名。 123456789101112Object.keys([]) // []Object.getOwnPropertyNames([]) // [ 'length' ]Object.keys(Object.prototype) // []Object.getOwnPropertyNames(Object.prototype)// ['hasOwnProperty',// 'valueOf',// 'constructor',// 'toLocaleString',// 'isPrototypeOf',// 'propertyIsEnumerable',// 'toString'] 上面代码中，数组自身的length属性是不可遍历的，Object.keys不会返回该属性。第二个例子的Object.prototype也是一个对象，所有实例对象都会继承它，它自身的属性都是不可遍历的。 Object.defineProperty(),Object.defineProperties()Object.defineProperty()方法接收三个参数，依次如下： Object：属性所在的对象 propertyName:字符串，表示属性名 attributesObject:属性描述对象 1234567891011var obj = Object.defineProperty(&#123;&#125;, 'p', &#123; value: 123, writable: false, enumerable: true, configurable: false&#125;);obj.p // 123obj.p = 246;obj.p // 123 Object.defineProperties()接收两个参数，如下： 123456789101112var obj = Object.defineProperties(&#123;&#125;, &#123; p1: &#123; value: 123, enumerable: true &#125;, p2: &#123; value: 'abc', enumerable: true &#125;, p3: &#123; get: function () &#123; return this.p1 + this.p2 &#125;, enumerable:true, configurable:true &#125;&#125;);obj.p1 // 123obj.p2 // "abc"obj.p3 // "123abc" Object.prototype.propertyIsEnumerable()返回一个布尔值，用来判断自身属性是否可以遍历。Object.keys是返回所有可以遍历的属性，不包括继承的属性，而Object.getOwnPropertyNames则是返回包括继承的属性在内的所有可以遍历的属性。 ​ es6入门len和const命令lenlen没有变量提升必须先声明后才能使用，注意len变量只在块区域内有用，是块级作用域，而var不同，var具有全局作用域、函数作用域； 暂时性死区123456var tmp = 123;if (true) &#123; tmp = 'abc'; // ReferenceError let tmp;&#125; 在代码块内，使用let命令声明变量之前，该变量都是不可用的。这在语法上，称为“暂时性死区”（temporal dead zone，简称 TDZ）。非常隐蔽的“死区”： 12345function bar(x = y, y = 2) &#123; return [x, y];&#125;bar(); // 报错 顶层对象的属性顶层对象，在浏览器环境指的是window对象，在 Node 指的是global对象。ES5 之中，顶层对象的属性与全局变量是等价的。但在ES6中，var命令和function命令声明的全局变量，依旧是顶层对象的属性；另一方面规定，let命令、const命令、class命令声明的全局变量，不属于顶层对象的属性。也就是说，从 ES6 开始，全局变量将逐步与顶层对象的属性脱钩。 1234567var a = 1;// 如果在 Node 的 REPL 环境，可以写成 global.a// 或者采用通用方法，写成 this.awindow.a // 1let b = 1;window.b // undefined node js入门]]></content>
      <categories>
        <category>前端学习</category>
      </categories>
      <tags>
        <tag>js</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Microsoft Remote Desktop for mac 和 Spark邮箱]]></title>
    <url>%2F2019%2F09%2F21%2F%E4%BD%BF%E7%94%A8Microsoft-Remote-Desktop-for-mac-%E5%92%8C-Spark%E9%82%AE%E7%AE%B1.html</url>
    <content type="text"><![CDATA[前言由于近期需要远程控制windows服务器，所以需要选择一个远程操控的软件，由于mac端并没有嵌入相关的功能，只能选择Microsoft Remote Desktop，并且遗憾的是，在中国区的app store并没有允许其上线，所以要做的第一步就是申请一个美区账号。 步骤 chrome打开apple id注册 按步骤注册，国家填写美国，注册完成之后需要登录一下 登录的时候会要求填写地址，以下是可用的信息街道: 4114 Sepulveda Blvd，城市: Culver City，州: CA邮编: 90230电话: (626) 339-6***商家名称: Clippinger Chevrolet Oldsmobile注意电话号码后三位随便填写几个数字 登录App Store，然后下载即可使用 Spark写这个主要是想吐槽一下，看到网上b乎各种说spark可以接收推送，即使没有打开软件，我想说简直是放屁啊，导致我查了几个小时都没能成功，根本做不到好吗！！！不信？看官网怎么说的 ——–&gt;Spark帮助中心所以只能一直开着了…垃圾的一批！！！！！！]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask + PyJWT 实现基于Json Web Token的用户认证授权]]></title>
    <url>%2F2019%2F08%2F23%2FFlask-PyJWT-%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EJson-Web-Token%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E6%8E%88%E6%9D%83.html</url>
    <content type="text"><![CDATA[前言这注定又将是一篇长文，接触到的第一个关于python的框架flask，本文将从最开始的安装flask，到与mysql结合，最后到用pyjwt和flask-jwt和restful写一个带有token的接口。 技术栈简介Flask是一个使用Python编写的轻量级Web应用框架。基于Werkzeug WSGI(PythonWeb服务器网关接口（Python Web Server Gateway Interface，缩写为WSGI)是Python应用程序或框架和Web服务器之间的一种接口，已经被广泛接受, 它已基本达成它的可移植性方面的目标)工具箱和Jinja2 模板引擎。 Flask使用BSD授权。 Flask也被称为“microframework”，因为它使用简单的核心，用extension增加其他功能。Flask没有默认使用的数据库、窗体验证工具。然而，Flask保留了扩增的弹性，可以用Flask-extension加入这些功能：ORM、窗体验证工具、文件上传、各种开放式身份验证技术 SQLAlchemyFlask是一个微型框架，自身没有提供数据库管理，表单验证，cookie处理等功能，很多功能需要通过扩展才能实现，数据库管理就需要SQLAlchemy。SqlAlchemy是Python编程语言下的一款ORM框架，该框架建立在数据库API之上，使用关系对象映射进行数据库操作，简而言之：将对象转换成SQL，然后使用数据API执行SQL并获取执行结果。 The first demo123456789from flask import Flaskapp = Flask(__name__)@app.route("/")def index(): return "hello world"if __name__ == '__main__': app.run() 第一行 from…import…就是导入flask中的Flask模块，不同于直接import，from…import简单来说就是帮我从车里拿瓶水，而import则是把车拿过来，所以二者在使用的时候还是有些区别的，在导入某个.py文件时，使用from…import…可以直接在该文件夹下使用import后的函数名，而如果是import，如果要调用函数必须用类.模块。 第二行 这行代码里有一个参数name，这个参数用到告诉flask你的application的名字，官方有一句话： 12If you are using a single module,__name__ is always the correct value.If you however are using a package, it’s usually recommended to hardcode the name ofyour package there. 意思就是说，如果是单一的应用，用name就可以了，如果是一个应用程序包，就hardcode一个名字给这个参数。比如： 1app = Flask(“myApp”) 由于目前我们的应用都相对简单，所以统一使用name作为参数。 第三行 使用route()修饰器注明通过什么样的url可以访问我们的函数，同时在函数中返回要显示在浏览器中的信息。 最后 调用run()方法，运行flask web应用程序 12if __name__ == '__main__': app.run() 其中if __name__==&#39;__main__&#39;的意思是，如果此文件是直接运行的才会执行app.run()这个方法，如果是通过import在其它py文件中调用的话是不会执行的。 比如我们修改code.py中的hello_world方法，如下： 1234567@app.route('/index')def hello_world(): if __name__=='main': return 'Hello World!' else: return "hello my name is "+__name__ 即当name为main时还是执行原来的逻辑，返回hello world，如果不是则输出此时的名字。 然后我们新建一个sub.py文件然后导入code.py，并且执行hello_world方法: 1234567import codedef CallCodeFun(): result = code.hello_world() print(result)CallCodeFun()print(__name__) 此时的name是Code而不是main，而此时，在sub.py中加一句print(name)可以发现sub.py中的name变成了main 由此我们可以得出 name 如果是 main 那么代表他是一个入口文件，直接执行的。 tip:建文件时文件名最好不要叫code，因为python有模块名就叫code。 flask-sqlalchemy 数据库配置，上demo 12345678910111213141516171819202122from flask_sqlalchemy import SQLAlchemyfrom flask import Flaskimport configparser# 告诉flask app的名字app = Flask(__name__)//通过configparser获取读取配置文件的解释器my_config = configparser.ConfigParser()my_config.read('db.conf')# dialect+driver://username:password@host:port/database?charset=utf8# 配置 sqlalchemy 数据库驱动://数据库用户名:密码@主机地址:端口/数据库?编码# py3无法继续使用sqldb，所以采用pymysqlapp.config['SQLALCHEMY_DATABASE_URI'] = 'mysql://' + my_config.get('DB', 'DB_USER') + ':' + \ my_config.get('DB', 'DB_PASSWORD') + '@' + my_config.get('DB', 'DB_HOST') + '/' + \ my_config.get('DB', 'DB_DB') # 设为True 表示每次请求结束后都会自动提交数据库的变动，但像add delete insert等仍需要commit,建议不要设为trueapp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = Truemydb = SQLAlchemy()mydb.init_app(app)if __name__ == "__main__": app.run(debug=True) 为了使代码结构更清晰，这里使用另一种配置方式，即使用单独配置文件的文件来做全局Flask配置 管控所有的配置文件conf.py: 1234567DB_USER = 'root'DB_PASSWORD = 'jwyjwy9951206-=-'DB_HOST = 'localhost'DB_DB = 'flask_migrate_demo'DEBUG = True SQLALCHEMY_TRACK_MODIFICATIONS = FalseSQLALCHEMY_DATABASE_URI = 'mysql+pymysql://' + DB_USER + ':' + DB_PASSWORD + '@' + DB_HOST + '/' + DB_DB SQLAlchemy允许我们根据数据库的表结构来创建数据模型，反之亦可。 所以我们一般无须手动的登录到数据库中使用 SQL 语句来创建表, 我们只需把数据模型定义好了之后, 表结构也就有了 新建model.py，并定义一个用户表数据模型 12345678from flask_sqlalchemy import SQLAlchemydb = SQLAlchemy()class User(db.Model):user_id = db.Column(db.Integer, primary_key=True)user_name = db.Column(db.String(60), nullable=False)user_password = db.Column(db.String(30), nullable=False)user_nickname = db.Column(db.String(50))user_email = db.Column(db.String(30), nullable=False) 新建db.py，利用Flask-Script和Flask-Migrate，搭建db和app之间的桥梁 1234567891011from flask import Flaskfrom flask_script import Managerfrom flask_migrate import Migrate, MigrateCommandfrom model import dbapp = Flask(__name__)app.config.from_object('conf')migrate = Migrate(app, db)manager = Manager(app)manager.add_command('db', MigrateCommand)if __name__ == '__main__':manager.run() Flask-Script和Flask-Migrate使用 最后，在pycharm的控制台打下三行迁移代码，这样就能在数据库中看见跟model字段一样的表了 123python db.py db initpython db.py db migratepython db.py db upgrade 示例接口实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091# 用到flask中的Flask框架，jsonify返回json数据，request将前端字段传入后端from flask import Flask, jsonify, request# 将model中的User，db对象传入from model import db, User# 告诉flask这个app要用它app = Flask(__name__)# 读取配置文件，主要是连接mysqlapp.config.from_object('conf')# 初始化db.init_app(app)# 开始路由@app.route('/')def index(): return '&lt;h1&gt;Hello Flask!&lt;/h1&gt;'# 增@app.route('/user', methods=['POST'])def addUser(): user_name = request.form.get('user_name') user_password = request.form.get('user_password') user_nickname = request.form.get('user_nickname') user_email = request.form.get('user_email') user = User(user_name=user_name, user_password=user_password, user_nickname=user_nickname, user_email=user_email) try: db.session.add(user) db.session.commit() except: db.session.rollback() db.session.flush() userId = user.user_id if (user.user_id is None): result = &#123;'msg': '添加失败'&#125; return jsonify(data=result) data = User.query.filter_by(user_id=userId).first() result = &#123;'user_id': data.user_id, 'user_name': data.user_name, 'user_nickname': data.user_nickname, 'user_email': data.user_email&#125; return jsonify(data=result)# 查@app.route('/user/&lt;int:userId&gt;', methods=['GET'])def getUser(userId): user = User.query.filter_by(user_id=userId).first() if (user is None): result = &#123;'msg': '找不到数据'&#125; else: result = &#123;'user_id': user.user_id, 'user_name': user.user_name, 'user_nickname': user.user_nickname, 'user_email': user.user_email&#125; return jsonify(data=result)# 改@app.route('/user/&lt;int:userId&gt;', methods=['PATCH'])def updateUser(userId): user_name = request.form.get('user_name') user_password = request.form.get('user_password') user_nickname = request.form.get('user_nickname') user_email = request.form.get('user_email') try: user = User.query.filter_by(user_id=userId).first() if (user is None): result = &#123;'msg': '找不到要修改的记录'&#125; return jsonify(data=result) else: user.user_name = user_name user.user_password = user_password user.user_nickname = user_nickname user.user_email = user_email db.session.commit() except: db.session.rollback() db.session.flush() userId = user.user_id data = User.query.filter_by(user_id=userId).first() result = &#123;'user_id': data.user_id, 'user_name': data.user_name, 'user_password': data.user_password, 'user_nickname': data.user_nickname, 'user_email': data.user_email&#125; return jsonify(data=result)# 查全部@app.route('/user', methods=['GET'])def getUsers(): data = User.query.all() data_all = [] for user in data: data_all.append(&#123;'user_id': user.user_id, 'user_name': user.user_name, 'user_nickname': user.user_nickname, 'user_email': user.user_email&#125;) return jsonify(users=data_all)# 删@app.route('/user/&lt;int:userId&gt;', methods=['DELETE'])def deleteUser(userId): # 删除数据 User.query.filter_by(user_id=userId).delete() db.session.commit() return getUsers()if __name__ == '__main__': app.run(debug=app.config['DEBUG']) ​ SQLAlchemy 几种查询方式总结 使用Flask-RESTful快速创建RESTful API接口先容我把项目做完…待续…….无聊看看文档：flask-restful中文官方文档 使用Flask + PyJWT 实现基于Json Web Token的用户认证授权待更…无聊看看文档：Flask + PyJWT 实现基于Json Web Token的用户认证授权 问题1.主要问题就是flask和mysql的连接问题，出现的最棘手的问题首先是mysqldb是py2的，py3没有，用pymysql代替，在SQLALCHEMY_DATABASE_URI注意修改driver驱动。2.最终修改方案是自己用一个demo建好了数据库和表(记得保存好demo)，demo如下，然后再运行就可以了。 12345678910111213141516from sqlalchemy import create_engine,Table,Column,Integer,String,MetaData,ForeignKeyengine=create_engine("mysql+pymysql://root:a5230411@localhost:3306/test",echo=True)metadata=MetaData(engine)user=Table('user',metadata, Column('id',Integer,primary_key=True), Column('name',String(20)), Column('fullname',String(40)), )address_table = Table('address', metadata, Column('id', Integer, primary_key=True), Column('user_id', None, ForeignKey('user.id')), Column('email', String(128), nullable=False) )metadata.create_all() 3.解决 sqlalchemy 报错:(1193, “Unknown system variable ‘tx_isolation’”): 传送门 参考网址 Flask + PyJWT 实现基于Json Web Token的用户认证授权 使用 Flask 设计 RESTful 的认证 使用Flask-RESTful快速创建RESTful API接口 Flask + flask-jwt 实现基于Json Web Token的用户认证授权 Flask程序目录结构——构建可扩展的Flask应用程序 Python的flask：models.py来创建mysql数据库 python使用sqlalchemy连接mysql数据库 深入浅出Flask(学习教程) 深入浅出理解Python装饰器 flask-restful中文官方文档]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>flask</tag>
        <tag>restful</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java核心技术]]></title>
    <url>%2F2019%2F08%2F20%2FJava%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF.html</url>
    <content type="text"><![CDATA[第一章Java的11个关键术语 简单性 面向对象 分布式 健壮性 安全性 体系结构中立 可移植性 解释型 高性能 多线程 动态性 Java applet在网页中运行的Java程序称为applet 第二章部分术语 JDK(java development kit):java开发工具包 JRE(Java Runtime Environment):运行java程序用户所使用的软件 SDK(Software Development Kit):软件开发包 安装过程MAC中JAVA环境变量配置以及Intellij IDEA如何配置JDKIDEA的校园邮箱激活方式IntelliJ IDEA 常用快捷键 之 Mac 版Intellij IDEA快捷生成常用代码 琐碎知识点 0x1.0p-3:0x表示16进制，使用p表示指数，所以就是1.0*2的-3次方 三个特殊的浮点数值：正无穷大(Double.POSITIVE_INFINITY)、负无穷大(Double.NEGATIVE_INFINITY)、NaN(Double.NaN) 浮点数值不适用于无法接受舍入误差的计算中，要想完全没有误差，需要使用BigDecimal类 部分特殊字符的转义序列：\b 退格 \t 制表 \n 换行 \r 回车 \“双引号 \反斜杠 强烈建议不要在程序中使用char类型，因为unicode早已经超过了65536个 第三章部分术语 类常量：static final,类常量的定义在main方法的外部 整数被0除将会产生异常，浮点数被0除将会得到无穷大或NaN结果 public static strictfp void main(String[] args)，在该main函数中所有指令都将使用严格的浮点计算 condition ? expression1 : expression2 eg: x&lt;y ? x : y 位运算符：&amp;、|、^、~拓展：奇数个数，两个两个成对最有一个单独，如何找出这个单独的数字是几，复杂度在O(n)以内：用异或的思想即可，全部异或 移位运算符: &lt;&lt;(算数移位)&gt;&gt; &gt;&gt;&gt;(逻辑右移),其中&gt;&gt;&gt;会用0填充高位，移位运算符的右操作数要满足模32的运算(如果左边操作数是long则要满足模64)，1&lt;&lt;35 == 1&lt;&lt;3 == 8 枚举类型：enum Size{SMALL,MEDIUM,LARGE } 字符串 子串: 12String greeting = "Hello";String s = greeting.substring(0,3); 拼接:直接用+连接、如果需要多个字符串放在一起，并且用一个定界符分割，可以采用静态join方法： 12String all = String.join("/","S","M","L","XL")// string: "S/M/L/XL" java中不允许修改字符，但是可以修改字符串变量*字符串比较用equals方法，例如：”Hello”.equals(greeting),”Hello”.equalsIgnoreCase(“hello”)「无视大小写的比较」 java.lang.string部分apiString类常用方法之charAt()、codePointAt()示例offsetByCodePoints()与codePointAt() 注意lastIndexOf(int cp,int fromindex)，是返回从index位置开始找到的第一个符合的字符串的位置 12345678910111213141516public int lastIndexOf(int ch, int fromIndex) &#123; if (ch &lt; Character.MIN_SUPPLEMENTARY_CODE_POINT) &#123; // handle most cases here (ch is a BMP code point or a // negative value (invalid code point)) final char[] value = this.value; int i = Math.min(fromIndex, value.length - 1); for (; i &gt;= 0; i--) &#123; if (value[i] == ch) &#123; return i; &#125; &#125; return -1; &#125; else &#123; return lastIndexOfSupplementary(ch, fromIndex); &#125; &#125; input Scanner在util包中，当使用的类不在java.lang中时，都需要导包 1234567Scanner sc = new Scanner(System.in); String nextLine = sc.nextLine(); System.out.println("please input your name:" + nextLine); int nextInt = sc.nextInt(); System.out.println("please input your age:" + nextInt);String next = sc.next();System.out.println("please input your name:" + next);]]></content>
      <categories>
        <category>Java 基础</category>
      </categories>
      <tags>
        <tag>java笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka]]></title>
    <url>%2F2019%2F08%2F18%2Fkafka.html</url>
    <content type="text"><![CDATA[初识kafkakafka概念Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据,是用于构建实时数据管道和流应用程序。 四个术语 TopicKafka将消息种子(Feed)分门别类，每一类的消息称之为一个主题(Topic)。 Producer发布消息的对象称之为主题生产者(Kafka topic producer)。生产者发布消息时要选定Topic上的分区。 Consumer订阅消息并处理发布的消息的种子的对象称之为主题消费者(consumers)。一般消费者模型可以分为两类：队列和发布-订阅式。队列的处理方式就是一条消息只有一个消费者知道并处理，发布-订阅的处理方式则是消息被所有人都知道，所有人都可以来处理该消息，kafka为这两种模型提供了单一抽象模型：消费者组（cosumer group）。每个消费者都有一个组名，当所有人的组名都不一样的时候，这个时候就是发布-订阅模式，因为消息要按照消费者组为单位发出的，这意味着此时每个消费者都会收到消息并可以进行处理；当所有人的消费者组名字都一样时，意味着只会有一个消费者收到消息并可以进行处理，此时就是队列模式了。 Broker已发布的消息保存在一组服务器中，称之为Kafka集群。集群中的每一个服务器都是一个代理(Broker). 消费者可以订阅一个或多个主题（topic），并从Broker拉数据，从而消费这些已发布的消息。 三个关键能力 发布和订阅消息流，在这方面，它类似于一个消息队列或企业消息系统 以容错的方式存储消息（流) 在消息流发生时处理他们 四个核心API 应用程序使用 Producer API 发布消息到1个或多个topic（主题）。 应用程序使用 Consumer API 来订阅一个或多个topic，并处理产生的消息 应用程序使用 Streams API 充当一个流处理器，从1个或多个topic消费输入流，并生产一个输出流到1个或多个输出topic，有效地将输入流转换到输出流。 Connector API允许构建或运行可重复使用的生产者或消费者，将topic连接到现有的应用程序或数据系统。例如，一个关系数据库的连接器可捕获每一个变化。 kafka安装过程 kafka的安装 1brew install kafka 安装会依赖zookeeper 注意：安装目录：/usr/local/Cellar/kafka/0.10.2.0 安装的配置文件位置/usr/local/etc/kafka/server.properties/usr/local/etc/kafka/zookeeper.properties 启动zookeeper 1zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp; 启动kafka 1kafka-server-start /usr/local/etc/kafka/server.properties &amp; 创建topic让我们使用单个分区和只有一个副本创建一个名为“test”的主题 1kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看创建的topic我们现在可以看到该主题，如果我们运行list topic命令： 1kafka-topics --list --zookeeper localhost:2181 发送消息Kafka提供了一个命令行客户端，它将从文件或标准输入接收输入，并将其作为消息发送到Kafka集群。默认情况下，每行都将作为单独的消息发送。运行生产者，然后在控制台中键入一些消息发送到服务器。 1kafka-console-producer --broker-list localhost:9092 --topic test 消费消息Kafka还有一个命令行消费者，将消息转储到标准输出。 1kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning TipQ:安装kafka出现错误： 1234kafka: Java 1.8 is required to install this formula.Install AdoptOpenJDK 8 with Homebrew Cask: brew cask install homebrew/cask-versions/adoptopenjdk8Error: An unsatisfied requirement failed this build. A: 1brew cask install homebrew/cask-versions/adoptopenjdk8 bilibili学习kafkahttps://www.bilibili.com/video/av36607048?p=2 zookeeper 默认是前台启动(关闭了控制台停止)，可以在最前面加上 nohup，这样就可以后台启动了。 三个目录 bin目录1cd usr/local/Cellar/kafka/2.2.1/bin * config文件 1/usr/local/etc/kafka/server.properties * 日志 1234############################# Log Basics ############################## A comma separated list of directories under which to store log fileslog.dirs=/usr/local/var/lib/kafka-logs 删除topic1kafka-topics --delete --zookeeper localhost:2181 --topic test]]></content>
      <categories>
        <category>中间件系列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python实践winrm，实现远程连接调度Windows服务器]]></title>
    <url>%2F2019%2F08%2F17%2Fpython%E5%AE%9E%E8%B7%B5winrm%EF%BC%8C%E5%AE%9E%E7%8E%B0%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E8%B0%83%E5%BA%A6Windows%E6%9C%8D%E5%8A%A1%E5%99%A8.html</url>
    <content type="text"><![CDATA[前言最近有个需求：用自己主机控制多个windows节点，实时获取他们的cpu和运存占用情况，通过查询，决定采用python中的winrm。Tip:winrm服务是windows 一种方便远程管理的服务；开启winrm service,便于在日常工作中，远程管理服务器，或通过脚本，同时管理多台服务器，来提高工作效率。 常规做法配置远程主机,使其支持被远程控制 查看winrm service listener（分为http和https）:1winrm e winrm/config/listener 如果没有返回，则没有开启winrm服务。 开启winrm服务： 1winrm quickconfig 为winrm service 配置auth: 1winrm set winrm/config/service/auth @&#123;Basic="true"&#125; 为winrm service 配置加密方式为允许非加密： 1winrm set winrm/config/service @&#123;AllowUnencrypted="true"&#125; 查看winrm服务的配置： 1winrm get winrm/config 客户端主机 装winrm 1pip install pywinrm 测试 1234import winrmwintest = winrm.Session('http://47.98.149.160:5985/wsman',auth=('Administrator','Jwy12345a'))ret = wintest.run_cmd('ipconfig')print(ret) 遇到的问题及解决措施 在测试环节，导入winrm包后，控制台显示：winrm has no attribute session 解决办法：pip导错了包，应该是pywinrm这个包，将pip原来的包删除，重新导入一遍。 pycharm问题：module ‘pip’ has no attribute ‘main’ 解决办法：pip升级导致无法导包 测试的时候，报出以下错误： 1requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='47.98.149.160', port=5985): Max retries exceeded with url: /wsman (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPConnection object at 0x104921650&gt;, 'Connection to 47.98.149.160 timed out. (connect timeout=30)')) 解决办法：由于云服务器有公网ip地址，所以本机可以ping通服务器，但是由于本机不存在公网地址，是局域网地址，所以服务器不能ping通主机，导致二者不能正常通信，只能换一种方法，也就是在云服务器端运行程序，程序提供了当前cpu运行情况和运存情况，本机可以随时访问并得到信息。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import timeimport psutil # cd C:\Python36-32\Scripts pip install psutil# 获取本机磁盘使用率和剩余空间G信息import pymysqlconn = pymysql.connect(user="root", passwd="No.93329332", host="134.175.5.88", port=3306, db="node_status")cur = conn.cursor()flag = 0while(True): # 循环磁盘分区 content = "" for disk in psutil.disk_partitions(): # 读写方式 光盘 or 有效磁盘类型 if 'cdrom' in disk.opts or disk.fstype == '': continue disk_name_arr = disk.device.split(':') disk_name = disk_name_arr[0] disk_info = psutil.disk_usage(disk.device) # 磁盘剩余空间，单位G free_disk_size = disk_info.free // 1024 // 1024 // 1024 # 当前磁盘使用率和剩余空间G信息 if (disk_name == 'C'): disk_c_name = disk_name disk_c_info = disk_info free_c_disk_size = free_disk_size info = "%s盘使用率：%s%%， 剩余空间：%iG" % (disk_c_name, str(disk_c_info.percent), free_c_disk_size) # print(info) # print(disk_name) if (disk_name == 'C'): # 拼接多个磁盘的信息 content = content + info print(content) # return content # cpu信息 cpu_percent = psutil.cpu_percent(interval=1) cpu_info = "CPU使用率：%i%%" % cpu_percent print(cpu_info) # return cpu_info # 内存信息 virtual_memory = psutil.virtual_memory() used_memory = virtual_memory.used / 1024 / 1024 / 1024 free_memory = virtual_memory.free / 1024 / 1024 / 1024 memory_percent = virtual_memory.percent memory_info = "内存使用：%0.2fG，使用率%0.1f%%，剩余内存：%0.2fG" % (used_memory, memory_percent, free_memory) print(memory_info) # return memory_info now = time.asctime() if(flag == 0): sql = "INSERT INTO status(ip, \ disk_percent, disk_freesize, CPU_percent, memory_used,memory_free,memory_percent,mytime) \ VALUES ('%s', '%s', '%s', '%s', '%s','%s','%s','%s')" % \ ('106.13.70.159', str(disk_c_info.percent) + "%", str(free_c_disk_size) + "G", str(cpu_percent) + "%",str(used_memory)[:3] + "G",str(free_memory)[:3] + "G",str(memory_percent) + "%",now) flag = 1 else: used_memory_float = str(used_memory)[:3] + 'G' free_memory_folat = str(free_memory)[:3] + 'G' disk_c_info_percent_str = str(disk_c_info.percent) + '%' free_c_disk_size_str = str(free_c_disk_size) + 'G' cpu_percent_str = str(cpu_percent) + '%' memory_percent_str = str(memory_percent) + '%' sql = "update status set disk_percent = '%s', disk_freesize = '%s', \ CPU_percent = '%s', memory_used = '%s' , memory_free = '%s' ,\ memory_percent = '%s' , mytime = '%s' where ip = '106.13.70.159'" % \ (disk_c_info_percent_str,free_c_disk_size_str,cpu_percent_str,used_memory_float,free_memory_folat,memory_percent_str,now) flag = 2 try: # 执行sql语句 cur.execute(sql) # 执行sql语句 conn.commit() if(flag==1): print("insert ok") else: print("update ok") except Exception as e: # 发生错误时回滚 conn.rollback() print("failed") print(e) time.sleep(60)# 关闭数据库连接conn.close() sql中update也可以 1sql = "update table set ss = '%s'where id = '1'"% disk]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>winrm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo的NexT主题---从入门到入土]]></title>
    <url>%2F2019%2F08%2F15%2F%E5%9F%BA%E4%BA%8EHexo%E7%9A%84NexT%E4%B8%BB%E9%A2%98-%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F.html</url>
    <content type="text"><![CDATA[前言本篇文章主要是针对nexT主题的优化说明，使用hexo其他主题的童靴请绕道~~整理一下大概优化：1.标题部分优化，颜色样式；2.设置博客文章连接为year/month/day/title.html格式3.Menu增加关于、标签、分类、互动、搜索菜单4.禁用关于、标签、分类菜单评论功能5.添加RSS6.设置背景图片7.Canvas_nest动态背景8.图片快速加载设置9.微信支付宝打赏功能10.点击出现桃心效果11.主页文章添加阴影效果12.设置代码高亮13.顶栏背景色14.底栏背景色15.修改文章内链接文本样式16.修改文章底部标签样式17.在文章末尾添加“文章结束”标记18.设置头像19.网站底部加上访问量20.网站底部字数统计21.网站底部添加网站运行时间22.网站底部添加动态桃心23.底部隐藏由Hexo强力驱动、主题–NexT.Mist24.设置网站的图标Favicon25.实现文章文字统计功能和阅读时长26.加来必力云跟帖功能27.去掉底部重复字数统计28.修改字体大小29.侧边栏社交小图标设置30.添加侧栏推荐阅读31.修改侧边栏背景图片32.添加侧边栏音乐33.修改侧边栏文字颜色34.在文章底部增加版权信息35.Hexo博客添加站内搜索36.修改选中字符的颜色37.添加aplay音乐播放38.添加博客左下角门神(看门🐶)39.增加了3D库three_waves，默认关闭40.增加了canvas页面丝带(话说这玩意真的很吃cpu，一开我的mac就铁板烧了)41.增加了首页pace加载进度42.增加图片懒加载lazyload43.增加了fancybox44.增加了fastclick解决延迟问题45.增加了gulp压缩网页css js样式46.截断首页文章内容47.代码复制功能 hexo目录结构主目录结构 _config.yml全局配置文件，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.jsonhexo框架的参数和所依赖插件。 scaffoldsscaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new ‘title’）的时候，hexo是根据这个目录下的文件进行构建的。基本不用关心。 source这个目录很重要，新建的文章都是在保存在这个目录下的._posts，需要新建的博文都放在_posts目录下。_posts目录下是一个个 markdown 文件。你应该可以看到一个 hello-world.md 的文件，文章就在这个文件中编辑。_posts 目录下的md文件，会被编译成html文件，放到 public （此文件现在应该没有，因为你还没有编译过）文件夹下。里面还有一个重要文件夹：images，是用来存放博文的图片的，这里建议使用七牛云图床来存储图片，因为可以更快的加载图片。同时强烈建议大家用一款hexo—client来进行可视化博客的书写，一站式服务，支持七牛云图床，接好传送门：hexoclient。 themes网站主题目录，我用的就是nexT主题。 node_moduleshexo依赖的组件都在其中。 主题目录结构12345678910111213141516171819202122232425262728293031323334353637├── .github #git信息├── languages #多语言| ├── default.yml #默认语言| └── zh-Hans.yml #简体中文| └── zh-tw.yml #繁体中文├── layout #布局，根目录下的*.ejs文件是对主页，分页，存档等的控制| ├── _custom #可以自己修改的模板，覆盖原有模板| | ├── _header.swig #头部样式| | ├── _sidebar.swig #侧边栏样式| ├── _macro #可以自己修改的模板，覆盖原有模板| | ├── post.swig #文章模板| | ├── reward.swig #打赏模板| | ├── sidebar.swig #侧边栏模板| ├── _partial #局部的布局| | ├── head #头部模板| | ├── search #搜索模板| | ├── share #分享模板| ├── _script #局部的布局| ├── _third-party #第三方模板| ├── _layout.swig #主页面模板| ├── index.swig #主页面模板| ├── page #页面模板| └── tag.swig #tag模板├── scripts #script源码| ├── tags #tags的script源码| ├── marge.js #页面模板├── source #源码| ├── css #css源码| | ├── _common #*.styl基础css| | ├── _custom #*.styl局部css| | └── _mixins #mixins的css| ├── fonts #字体| ├── images #图片| ├── uploads #添加的文件| └── js #javascript源代码├── _config.yml #主题配置文件└── README.md #用GitHub的都知道 高度定制优化篇标题部分优化，颜色样式首先你要确定你是主题中的哪个scheme：Muse、Mist、Pisces、Gemini，我的是Mist,所以我就去themes/nexT/source/css/_schemes/Mist/_header.styl下，添加以下代码：12345//可以加图片，也可以自己调色彩.header &#123; //background: url('/images/headbg.jpg') repeat !important; background: rgba(#EBF2EA,1) none repeat scroll !important; &#125; 设置背景图片默认禁用，可以在themes/nexT/source/css/_custom/custon.styl文件中启用12345678//设置背景图片body &#123; background:url(http://pw5u1sbg2.bkt.clouddn.com/145676.jpg); background-repeat: repeat; background-attachment:fixed; background-position:50% 50%; background-size:cover;&#125; Canvas_nest动态背景背景的几何线条是采用的nest效果, 一个基于html5 canvas绘制的网页背景效果, 非常赞！来自github的开源项目canvas-nest 特性 不依赖任何框架或者内库，比如不依赖jQuery，使用原生的javascript 非常小，只有1.66kb，如果开启gzip，可以更小 非常容易实现，配置简单，即使你不是web开发者，也能简单搞定说明 color：线条颜色，默认： ‘0,0,0’ ；三个数字分别为(R,G,B)，这里推荐一个颜色采集器：ColorSlurp，在AppStore中就可以搜到 opacity：线条透明度（0~1），默认: 0.5 count：线条的总数量， 默认：150 zIndex：背景的z-index属性，css属性用于控制所在层的位置，默认：-1不足内存占用过高做法直接找到主题配置文件，将canvas改为true即可。RSS在你的hexo站点目录下：1$ npm install hexo-generator-feed --save 打开站点目录下的_config.yml:12345678# feed# Dependencies: https://github.com/hexojs/hexo-generator-feedfeed: type: atom path: atom.xml limit: 20 hub: content: 来必力评论系统等第三方插件登录 来必力 ，获取LiveRe UID，编辑主题配置文件，编辑livere_uid字段，将id填入即可，更多第三方集成插件可见：nexT官网，包括评论系统、数据统计与分析、内容分享服务、搜索服务等等。 打赏功能nexT集成了支付宝微信打赏功能，直接在主题配置文件搜索wechat或者alipay即可,然后跟上自己的微信和支付宝的收钱码图片地址即可。 点击出现桃心效果1!function(e,t,a)&#123;function n()&#123;c(".heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;"),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText="left:"+d[e].x+"px;top:"+d[e].y+"px;opacity:"+d[e].alpha+";transform:scale("+d[e].scale+","+d[e].scale+") rotate(45deg);background:"+d[e].color+";z-index:99999");requestAnimationFrame(r)&#125;function o()&#123;var t="function"==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement("div");a.className="heart",d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement("style");a.type="text/css";try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName("head")[0].appendChild(a)&#125;function s()&#123;return"rgb("+~~(255*Math.random())+","+~~(255*Math.random())+","+~~(255*Math.random())+")"&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document); 新建 clicklove.js 文件并且将以上代码复制进去，然后保存。 将 clicklove.js文件放到路径 /themes/next/source/js/src 里面 然后打开 \themes\next\layout\_layout.swig 文件,在末尾（在前面引用会出现找不到的bug）添加以下代码：12&lt;!-- 页面点击小红心 --&gt;&lt;script type="text/javascript" src="/js/src/clicklove.js"&gt;&lt;/script&gt; 主页文章添加阴影效果打开 themes/next/source/css/_custom/custom.styl，向里面加代码:12345678// 主页文章添加阴影效果.post &#123; margin-top: 0px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5);&#125; 代码高亮直接在主题配置文件搜索 highlight_theme：1234# Code Highlight theme# Available values: normal | night | night eighties | night blue | night bright# https://github.com/chriskempson/tomorrow-themehighlight_theme: night bright 顶部底部背景色改变首先你要确定你是主题中的哪个scheme：Muse、Mist、Pisces、Gemini，我的是Mist,所以我就themes/nexT/source/css/_schemes/Mist/_header.styl下，将颜色进行修改即可。底部颜色则在themes/nexT/source/css/_schemes/Mist/index.styl中进行修改：123456789101112131415161718// Footer// --------------------------------------------------.footer &#123; margin-top: 80px; padding: 10px 0; //background: url('/images/headbg.jpg') repeat !important; background: rgba(#EBF2EA,1) none repeat scroll !important; color: $grey-dim;&#125;.footer-inner &#123; margin: 0 auto; text-align: left; +mobile() &#123; width: auto; text-align: center; &#125;&#125; 修改文章内文本连接样式打开 themes/next/source/css/_custom/custom.styl,添加代码：1234567891011// 文章内链接文本样式.post-body p a&#123; color: #0593d3; border-bottom: none; border-bottom: 1px solid #0593d3; &amp;:hover &#123; color: #fc6423; border-bottom: none; border-bottom: 1px solid #fc6423; &#125;&#125; 修改网页底部 在图标库中找到你自己喜欢的图标, 修改桃心,打开 themes/next_config.yml ,搜索关键字 authoricon,替换图标名： 12# icon between year and author @Footerauthoricon: id-card 隐藏网页底部 Hexo 强力驱动 打开主题配置文件,搜索关键字 copyright ，如下: 12# Footer `powered-by` and `theme-info` copyrightcopyright: false 添加文章结束标记在themes/next/layout/_macro/post.swig中, 在wechat-subscriber.swig之前添加如下代码:123&lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;---------------- The End ----------------&lt;/div&gt; 统计功能，统计功能,显示文章字数统计,阅读时长,总字数在站点的根目录下：1$ npm i --save hexo-wordcount 打开 themes/next/_config.yml ，搜索关键字 post_wordcount：1234567891011# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true #字数统计 wordcount: true #预览时间 min2read: true #总字数,显示在页面底部 totalcount: true separated_meta: true 设置头像打开 themes/next/_config.yml ，搜索关键字 avatar：123456789101112# Sidebar Avataravatar: # In theme directory (source/images): /images/avatar.gif # In site directory (source/uploads): /uploads/avatar.gif # You can also use other linking images. url: http://pw5u1sbg2.bkt.clouddn.com/avatar.png #/images/avatar.png # If true, the avatar would be dispalyed in circle. rounded: true # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false 底部添加访问量和字数统计运用第三方插件，见nexT官网，推荐百度统计和不蒜子统计。 底部添加运行时间找到\themes\next\layout\_partials\下面的footer.swig文件，在末尾添加所示代码：12345678910111213141516171819&lt;div class="run_time" style=" text-align:center;"&gt; &lt;span id="timeDate"&gt;载入天数...&lt;/span&gt;&lt;span id="times"&gt;载入时分秒...&lt;/span&gt; &lt;script&gt; var now = new Date(); function createtime() &#123; var grt= new Date("08/06/2019 19:00:00");//此处修改你的建站时间或者网站上线时间 now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 )&#123;hnum = "0" + hnum;&#125; minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 )&#123;mnum = "0" + mnum;&#125; seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 )&#123;snum = "0" + snum;&#125; document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; &#125; setInterval("createtime()",250); &lt;/script&gt;&lt;/div&gt; 底部添加红心打开 themes/next/_config.yml ，搜索关键字 footer：123456789101112131415footer: # Specify the date when the site was setup. If not defined, current year will be used. since: 2019 # visitors count counter: true # Icon between year and copyright info. icon: # Icon name in fontawesome, see: https://fontawesome.com/v4.7.0/icons/ # `heart` is recommended with animation in red (#ff0000). name: heart # If you want to animate the icon, set it to true. animated: true # Change the color of icon, using Hex Code. color: "#ff0000" 添加侧边栏音乐框去往网易云音乐搜索喜欢的音乐，点击生成外链播放器， 复制代码直接放到博文末尾即可，height设为0可隐藏播放器，但仍然可以播放音乐，auto设成0可手动播放，默认是1自动播放，可把代码放到themes/next/layout/_custom/sidebar.swig文件里，播放器会显示在站点预览中,示例代码如下：12&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=110 src="//music.163.com/outchain/player?type=0&amp;id=2588481240&amp;auto=1&amp;height=90"&gt;&lt;/iframe&gt; 添加aplayer音乐播放由于我想在单独的页面加入歌单，所以额外创了个页面，也可以直接在文章中插入，原理都是一样的。 新建页面，命名为guestbook： 1hexo new page guestbook 这时候在 /Hexo/source 文件夹下会生成一个guestbook文件夹，打开里面的index.md，示例如下： 123456789101112---title: 留言互动date: 2019-08-15 12:18:09type: "guestbook"---&lt;div align="center"&gt; &lt;img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1558956326532&amp;di=82cc9907fc903cfb978a35206986d3f6&amp;imgtype=0&amp;src=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20160809%2F31283a3e2d7f411492d3fb27297180ec_th.jpg" /&gt;&lt;/div&gt;[//]: #(aplay音频播放https://github.com/MoePlayer/hexo-tag-aplayer)&#123;% meting "2331951308" "netease" "playlist" "autoplay" "mutex:false" "order:random" "listmaxheight:250px" "preload:none" "theme:#f7f7f7"%&#125; meting中配置参数含义如下： 选项 默认值 描述 id 必须值 歌曲 id / 播放列表 id / 相册 id / 搜索关键字 server 必须值 音乐平台: netease, tencent, kugou, xiami, baidu type 必须值 song, playlist, album, search, artist fixed false 开启固定模式 mini false 开启固定模式 loop all 列表循环模式：all, one,none order list 列表播放模式： list, random volume 0.7 播放器音量 lrctype 0 歌词格式类型 listfolded false 指定音乐播放列表是否折叠 storagename metingjs LocalStorage 中存储播放器设定的键名 autoplay true 自动播放，移动端浏览器暂时不支持此功能 mutex true 该选项开启时，如果同页面有其他 aplayer 播放，该播放器会暂停 theme #ad7a86 播放器风格色彩设置 listmaxheight 340px 播放列表的最大长度 preload auto 音乐文件预载入模式，可选项： none, metadata, auto 打开主题_config.yml文件，在menu下新建一个名为guestbook的类，完成后如下所示： 1234567menu: home: / || home tags: /tags/ || tags categories: /categories/ || bookmark archives: /archives/ || archive about: /about/ || user 互动: /guestbook/ || comments 打开/Hexo/themes/hexo-theme-next/languages/zh-Hans.yml，添加对应的中文翻译： 12menu: guestbook: 互动 至此，歌单页面创建完成，现在只需要在站点配置文件中开启meting模式，在_config.yml中搜索meting： 12aplayer: meting: true Tip同一个歌单不能做到实时刷新，需要24小时后aplayer才会自动更新缓存 添加博客左下角门神安装依赖:12npm install --save hexo-helper-live2dnpm install --save live2d-widget-model-wanko 站点配置添加:12345678910111213141516171819202122# hexo-helper-live2d配置, 参考https://github.com/EYHN/hexo-helper-live2d/blob/master/README.zh-CN.mdlive2d: enable: true scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false debug: false model: scale: 1 use: live2d-widget-model-wanko display: superSample: 2 # 超采样等级 width: 100 height: 100 position: left # 位置 mobile: show: false react: opacityDefault: 0.9 # 默认透明度 opacityOnHover: 0.5 # 鼠标移上透明度 增加首页pace加载进度打开主题配置文件：12345678# Progress bar in the top during page loading.# Dependencies: https://github.com/theme-next/theme-next-pacepace: true# Themes list:# pace-theme-big-counter | pace-theme-bounce | pace-theme-barber-shop | pace-theme-center-atom# pace-theme-center-circle | pace-theme-center-radar | pace-theme-center-simple | pace-theme-corner-indicator# pace-theme-fill-left | pace-theme-flash | pace-theme-loading-bar | pace-theme-mac-osx | pace-theme-minimalpace_theme: pace-theme-bounce 增加图片懒加载lazyload打开主题配置文件：123# Vanilla JavaScript plugin for lazyloading images.# Dependencies: https://github.com/theme-next/theme-next-jquery-lazyloadlazyload: true 增加fancybox和fastclick解决延迟问题打开主题配置文件：123456789# Fancybox. There is support for old version 2 and new version 3.# Choose only one variant, do not need to install both.# To install 2.x: https://github.com/theme-next/theme-next-fancybox# To install 3.x: https://github.com/theme-next/theme-next-fancybox3fancybox: true# Polyfill to remove click delays on browsers with touch UIs.# Dependencies: https://github.com/theme-next/theme-next-fastclickfastclick: true 截断首页文章内容打开主题配置文件，搜索auto_excerpt：12345# Automatically Excerpt (Not recommend).# Use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 代码复制功能 复制该网页的代码，传送门：clipboard.min.js，然后在themes\next\source\js\src下新建clipboard.min.js文件，将以上内容复制进去即可； 在themes\next\source\js\src目录下，创建clipboard-use.js，文件内容如下： 123456789101112131415161718 /*页面载入完成后，创建复制按钮*/ !function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = ''; copyHtml += '&lt;button class="btn-copy" data-clipboard-snippet=""&gt;'; //fa fa-globe可以去字体库替换自己想要的图标copyHtml += ' &lt;i class="fa fa-clipboard"&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;'; copyHtml += '&lt;/button&gt;'; $(".highlight .code pre").before(copyHtml); new ClipboardJS('.btn-copy', &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode(); &#125;(window, document); 在themes\next\source\css\_custom\custom.styl样式文件中添加下面代码： 123456789101112131415161718192021222324252627282930313233343536//代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 在themes\next\layout\_layout.swig文件中，添加引用（注：在 swig 末尾或 body 结束标签（L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"superSample":1,"width":250,"height":300,"position":"left"},"mobile":{"show":false},"react":{"opacityDefault":0.9,"opacityOnHover":0.5},"log":false});）之前添加）： 123&lt;!-- 代码块复制功能 --&gt;&lt;script type="text/javascript" src="/js/src/clipboard.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="/js/src/clipboard-use.js"&gt;&lt;/script&gt; 尝试了但未实现的功能1.用mob_share的app_key未能实现share的功能，经过其官方人员电话沟通，已经不再支持pc端博客服务2.aplay音乐播放歌单，不能做到实时更新歌单，究其原因是其api设置了24小时缓存，同一个歌单24小时内不会发生改变 非常有益的网址 NexT主题进阶 基于Hexo的个人博客搭建(进阶版) Hexo+Next主题优化]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>NexT</tag>
        <tag>功能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer算法题]]></title>
    <url>%2F2019%2F08%2F12%2F%E5%89%91%E6%8C%87offer%E7%AE%97%E6%B3%95%E9%A2%98.html</url>
    <content type="text"><![CDATA[二维数组中的查找题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 解题思路要求时间复杂度 O(M + N)，空间复杂度 O(1)。其中 M 为行数，N 为列数。该二维数组中的一个数，小于它的数一定在其左边，大于它的数一定在其下边。因此，从右上角开始查找，就可以根据 target 和当前元素的大小关系来缩小查找区间，当前元素的查找区间为左下角的所有元素。 My code1234567891011121314151617# -*- coding:utf-8 -*-class Solution: # array 二维列表 def Find(self, target, array): # write code here row = len(array) col = len(array[0]) c = col - 1 r = 0 while(r &lt;= row-1 and c &gt;=0): if(target == array[r][c]): return True elif(target &gt; array[r][c]): r+=1 else: c-=1 return False 替换空格题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 解题思路将字符串复制给一个新的列表，碰到空格则替换，没有则copy原字符串，最后对列表进行join，返回字符串。 易错点 My code12345678910111213141516171819202122# -*- coding:utf-8 -*-class Solution: # s 源字符串 def replaceSpace(self, s): # write code here t = len(s) j = 0 m = list(s) for i in range(0,t): if(s[i] == ' '): m[j] = '%20' j+=1 else: m[j] = s[i] j+=1 pass return ''.join(m)# s = 'I want to sleep'# solution = Solution()# ss = solution.replaceSpace(s)# print(ss) 从尾到头打印链表题目描述输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 解题思路知识点补充python中链表的操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class Node(): '创建节点' def __init__(self,data): self.data = data self.next = None class LinkList(): '创建列表' def __init__(self, node): '初始化列表' self.head = node self.head.next = None self.tail = self.head def add_node(self, node): '添加节点' self.tail.next = node self.tail = self.tail.next def view(self): '查看列表' node = self.head link_str = '' while node is not None: if node.next is not None: link_str += str(node.data) + '--&gt;' else: link_str += str(node.data) node = node.next print ('The Linklist is:' + link_str) def length(self): '列表长度' node = self.head count = 1 while node.next is not None: count += 1 node = node.next print ('The length of linklist are %d' % count) return count def delete_node(self, index): '删除节点' if index+1 &gt; self.length(): raise IndexError('index out of bounds') num = 0 node = self.head while True: if num == index-1: break node = node.next num += 1 tmp_node = node.next node.next = node.next.next return tmp_node.data def find_node(self, index): '查看具体节点' if index+1 &gt; self.length(): raise IndexError('index out of bounds') num = 0 node = self.head while True: if num == index: break node = node.next num += 1 return node.data My code123456789101112131415# -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): # write code here res=[] while listNode: res.append(listNode.val) listNode=listNode.next return res[::-1] #逆序打印 重建二叉树题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 解题思路 递归 整体思路就是：利用递归的思想，跟归并和快排有异曲同工之妙。抓住第一个节点是根节点，然后在中序遍历中定位该节点，左边即为根节点的左子树，右边即为根节点的右子树。然后对左子树和右子树分别进行递归即可，直到左子树和右子树都没了。 非递归 整体思路：我还没认真看呢！！！！！！！！ 用两个堆栈来分别存储二叉树和先序中序序列索引值，跟递归思路一样，先找到根节点，然后分别判断在中序遍历中是否还有左右子树，如果有则计数值count+=1,当找到一个节点之后就将计数值count-=1，直到中序遍历中再也没有左右子树。 My code递归1234567891011121314151617181920212223242526272829303132333435/** * Definition for binary tree * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre, int [] in) &#123; if(pre == null || in == null || pre.length == 0 || in.length == 0)&#123; return null; &#125; return constructBinaryTree(pre,in,0,pre.length-1,0,in.length-1); // 首先看先序遍历的第一个数，这是根节点，然后根据中序遍历然后分为左右两个节点块 // 再看左边的第一个数，这个肯定是根节点，然后定位到中序遍历该数的位置，再将其分为左右两个节点块，以此类推。 &#125; private TreeNode constructBinaryTree(int[] pre, int[] in, int preStart, int preEnd, int inStart, int inEnd) &#123; if(preStart &gt; preEnd || inStart &gt; inEnd)&#123; return null; &#125; TreeNode node = new TreeNode(pre[preStart]); int mid = inStart; while (pre[preStart] != in[mid]) mid++; int preNumber = mid - inStart; node.left = constructBinaryTree(pre,in,preStart + 1,preStart + preNumber,inStart,mid - 1); node.right = constructBinaryTree(pre,in,preStart + preNumber + 1,preEnd,mid + 1,inEnd); return node; &#125;&#125; 非递归123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Definition for binary tree * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) &#123; val = x; &#125; * &#125; */import java.util.*;public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; //p用来保存两个序列的索引值 Stack&lt;Integer&gt; p = new Stack&lt;Integer&gt;(); Stack&lt;TreeNode&gt; l = new Stack&lt;TreeNode&gt;(); TreeNode cur = null; p.push(0); p.push(pre.length - 1); p.push(0); p.push(in.length - 1); int count = 1; int pStart = 0; int pEnd = pre.length - 1; int iStart = 0; int iEnd = in.length - 1; TreeNode root = new TreeNode(-1); l.push(root); while( count != 0)&#123; iEnd = p.pop(); iStart = p.pop(); pEnd = p.pop(); pStart = p.pop(); cur = l.pop(); count -= 1; int index = find(pre[pStart], in, iStart, iEnd); cur.val = pre[pStart]; int lenL = index - iStart; if(lenL == 0)&#123; &#125;else&#123; p.push(pStart + 1); p.push(pStart + lenL); p.push(iStart); p.push(index - 1); count += 1; cur.left = new TreeNode(-1); l.push(cur.left); &#125; int lenR = iEnd - index; if(lenR == 0)&#123; &#125;else&#123; p.push(pStart + lenL + 1); p.push(pEnd); p.push(index + 1); p.push(iEnd); count += 1; cur.right = new TreeNode(-1); l.push(cur.right); &#125; &#125; return root; &#125; int find(int target, int[] all, int start , int end)&#123; for(int i = start; i &lt;= end; ++i)&#123; if(all[i] == target) return i; &#125; return -1; &#125;&#125; 知识点补充Tips: 二叉树的前序，中序，后序遍历方法总结 前序递归和非递归算法前序递归递归的方法很容易实现，也很容易理解：我们先访问根节点，然后递归访问左子树，再递归访问右子树，即实现了根-&gt;左-&gt;右的访问顺序，因为使用的是递归方法，所以每一个子树都实现了这样的顺序。 1234567891011121314class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); preorderHelper(root, result); return result; &#125; private void preorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; result.add(root.val); // 访问根节点 preorderHelper(root.left, result); // 递归遍历左子树 preorderHelper(root.right, result); //递归遍历右子树 &#125;&#125; 前序非递归在迭代法中，我们使用栈来实现。由于出栈顺序和入栈顺序相反，所以每次添加节点的时候先添加右节点，再添加左节点。这样在下一轮访问子树的时候，就会先访问左子树，再访问右子树： 123456789101112131415161718class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); if (root == null) return result; Stack&lt;TreeNode&gt; toVisit = new Stack&lt;&gt;(); toVisit.push(root); TreeNode cur; while (!toVisit.isEmpty()) &#123; cur = toVisit.pop(); result.add(cur.val); // 访问根节点 if (cur.right != null) toVisit.push(cur.right); // 右节点入栈 if (cur.left != null) toVisit.push(cur.left); // 左节点入栈 &#125; return result; &#125;&#125; 中序递归和非递归算法中序递归无论对于哪种方式，递归的方法总是很容易实现的，也是很符合直觉的。对于中序遍历，就是先访问左子树，再访问根节点，再访问右子树，即 左-&gt;根-&gt;右： 1234567891011121314class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); inorderHelper(root, result); return result; &#125; private void inorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if(root == null) return; inorderHelper(root.left, result); // 递归遍历左子树 result.add(root.val); // 访问根节点 inorderHelper(root.right, result); // 递归遍历右子树 &#125;&#125; 中序非递归中序遍历的迭代法要稍微复杂一点，因为最先遇到的根节点不是最先访问的，我们需要先访问左子树，再回退到根节点，再访问根节点的右子树，这里的一个难点是从左子树回退到根节点的操作，虽然可以用栈来实现回退，但是要注意在出栈时保存根节点的引用，因为我们还需要通过根节点来访问右子树： 123456789101112131415161718class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); Stack&lt;TreeNode&gt; toVisit = new Stack&lt;&gt;(); TreeNode cur = root; while (cur != null || !toVisit.isEmpty()) &#123; while (cur != null) &#123; toVisit.push(cur); // 添加根节点 cur = cur.left; // 循环添加左节点 &#125; cur = toVisit.pop(); // 当前栈顶已经是最底层的左节点了，取出栈顶元素，访问该节点 result.add(cur.val); cur = cur.right; // 添加右节点 &#125; return result; &#125;&#125; 在看这部分代码中，脑海中要有一个概念：当前树的根节点的左节点，是它的左子树的根节点。因此从不同的层次上看，左节点也是根节点。另外，LeetCode上也提供了关于中序遍历的动态图的演示，感兴趣的读者可以去看一看。 后序递归和非递归算法后序递归无论对于哪种方式，递归的方法总是很容易实现的，也是很符合直觉的。对于后序遍历，就是先访问左子树，再访问右子树，再访问根节点，即 左-&gt;右-&gt;根： 1234567891011121314class Solution &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); postorderHelper(root, result); return result; &#125; private void postorderHelper(TreeNode root, List&lt;Integer&gt; result) &#123; if (root == null) return; postorderHelper(root.left, result); // 遍历左子树 postorderHelper(root.right, result); // 遍历右子树 result.add(root.val); // 访问根节点 &#125;&#125; 后序非递归前面说过，与中序遍历不同的是，后序遍历在访问完左子树向上回退到根节点的时候不是立马访问根节点的，而是得先去访问右子树，访问完右子树后在回退到根节点，因此，在迭代过程中要复杂一点： 1234567891011121314151617181920212223242526class Solution &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; List&lt;Integer&gt; result = new LinkedList&lt;&gt;(); Stack&lt;TreeNode&gt; toVisit = new Stack&lt;&gt;(); TreeNode cur = root; TreeNode pre = null; while (cur != null || !toVisit.isEmpty()) &#123; while (cur != null) &#123; toVisit.push(cur); // 添加根节点 cur = cur.left; // 递归添加左节点 &#125; cur = toVisit.peek(); // 已经访问到最左的节点了 //在不存在右节点或者右节点已经访问过的情况下，访问根节点 if (cur.right == null || cur.right == pre) &#123; toVisit.pop(); result.add(cur.val); pre = cur; cur = null; &#125; else &#123; cur = cur.right; // 右节点还没有访问过就先访问右节点 &#125; &#125; return result; &#125;&#125; 这里尤其注意后续遍历和中序遍历中对于从最左侧节点向上回退时的处理： 在后序遍历中，我们首先使用的是： 1cur = toVisit.peek(); 注意，这里使用的是peek而不是pop，这是因为我们需要首先去访问右节点，下面的： 1if (cur.right == null || cur.right == pre) 就是用来判断是否存在右节点，或者右节点是否已经访问过了，如果右节点已经访问过了，则接下来的操作就和中序遍历的情况差不多了，所不同的是，这里多了两步： 12pre = cur;cur = null; 这两步的目的都是为了在下一轮遍历中不再访问自己，cur = null很好理解，因为我们必须在一轮结束后改变cur的值，以添加下一个节点，所以它和cur = cur.right一样，目的都是指向下一个待遍历的节点，只是在这里，右节点已经访问过了，则以当前节点为根节点的整个子树都已经访问过了，接下来应该回退到当前节点的父节点，而当前节点的父节点已经在栈里了，所以我们并没有新的节点要添加，直接将cur设为null即可。 pre = cur 的目的有点类似于将当前节点标记为已访问，它是和if条件中的cur.right == pre配合使用的。注意这里的两个cur指的不是同一个节点。我们假设当前节点为C，当前节点的父节点为A，而C是A的右孩子，则当前cur是C，但在一轮中，cur将变成A，则： 123 A / \B C (pre) pre = cur 就是 pre = C if (cur.right == null || cur.right == pre) 就是 if (A.right == null || A.right == pre) 这里，由于A是有右节点的，它的右节点就是C，所以A.right == null不成立。但是C节点我们在上一轮已经访问过了，所以这里为了防止进入else语句重复添加节点，我们多加了一个A.right == pre条件，它表示A的右节点已经访问过了，我们得以进入if语句内，直接访问A节点。 用两个栈实现队列旋转数组的最小数字tips：只要是关于有序数组，基本就在考察二分法 斐波那契数列变态跳台阶]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>剑指 offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在macOS上使用GitHub Pages+Hexo搭建个人博客]]></title>
    <url>%2F2019%2F08%2F08%2F%E5%9C%A8macOS%E4%B8%8A%E4%BD%BF%E7%94%A8GitHub-Pages-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2.html</url>
    <content type="text"><![CDATA[搭建环境操作系统：macos 10.14.5工具：github Desktop, Node.js, Git框架：Hexo, Github Pages 具体步骤安装git直接在终端输入git --version判断自己系统是否已有git，如果没有，点击该链接git下载，安装完后进入终端输入git --version检查是否安装成功。 安装node.js话不多说，上链接：node.js下载，然后傻瓜式安装就行。安装完之后请记得去控制台看看是否安装成功node --version,然后更新一下npm,输入:npm install -g npm 注册账号并创建博客仓库a.首先进入github官网：github,进行相应的注册登录，然后创建仓库，就在左上角那个猫猫旁边。b.戳进去之后,Respository name必须是:您的github账户名.github.io,然后勾选下面的创建readme，创建完成。c.进入仓库，点击Setting，启用GitHub Pages,此时你输入您的github账户名.github.io,就能看到不是404了，说明此时博客已经建立了，接下来就是利用Hexo框架让你的博客美起来啦！！！！ 安装配置Hexoa.首先建立一个文件夹(随便你建在哪)b.在该文件夹内打开终端，输入：1$ npm install hexo-cli -g 接下来你就能看到文件夹内的东西多了起来哈哈哈，如下图所示的文件结构：c.Hexo已经有了，github pages也有了，现在就剩把他们联系起来了，所以这步就是关键咯，打开上图第一个文件：_config.yml,然后在文本的结尾加上下面的一段代码：1234deploy: type: git repository: https://github.com/jeromememory/jeromememory.github.io.git branch: master 其中repository就是你最开始设置的仓库哟，记得替换自己的github用户名。 检测是否部署成功首先如果你想在本地预览你的博客，只需要打开终端,首先进入Hexo文件夹内，然后进行 hexo s即可：12$ cd /Users/jerome/Hexo $ hexo s 然后你就会看到在终端提示你在localhost:4000预览你的博客啦！！如果你想在网页上看到你的博客，只需要在终端输入：12$ hexo g$ hexo d 稍等一小会(github pages需要一小段时间才会更新)，你就能在你仓库对应的那个网址(您的github用户名.io.git)看到您的博客了，至此，您的博客应该就已经搭建完成了，当然您肯定会说这也太丑了吧，对，我也觉得，所以接下来请继续看下面的操作吧！！！ Hexo主题配置默认的主题很丑，所以这个时候我们就可以去github这个最大的开源网站找了，我推荐Hexo的next主题，我自己用的就是这个，极简至极，找到别人的主题之后，把他的代码clone下来，放到/Hexo/themes这个文件夹下：然后去开始的_config.yml中定位到theme，将后面的值改为你的主题名，别忘了，改完这些后，打开终端，输入：12$ hexo g$ hexo d 过一小会你就能看到自己的主题发生变化啦！剩下的具体配置，我就偷偷懒放个链接：NexT主题，其他主题的同学，你们也可以自己查一下对应的官网哟，都写的贼详细，好用！ 总结至此，个人博客就应该搭建好了，希望对大家有所收获！!(话说第一篇博客果然是狗屁不通啊大家凑合看吧)]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>GitHub Pages</tag>
      </tags>
  </entry>
</search>
